{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding of key concepts on summarization evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Abstractive and Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt_template = \"\"\"\n",
    "Answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. \n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What is Abstractive Summary?\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"What is Extractive Summary?\"\n",
    "    }    \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Abstractive summary is a type of summarization that involves interpreting and rephrasing the original text in a new way, rather than simply extracting and condensing existing sentences. It aims to capture the main ideas and concepts of the original text in a more concise and coherent manner. It is often used in natural language processing and machine learning applications.'},\n",
       " {'text': 'An extractive summary is a type of summary that selects and extracts key information directly from the original text. It does not involve any interpretation or rewriting of the content. Extractive summaries are often used in natural language processing and text analysis.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt_template = \"\"\"\n",
    "Answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. \n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be concise?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"A concise podcast summary is one that provides a brief and clear overview of the episode's content. It should include the main topics discussed and key takeaways, without unnecessary details or lengthy explanations. A concise summary helps listeners quickly understand what the episode is about and decide if they want to listen.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be accurate?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'An accurate podcast summary means that it effectively and truthfully represents the content of the podcast episode. It should provide a clear and concise overview of the main topics and themes discussed. Accuracy is important for helping listeners decide if the episode is relevant to their interests.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good explanation, better thatn ollama openchat model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be complete?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"A complete podcast summary should provide a brief overview of the episode's main topics and key points. It should also include any important quotes or insights shared by the host or guests. Additionally, a complete summary should give listeners a clear understanding of what they can expect to learn or gain from listening to the episode.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be catchy?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"A catchy podcast summary is one that grabs the listener's attention and entices them to listen to the episode. It should be concise, engaging, and highlight the most interesting aspects of the episode. A catchy summary can help attract new listeners and keep existing ones engaged.\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear\n",
    "(Clear,Concise,Consistent) https://www.linkedin.com/pulse/clear-concise-consistent-three-fundamentals-good-writing-gisler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be clear?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"A clear podcast summary provides a brief and accurate overview of the episode's content. It should effectively communicate the main topics and key points discussed. A clear summary helps listeners understand what to expect from the episode.\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ollama Openchat seems to be better. It mentions about 'context', \n",
    "and Openai last sentence about helping listeners what to expect seems to be redundant and not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[])\n",
    "\n",
    "# Define the LLMs\n",
    "map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "input_data = [\n",
    "    {\n",
    "        'question': \"What does it mean for a podcast summary to be consistent?\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "# Run the input through the LLM chain (works in parallel)\n",
    "map_llm_chain_results = map_llm_chain.apply(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'A consistent podcast summary means that the information provided in each summary is uniform and follows a similar format or structure. It also means that the tone and style of the summaries remain consistent throughout. Consistency in podcast summaries helps to create a cohesive and professional image for the podcast.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_llm_chain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pretty good, last sentence talks about cohesive and professional image which is also relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
