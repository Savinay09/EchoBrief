{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "\n",
    "PODCAST_DATA = '../summarize/vtt_data.csv'\n",
    "\n",
    "with open(PODCAST_DATA) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "\n",
    "print(len(podcast_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "episodes = [22,23,79,94]\n",
    "\n",
    "baseline_podcasts = {}\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    if int(podcast['episode_index']) in episodes:\n",
    "        baseline_podcasts[podcast['episode_index']] = podcast\n",
    "        \n",
    "\n",
    "print(len(baseline_podcasts))\n",
    "\n",
    "# baseline_podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt_template = \"\"\"You are comparing the summary text and it's original document and \n",
    "trying to determine if the summary is accurate using the original document as the source of truth.\n",
    "\n",
    "Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Summary]: {summary}\n",
    "************\n",
    "[Original Document]: {document}\n",
    "[END DATA]\n",
    "\n",
    "Compare the Summary above to the Original Document and determine if the Summary is\n",
    "accurate.\n",
    "\n",
    "Your response must be either Very, Most, Somewhat, or Not. Your response should not contain any text\n",
    "or characters aside from that.\n",
    "\n",
    "The string Very means that summary is a very accurate.\n",
    "\n",
    "The string Mostly means that summary is a mostly accurate.\n",
    "\n",
    "The string Somewhat means that summary is a somewhat accurate.\n",
    "\n",
    "The string Not means that summary is not accurate.\n",
    "\n",
    "You response should also contain reasons behind your evaluation.\n",
    "\n",
    "Return your answer in the following format:\n",
    "  Very/Mostly/Somewhat/Not | reasons...\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  Episode 22\n",
      "## Transcript\n",
      "The following is a conversation with Rajat Manga. He's an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work going on in the world in deep learning, both the cutting edge research and the large scale application of learning based approaches. But it's quickly becoming much more than a software library. It's now an ecosystem of tools for the deployment of machine learning in the cloud, on the phone, in the browser, on both generic and specialized hardware. TPU, GPU, and so on. Plus, there's a big emphasis on growing a passionate community of developers. Rajat, Jeff Dean, and a large team of engineers at Google Brain are working to define the future of machine learning with TensorFlow 2.0, which is now in alpha. I think the decision to open source TensorFlow is a definitive moment in the tech industry. It showed that open innovation can be successful and inspire many companies to open source their code, to publish, and in general engage in the open exchange of ideas. This conversation is part of the Artificial Intelligence podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter at Lex Friedman, spelled F R I D. And now, here's my conversation with Rajat Manga. You were involved with Google Brain since its start in 2011 with Jeff Dean. It started with this belief, the proprietary machine learning library, and turned into TensorFlow in 2014, the open source library. So what were the early days of Google Brain like? What were the goals, the missions? How do you even proceed forward once there's so much possibilities before you? It was interesting back then when I started, or when you were even just talking about it, the idea of deep learning was interesting and intriguing in some ways. It hadn't yet taken off, but it held some promise. It had shown some very promising and early results. I think the idea where Andrew and Jeff had started was, what if we can take this work people are doing in research and scale it to what Google has in terms of the compute power, and also put that kind of data together? What does it mean? And so far, the results had been, if you scale the compute, scale the data, it does better. And would that work? And so that was the first year or two, can we prove that out? And with this belief, when we started the first year, we got some early wins, which is always great. What were the wins like? What was the wins where you were, there's some problems to this, this is going to be good? I think there are two early wins where one was speech, that we collaborated very closely with the speech research team, who was also getting interested in this. And the other one was on images, where the cat paper, as we call it, that was covered by a lot of folks. And the birth of Google Brain was around neural networks. So it was deep learning from the very beginning. That was the whole mission. So what would, in terms of scale, what was the sort of dream of what this could become? Were there echoes of this open source TensorFlow community that might be brought in? Was there a sense of TPUs? Was there a sense of machine learning is now going to be at the core of the entire company, is going to grow into that direction? Yeah, I think, so that was interesting. And if I think back to 2012 or 2011, and first was can we scale it in the year or so, we had started scaling it to hundreds and thousands of machines. In fact, we had some runs even going to 10,000 machines. And all of those shows great promise. In terms of machine learning at Google, the good thing was Google's been doing machine learning for a long time. Deep learning was new, but as we scaled this up, we showed that, yes, that was possible. And it was going to impact lots of things. Like we started seeing real products wanting to use this. Again, speech was the first, there were image things that photos came out of and then many other products as well. So that was exciting. As we went into that a couple of years, externally also academia started to, there was lots of push on, okay, deep learning is interesting, we should be doing more and so on. And so by 2014, we were looking at, okay, this is a big thing, it's going to grow. And not just internally, externally as well. Yes, maybe Google's ahead of where everybody is, but there's a lot to do. So a lot of this started to make sense and come together. So the decision to open source, I was just chatting with Chris Glatner about this. The decision to go open source with TensorFlow, I would say sort of for me personally, seems to be one of the big seminal moments in all of software engineering ever. I think that's when a large company like Google decides to take a large project that many lawyers might argue has a lot of IP, just decide to go open source with it, and in so doing lead the entire world and saying, you know what, open innovation is a pretty powerful thing, and it's okay to do. That was, I mean, that's an incredible moment in time. So do you remember those discussions happening? Whether open source should be happening? What was that like? I would say, I think, so the initial idea came from Jeff, who was a big proponent of this. I think it came off of two big things. One was research wise, we were a research group. We were putting all our research out there. If you wanted to, we were building on others research and we wanted to push the state of the art forward. And part of that was to share the research. That's how I think deep learning and machine learning has really grown so fast. So the next step was, okay, now, would software help with that? And it seemed like they were existing a few libraries out there, Tiano being one, Torch being another, and a few others, but they were all done by academia and so the level was significantly different. The other one was from a software perspective, Google had done lots of software or that we used internally, you know, and we published papers. Often there was an open source project that came out of that that somebody else picked up that paper and implemented and they were very successful. Back then it was like, okay, there's Hadoop, which has come off of tech that we've built. We know the tech we've built is way better for a number of different reasons. We've invested a lot of effort in that. And turns out we have Google Cloud and we are now not really providing our tech, but we are saying, okay, we have Bigtable, which is the original thing. We are going to now provide H base APIs on top of that, which isn't as good, but that's what everybody's used to. So there's like, can we make something that is better and really just provide, helps the community in lots of ways, but also helps push a good standard forward. So how does Cloud fit into that? There's a TensorFlow open source library and how does the fact that you can use so many of the resources that Google provides and the Cloud fit into that strategy? So TensorFlow itself is open and you can use it anywhere, right? And we want to make sure that continues to be the case. On Google Cloud, we do make sure that there's lots of integrations with everything else and we want to make sure that it works really, really well there. You're leading the TensorFlow effort. Can you tell me the history and the timeline of TensorFlow project in terms of major design decisions, so like the open source decision, but really what to include and not? There's this incredible ecosystem that I'd like to talk about. There's all these parts, but what if just some sample moments that defined what TensorFlow eventually became through its, I don't know if you're allowed to say history when it's just, but in deep learning, everything moves so fast and just a few years is already history. Yes, yes, so looking back, we were building TensorFlow. I guess we open sourced it in 2015, November 2015. We started on it in summer of 2014, I guess. And somewhere like three to six, late 2014, by then we had decided that, okay, there's a high likelihood we'll open source it. So we started thinking about that and making sure we're heading down that path. At that point, by that point, we had seen a few, lots of different use cases at Google. So there were things like, okay, yes, you wanna run it at large scale in the data center. Yes, we need to support different kind of hardware. We had GPUs at that point. We had our first GPU at that point or was about to come out roughly around that time. So the design sort of included those. We had started to push on mobile. So we were running models on mobile. At that point, people were customizing code. So we wanted to make sure TensorFlow could support that as well. So that sort of became part of that overall design. When you say mobile, you mean like a pretty complicated algorithms running on the phone? That's correct. So when you have a model that you deploy on the phone and run it there, right? So already at that time, there was ideas of running machine learning on the phone. That's correct. We already had a couple of products that were doing that by then. And in those cases, we had basically customized handcrafted code or some internal libraries that we're using. So I was actually at Google during this time in a parallel, I guess, universe, but we were using Theano and Caffe. Was there some degree to which you were bouncing, like trying to see what Caffe was offering people, trying to see what Theano was offering that you want to make sure you're delivering on whatever that is? Perhaps the Python part of thing, maybe did that influence any design decisions? Totally. So when we built this belief and some of that was in parallel with some of these libraries coming up, I mean, Theano itself is older, but we were building this belief focused on our internal thing because our systems were very different. By the time we got to this, we looked at a number of libraries that were out there. Theano, there were folks in the group who had experience with Torch, with Lua. There were folks here who had seen Caffe. I mean, actually, Yang Jing was here as well. There's what other libraries? I think we looked at a number of things. Might even have looked at JNR back then. I'm trying to remember if it was there. In fact, yeah, we did discuss ideas around, okay, should we have a graph or not? So putting all these together was definitely, they were key decisions that we wanted. We had seen limitations in our prior disbelief things. A few of them were just in terms of research was moving so fast, we wanted the flexibility. The hardware was changing fast. We expected to change that so that those probably were two things. And yeah, I think the flexibility in terms of being able to express all kinds of crazy things was definitely a big one then. So what, the graph decisions though, with moving towards TensorFlow 2.0, there's more, by default, there'll be eager execution. So sort of hiding the graph a little bit because it's less intuitive in terms of the way people develop and so on. What was that discussion like in terms of using graphs? It seemed, it's kind of the Theano way. Did it seem the obvious choice? So I think where it came from was our disbelief had a graph like thing as well. A much more simple, it wasn't a general graph, it was more like a straight line thing. More like what you might think of cafe, I guess in that sense. But the graph was, and we always cared about the production stuff. Like even with disbelief, we were deploying a whole bunch of stuff in production. So graph did come from that when we thought of, okay, should we do that in Python? And we experimented with some ideas where it looked a lot simpler to use, but not having a graph meant, okay, how do you deploy now? So that was probably what tilted the balance for us and eventually we ended up with a graph. And I guess the question there is, did you, I mean, so production seems to be the really good thing to focus on, but did you even anticipate the other side of it where there could be, what is it? What are the numbers? It's been crazy, 41 million downloads. Yep. I mean, was that even like a possibility in your mind that it would be as popular as it became? So I think we did see a need for this a lot from the research perspective and like early days of deep learning in some ways. 41 million, no, I don't think I imagined this number. Then it seemed like there's a potential future where lots more people would be doing this and how do we enable that? I would say this kind of growth, I probably started seeing somewhat after the open sourcing where it was like, okay, deep learning is actually growing way faster for a lot of different reasons. And we are in just the right place to push on that and leverage that and deliver on lots of things that people want. So what changed once you open sourced? Like how this incredible amount of attention from a global population of developers, how did the project start changing? I don't even actually remember during those times. I know looking now, there's really good documentation, there's an ecosystem of tools, there's a community, there's a blog, there's a YouTube channel now, right? Yeah. It's very community driven. Back then, I guess 0.1 version, is that the version? I think we call it 0.6 or five, something like that, I forget. What changed leading into 1.0? It's interesting. I think we've gone through a few things there. When we started out, when we first came out, people loved the documentation we have because it was just a huge step up from everything else because all of those were academic projects, people doing, who don't think about documentation. I think what that changed was, instead of deep learning being a research thing, some people who were just developers could now suddenly take this out and do some interesting things with it, right? Who had no clue what machine learning was before then. And that I think really changed how things started to scale up in some ways and pushed on it. Over the next few months as we looked at how do we stabilize things, as we look at not just researchers, now we want stability, people want to deploy things. That's how we started planning for 1.0 and there are certain needs for that perspective. And so again, documentation comes up, designs, more kinds of things to put that together. And so that was exciting to get that to a stage where more and more enterprises wanted to buy in and really get behind that. And I think post 1.0 and over the next few releases, that enterprise adoption also started to take off. I would say between the initial release and 1.0, it was, okay, researchers of course, then a lot of hobbies and early interest, people excited about this who started to get on board and then over the 1.x thing, lots of enterprises. I imagine anything that's below 1.0 gives pressure to be, the enterprise probably wants something that's stable. Exactly. And do you have a sense now that TensorFlow is stable? Like it feels like deep learning in general is extremely dynamic field, so much is changing. And TensorFlow has been growing incredibly. Do you have a sense of stability at the helm of it? I mean, I know you're in the midst of it, but. Yeah, I think in the midst of it, it's often easy to forget what an enterprise wants and what some of the people on that side want. There are still people running models that are three years old, four years old. So Inception is still used by tons of people. Even ResNet 50 is what, couple of years old now or more, but there are tons of people who use that and they're fine. They don't need the last couple of bits of performance or quality, they want some stability in things that just work. And so there is value in providing that with that kind of stability and making it really simpler because that allows a lot more people to access it. And then there's the research crowd which wants, okay, they wanna do these crazy things exactly like you're saying, right? Not just deep learning in the straight up models that used to be there, they want RNNs and even RNNs are maybe old, they are transformers now. And now it needs to combine with RL and GANs and so on. So there's definitely that area that like the boundary that's shifting and pushing the state of the art. But I think there's more and more of the past that's much more stable and even stuff that was two, three years old is very, very usable by lots of people. So that part makes it a lot easier. So I imagine, maybe you can correct me if I'm wrong, one of the biggest use cases is essentially taking something like ResNet 50 and doing some kind of transfer learning on a very particular problem that you have. It's basically probably what majority of the world does. And you wanna make that as easy as possible. So I would say for the hobbyist perspective, that's the most common case, right? In fact, the apps and phones and stuff that you'll see, the early ones, that's the most common case. I would say there are a couple of reasons for that. One is that everybody talks about that. It looks great on slides. That's a presentation, yeah, exactly. What enterprises want is that is part of it, but that's not the big thing. Enterprises really have data that they wanna make predictions on. This is often what they used to do with the people who were doing ML was just regression models, linear regression, logistic regression, linear models, or maybe gradient booster trees and so on. Some of them still benefit from deep learning, but they want that's the bread and butter, or like the structured data and so on. So depending on the audience you look at, they're a little bit different. And they just have, I mean, the best of enterprise probably just has a very large data set, or deep learning can probably shine. That's correct, that's right. And then I think the other pieces that they wanted, again, with 2.0, the developer summit we put together is the whole TensorFlow Extended piece, which is the entire pipeline. They care about stability across doing their entire thing. They want simplicity across the entire thing. I don't need to just train a model. I need to do that every day again, over and over again. I wonder to which degree you have a role in, I don't know, so I teach a course on deep learning. I have people like lawyers come up to me and say, when is machine learning gonna enter legal, the legal realm? The same thing in all kinds of disciplines, immigration, insurance, often when I see what it boils down to is these companies are often a little bit old school in the way they organize the data. So the data is just not ready yet, it's not digitized. Do you also find yourself being in the role of an evangelist for like, let's get, organize your data, folks, and then you'll get the big benefit of TensorFlow. Do you get those, have those conversations? Yeah, yeah, you know, I get all kinds of questions there from, okay, what do I need to make this work, right? Do we really need deep learning? I mean, there are all these things, I already use this linear model, why would this help? I don't have enough data, let's say, or I wanna use machine learning, but I have no clue where to start. So it varies, that to all the way to the experts to why support very specific things, it's interesting. Is there a good answer? It boils down to oftentimes digitizing data. So whatever you want automated, whatever data you want to make prediction based on, you have to make sure that it's in an organized form. Like within the TensorFlow ecosystem, there's now, you're providing more and more data sets and more and more pre trained models. Are you finding yourself also the organizer of data sets? Yes, I think the TensorFlow data sets that we just released, that's definitely come up where people want these data sets, can we organize them and can we make that easier? So that's definitely one important thing. The other related thing I would say is I often tell people, you know what, don't think of the most fanciest thing that the newest model that you see, make something very basic work and then you can improve it. There's just lots of things you can do with it. Yeah, start with the basics, true. One of the big things that makes TensorFlow even more accessible was the appearance whenever that happened of Keras, the Keras standard sort of outside of TensorFlow. I think it was Keras on top of Tiano at first only and then Keras became on top of TensorFlow. Do you know when Keras chose to also add TensorFlow as a backend, who was the, was it just the community that drove that initially? Do you know if there was discussions, conversations? Yeah, so Francois started the Keras project before he was at Google and the first thing was Tiano. I don't remember if that was after TensorFlow was created or way before. And then at some point, when TensorFlow started becoming popular, there were enough similarities that he decided to create this interface and put TensorFlow as a backend. I believe that might still have been before he joined Google. So we weren't really talking about that. He decided on his own and thought that was interesting and relevant to the community. In fact, I didn't find out about him being at Google until a few months after he was here. He was working on some research ideas and doing Keras on his nights and weekends project. Oh, interesting. He wasn't like part of the TensorFlow. He didn't join initially. He joined research and he was doing some amazing research. He has some papers on that and research, so he's a great researcher as well. And at some point we realized, oh, he's doing this good stuff. People seem to like the API and he's right here. So we talked to him and he said, okay, why don't I come over to your team and work with you for a quarter and let's make that integration happen. And we talked to his manager and he said, sure, quarter's fine. And that quarter's been something like two years now. And so he's fully on this. So Keras got integrated into TensorFlow in a deep way. And now with 2.0, TensorFlow 2.0, sort of Keras is kind of the recommended way for a beginner to interact with TensorFlow. Which makes that initial sort of transfer learning or the basic use cases, even for an enterprise, super simple, right? That's correct, that's right. So what was that decision like? That seems like it's kind of a bold decision as well. We did spend a lot of time thinking about that one. We had a bunch of APIs, some built by us. There was a parallel layers API that we were building. And when we decided to do Keras in parallel, so there were like, okay, two things that we are looking at. And the first thing we was trying to do is just have them look similar, like be as integrated as possible, share all of that stuff. There were also like three other APIs that others had built over time because we didn't have a standard one. But one of the messages that we kept hearing from the community, okay, which one do we use? And they kept seeing like, okay, here's a model in this one and here's a model in this one, which should I pick? So that's sort of like, okay, we had to address that straight on with 2.0. The whole idea was we need to simplify. We had to pick one. Based on where we were, we were like, okay, let's see what are the people like? And Keras was clearly one that lots of people loved. There were lots of great things about it. So we settled on that. Organically, that's kind of the best way to do it. It was great. It was surprising, nevertheless, to sort of bring in an outside. I mean, there was a feeling like Keras might be almost like a competitor in a certain kind of, to TensorFlow. And in a sense, it became an empowering element of TensorFlow. That's right. Yeah, it's interesting how you can put two things together, which can align. In this case, I think Francois, the team, and a bunch of us have chatted, and I think we all want to see the same kind of things. We all care about making it easier for the huge set of developers out there, and that makes a difference. So Python has Guido van Rossum, who until recently held the position of benevolent dictator for life. All right, so there's a huge successful open source project like TensorFlow need one person who makes a final decision. So you've did a pretty successful TensorFlow Dev Summit just now, last couple of days. There's clearly a lot of different new features being incorporated, an amazing ecosystem, so on. Who's, how are those design decisions made? Is there a BDFL in TensorFlow, or is it more distributed and organic? I think it's somewhat different, I would say. I've always been involved in the key design directions, but there are lots of things that are distributed where there are a number of people, Martin Wick being one, who has really driven a lot of our open source stuff, a lot of the APIs, and there are a number of other people who've been, you know, pushed and been responsible for different parts of it. We do have regular design reviews. Over the last year, we've had a lot of we've really spent a lot of time opening up to the community and adding transparency. We're setting more processes in place, so RFCs, special interest groups, to really grow that community and scale that. I think the kind of scale that ecosystem is in, I don't think we could scale with having me as the lone point of decision maker. I got it. So, yeah, the growth of that ecosystem, maybe you can talk about it a little bit. First of all, it started with Andrej Karpathy when he first did ComNetJS. The fact that you can train and you'll network in the browser was, in JavaScript, was incredible. So now TensorFlow.js is really making that a serious, like a legit thing, a way to operate, whether it's in the backend or the front end. Then there's the TensorFlow Extended, like you mentioned. There's TensorFlow Lite for mobile. And all of it, as far as I can tell, it's really converging towards being able to save models in the same kind of way. You can move around, you can train on the desktop and then move it to mobile and so on. That's right. So there's that cohesiveness. So can you maybe give me, whatever I missed, a bigger overview of the mission of the ecosystem that's trying to be built and where is it moving forward? Yeah. So in short, the way I like to think of this is our goals to enable machine learning. And in a couple of ways, you know, one is we have lots of exciting things going on in ML today. We started with deep learning, but we now support a bunch of other algorithms too. So one is to, on the research side, keep pushing on the state of the art. Can we, you know, how do we enable researchers to build the next amazing thing? So BERT came out recently, you know, it's great that people are able to do new kinds of research. And there are lots of amazing research that happens across the world. So that's one direction. The other is how do you take that across all the people outside who want to take that research and do some great things with it and integrate it to build real products, to have a real impact on people. And so if that's the other axes in some ways, you know, at a high level, one way I think about it is there are a crazy number of compute devices across the world. And we often used to think of ML and training and all of this as, okay, something you do either in the workstation or the data center or cloud. But we see things running on the phones. We see things running on really tiny chips. I mean, we had some demos at the developer summit. And so the way I think about this ecosystem is how do we help get machine learning on every device that has a compute capability? And that continues to grow and so in some ways this ecosystem is looked at, you know, various aspects of that and grown over time to cover more of those. And we continue to push the boundaries. In some areas we've built more tooling and things around that to help you. I mean, the first tool we started was TensorBoard. You wanted to learn just the training piece, the effects or TensorFlow extended to really do your entire ML pipelines. If you're, you know, care about all that production stuff, but then going to the edge, going to different kinds of things. And it's not just us now. We are a place where there are lots of libraries being built on top. So there are some for research, maybe things like TensorFlow agents or TensorFlow probability that started as research things or for researchers for focusing on certain kinds of algorithms, but they're also being deployed or used by, you know, production folks. And some have come from within Google, just teams across Google who wanted to build these things. Others have come from just the community because there are different pieces that different parts of the community care about. And I see our goal as enabling even that, right? It's not, we cannot and won't build every single thing. That just doesn't make sense. But if we can enable others to build the things that they care about, and there's a broader community that cares about that, and we can help encourage that, and that's great. That really helps the entire ecosystem, not just those. One of the big things about 2.0 that we're pushing on is, okay, we have these so many different pieces, right? How do we help make all of them work well together? So there are a few key pieces there that we're pushing on, one being the core format in there and how we share the models themselves through save model and TensorFlow hub and so on. And a few of the pieces that we really put this together. I was very skeptical that that's, you know, when TensorFlow.js came out, it didn't seem, or deep learning JS as it was earlier. Yeah, that was the first. It seems like technically very difficult project. As a standalone, it's not as difficult, but as a thing that integrates into the ecosystem, it seems very difficult. So, I mean, there's a lot of aspects of this you're making look easy, but, and the technical side, how many challenges have to be overcome here? A lot. And still have to be overcome. That's the question here too. There are lots of steps to it, right? And we've iterated over the last few years, so there's a lot we've learned. I, yeah, and often when things come together well, things look easy and that's exactly the point. It should be easy for the end user, but there are lots of things that go behind that. If I think about still challenges ahead, there are, you know, we have a lot more devices coming on board, for example, from the hardware perspective. How do we make it really easy for these vendors to integrate with something like TensorFlow, right? So there's a lot of compiler stuff that others are working on. There are things we can do in terms of our APIs and so on that we can do. As we, you know, TensorFlow started as a very monolithic system and to some extent it still is. There are less, lots of tools around it, but the core is still pretty large and monolithic. One of the key challenges for us to scale that out is how do we break that apart with clearer interfaces? It's, you know, in some ways it's software engineering 101, but for a system that's now four years old, I guess, or more, and that's still rapidly evolving and that we're not slowing down with, it's hard to change and modify and really break apart. It's sort of like, as people say, right, it's like changing the engine with a car running or trying to fix that. That's exactly what we're trying to do. So there's a challenge here because the downside of so many people being excited about TensorFlow and coming to rely on it in many of their applications is that you're kind of responsible, like it's the technical debt. You're responsible for previous versions to some degree still working. So when you're trying to innovate, I mean, it's probably easier to just start from scratch every few months. Absolutely. So do you feel the pain of that? 2.0 does break some back compatibility, but not too much. It seems like the conversion is pretty straightforward. Do you think that's still important given how quickly deep learning is changing? Can you just, the things that you've learned, can you just start over or is there pressure to not? It's a tricky balance. So if it was just a researcher writing a paper who a year later will not look at that code again, sure, it doesn't matter. There are a lot of production systems that rely on TensorFlow, both at Google and across the world. And people worry about this. I mean, these systems run for a long time. So it is important to keep that compatibility and so on. And yes, it does come with a huge cost. There's, we have to think about a lot of things as we do new things and make new changes. I think it's a trade off, right? You can, you might slow certain kinds of things down, but the overall value you're bringing because of that is much bigger because it's not just about breaking the person yesterday. It's also about telling the person tomorrow that, you know what, this is how we do things. We're not gonna break you when you come on board because there are lots of new people who are also gonna come on board. And, you know, one way I like to think about this, and I always push the team to think about it as well, when you wanna do new things, you wanna start with a clean slate. Design with a clean slate in mind, and then we'll figure out how to make sure all the other things work. And yes, we do make compromises occasionally, but unless you design with the clean slate and not worry about that, you'll never get to a good place. Oh, that's brilliant, so even if you are responsible when you're in the idea stage, when you're thinking of new, just put all that behind you. Okay, that's really, really well put. So I have to ask this because a lot of students, developers ask me how I feel about PyTorch versus TensorFlow. So I've recently completely switched my research group to TensorFlow. I wish everybody would just use the same thing, and TensorFlow is as close to that, I believe, as we have. But do you enjoy competition? So TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, in terms of number of users, momentum, power, production levels, so on, but a lot of researchers are now also using PyTorch. Do you enjoy that kind of competition or do you just ignore it and focus on making TensorFlow the best that it can be? So just like research or anything people are doing, it's great to get different kinds of ideas. And when we started with TensorFlow, like I was saying earlier, one, it was very important for us to also have production in mind. We didn't want just research, right? And that's why we chose certain things. Now PyTorch came along and said, you know what, I only care about research. This is what I'm trying to do. What's the best thing I can do for this? And it started iterating and said, okay, I don't need to worry about graphs. Let me just run things. And I don't care if it's not as fast as it can be, but let me just make this part easy. And there are things you can learn from that, right? They, again, had the benefit of seeing what had come before, but also exploring certain different kinds of spaces. And they had some good things there, building on say things like JNR and so on before that. So competition is definitely interesting. It made us, you know, this is an area that we had thought about, like I said, way early on. Over time we had revisited this a couple of times, should we add this again? At some point we said, you know what, it seems like this can be done well, so let's try it again. And that's how we started pushing on eager execution. How do we combine those two together? Which has finally come very well together in 2.0, but it took us a while to get all the things together and so on. So let me ask, put another way, I think eager execution is a really powerful thing that was added. Do you think it wouldn't have been, you know, Muhammad Ali versus Frasier, right? Do you think it wouldn't have been added as quickly if PyTorch wasn't there? It might have taken longer. No longer? Yeah, it was, I mean, we had tried some variants of that before, so I'm sure it would have happened, but it might have taken longer. I'm grateful that TensorFlow is finally in the way they did. It's doing some incredible work last couple years. What other things that we didn't talk about are you looking forward in 2.0? That comes to mind. So we talked about some of the ecosystem stuff, making it easily accessible to Keras, eager execution. Is there other things that we missed? Yeah, so I would say one is just where 2.0 is, and you know, with all the things that we've talked about, I think as we think beyond that, there are lots of other things that it enables us to do and that we're excited about. So what it's setting us up for, okay, here are these really clean APIs. We've cleaned up the surface for what the users want. What it also allows us to do a whole bunch of stuff behind the scenes once we are ready with 2.0. So for example, in TensorFlow with graphs and all the things you could do, you could always get a lot of good performance if you spent the time to tune it, right? And we've clearly shown that, lots of people do that. With 2.0, with these APIs, where we are, we can give you a lot of performance just with whatever you do. You know, because we see these, it's much cleaner. We know most people are gonna do things this way. We can really optimize for that and get a lot of those things out of the box. And it really allows us, you know, both for single machine and distributed and so on, to really explore other spaces behind the scenes after 2.0 in the future versions as well. So right now the team's really excited about that, that over time I think we'll see that. The other piece that I was talking about in terms of just restructuring the monolithic thing into more pieces and making it more modular, I think that's gonna be really important for a lot of the other people in the ecosystem, other organizations and so on that wanted to build things. Can you elaborate a little bit what you mean by making TensorFlow ecosystem more modular? So the way it's organized today is there's one, there are lots of repositories in the TensorFlow organization at GitHub. The core one where we have TensorFlow, it has the execution engine, it has the key backends for CPUs and GPUs, it has the work to do distributed stuff. And all of these just work together in a single library or binary. There's no way to split them apart easily. I mean, there are some interfaces, but they're not very clean. In a perfect world, you would have clean interfaces where, okay, I wanna run it on my fancy cluster with some custom networking, just implement this and do that. I mean, we kind of support that, but it's hard for people today. I think as we are starting to see more interesting things in some of these spaces, having that clean separation will really start to help. And again, going to the large size of the ecosystem and the different groups involved there, enabling people to evolve and push on things more independently just allows it to scale better. And by people, you mean individual developers and? And organizations. And organizations. That's right. So the hope is that everybody sort of major, I don't know, Pepsi or something uses, like major corporations go to TensorFlow to this kind of. Yeah, if you look at enterprises like Pepsi or these, I mean, a lot of them are already using TensorFlow. They are not the ones that do the development or changes in the core. Some of them do, but a lot of them don't. I mean, they touch small pieces. There are lots of these, some of them being, let's say, hardware vendors who are building their custom hardware and they want their own pieces. Or some of them being bigger companies, say, IBM. I mean, they're involved in some of our special interest groups, and they see a lot of users who want certain things and they want to optimize for that. So folks like that often. Autonomous vehicle companies, perhaps. Exactly, yes. So, yeah, like I mentioned, TensorFlow has been downloaded 41 million times, 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. So I'm not sure if you can explain it, but what does it take to build a community like that? In retrospect, what do you think, what is the critical thing that allowed for this growth to happen, and how does that growth continue? Yeah, yeah, that's an interesting question. I wish I had all the answers there, I guess, so you could replicate it. I think there are a number of things that need to come together, right? One, just like any new thing, it is about, there's a sweet spot of timing, what's needed, does it grow with, what's needed, so in this case, for example, TensorFlow's not just grown because it was a good tool, it's also grown with the growth of deep learning itself. So those factors come into play. Other than that, though, I think just hearing, listening to the community, what they do, what they need, being open to, like in terms of external contributions, we've spent a lot of time in making sure we can accept those contributions well, we can help the contributors in adding those, putting the right process in place, getting the right kind of community, welcoming them and so on. Like over the last year, we've really pushed on transparency, that's important for an open source project. People wanna know where things are going, and we're like, okay, here's a process where you can do that, here are our RFCs and so on. So thinking through, there are lots of community aspects that come into that you can really work on. As a small project, it's maybe easy to do because there's like two developers and you can do those. As you grow, putting more of these processes in place, thinking about the documentation, thinking about what two developers care about, what kind of tools would they want to use, all of these come into play, I think. So one of the big things I think that feeds the TensorFlow fire is people building something on TensorFlow, and implement a particular architecture that does something cool and useful, and they put that on GitHub. And so it just feeds this growth. Do you have a sense that with 2.0 and 1.0 that there may be a little bit of a partitioning like there is with Python 2 and 3, that there'll be a code base and in the older versions of TensorFlow, they will not be as compatible easily? Or are you pretty confident that this kind of conversion is pretty natural and easy to do? So we're definitely working hard to make that very easy to do. There's lots of tooling that we talked about at the developer summit this week, and we'll continue to invest in that tooling. It's, you know, when you think of these significant version changes, that's always a risk, and we are really pushing hard to make that transition very, very smooth. So I think, so at some level, people wanna move and they see the value in the new thing. They don't wanna move just because it's a new thing, and some people do, but most people want a really good thing. And I think over the next few months, as people start to see the value, we'll definitely see that shift happening. So I'm pretty excited and confident that we will see people moving. As you said earlier, this field is also moving rapidly, so that'll help because we can do more things and all the new things will clearly happen in 2.x, so people will have lots of good reasons to move. So what do you think TensorFlow 3.0 looks like? Is there, are things happening so crazily that even at the end of this year seems impossible to plan for? Or is it possible to plan for the next five years? I think it's tricky. There are some things that we can expect in terms of, okay, change, yes, change is gonna happen. Are there some things gonna stick around and some things not gonna stick around? I would say the basics of deep learning, the, you know, say convolution models or the basic kind of things, they'll probably be around in some form still in five years. Will RL and GAN stay? Very likely, based on where they are. Will we have new things? Probably, but those are hard to predict. And some directionally, some things that we can see is, you know, in things that we're starting to do, right, with some of our projects right now is just 2.0 combining eager execution and graphs where we're starting to make it more like just your natural programming language. You're not trying to program something else. Similarly, with Swift for TensorFlow, we're taking that approach. Can you do something ground up, right? So some of those ideas seem like, okay, that's the right direction. In five years, we expect to see more in that area. Other things we don't know is, will hardware accelerators be the same? Will we be able to train with four bits instead of 32 bits? And I think the TPU side of things is exploring that. I mean, TPU is already on version three. It seems that the evolution of TPU and TensorFlow are sort of, they're coevolving almost in terms of both are learning from each other and from the community and from the applications where the biggest benefit is achieved. That's right. You've been trying to sort of, with Eager, with Keras, to make TensorFlow as accessible and easy to use as possible. What do you think, for beginners, is the biggest thing they struggle with? Have you encountered that? Or is basically what Keras is solving is that Eager, like we talked about? Yeah, for some of them, like you said, right, the beginners want to just be able to take some image model, they don't care if it's Inception or ResNet or something else, and do some training or transfer learning on their kind of model. Being able to make that easy is important. So in some ways, if you do that by providing them simple models with say, in hub or so on, they don't care about what's inside that box, but they want to be able to use it. So we're pushing on, I think, different levels. If you look at just a component that you get, which has the layers already smooshed in, the beginners probably just want that. Then the next step is, okay, look at building layers with Keras. If you go out to research, then they are probably writing custom layers themselves or doing their own loops. So there's a whole spectrum there. And then providing the pre trained models seems to really decrease the time from you trying to start. You could basically in a Colab notebook achieve what you need. So I'm basically answering my own question because I think what TensorFlow delivered on recently is trivial for beginners. So I was just wondering if there was other pain points you're trying to ease, but I'm not sure there would. No, those are probably the big ones. I see high schoolers doing a whole bunch of things now, which is pretty amazing. It's both amazing and terrifying. Yes. In a sense that when they grow up, it's some incredible ideas will be coming from them. So there's certainly a technical aspect to your work, but you also have a management aspect to your role with TensorFlow leading the project, a large number of developers and people. So what do you look for in a good team? What do you think? Google has been at the forefront of exploring what it takes to build a good team and TensorFlow is one of the most cutting edge technologies in the world. So in this context, what do you think makes for a good team? It's definitely something I think a favorite about. I think in terms of the team being able to deliver something well, one of the things that's important is a cohesion across the team. So being able to execute together in doing things that's not an end, like at this scale, an individual engineer can only do so much. There's a lot more that they can do together, even though we have some amazing superstars across Google and in the team, but there's, you know, often the way I see it as the product of what the team generates is way larger than the whole or the individual put together. And so how do we have all of them work together, the culture of the team itself, hiring good people is important. But part of that is it's not just that, okay, we hire a bunch of smart people and throw them together and let them do things. It's also people have to care about what they're building, people have to be motivated for the right kind of things. That's often an important factor. And, you know, finally, how do you put that together with a somewhat unified vision of where we wanna go? So are we all looking in the same direction or each of us going all over? And sometimes it's a mix. Google's a very bottom up organization in some sense, also research even more so, and that's how we started. But as we've become this larger product and ecosystem, I think it's also important to combine that well with a mix of, okay, here's the direction we wanna go in. There is exploration we'll do around that, but let's keep staying in that direction, not just all over the place. And is there a way you monitor the health of the team? Sort of like, is there a way you know you did a good job? The team is good? Like, I mean, you're sort of, you're saying nice things, but it's sometimes difficult to determine how aligned. Yes. Because it's not binary. It's not like there's tensions and complexities and so on. And the other element of the mission of superstars, there's so much, even at Google, such a large percentage of work is done by individual superstars too. So there's a, and sometimes those superstars can be against the dynamic of a team and those tensions. I mean, I'm sure in TensorFlow it might be a little bit easier because the mission of the project is so sort of beautiful. You're at the cutting edge, so it's exciting. But have you had struggle with that? Has there been challenges? There are always people challenges in different kinds of ways. That said, I think we've been what's good about getting people who care and are, you know, have the same kind of culture, and that's Google in general to a large extent. But also, like you said, given that the project has had so many exciting things to do, there's been room for lots of people to do different kinds of things and grow, which does make the problem a bit easier, I guess. And it allows people, depending on what they're doing, if there's room around them, then that's fine. But yes, we do care about whether a superstar or not, that they need to work well with the team across Google. That's interesting to hear. So it's like superstar or not, the productivity broadly is about the team. Yeah, yeah. I mean, they might add a lot of value, but if they're hurting the team, then that's a problem. So in hiring engineers, it's so interesting, right, the hiring process. What do you look for? How do you determine a good developer or a good member of a team from just a few minutes or hours together? Again, no magic answers, I'm sure. Yeah, I mean, Google has a hiring process that we've refined over the last 20 years, I guess, and that you've probably heard and seen a lot about. So we do work with the same hiring process and that's really helped. For me in particular, I would say, in addition to the core technical skills, what does matter is their motivation in what they wanna do. Because if that doesn't align well with where we wanna go, that's not gonna lead to long term success for either them or the team. And I think that becomes more important the more senior the person is, but it's important at every level. Like even the junior most engineer, if they're not motivated to do well at what they're trying to do, however smart they are, it's gonna be hard for them to succeed. Does the Google hiring process touch on that passion? So like trying to determine, because I think as far as I understand, maybe you can speak to it, that the Google hiring process sort of helps in the initial like determines the skill set there, is your puzzle solving ability, problem solving ability good? But like, I'm not sure, but it seems that the determining whether the person is like fire inside them, that burns to do anything really, it doesn't really matter. It's just some cool stuff, I'm gonna do it. Is that something that ultimately ends up when they have a conversation with you or once it gets closer to the team? So one of the things we do have as part of the process is just a culture fit, like part of the interview process itself, in addition to just the technical skills and each engineer or whoever the interviewer is, is supposed to rate the person on the culture and the culture fit with Google and so on. So that is definitely part of the process. Now, there are various kinds of projects and different kinds of things. So there might be variants and of the kind of culture you want there and so on. And yes, that does vary. So for example, TensorFlow has always been a fast moving project and we want people who are comfortable with that. But at the same time now, for example, we are at a place where we are also very full fledged product and we wanna make sure things that work really, really work, right? You can't cut corners all the time. So balancing that out and finding the people who are the right fit for those is important. And I think those kinds of things do vary a bit across projects and teams and product areas across Google. And so you'll see some differences there in the final checklist. But a lot of the core culture, it comes along with just the engineering excellence and so on. What is the hardest part of your job? I'll take your pick, I guess. It's fun, I would say, right? Hard, yes. I mean, lots of things at different times. I think that does vary. So let me clarify that difficult things are fun when you solve them, right? So it's fun in that sense. I think the key to a successful thing across the board and in this case, it's a large ecosystem now, but even a small product, is striking that fine balance across different aspects of it. Sometimes it's how fast do you go versus how perfect it is. Sometimes it's how do you involve this huge community? Who do you involve or do you decide, okay, now is not a good time to involve them because it's not the right fit. Sometimes it's saying no to certain kinds of things. Those are often the hard decisions. Some of them you make quickly because you don't have the time. Some of them you get time to think about them, but they're always hard. So both choices are pretty good, those decisions. What about deadlines? Is this, do you find TensorFlow, to be driven by deadlines to a degree that a product might? Or is there still a balance to where it's less deadline? You had the Dev Summit today that came together incredibly. Looked like there's a lot of moving pieces and so on. So did that deadline make people rise to the occasion releasing TensorFlow 2.0 alpha? I'm sure that was done last minute as well. I mean, up to the last point. Again, it's one of those things that you need to strike the good balance. There's some value that deadlines bring that does bring a sense of urgency to get the right things together. Instead of getting the perfect thing out, you need something that's good and works well. And the team definitely did a great job in putting that together. So I was very amazed and excited by everything how that came together. That said, across the year, we try not to put out official deadlines. We focus on key things that are important, figure out how much of it's important. And we are developing in the open, both internally and externally, everything's available to everybody. So you can pick and look at where things are. We do releases at a regular cadence. So fine, if something doesn't necessarily end up this month, it'll end up in the next release in a month or two. And that's okay, but we want to keep moving as fast as we can in these different areas. Because we can iterate and improve on things, sometimes it's okay to put things out that aren't fully ready. We'll make sure it's clear that okay, this is experimental, but it's out there if you want to try and give feedback. That's very, very useful. I think that quick cycle and quick iteration is important. That's what we often focus on rather than here's a deadline where you get everything else. Is 2.0, is there pressure to make that stable? Or like, for example, WordPress 5.0 just came out and there was no pressure to, it was a lot of build updates delivered way too late, but, and they said, okay, well, but we're gonna release a lot of updates really quickly to improve it. Do you see TensorFlow 2.0 in that same kind of way or is there this pressure to once it hits 2.0, once you get to the release candidate and then you get to the final, that's gonna be the stable thing? So it's gonna be stable in, just like when NodeX was where every API that's there is gonna remain in work. It doesn't mean we can't change things under the covers. It doesn't mean we can't add things. So there's still a lot more for us to do and we'll continue to have more releases. So in that sense, there's still, I don't think we'll be done in like two months when we release this. I don't know if you can say, but is there, there's not external deadlines for TensorFlow 2.0, but is there internal deadlines, the artificial or otherwise, that you're trying to set for yourself or is it whenever it's ready? So we want it to be a great product, right? And that's a big important piece for us. TensorFlow's already out there. We have 41 million downloads for 1.0 X. So it's not like we have to have this. Yeah, exactly. So it's not like, a lot of the features that we've really polishing and putting them together are there. We don't have to rush that just because. So in that sense, we wanna get it right and really focus on that. That said, we have said that we are looking to get this out in the next few months, in the next quarter. And as far as possible, we'll definitely try to make that happen. Yeah, my favorite line was, spring is a relative concept. I love it. Yes. Spoken like a true developer. So something I'm really interested in and your previous line of work is, before TensorFlow, you led a team at Google on search ads. I think this is a very interesting topic on every level, on a technical level, because at their best, ads connect people to the things they want and need. So, and at their worst, they're just these things that annoy the heck out of you to the point of ruining the entire user experience of whatever you're actually doing. So they have a bad rep, I guess. And on the other end, so that this connecting users to the thing they need and want is a beautiful opportunity for machine learning to shine. Like huge amounts of data that's personalized and you kind of map to the thing they actually want won't get annoyed. So what have you learned from this, Google that's leading the world in this aspect, what have you learned from that experience and what do you think is the future of ads? Take you back to that. Yeah, yes, it's been a while, but I totally agree with what you said. I think the search ads, the way it was always looked at and I believe it still is, is it's an extension of what search is trying to do. And the goal is to make the information and make the world's information accessible. That's it's not just information, but maybe products or other things that people care about. And so it's really important for them to align with what the users need. And in search ads, there's a minimum quality level before that ad would be shown. If you don't have an ad that hits that quality, but it will not be shown even if we have it and okay, maybe we lose some money there, that's fine. That is really, really important. And I think that that is something I really liked about being there. Advertising is a key part. I mean, as a model, it's been around for ages, right? It's not a new model, it's been adapted to the web and became a core part of search and many other search engines across the world. And I do hope, like you said, there are aspects of ads that are annoying and I go to a website and if it just keeps popping an ad in my face not to let me read, that's gonna be annoying clearly. So I hope we can strike that balance between showing a good ad where it's valuable to the user and provides the monetization to the service. And this might be search, this might be a website, all of these, they do need the monetization for them to provide that service. But if it's done in a good balance between showing just some random stuff that's distracting versus showing something that's actually valuable. So do you see it moving forward as to continue being a model that funds businesses like Google, that's a significant revenue stream? Because that's one of the most exciting things but also limiting things in the internet is nobody wants to pay for anything. And advertisements, again, coupled at their best, are actually really useful and not annoying. Do you see that continuing and growing and improving or is there, do you see sort of more Netflix type models where you have to start to pay for content? I think it's a mix. I think it's gonna take a long while for everything to be paid on the internet, if at all, probably not. I mean, I think there's always gonna be things that are sort of monetized with things like ads. But over the last few years, I would say we've definitely seen that transition towards more paid services across the web and people are willing to pay for them because they do see the value. I mean, Netflix is a great example. I mean, we have YouTube doing things. People pay for the apps they buy. More people I find are willing to pay for newspaper content for the good news websites across the web. That wasn't the case a few years, even a few years ago, I would say. And I just see that change in myself as well and just lots of people around me. So definitely hopeful that we'll transition to that mix model where maybe you get to try something out for free, maybe with ads, but then there's a more clear revenue model that sort of helps go beyond that. So speaking of revenue, how is it that a person can use the TPU in a Google call app for free? So what's the, I guess the question is, what's the future of TensorFlow in terms of empowering, say, a class of 300 students? And I'm asked by MIT, what is going to be the future of them being able to do their homework in TensorFlow? Like, where are they going to train these networks, right? What's that future look like with TPUs, with cloud services, and so on? I think a number of things there. I mean, any TensorFlow open source, you can run it wherever, you can run it on your desktop and your desktops always keep getting more powerful, so maybe you can do more. My phone is like, I don't know how many times more powerful than my first desktop. You'll probably train it on your phone though, yeah, that's true. Right, so in that sense, the power you have in your hands is a lot more. Clouds are actually very interesting from, say, students or courses perspective, because they make it very easy to get started. I mean, Colab, the great thing about it is, go to a website and it just works. No installation needed, nothing to, you're just there and things are working. That's really the power of cloud as well. And so I do expect that to grow. Again, Colab is a free service. It's great to get started, to play with things, to explore things. That said, with free, you can only get so much. You'd be, yeah. So just like we were talking about, free versus paid, yeah, there are services you can pay for and get a lot more. Great, so if I'm a complete beginner interested in machine learning and TensorFlow, what should I do? Probably start with going to our website and playing there. So just go to TensorFlow.org and start clicking on things. Yep, check out tutorials and guides. There's stuff you can just click there and go to a Colab and do things. No installation needed, you can get started right there. Okay, awesome, Rajit, thank you so much for talking today. Thank you, Lex, it was great.\n",
      "## End of Transcript\n",
      "\n",
      "## Summary\n",
      "The podcast describes the growth of deep learning and machine learning, highlighting the success of AI projects and the open-source nature of TensorFlow. It also discusses the impact of open-source projects on technology and the overall impact of TensorFlow on the AI community.\n",
      "\n",
      "The podcast summarizes the key points of various articles about paid services, advertising on the internet, AI, and its potential impact on education and advertising. It highlights the accessibility and power of AI tools like TPUs, cloud services, and TensorFlow, as well as the benefits of platforms like Colab for machine learning beginners. Additionally, it explores the impact of advertising on information accessibility and its potential for connecting users to desired products.\n",
      "\n",
      "The podcast concludes by discussing the future of advertising and monetization on the internet. It emphasizes the potential of AI to revolutionize the advertising industry and its ability to connect users with relevant content.\n",
      "## End of Summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completeness evaluation score for episode 22\n",
      "[{'text': 'Mostly | The summary accurately captures the key points of the original document, including the discussion of TensorFlow, the impact of open-source projects, the future of advertising, and the use of TPUs and cloud services. It also covers the growth of deep learning and machine learning, as well as the accessibility of platforms like Colab for beginners. However, the summary could have included more specific details and examples from the original document to provide a more comprehensive overview.'}]\n",
      "##############################################\n",
      "##############################################\n",
      "\n",
      "#######  Episode 23\n",
      "## Transcript\n",
      "The following is a conversation with Gavin Miller, he's the head of Adobe Research. Adobe has empowered artists, designers, and creative minds from all professions working in the digital medium for over 30 years with software such as Photoshop, Illustrator, Premiere, After Effects, InDesign, Audition, software that work with images, video, and audio. Adobe Research is working to define the future evolution of these products in a way that makes the life of creatives easier, automates the tedious tasks, and gives more and more time to operate in the idea space instead of pixel space. This is where the cutting edge, deep learning methods of the past decade can really shine more than perhaps any other application. Gavin is the embodiment of combining tech and creativity. Outside of Adobe Research, he writes poetry and builds robots, both things that are near and dear to my heart as well. This conversation is part of the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter at Lux Friedman spelled F R I D. And now, here's my conversation with Gavin Miller. You're head of Adobe Research, leading a lot of innovative efforts and applications of AI, creating images, video, audio, language, but you're also yourself an artist, a poet, a writer, and even a roboticist. So, while I promised to everyone listening that I will not spend the entire time we have together reading your poetry, which I love, I have to sprinkle it in at least a little bit. So, some of them are pretty deep and profound and some are light and silly. Let's start with a few lines from the silly variety. You write in Je Ne Vinaigrette Rien, a poem that beautifully parodies both Edith Piaf's Je Ne Vinaigrette Rien and My Way by Frank Sinatra. So, it opens with, and now dessert is near. It's time to pay the final total. I've tried to slim all year, but my diets have been anecdotal. So, where does that love for poetry come from for you? And if we dissect your mind, how does it all fit together in the bigger puzzle of Dr. Gavin Miller? Oh, well, interesting you chose that one. That was a poem I wrote when I'd been to my doctor and he said you really need to lose some weight and go on a diet. And whilst the rational part of my brain wanted to do that, the irrational part of my brain was protesting and sort of embraced the opposite idea. I regret nothing hence. Yes, exactly. Taken to an extreme, I thought it would be funny. Obviously, it's a serious topic for some people. But I think for me, I've always been interested in writing since I was in high school, as well as doing technology and invention. And sometimes there are parallel strands in your life that carry on. And one is more about your private life and one's more about your technological career. And then at sort of happy moments along the way, sometimes the two things touch. One idea informs the other. And we can talk about that as we go. Do you think your writing, the art, the poetry contribute indirectly or directly to your research, to your work in Adobe? Well, sometimes it does if I say, imagine a future in a science fiction kind of way. And then once it exists on paper, I think, well, why shouldn't I just build that? There was an example where when realistic voice synthesis first started in the 90s at Apple, where I worked in research, it was done by a friend of mine. I sort of sat down and started writing a poem, which each line I would enter into the voice synthesizer and see how it sounded, and sort of wrote it for that voice. And at the time, the agents weren't very sophisticated. So they'd sort of add random intonation. And I kind of made up the poem to sort of match the tone of the voice. And it sounded slightly sad and depressed. So I pretended it was a poem written by an intelligent agent, sort of telling the user to go home and leave them alone. But at the same time, they were lonely and wanted to have company and learn from what the user was saying. And at the time, it was way beyond anything that AI could possibly do. But since then, it's becoming more within the bounds of possibility. And then at the same time, I had a project at home where I did sort of a smart home. This was probably 93, 94. And I had the talking voice who'd remind me when I walked in the door of what things I had to do. I had buttons on my washing machine because I was a bachelor and I'd leave the clothes in there for three days and they got moldy. So as I got up in the morning, it would say, don't forget the washing and so on. I made photo albums that use light sensors to know which page you were looking at would send that over wireless radio to the agent who would then play sounds that match the image you were looking at in the book. So I was kind of in love with this idea of magical realism and whether it was possible to do that with technology. So that was a case where the sort of the agent sort of intrigued me from a literary point of view and became a personality. I think more recently, I've also written plays and when plays you write dialogue and obviously you write a fixed set of dialogue that follows a linear narrative. But with modern agents, as you design a personality or a capability for conversation, you're sort of thinking of, I kind of have imaginary dialogue in my head. And then I think, what would it take not only to have that be real, but for it to really know what it's talking about. So it's easy to fall into the uncanny valley with AI where it says something it doesn't really understand, but it sounds good to the person. But you rapidly realize that it's kind of just stimulus response. It doesn't really have real world knowledge about the thing it's describing. And so when you get to that point, it really needs to have multiple ways of talking about the same concept. So it sounds as though it really understands it. Now, what really understanding means is in the eye of the beholder, right? But if it only has one way of referring to something, it feels like it's a canned response. But if it can reason about it, or you can go at it from multiple angles and give a similar kind of response that people would, then it starts to seem more like there's something there that's sentient. You can say the same thing, multiple things from different perspectives. I mean, with the automatic image captioning that I've seen the work that you're doing, there's elements of that, right? Being able to generate different kinds of statements about the same picture. Right. So in my team, there's a lot of work on turning a medium from one form to another, whether it's auto tagging imagery or making up full sentences about what's in the image, then changing the sentence, finding another image that matches the new sentence or vice versa. And in the modern world of GANs, you sort of give it a description and it synthesizes an asset that matches the description. So I've sort of gone on a journey. My early days in my career were about 3D computer graphics, the sort of pioneering work, sort of before movies had special effects done with 3D graphics, and sort of rode that revolution. And that was very much like the Renaissance where people would model light and color and shape and everything. And now we're kind of in another wave where it's more impressionistic and it's sort of the idea of something can be used to generate an image directly, which is sort of the new frontier in computer image generation using AI algorithms. So the creative process is more in the space of ideas or becoming more in the space of ideas versus in the raw pixels? Well, it's interesting. It depends. I think at Adobe, we really want to span the entire range from really, really good, what you might call low level tools by low level as close to say, analog workflows as possible. So what we do there is we make up systems that do really realistic oil paint and watercolor simulations. So if you want every bristle to behave as it would in the real world and leave a beautiful analog trail of water and then flow after you've made the brushstroke, you can do that. And that's really important for people who want to create something really expressive or really novel because they have complete control. And then as certain other tasks become automated, it frees the artists up to focus on the inspiration and less of the perspiration. So thinking about different ideas, obviously. Once you finish the design, there's a lot of work to, say, do it for all the different aspect ratio of phones or websites and so on. And that used to take up an awful lot of time for artists. It still does for many what we call content velocity. And one of the targets of AI is actually to reason about from the first example of what are the likely intent for these other formats? Maybe if you change the language to German and the words are longer, how do you reflow everything so that it looks nicely artistic in that way? And so the person can focus on the really creative bit in the middle, which is what is the look and style and feel and what's the message and what's the story and the human element? So I think creativity is changing. So that's one way in which we're trying to just make it easier and faster and cheaper to do so that there can be more of it, more demand because it's less expensive. So everyone wants beautiful artwork for everything from a school website to Hollywood movie. On the other side, as some of these things have automatic versions of them, people will possibly change role from being the hands on artisan to being either the art director or the conceptual artist. And then the computer will be a partner to help create polished examples of the idea that they're exploring. Let's talk about Adobe products, AI and Adobe products. Just so you know where I'm coming from, I'm a huge fan of Photoshop for images, Premiere for video, Audition for audio. I'll probably use Photoshop to create the thumbnail for this video, Premiere to edit the video, Audition to do the audio. That said, everything I do is really manually and I set up, I use this old school Kinesis keyboard and I have auto hotkey that just, it's really about optimizing the flow. Of just making sure there's as few clicks as possible, so just being extremely efficient, something you started to speak to. So before we get into the fun sort of awesome deep learning things, where does AI, if you could speak a little more to it, AI or just automation in general, do you see in the coming months and years or in general, prior in 2018, fitting into making the life, the low level pixel work flow easier? Yeah, that's a great question. So we have a very rich array of algorithms already in Photoshop, just classical procedural algorithms as well as ones based on data. In some cases, they end up with a large number of sliders and degrees of freedom. So one way in which AI can help is just an auto button, which comes up with default settings based on the content itself rather than default values for the tool. At that point, you then start tweaking. So that's a very kind of make life easier for people whilst making use of common sense from other example images. So like smart defaults. Smart defaults, absolutely. Another one is something we've spent a lot of work over the last 20 years I've been at Adobe, or 19, thinking about selection, for instance, where, you know, with quick select, you would look at color boundaries and figure out how to sort of flood fill into regions that you thought were physically connected in the real world. But that algorithm had no visual common sense about what a cat looks like or a dog. It would just do it based on rules of thumb, which were applied to graph theory. And it was a big improvement over the previous work where you had sort of almost click everything by hand. Or if it just did similar colors, it would do little tiny regions that wouldn't be connected. But in the future, using neural nets to actually do a great job with, say, a single click or even in the case of well known categories like people or animals, no click where you just say select the object and it just knows the dominant object is a person in the middle of the photograph. Those kinds of things are really valuable if they can be robust enough to give you good quality results or they can be a great start for like tweaking it. So, for example, background removal. Correct. Like one thing I'll, in a thumbnail, I'll take a picture of you right now and essentially remove the background behind you. And I want to make that as easy as possible. You don't have flowing hair, like rich at the moment. I had it in the past. It may come again in the future. So that sometimes makes it a little more challenging to remove the background. How difficult do you think is that problem for AI for basically making the quick selection tool smarter and smarter and smarter? Well, we have a lot of research on that already. If you want a sort of quick, cheap and cheerful, look, I'm pretending I'm in Hawaii, but it's sort of a joke, then you don't need perfect boundaries. And you can do that today with a single click with the algorithms we have. We have other algorithms where with a little bit more guidance on the boundaries, like you might need to touch it up a little bit. We have other algorithms that can pull a nice mat from a crude selection. So we have combinations of tools that can do all of that. And at our recent Max conference at Adobe Max, we demonstrated how very quickly, just by drawing a simple polygon around the object of interest, we could not only do it for a single still, but we could pull a mat, well, pull at least a selection mask from a moving target, like a person dancing in front of a brick wall or something. And so it's going from hours to a few seconds for workflows that are really nice, and then you might go in and touch up a little. So that's a really interesting question. You mentioned the word robust. You know, there's like a journey for an idea, right? And what you presented probably at Max has elements of just sort of, it inspires the concept, it can work pretty well in a majority of cases. But how do you make something that works, well, in majority of cases, how do you make something that works, maybe in all cases, or it becomes a robust tool that can... Well, there are a couple of things. So that really touches on the difference between academic research and industrial research. So in academic research, it's really about who's the person to have the great new idea that shows promise. And we certainly love to be those people too. But we have sort of two forms of publishing. One is academic peer review, which we do a lot of, and we have great success there as much as some universities. But then we also have shipping, which is a different type of... And then we get customer review, as well as, you know, product critics. And that might be a case where it's not about being perfect every single time, but perfect enough of the time, plus a mechanism to intervene and recover where you do have mistakes. So we have the luxury of very talented customers. We don't want them to be overly taxed doing it every time. But if they can go in and just take it from 99 to 100 with the touch of a mouse or something, then for the professional end, that's something that we definitely want to support as well. And for them, it went from having to do that tedious task all the time to much less often. So I think that gives us an out. If it had to be 100% automatic all the time, then that would delay the time at which we could get to market. So on that thread, maybe you can untangle something. Again, I'm sort of just speaking to my own experience. Maybe that is the most useful. Absolutely. So I think Photoshop, as an example, or Premiere, has a lot of amazing features that I haven't touched. And so in terms of AI helping make my life or the life of creatives easier, this collaboration between human and machine, how do you learn to collaborate better? How do you learn the new algorithms? Is it something where you have to watch tutorials and you have to watch videos and so on? Or do you think about the experience itself through exploration, being the teacher? We absolutely do. So I'm glad that you brought this up. We sort of think about two things. One is helping the person in the moment to do the task that they need to do, but the other is thinking more holistically about their journey learning a tool. And when it's like, think of it as Adobe University, where you use the tool long enough, you become an expert. And not necessarily an expert in everything. It's like living in a city. You don't necessarily know every street, but you know the important ones you need to get to. So we have projects in research, which actually look at the thousands of hours of tutorials online and try to understand what's being taught in them. And then we had one publication at CHI where it was looking at, given the last three or four actions you did, what did other people in tutorials do next? So if you want some inspiration for what you might do next, or you just want to watch the tutorial and see, learn from people who are doing similar workflows to you, you can without having to go and search on keywords and everything. So really trying to use the context of your use of the app to make intelligent suggestions, either about choices that you might make, or in a more assistive way, where it could say, if you did this next, we could show you. And that's basically the frontier that we're exploring now, which is, if we really deeply understand the domain in which designers and creative people work, can we combine that with AI and pattern matching of behavior to make intelligent suggestions, either through, you know, verbal, possibilities, or just showing the results of if you try this. And that's really the sort of, you know, I was in a meeting today thinking about these things. Well, it's still a grand challenge. You know, we'd all love an artist over one shoulder and a teacher over the other, right? And we hope to get there. And the right thing to do is to give enough at each stage that it's useful in itself, but it builds a foundation for the next stage. Give enough at each stage that it's useful in itself, but it builds a foundation for the next level of expectation. Are you aware of this gigantic medium of YouTube that's creating just a bunch of creative people, both artists and teachers of different kinds? Absolutely. And the more we can understand those media types, both visually and in terms of transcripts and words, the more we can bring the wisdom that they embody into the guidance that's embedded in the tool. That would be brilliant to remove the barrier from having to yourself type in the keyword searching, so on. Absolutely. And then in the longer term, an interesting discussion is, does it ultimately not just assist with learning the interface we have, but does it modify the interface to be simpler? Or do you fragment into a variety of tools, each of which has a different level of visibility of the functionality? I like to say that if you add a feature to a GUI, you have to have yet more visual complexity confronting the new user. Whereas if you have an assistant with a new skill, if you know they have it, so you know to ask for it, then it's sort of additive without being more intimidating. So we definitely think about new users and how to onboard them. Many actually value the idea of being able to master that complex interface and keyboard shortcuts like you were talking about earlier, because with great familiarity, it becomes a musical instrument for expressing your visual ideas. And other people just want to get something done quickly in the simplest way possible. And that's where a more assistive version of the same technology might be useful, maybe on a different class of device, which is more in context for CAPTCHA, say. Whereas somebody who's in a deep post production workflow maybe want to be on a laptop or a big screen desktop and have more knobs and dials to really express the subtlety of what they want to do. So there's so many exciting applications of computer vision and machine learning that Adobe is working on, like scene stitching, sky replacement, foreground, background removal, spatial object based image search, automatic image captioning, like we mentioned, project cloak, project deep fill, filling in parts of the images, project scribbler, style transform video, style transform faces and video with project puppetron, best name ever. Can you talk through a favorite or some of them or examples that popped in mind? I'm sure I'll be able to provide links to other ones we don't talk about because there's visual elements to all of them that are exciting. Why they're interesting for different reasons might be a good way to go. So I think sky replace is interesting because we talked about selection being sort of an atomic operation. It's almost like if you think of an assembly language, it's like a single instruction. Whereas sky replace is a compound action where you automatically select the sky, you look for stock content that matches the geometry of the scene. You try to have variety in your choices so that you do coverage of different moods. It then mats in the sky behind the foreground. But then importantly, it uses the foreground of the other image that you just searched on to recolor the foreground of the image that you're editing. So if you say go from a midday sky to an evening sky, it will actually add sort of an orange glow to the foreground objects as well. I was a big fan in college of Magritte and he has a number of paintings where it's surrealism because he'll like do a composite, but the foreground building will be at night and the sky will be during the day. There's one called The Empire of Light, which was on my wall in college. And we're trying not to do surrealism. It can be a choice, but we'd rather have it be natural by default rather than it looking fake. And then you have to do a whole bunch of post production to fix it. So that's a case where we're kind of capturing an entire workflow into a single action and doing it in about a second rather than a minute or two. And when you do that, you can not just do it once, but you can do it for say like 10 different backgrounds. And then you're almost back to this inspiration idea of I don't know quite what I want, but I'll know it when I see it. And you can just explore the design space as close to final production value as possible. And then when you really pick one, you might go back and slightly tweak the selection mask just to make it perfect and do that kind of polish that professionals like to bring to their work. So then there's this idea of, you mentioned the sky, replacing it to different stock images of the sky. But in general, you have this idea. Or it could be on your disc or whatever. Disc, right. But making even more intelligent choices about ways to search stock images, which is really interesting. It's kind of spatial. Absolutely. Right. So that was something we called concept canvas. So normally when you do a say an image search, you would I assuming it's just based on text, you would give the keywords of the things you want to be in the image, and it would find the nearest one that had those tags. For many tasks, you really want, you know, to be able to say I want a big person in the middle or in a dog to the right and umbrella above the left because you want to leave space for the text or whatever for the and so concept canvas lets you assign spatial regions to the keywords. And then we've already pre indexed the images to know where the important concepts are in the picture. So we then go through that index matching to assets. And even though it's just another form of search, because you're doing spatial design or layout, it starts to feel like design, you sort of feel oddly responsible for the image that comes back as if you invented it. Yeah. So it's, it's a it's a good example where giving enough control starts to make people have a sense of ownership over the outcome of the event. And then we also have technologies in Photoshop, we physically can move the dog in post as well. But for concept canvas, it was just a very fast way to sort of loop through and be able to lay things out. And in terms of being able to remove objects from a scene and fill in the background, right, automatically. I so that's extremely exciting. And that's so neural networks are stepping in there. I just talked this week, Ian Goodfellow, so the GANs for doing that is definitely one approach. So that is that is that a really difficult problem? Is it as difficult as it looks, again, to take it to a robust product level? Well, there are certain classes of image for which the traditional algorithms like content aware fill work really well, like if you have a naturalistic texture, like a gravel path or something, because it's patch based, it will make up a very plausible looking intermediate thing and fill in the hole. And then we use some algorithms to sort of smooth out the lighting so you don't see any brightness contrast in that region, or you've gradually ramped from one from dark to light, if it straddles the boundary, where it gets complicated as if you have to infer invisible structure behind behind the person in front. And that really requires a common sense knowledge of the world to know what, you know, if I see three quarters of a house, do I have a rough sense of what the rest of the house looks like? If you just fill it in with patches, it can end up sort of doing things that make sense locally, but you look at the global structure, and it looks like it's just sort of crumpled or messed up. And so what GANs and neural nets bring to the table is this common sense learned from the training set. And the challenge right now is that the generative methods that can make up missing holes using that kind of technology are still only stable at low resolutions. And so you either need to then go from a low resolution to a high resolution using some other algorithm, or we need to push the state of the art and it's still in research to get to that point. Of course, if you show it something, say it's trained on houses, and then you show it an octopus, it's not going to do a very good job of showing common sense about octopuses. So again, you're asking about how you know that it's ready for primetime. You really need a very diverse training set of images. And ultimately, that may be a case where you put it out there with some guardrails where you might do a detector which looks at the image and sort of estimates its own competence of how well a job could this algorithm do. So eventually, there may be this idea of what we call an ensemble of experts where any particular expert is specialized in certain things. And then there's sort of a, either they vote to say how confident they are about what to do, this is sort of more future looking, or there's some dispatcher which says you're good at houses, you're good at trees. So I mean, all this adds up to a lot of work because each of those models will be a whole bunch of work. But I think over time, you'd gradually fill out the set and initially focus on certain workflows and then sort of branch out as you get more capable. You mentioned workflows, and have you considered maybe looking far into the future? First of all, using the fact that there is a huge amount of people that use Photoshop, for example, and have certain workflows, being able to collect the information by which they, you know, basically get information about their workflows, about what they need, the ways to help them, whether it is houses or octopus that people work on more, you know, like basically getting a beat on what kind of data is needed to be annotated and collected for people to build tools that actually work well for people. Right, absolutely. And this is a big topic in the whole world of AI is what data can you gather and why? Right. At one level, a way to think about it is we not only want to train our customers in how to use our products, but we want them to teach us what's important and what's useful. At the same time, we want to respect their privacy. And obviously, we wouldn't do things without their explicit permission. And I think the modern spirit of the age around this is you have to demonstrate to somebody how they're benefiting from sharing their data with the tool. Either it's helping in the short term to understand their intent, so you can make better recommendations, or if they're friendly to your cause, or your tool, or they want to help you evolve quickly, because they depend on you for their livelihood, they may be willing to share some of their workflows or choices with the data set to be then trained. There are technologies for looking at learning without necessarily storing all the information permanently, so that you can sort of learn on the fly, but not keep a record of what somebody did. So we're definitely exploring all of those possibilities. And I think Adobe exists in a space where Photoshop, like if I look at the data I've created and own, you know, I'm less comfortable sharing data with social networks than I am with Adobe, because there's a, just exactly as you said, there's an obvious benefit for sharing for sharing the data that I use to create in Photoshop, because it's helping improve the workflow in the future, as opposed to it's not clear what the benefit is in social networks. It's nice for you to say that. I mean, I think there are some professional workflows where people might be very protective of what they're doing, such as if I was preparing evidence for a legal case, I wouldn't want any of that, you know, phoning home to help train the algorithm or anything. There may be other cases where people are, say, having a trial version, or they're doing some, I'm not saying we're doing this today, but there's a future scenario where somebody has a more permissive relationship with Adobe, where they explicitly say, I'm fine, I'm only doing hobby projects, or things which are non confidential. And in exchange for some benefit, tangible or otherwise, I'm willing to share very fine grained data. So another possible scenario is to capture relatively crude, high level things from more people, and then more detailed knowledge from people who are willing to participate. We do that today with explicit customer studies where, you know, we go and visit somebody and ask them to try the tool and we human observe what they're doing. In the future, to be able to do that enough to be able to train an algorithm, we'd need a more systematic process. But we'd have to do it very consciously, because is one of the things people treasure about Adobe is a sense of trust. And we don't want to endanger that through overly aggressive data collection. So we have a chief privacy officer. And it's definitely front and center of thinking about AI rather than an afterthought. Well, when you start that program, sign me up. Okay, happy to. Is there other projects that you wanted to mention that that I didn't perhaps that pop into mind? Well, you covered the number, I think you mentioned Project Puppetron, I think that one is interesting, because it's, you might think of Adobe as only thinking in 2d. And that's a good example where we're actually thinking more three dimensionally about how to assign features to faces so that we can, you know, if you take so what puppet run does, it takes either a still or a video of a person talking, and then it can take a painting of somebody else and then apply the style of the painting to the person who's talking in the video. And it's unlike a sort of screen door post filter effect that you sometimes see online, it really looks as though it's sort of somehow attached or reflecting the motion of the face. And so that's the case where even to do a 2d workflow, like stylization, you really need to infer more about the 3d structure of the world. And I think, as 3d computer vision algorithms get better, initially, they'll focus on particular domains, like faces, where you have a lot of prior knowledge about structure, and you can maybe have a parameterized template that you fit to the image. But over time, this should be possible for more general content. And it might even be invisible to the user that you're doing 3d reconstruction, but under the hood, but it might then let you do edits much more reliably or correctly than you would otherwise. And, you know, the face is a very important application, right? Absolutely. So making things work. And a very sensitive one. If you do something uncanny, it's very disturbing. That's right. You have to get it right. So in the space of augmented reality and virtual reality, what do you think is the role of AR and VR and in the content we consume as people, as consumers, and the content we create as creators? Now, that's a great question. We think about this a lot, too. So I think VR and AR serve slightly different purposes. So VR can really transport you to an entire immersive world, no matter what your personal situation is. To that extent, it's a bit like a really, really widescreen television, where it sort of snaps you out of your context and puts you in a new one. And I think it's still evolving in terms of the hardware. I actually worked on VR in the 90s trying to solve the latency and sort of nausea problem, which we did, but it was very expensive and a bit early. There's a new wave of that now, I think. And increasingly, those devices are becoming all in one rather than something that's tethered to a box. I think the market seems to be bifurcating into things for consumers and things for professional use cases, like for architects and people designing where your product is a building and you really want to experience it better than looking at a scale model or a drawing, I think, or even than a video. So I think for that, where you need a sense of scale and spatial relationships, it's great. I think AR holds the promise of sort of taking digital assets off the screen and putting them in context in the real world on the table in front of you, on the wall behind you. And that has the corresponding need that the assets need to adapt to the physical context in which they're being placed. I mean, it's a bit like having a live theater troupe come to your house and put on Hamlet. My mother had a friend who used to do this at Stately Homes in England for the National Trust. And they would adapt the scenes and even they'd walk the audience through the rooms to see the action based on the country house they found themselves in for two days. And I think AR will have the same issue that, you know, if you have a tiny table and a big living room or something, it'll try to figure out what can you change and what's fixed. And there's a little bit of a tension between fidelity where if you captured, say, Nureyev doing a fantastic ballet, you'd want it to be sort of exactly reproduced. And maybe all you could do is scale it down. Whereas somebody telling you a story might be walking around the room doing some gestures and that could adapt to the room in which they were telling the story. And do you think fidelity is that important in that space or is it more about the storytelling? I think it may depend on the characteristic of the media. If it's a famous celebrity, then it may be that you want to catch every nuance and they don't want to be reanimated by some algorithm. It could be that if it's really, you know, a lovable frog telling you a story and it's about a princess and a frog, then it doesn't matter if the frog moves in a different way. I think a lot of the ideas that have sort of grown up in the game world will now come into the broader commercial sphere once they're needing adaptive characters in AR. Are you thinking of engineering tools that allow creators to create in the augmented world, basically making a Photoshop for the augmented world? Well, we have shown a few demos of sort of taking a Photoshop layer stack and then expanding it into 3D. That's actually been shown publicly as one example in AR. Where we're particularly excited at the moment is in 3D. 3D design is still a very challenging space. And we believe that it's a worthwhile experiment to try to figure out if AR or immersive makes 3D design more spontaneous. Can you give me an example of 3D design, just like applications? Literally, a simple one would be laying out objects, right? So on a conventional screen, you'd sort of have a plan view and a side view and a perspective view, and you'd sort of be dragging it around with a mouse. And if you're not careful, it would go through the wall and all that. Whereas if you were really laying out objects, say, in a VR headset, you could literally move your head to see a different viewpoint. They'd be in stereo. So you'd have a sense of depth because you're already wearing the depth glasses, right? So it would be those sort of big gross motor move things around kind of skills seem much more spontaneous, just like they are in the real world. The frontier for us, I think, is whether that same medium can be used to do fine grained design tasks, like very accurate constraints on, say, a CAD model or something that may be better done on a desktop, but it may just be a matter of inventing the right UI. So we're hopeful that because there will be this potential explosion of demand for 3D assets driven by AR and more real time animation on conventional screens, that those tools will also help with, or those devices will help with designing the content as well. You've mentioned quite a few interesting sort of new ideas. And at the same time, there's old timers like me that are stuck in their old ways and are... Well, I think I'm the old timer. Okay. All right. All right. But the opposed all change at all costs. Yes. When you're thinking about creating new interfaces, do you feel the burden of just this giant user base that loves the current product? So anything new you do, any new idea comes at a cost that you'll be resisted? Well, I think if you have to trade off control for convenience, then our existing user base would definitely be offended by that. I think if there are some things where you have more convenience and just as much control, that may be more welcome. We do think about not breaking well known metaphors for things. So things should sort of make sense. Photoshop has never been a static target. It's always been evolving and growing. And to some extent, there's been a lot of brilliant thought along the way of how it works today. So we don't want to just throw all that out. If there's a fundamental breakthrough, like a single click is good enough to select an object rather than having to do lots of strokes, that actually fits in quite nicely to the existing toolset, either as an optional mode or as a starting point. I think where we're looking at radical simplicity, where you could encapsulate an entire workflow with a much simpler UI, then sometimes that's easier to do in the context of either a different device, like a mobile device, where the affordances are naturally different. Or in a tool that's targeted at a different workflow, where it's about spontaneity and velocity rather than precision. And we have projects like Rush, which can let you do professional quality video editing for a certain class of media output that is targeted very differently in terms of users and the experience. And ideally, people would go, if I'm feeling like doing Premiere, big project, I'm doing a four part television series, that's definitely a Premiere thing. But if I want to do something to show my recent vacation, maybe I'll just use Rush because I can do it in the half an hour I have free at home rather than the four hours I need to do it at work. And for the use cases, which we can do well, it really is much faster to get the same output. But the more professional tools obviously have a much richer toolkit and more flexibility in what they can do. And then at the same time with the flexibility and control, I like this idea of smart defaults, of using AI to coach you to like what Google has, I'm feeling lucky button. Or one button kind of gives you a pretty good set of settings. And then that's almost an educational tool to show. Because sometimes when you have all this control, you're not sure about the correlation between the different bars that control different elements of the image and so on. And sometimes there's a degree of, you don't know what the optimal is. And then some things are sort of on demand, like help, right? Where I'm stuck, I need to know what to look for. I'm not quite sure what it's called. And something that was proactively making helpful suggestions or, you could imagine a make a suggestion button where you'd use all of that knowledge of workflows and everything to maybe suggest something to go and learn about or just to try or show the answer. And maybe it's not one intelligent default, but it's like a variety of defaults. And then you go, I like that one. Yeah. Yeah. Several options. So back to poetry. Ah, yes. We're going to interleave. So first few lines of a recent poem of yours before I ask the next question. This is about the smartphone. Today I left my phone at home and went down to the sea. The sand was soft, the ocean glass, but I was still just me. This is a poem about you leaving your phone behind and feeling quite liberated because of it. So this is kind of a difficult topic and let's see if we can talk about it, figure it out. But so with the help of AI more and more, we can create sort of versions of ourselves, versions of reality that are in some ways more beautiful than actual reality. And some of the creative ways that we can do that, some of the creative effort there is part of creating this illusion. So of course this is inevitable, but how do you think we should adjust as human beings to live in this digital world that's partly artificial, that's better than the world that we lived in a hundred years ago when you didn't have Instagram and Facebook versions of ourselves and the online Oh, this is sort of showing off better versions of ourselves. We're using the tooling of modifying the images or even with artificial intelligence ideas of deep fakes and creating adjusted or fake versions of ourselves and reality. I think it's an interesting question. You're all sort of historical bent on this. So I actually wonder if 18th century aristocrats who commissioned famous painters to paint portraits of them had portraits that were slightly nicer than they actually looked in practice. So human desire to put your best foot forward has always been true. I think it's interesting. You sort of framed it in two ways. One is if we can imagine alternate realities and visualize them, is that a good or bad thing? In the old days, you do it with storytelling and words and poetry, which still resides sometimes on websites, but we've become a very visual culture in particular. In the 19th century, we're very much a text based culture. People would read long tracks, political speeches were very long. Nowadays, everything's very kind of quick and visual and snappy. I think it depends on how harmless your intent. A lot of it's about intent. So if you have a somewhat flattering photo that you pick out of the photos that you have in your inbox to say, this is what I look like, it's probably fine. If someone's going to judge you by how you look, then they'll decide soon enough when they meet you whether the reality, you know. Yeah, right. I think where it can be harmful is if people hold themselves up to an impossible standard, which they then feel bad about themselves for not meeting. I think that definitely can be an issue. But I think the ability to imagine and visualize an alternate reality, which sometimes you then go off and build later, can be a wonderful thing too. People can imagine architectural styles, which they then, you know, have a startup, make a fortune, and then build a house that looks like their favorite video game. Is that a terrible thing? I think I used to worry about exploration, actually, that part of the joy of going to the moon. When I was a tiny child, I remember it in grainy black and white, was to know what it would look like when you got there. And I think now we have such good graphics for visualizing the experience before it happens, that I slightly worry that it may take the edge off actually wanting to go, you know what I mean? Because we've seen it on TV. We kind of, oh, you know, by the time we finally get to Mars, we'll go, yeah, yeah, so it's Mars. That's what it looks like. But then, you know, the outer exploration, I mean, I think Pluto was a fantastic recent discovery where nobody had any idea what it looked like. And it was just breathtakingly varied and beautiful. So I think expanding the ability of the human toolkit to imagine and communicate on balance is a good thing. I think there are abuses, we definitely take them seriously and try to discourage them. I think there's a parallel side where the public needs to know what's possible through events like this, right? So that you don't believe everything you read in print anymore. And it may over time become true of images as well. Or you need multiple sets of evidence to really believe something rather than a single media asset. So I think it's a constantly evolving thing. It's been true forever. There's a famous story about Anne of Cleves and Henry VIII where luckily for Anne, they didn't get married, right? So, or they got married and broke up in it. What's the story? Oh, so Holbein went and painted a picture and then Henry VIII wasn't pleased and, you know, history doesn't record whether Anne was pleased, but I think she was pleased not to be married more than a day or something. So, I mean, this has gone on for a long time, but I think it's just a part of the magnification of human capability. You've kind of built up an amazing research environment here, research culture, research lab, and you've written that the secret to a thriving research lab is interns. Can you unpack that a little bit? Oh, absolutely. So a couple of reasons. As you see looking at my personal history, there are certain ideas you bond with at a certain stage of your career and you tend to keep revisiting them through time. If you're lucky, you pick one that doesn't just get solved in the next five years and then you're sort of out of luck. So I think a constant influx of new people brings new ideas with it. From the point of view of industrial research, because a big part of what we do is really taking those ideas to the point where they can ship as very robust features, you end up investing a lot in a particular idea. And if you're not careful, people can get too conservative in what they choose to do next, knowing that the product teams will want it. And interns let you explore the more fanciful or unproven ideas in a relatively lightweight way, ideally leading to new publications for the intern and for the researcher. And it gives you then a portfolio from which to draw which idea am I going to then try to take all the way through to being robust in the next year or two to ship. So it sort of becomes part of the funnel. It's also a great way for us to identify future full time researchers. Many of our greatest researchers were former interns. It builds a bridge to university departments so we can get to know and build an enduring relationship with the professors whom we often do academic give funds to as well as an acknowledgement of the value the interns add in their own collaborations. So it's sort of a virtuous cycle. And then the long term legacy of a great research lab hopefully will be not only the people who stay, but the ones who move through and then go off and carry that same model to other companies. And so we believe strongly in industrial research and how it can complement academia. And we hope that this model will continue to propagate and be invested in by other companies, which makes it harder for us to recruit, of course, but that's a sign of success. And a rising tide lifts all ships in that sense. And where's the idea born with the interns? Is there brainstorming? Is there discussions about, you know, like what? Where do the ideas come from? Yeah. As I'm asking the question, I realize how dumb it is, but I'm hoping you have a better answer. A question I ask at the beginning of every summer. So what will happen is we'll send out a call for interns. They'll, we'll have a number of resumes come in. People will contact the candidates, talk to them about their interests. They'll usually try to find some, somebody who has a reasonably good match to what they're already doing, or just has a really interesting domain that they've been pursuing in their PhD. And we think we'd love to do one of those projects too. And then the intern stays in touch with the mentor, as we call them. And then they come and at the end of two weeks, they have to decide. So they'll often have a general sense by the time they arrive. And we'll have internal discussions about what are all the general ideas that we're wanting to pursue to see whether two people have the same idea, and maybe they should talk and all that. But then once the intern actually arrives, sometimes the idea goes linearly. And sometimes it takes a giant left turn. And we go, that sounded good. But when we thought about it, there's this other project, or it's already been done. And we found this paper, we were scooped. But we have this other great idea. So it's pretty, pretty flexible at the beginning. One of the questions for research labs is who's deciding what to do? And then who's to blame if it goes wrong? Who gets the credit if it goes right? And so in Adobe, we push the needle very much towards freedom of choice of projects by the researchers and the interns. But then we reward people based on impact. So if the projects ultimately end up impacting the products and having papers and so on. And so your alternative model, just to be clear, is that you have one lab director who thinks he's a genius and tells everybody what to do, takes all the credit if it goes well, blames everybody else if it goes badly. So we don't want that model. And this helps new ideas percolate up. The art of running such a lab is that there are strategic priorities for the company. And there are areas where we do want to invest and pressing problems. And so it's a little bit of a trickle down and filter up meets in the middle. And so you don't tell people you have to do X, but you say X would be particularly appreciated this year. And then people reinterpret X through the filter of things they want to do and they're interested in. And miraculously, it usually comes together very well. One thing that really helps is Adobe has a really broad portfolio of products. So if we have a good idea, there's usually a product team that is intrigued or interested. So it means we don't have to qualify things too much ahead of time. Once in a while, the product teams sponsor extra intern, because they have a particular problem that they really care about, in which case it's a little bit more, we really need one of these. And then we sort of say, great, I get an extra intern, we find an intern who thinks that's a great problem. But that's not the typical model. That's sort of the icing on the cake as far as the budget is concerned. And all of the above end up being important. It's really hard to predict at the beginning of the summer, which we all have high hopes of all of the intern projects, but ultimately, some of them pay off and some of them sort of are a nice paper, but don't turn into a feature. Others turn out not to be as novel as we thought, but they'd be a great feature, but not a paper. And then others, we make a little bit of progress and we realize how much we don't know. And maybe we revisit that problem several years in a row until it, finally we have a breakthrough and then it becomes more on track to impact a product. Jumping back to a big overall view of Adobe research, what are you looking forward to in 2019 and beyond? What is, you mentioned there's a giant suite of products, a giant suite of ideas, new interns, a large team of researchers. What do you think the future holds? In terms of the technological breakthroughs? Technological breakthroughs, especially ones that will make it into product, will get to impact the world. So I think the creative or the analytics assistants that we talked about where they're constantly trying to figure out what you're trying to do and how can they be helpful and make useful suggestions is a really hot topic. And it's very unpredictable as to when it'll be ready, but I'm really looking forward to seeing how much progress we make against that. I think some of the core technologies like generative adversarial networks are immensely promising and seeing how quickly those become practical for mainstream use cases at high resolution with really good quality is also exciting. And they also have this sort of strange way of even the things they do oddly are odd in an interesting way. So it can look like dreaming or something. So that's fascinating. I think internally, we have a Sensei platform, which is a way in which we're pulling our neural nets and other intelligence models into a central platform, which can then be leveraged by multiple product teams at once. So we're in the middle of transitioning from once you have a good idea, you pick a product team to work with and they sort of hand design it for that use case to a more sort of Henry Ford standard up in a standard way, which can be accessed in a standard way, which should mean that the time between a good idea and impacting our products will be greatly shortened. And when one product has a good idea, many of the other products can just leverage it too. So it's sort of an economy of scale. So that's more about the how than the what. But that combination of this sort of renaissance in AI, there's a comparable one in graphics with real time ray tracing and other really exciting emerging technologies. And when these all come together, you'll sort of basically be dancing with light, right, where you'll have real time shadows, reflections and as if it's a real world in front of you. But then with all these magical properties brought by AI, where it sort of anticipates or modifies itself in ways that make sense based on how it understands the creative task you're trying to do. That's a really exciting future for creative for myself to the creator. So first of all, I work in autonomous vehicles. I'm a roboticist. I love robots. And I think you have a fascination with snakes, both natural and artificial robots. I share your fascination. I mean, their movement is beautiful, adaptable. The adaptability is fascinating. There are, I looked it up, 2,900 species of snakes in the world. Wow. 875 venomous. Some are tiny, some are huge. I saw that there's one that's 25 feet in some cases. So what's the most interesting thing that you connect with in terms of snakes, both natural and artificial? What was the connection with robotics AI and this particular form of a robot? Well, it actually came out of my work in the 80s on computer animation, where I started doing things like cloth simulation and other kind of soft body simulation. And you'd sort of drop it and it would bounce and then it would just sort of stop moving. And I thought, well, what if you animate the spring lengths and simulate muscles? And the simplest object I could do that for was an earthworm. So I actually did a paper in 1988 called The Motion Dynamics of Snakes and Worms. And I read the physiology literature on both how snakes and worms move and then did some of the early computer animation examples of that. And so your interest in robotics came out of simulation and graphics. When I moved from Alias to Apple, we actually did a movie called Her Majesty's Secret Serpent, which is about a secret agent snake that parachutes in and captures a film canister from a satellite, which tells you how old fashioned we were thinking back then. Sort of classic 1950s or 60s Bond movie kind of thing. And at the same time, I'd always made radio controlled chips when I was a child and from scratch. And I thought, well, how can it be to build a real one? And so then started what turned out to be like a 15 year obsession with trying to build better snake robots. And the first one that I built just sort of slithered sideways, but didn't actually go forward. Then I added wheels and building things in real life makes you honest about the friction. The thing that appeals to me is I love creating the illusion of life, which is what drove me to animation. And if you have a robot with enough degrees of coordinated freedom that move in a kind of biological way, then it starts to cross the Ancani Valley and to seem like a creature rather than a thing. And I certainly got that with the early snakes by S3, I had it able to sidewind as well as go directly forward. My wife to be suggested that it would be the ring bearer at our wedding. So it actually went down the aisle carrying the rings and got in the local paper for that, which was really fun. And this was all done as a hobby. And then I, at the time that can onboard compute was incredibly limited. It was sort of. Yeah. So you should explain that these things, the whole idea is that you would, you're trying to run it autonomously. Autonomously on board right. And so the very first one, I actually built the controller from discrete logic cause I used to do LSI, you know, circuits and things when I was a teenager. And then the second and third one, the eight bit microprocessors were available with like the whole 256 bytes of RAM, which you could just about squeeze in. So they were radio controlled rather than autonomous and really were more about the physicality and coordinated motion. I've occasionally taken a sidestep into, if only I could make it cheaply enough, bake a great toy, which has been a lesson in how clockwork is its own magical realm that you venture into and learn things about backlash and other things you don't take into account as a computer scientist, which is why what seemed like a good idea doesn't work. So it was quite humbling. And then more recently I've been building S9, which is a much better engineered version of S3 where the motors wore out and it doesn't work anymore. And you can't buy replacements, which is sad given that it was such a meaningful one. S5 was about twice as long and looked much more biologically inspired. Unlike the typical roboticist, I taper my snakes. There are good mechanical reasons to do that, but it also makes them look more biological, although it means every segment's unique rather than a repetition, which is why most engineers don't do it. It actually saves weight and leverage and everything. And that one is currently on display at the International Spy Museum in Washington, DC. Not that it's done any spying. It was on YouTube and it got its own conspiracy theory where people thought that it wasn't real because I work at Adobe, it must be fake graphics. And people would write to me, tell me it's real. You know, they say the background doesn't move and it's like, it's on a tripod, you know? So that one, but you can see the real thing, so it really is true. And then the latest one is the first one where I could put a Raspberry Pi, which leads to all sorts of terrible jokes about Pythons and things. But this one can have on board compute. And then where my hobby work and my work work are converging is you can now add vision accelerator chips, which can evaluate neural nets and do object recognition and everything. So both for the snakes and more recently for the spider that I've been working on, having, you know, desktop level compute is now opening up a whole world of true autonomy with onboard compute, onboard batteries, and still having that sort of biomimetic quality that appeals to children in particular. They are really drawn to them and adults think they look creepy, but children actually think they look charming. And I gave a series of lectures at Girls Who Code to encourage people to take an interest in technology. And at the moment, I'd say they're still more expensive than the value that they add, which is why they're a great hobby for me, but they're not really a great product. It makes me think about doing that very early thing I did at Alias with changing the muscle rest lengths. If I could do that with a real artificial muscle material, then the next snake ideally would use that rather than motors and gearboxes and everything. It would be lighter, much stronger, and more continuous and smooth. So it's, I like to say being in research is a license to be curious. And I have the same feeling with my hobby. It forced me to read biology and be curious about things that otherwise would have just been, you know, a National Geographic special. Suddenly I'm thinking, how does that snake move? Can I copy it? I look at the trails that sidewinding snakes leave in sand and see if my snake robots would do the same thing. So out of something inanimate, I like why you put it, try to bring life into it and beauty. Absolutely. And then ultimately give it a personality, which is where the intelligent agent research will converge with the vision and voice synthesis to give it a sense of having, not necessarily human level intelligence. I think the Turing test is such a high bar. It's a little bit self defeating, but having one that you can have a meaningful conversation with, especially if you have a reasonably good sense of what you can say. So not trying to have it so a stranger could walk up and have one, but so as a pet owner or a robot pet owner, you could know what it thinks about and what it can reason about. Or sometimes just the meaningful interaction. If you have the kind of interaction you have with the dog, sometimes you might have a conversation, but it's usually one way. Absolutely. And nevertheless, it feels like a meaningful and meaningful connection. And one of the things that I'm trying to do in the sample audio that will play you is beginning to get towards the point where the reasoning system can explain why it knows something or why it thinks something. And that again, creates the sense that it really does know what it's talking about, but also for debugging as you get more and more elaborate behavior, it's like, why did you decide to do that? You know, how do you know that? I think the robot's really my muse for helping me think about the future of AI and what to invent next. So even at Adobe, that's mostly operating in digital world. Correct. Do you ever, do you see a future where Adobe even expands into the more physical world perhaps? So bringing life not into animations, but bringing life into physical objects with, whether it's, well, I'd have to say at the moment, it's a twinkle in my eye. I think the more likely thing is that we will bring virtual objects into the physical world through augmented reality and many of the ideas that might take five years to build a robot to do, you can do in a few weeks with digital assets. So I think when really intelligent robots finally become commonplace, they won't be that surprising because we'll have been living with those personalities for in the virtual sphere for a long time. And then they'll just say, Oh, it's, you know, Siri with legs or Alexa, Alexa on hooves or something. So I can see that world coming. And for now, it's still an adventure, still an adventure. And we don't know quite what the experience will be like. And it's really exciting to sort of see all of these different strands of my career converge. Yeah. In interesting ways. And it is definitely a fun adventure. So let me end with my favorite poem, the last few lines of my favorite poem of yours that ponders mortality and in some sense, immortality, you know, as our ideas live through the ideas of others, through the work of others, it ends with do not weep or mourn. It was enough. The little enemies permitted just a single dance, scattered them as deep as your eyes can see. I'm content. They'll have another chance sweeping more centered parts along to join a jostling lifting throng as others danced in me. Beautiful poem. Beautiful way to end it. Gavin, thank you so much for talking today. And thank you for inspiring and empowering millions of people like myself for creating amazing stuff. Oh, thank you. Great conversation.\n",
      "## End of Transcript\n",
      "\n",
      "## Summary\n",
      "The podcast explores various topics related to creativity, technology, and personal growth, highlighting the interconnectedness of various fields and the potential impact of AI on various aspects of human experience. It covers topics such as AI and creativity, poetry and AI, home automation, the intersection of technology and literature, and the potential for AI to enhance learning and improve quality control. The podcast emphasizes the potential benefits of AI in streamlining processes, unlocking new possibilities, and fostering human-machine collaboration.\n",
      "\n",
      "Overall, the podcast explores the potential of AI to revolutionize various fields and aspects of human experience, emphasizing its ability to enhance creativity, streamline workflows, and facilitate learning and problem-solving.\n",
      "## End of Summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completeness evaluation score for episode 23\n",
      "[{'text': 'Not | The summary does not accurately capture the depth and breadth of the original document. The original document covers a wide range of topics, including creativity, technology, robotics, AI, and more, while the summary focuses mainly on the discussion of AI and robotics. Additionally, the summary does not capture the poetic and philosophical elements present in the original document.'}]\n",
      "##############################################\n",
      "##############################################\n",
      "\n",
      "#######  Episode 79\n",
      "## Transcript\n",
      "The following is a conversation with Lee Smolin. He's a theoretical physicist, co inventor of loop quantum gravity, and a contributor of many interesting ideas to cosmology, quantum field theory, the foundations of quantum mechanics, theoretical biology, and the philosophy of science. He's the author of several books, including one that critiques the state of physics and string theory called The Trouble with Physics. And his latest book, Einstein's Unfinished Revolution, The Search for What Lies Beyond the Quantum. He's an outspoken personality in the public debates on the nature of our universe, among the top minds in the theoretical physics community. This community has its respected academics, its naked emperors, its outcasts and its revolutionaries, its madmen and its dreamers. This is why it's an exciting world to explore through a long form conversation. I recommend you listen back to the episodes with Leonard Susskind, Sean Carroll, Michio Okaku, Max Tegmark, Eric Weinstein, and Jim Gates. You might be asking, why talk to physicists if you're interested in AI? To me, creating artificial intelligence systems requires more than Python and deep learning. It requires that we return to exploring the fundamental nature of the universe and the human mind. Theoretical physicists venture out into the dark, mysterious, psychologically challenging place of first principles more than almost any other discipline. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEXBODCAST. Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as one dollar. Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating. I recommend Ascent of Money as a great book on this history. Debits and credits on ledgers started around 30,000 years ago. The US dollar, of course, created over 200 years ago, and Bitcoin, the first decentralized cryptocurrency, was released just over 10 years ago. So given that history, cryptocurrency is still very much in its early days of development, but it still is aiming to and just might redefine the nature of money. If you get Cash App from the App Store or Google Play and use the code LEXBODCAST, you'll get $10, and Cash App will also donate $10 to First, one of my favorite organizations that is helping to advance robotics and STEM education for young people around the world. And now, here's my conversation with Lee Smolin. What is real? Let's start with an easy question. Put another way, how do we know what is real and what is merely a creation of our human perception and imagination? We don't know. We don't know. This is science. I presume we're talking about science. And we believe, or I believe, that there is a world that is independent of my existence and my experience about it and my knowledge of it, and this I call the real world. So you said science, but even bigger than science, what? Sure, sure. I need not have said this is science. I just was warming up. Warming up? Okay, now that we're warmed up, let's take a brief step outside of science. Is it completely a crazy idea to you that everything that exists is merely a creation of our mind? So there's a few, not many. This is outside of science now. People who believe sort of perception is fundamentally what's in our human perception, the visual cortex and so on, the cognitive constructs that's being formed there is the reality. And then anything outside is something that we can never really grasp. Is that a crazy idea to you? There's a version of that that is not crazy at all. What we experience is constructed by our brains and by our brains in an active mode. So we don't see the raw world. We see a very processed world. We feel something that's very processed through our brains and our brains are incredible. But I still believe that behind that experience, that mirror or veil or whatever you wanna call it, there is a real world and I'm curious about it. Can we truly, how do we get a sense of that real world? Is it through the tools of physics, from theory to the experiments? Or can we actually grasp it in some intuitive way that's more connected to our ape ancestors? Or is it still fundamentally the tools of math and physics that really allow us to grasp it? Well, let's talk about what tools they are. What you say are the tools of math and physics. I mean, I think we're in the same position as our ancestors in the caves or before the caves or whatever. We find ourselves in this world and we're curious. We also, it's important to be able to explain what happens when there are fires, when there are not fires, what animals and plants are good to eat and all that stuff. But we're also just curious. We look up in the sky and we see the sun and the moon and the stars and we see some of those move and we're very curious about that. And I think we're just naturally curious. So we make, this is my version of how we work. We make up stories and explanations. And where there are two things which I think are just true of being human, we make judgments fast because we have to. Where to survive, is that a tiger or is that not a tiger? And we go. Act. We have to act fast on incomplete information. So we judge quickly and we're often wrong or at least sometimes wrong, which is all I need for this. We're often wrong. So we fool ourselves and we fool other people readily. And so there's lots of stories that get told and some of them result in a concrete benefit and some of them don't. So you said we're often wrong, but what does it mean to be right? Right, that's an excellent question. To be right, well since I believe that there is a real world, I believe that to be, you can challenge me on this if you're not a realist. A realist is somebody who believes in this real objective world which is independent of our perception. If I'm a realist, I think that to be right is to come closer. I think first of all, there's a relative scale. There's not right and wrong. There's right or more right and less right. And you're more right if you come closer to an exact true description of that real world. Now can we know that for sure? No. And the scientific method is ultimately what allows us to get a sense of how close we're getting to that real world? No on two counts. First of all, I don't believe there's a scientific method. I was very influenced when I was in graduate school by the writings of Paul Fireman who was an important philosopher of science who argued that there isn't a scientific method. There is or there is not? There is not. Can you elaborate, I'm sorry if you were going to, but can you elaborate on what does it mean for there not to be a scientific method, this notion that I think a lot of people believe in in this day and age? Sure. Paul Fireman, he was a student of Popper who taught Karl Popper. And Fireman argued both by logic and by historical example that you name anything that should be part of the practice of science. Say you should always make sure that your theories agree with all the data that's already been taken. And he'll prove to you that there have to be times when science contradicts, when some scientist contradicts that advice for science to progress overall. So it's not a simple matter. I think that, I think of science as a community. Of people. Of people and as a community of people bound by certain ethical precepts, precepts, whatever that is. So in that community, a set of ideas they operate under, meaning ethically of kind of the rules of the game they operate under. Don't lie, report all your results, whether they agree or don't agree with your hypothesis. Check the training of a scientist. Mostly consists of methods of checking because again, we make lots of mistakes. We're very error prone. But there are tools both on the mathematics side and the experimental side to check and double check and triple check. And a scientist goes through a training and I think this is part of it. You can't just walk off the street and say, yo, I'm a scientist. You have to go through the training and the training, the test that lets you be done with the training is can you form a convincing case for something that your colleagues will not be able to shout down because they'll ask, did you check this? And did you check that? And did you check this? And what about seeming contradiction with this? And you've got to have answers to all those things or you don't get taken seriously. And when you get to the point where you can produce that kind of defense and argument, then they give you a PhD. And you're kind of licensed. You're still gonna be questioned and you still may propose or publish mistakes. But the community is gonna have to waste less time fixing your mistakes. Yes, but if you can maybe linger on it a little longer, what's the gap between the thing that that community does and the ideal of the scientific method? The scientific method is you should be able to repeat and experiment. There's a lot of elements to what construes the scientific method, but the final result, the hope of it is that you should be able to say with some confidence that a particular thing is close to the truth. Right, but there's not a simple relationship between experiment and hypothesis or theory. For example, Galileo did this experiment of dropping a ball from the top of a tower and it falls right at the base of the tower. And an Aristotelian would say, wow, of course it falls right to the base of the tower. That shows that the earth isn't moving while the ball is falling. And Galileo says, no way, there's a principle of inertia and it has an inertia in the direction where the earth isn't moving and the tower and the ball and the earth all move together. When the principle of inertia tells you it hits the bottom, it does look, therefore my principle of inertia is right. And Aristotelian says, no, our style of science is right. The earth is stationary. And so you gotta get an interconnected bunch of cases and work hard to line up and explain. It took centuries to make the transition from Aristotelian physics to the new physics. It wasn't done until Newton in 1680 something, 1687. So what do you think is the nature of the process that seems to lead to progress? If we at least look at the long arc of science, of all the community of scientists, they seem to do a better job of coming up with ideas that engineers can then take on and build rockets with or build computers with or build cool stuff with. I don't know, a better job than what? Than this previous century. So century by century, we'll talk about string theory and so on and kind of possible, what you might think of as dead ends and so on. Which is not the way I think of string theory. We'll straighten out, we'll get all the strings straight. But there is, nevertheless in science, very often, at least temporary dead ends. But if you look at the, through centuries, the century before Newton and the century after Newton, it seems like a lot of ideas came closer to the truth that then could be usable by our civilization to build the iPhone, right? To build cool things that improve our quality of life. That's the progress I'm kind of referring to. Let me, can I say that more precisely? Yes, well, it's a low bar. Because I think it's important to get the time places right. There was a scientific revolution that partly succeeded between about 1900 or late 1890s and into the 1930s, 1940s and so. And maybe some, if you stretched it, into the 1970s. And the technology, this was the discovery of relativity and that included a lot of developments of electromagnetism. The confirmation, which wasn't really well confirmed into the 20th century, that matter was made of atoms. And the whole picture of nuclei with electrons going around, this is early 20th century. And then quantum mechanics was from 1905, took a long time to develop, to the late 1920s. And then it was basically in final form. And the basis of this partial revolution, and we can come back to why it's only a partial revolution, is the basis of the technologies that you mentioned. All of, I mean, electrical technology was being developed slowly with this. And in fact, there's a close relation between the development of electricity and the electrification of cities in the United States and Europe and so forth. And the development of the science. The fundamental physics since the early 1970s doesn't have a story like that so far. There's not a series of triumphs and progresses and there's not any practical application. So just to linger briefly on the early 20th century and the revolutions in science that happened there, what was the method by which the scientific community kept each other in check about when you get something right, when you get something wrong? Is experimental validation ultimately the final test? It's absolutely necessary. And the key things were all validated. The key predictions of quantum mechanics and of the theory of electricity and magnetism. So before we talk about Einstein, your new book, before String Theory, Quantum Mechanics, so on, let's take a step back at a higher level question. What is that you mentioned? What is realism? What is anti realism? And maybe why do you find realism, as you mentioned, so compelling? Well, realism is the belief in an external world independent of our existence, our perception, our belief, our knowledge. A realist as a physicist is somebody who believes that there should be possible some completely objective description of each and every process at the fundamental level, which describes and explains exactly what happens and why it happens. That kind of implies that that system, in a realist view, is deterministic, meaning there's no fuzzy magic going on that you can never get to the bottom, or you can get to the bottom of anything and perfectly describe it. Some people would say that I'm not that interested in determinism, but I could live with the fundamental world, which had some chance in it. So do you, you said you could live with it, but do you think God plays dice in our universe? I think it's probably much worse than that. In which direction? I think that theories can change, and theories can change without warning. I think the future is open. You mean the fundamental laws of physics can change? Yeah. Oh, okay, we'll get there. I thought we would be able to find some solid ground, but apparently the entirety of it, temporarily so, probably. Okay, so realism is the idea that while the ground is solid, you can describe it. What's the role of the human being, our beautiful, complex human mind in realism? Do we have a, are we just another set of molecules connected together in a clever way, or the observer, does the observer, our human mind, consciousness, have a role in this realism view of the physical universe? There's two ways, there's two questions you could be asking. One, does our conscious mind, do our perceptions play a role in making things become, in making things real or things becoming? That's question one. Question two is, does this, we can call it a naturalist view of the world that is based on realism, allow a place to understand the existence of and the nature of perceptions and consciousness in mind, and that's question two. Question two, I do think a lot about, and my answer, which is not an answer, is I hope so, but it certainly doesn't yet. So what kind? Question one, I don't think so. But of course, the answer to question one depends on question two. Right. So I'm not up to question one yet. So question two is the thing that you can kind of struggle with at this time. Yes. That's, what about the anti realists? So what flavor, what are the different camps of anti realists that you've talked about? I think it would be nice if you can articulate for the people for whom there is not a very concrete real world, or there's divisions, or it's messier than the realist view of the universe, what are the different camps, what are the different views? I'm not sure I'm a good scholar and can talk about the different camps and analyze it, but some, many of the inventors of quantum physics were not realists, were anti realists. Their scholars, they lived in a very perilous time between the two world wars. And there were a lot of trends in culture which were going that way. But in any case, they said things like, the purpose of science is not to give an objective realist description of nature as it would be in our absence. This might be saying Niels Bohr. The purpose of science is as an extension of our conversations with each other to describe our interactions with nature. And we're free to invent and use terms like particle, or wave, or causality, or time, or space. If they're useful to us, and they carry some intuitive implication, but we shouldn't believe that they actually have to do with what nature would be like in our absence, which we have nothing to say about. Do you find any aspect of that, because you kind of said that we human beings tell stories, do you find aspects of that kind of anti realist view of Niels Bohr compelling? That we fundamentally are storytellers, and then we create tools of space, and time, and causality, and whatever this fun quantum mechanic stuff is to help us tell the story of our world. Sure, I just would like to believe that there's an aspiration for the other thing. The other thing being what? The realist point of view. Do you hope that the stories will eventually lead us to discovering the real world as it is? Yeah. Is perfection possible, by the way? Is it? No. Well that's, you mean will we ever get there and know that we're there? Yeah, exactly. That's not my, that's for people 5,000 years in the future. We're certainly nowhere near there yet. Do you think reality that exists outside of our mind, do you think there's a limit to our cognitive abilities? Is, again, descendants of apes, who are just biological systems, is there a limit to our mind's capability to actually understand reality? Sort of, there comes a point, even with the help of the tools of physics, that we just cannot grasp some fundamental aspects of that reality. Again, I think that's a question for 5,000 years in the future. We're not even close to that limit. I think there is a universality. Here, I don't agree with David Deutsch about everything, but I admire the way he put things in his last book. And he talked about the role of explanation. And he talked about the universality of certain languages or the universality of mathematics or of computing and so forth. And he believed that universality, which is something real, which somehow comes out of the fact that a symbolic system or a mathematical system can refer to itself and can, I forget what that's called, can reference back to itself and build, in which he argued for a universality of possibility for our understanding, whatever is out there. But I admire that argument, but it seems to me we're doing okay so far, but we'll have to see. Whether there is a limit or not. For now, we've got plenty to play with. Yeah. There are things which are right there in front of us which we miss. And I'll quote my friend, Eric Weinstein, in saying, look, Einstein carried his luggage. Freud carried his luggage. Marx carried his luggage. Martha Graham carried her luggage, et cetera. Edison carried his luggage. All these geniuses carried their luggage. And not once before relatively recently did it occur to anybody to put a wheel on luggage and pull it. And it was right there waiting to be invented for centuries. So this is Eric Weinstein. Yeah. What do the wheels represent? Are you basically saying that there's stuff right in front of our eyes? That once we, it just clicks, we put the wheels on the luggage, a lot of things will fall into place. Yes, I do, I do. And every day I wake up and think, why can't I be that guy who was walking through the airport? What do you think it takes to be that guy? Because like you said, a lot of really smart people carried their luggage. What, just psychologically speaking, so Eric Weinstein is a good example of a person who thinks outside the box. Yes. Who resists almost conventional thinking. You're an example of a person who by habit, by psychology, by upbringing, I don't know, but resists conventional thinking as well, just by nature. Thank you, that's a compliment. That's a compliment? Good. So what do you think it takes to do that? Is that something you were just born with? I doubt it. Well, from my studying some cases, because I'm curious about that, obviously, and just in a more concrete way, when I started out in physics, because I started a long way from physics, so it took me a long, not a long time, but a lot of work to get to study it and get into it, so I did wonder about that. And so I read the biographies, and in fact, I started with the autobiography of Einstein and Newton and Galileo and all those people. And I think there's a couple of things. Some of it is luck, being in the right place at the right time. Some of it is stubbornness and arrogance, which can easily go wrong. Yes. And I know all of these are doorways. If you go through them slightly at the wrong speed or in the wrong angle, they're ways to fail. But if you somehow have the right luck, the right confidence or arrogance, caring, I think Einstein cared to understand nature with ferocity and a commitment that exceeded other people of his time. So he asked more stubborn questions. He asked deeper questions. I think, and there's a level of ability and whether ability is born in or can be developed to the extent to which it can be developed, like any of these things like musical talent. So you mentioned ego. What's the role of ego in that process? Confidence. Confidence. But in your own life, have you found yourself walking that nice edge of too much or too little, so being overconfident and therefore leaning yourself astray or not sufficiently confident to throw away the conventional thinking of whatever the theory of the day, of theoretical physics? I don't know if I, I mean, I've contributed where I've contributed, whether if I had had more confidence in something, I would have gotten further. I don't know. Certainly, I'm sitting here at this moment with very much my own approach to nearly everything. And I'm calm, I'm happy about that. But on the other hand, I know people whose self confidence vastly exceeds mine. And sometimes I think it's justified and sometimes I think it's not justified. Your most recent book titled Einstein's Unfinished Revolution. So I have to ask, what is Einstein's unfinished revolution and also how do we finish it? Well, that's something I've been trying to do my whole life, but Einstein's unfinished revolution is the twin revolutions which invented relativity theory, special and especially general relativity, and quantum theory, which he was the first person to realize in 1905 that there would have to be a radically different theory which somehow realized or resolved the paradox of the duality of particle and wave for photons. And he was, I mean, people I think don't always associate Einstein with quantum mechanics because I think his connection with it, founding as one of the founders, I would say, of quantum mechanics, he kind of put it in the closet. Is it? Well, he didn't believe that the quantum mechanics as it was developed in the mid to late 1920s was completely correct. At first, he didn't believe it at all. Then he was convinced that it's consistent, but incomplete, and that also is my view. It needs, for various reasons, I can elucidate, to have additional degrees of freedom, particles, forces, something to reach the stage where it gives a complete description of each phenomenon, as I was saying, realism demands. So what aspect of quantum mechanics bothers you and Einstein the most? Is it some aspect of the wave function collapse discussions, the measurement problem? Is it the? The measurement problem. I'm not gonna speak for Einstein. But the measurement problem, basically, and the fact that. What is the measurement problem, sorry? The basic formulation of quantum mechanics gives you two ways to evolve situations in time. One of them is explicitly when no observer is observing and no measurement is taking place. And the other is when a measurement or an observation is taking place. And they basically contradict each other. But there's another reason why the revolution was incomplete, which is we don't understand the relationship between these two parts. General relativity, which became our best theory of space and time and gravitation and cosmology, and quantum theory. So for the most part, general relativity describes big things. Quantum theory describes little things. And that's the revolution that we found really powerful tools to describe big things and little things. And it's unfinished because we have two totally separate things and we need to figure out how to connect them so we can describe everything. Right, and we either do that if we believe quantum mechanics as understood now is correct by bringing general relativity or some extension of general relativity that describes gravity and so forth into the quantum domain that's called quantize, the theory of gravity. Or if you believe with Einstein that quantum mechanics needs to be completed, and this is my view, then part of the job of finding the right completion or extension of quantum mechanics would be one that incorporated space, time, and gravity. So, where do we begin? So first, let me ask, perhaps you can give me a chance, if I could ask you some just really basic questions. Well, they're not at all. The basic questions are the hardest, but you mentioned space, time. What is space, time? Space, time, you talked about a construction. So I believe the space, time is an intellectual construction that we make of the events in the universe. I believe the events are real, and the relationships between the events, which cause which are real. But the idea that there's a four dimensional smooth geometry which has a metric and a connection and satisfies the equations that Einstein wrote, it's a good description to some scale. It's a good approximation, it captures some of what's really going on in nature. But I don't believe it for a minute is fundamental. So, okay, we're gonna allow me to linger on that. So the universe has events, events cause other events. This is the idea of causality. Okay, so that's real. That's in my. In your view is real. Or hypothesis, or the theories that I have been working to develop make that assumption. So space, time, you said four dimensional space is kind of the location of things, and time is whatever the heck time is. And you're saying that space, time is, both space and time are emergent and not fundamental? No. Sorry, before you correct me, what does it mean to be fundamental or emergent? Fundamental means it's part of the description as far down as you go. We have this notion. As real. Yes. As real as real it could be. Yeah, so I think that time is fundamental, and quote goes all the way down, and space does not, and the combination of them we use in general relativity that we call space time also does not. But what is time then? I think that time, the activity of time is a continual creation of events from existing events. So if there's no events, there's no time. Then there's not only no time, there's no nothing. So I believe the universe has a history which goes to the past. I believe the future does not exist. There's a notion of the present and a notion of the past, and the past consists of, is a story about events that took place to our past. So you said the future doesn't exist. Yes. Could you say that again? Can you try to give me a chance to understand that one more time? So events cause other events. What is this universe? Cause we'll talk about locality and nonlocality. Good. Cause it's a crazy, I mean it's not crazy, it's a beautiful set of ideas that you propose. But, and if Kozali is fundamental, I'd just like to understand it better. What is the past? What is the future? What is the flow of time? Even the error of time in our universe, in your view. And maybe what's an event, right? Oh, an event is where something changes, or where two, it's hard to say because it's a primitive concept. An event is a moment of time within space. This is the view in general relativity, where two particles intersect in their paths, or something changes in the path of a particle. Now, we are postulating that there is, at the fundamental level, a notion, which is an elementary notion, so it doesn't have a definition in terms of other things, but it is something elementary happening. And it doesn't have a connection to energy, or matter, or exchange of energy? It does have a connection to energy and matter. So it's at that level. Yeah, it involves, and that's why the version of a theory of events that I've developed with Marina Cortez, and it's, by the way, I wanna mention my collaborators, because they've been at least as important in this work as I have. It's Marina Cortez in all the work since about 2013, 2012, 2013, about causality, causal sets. And in the period before that, Roberta Mangibera Unger, who is a philosopher and a professor of law. And that's in your efforts, together with your collaborators, to finish the unfinished revolution. Yes. And focus on causality as a fundamental. Yes. As fundamental to physics. So. And there's certainly other people we've worked with, but those two people's thinking had a huge influence on my own thinking. So in the way you describe causality, that's what you mean of time being fundamental. That causality is fundamental. Yes. And what does it mean for space to not be fundamental, to be emergent? That's very good. There's a level of description in which there are events, there are events create other events, but there's no space. They don't live in space. They have an order in which they caused each other. And that is part of the nature of time for us. But there is an emergent approximate description. And you asked me to define emergent. I didn't. An emergent property is a property that arises at some level of complexity, larger than and more complex than the fundamental level, which requires some property to describe it, which is not directly explicable or derivable is the word I want from the properties of the fundamental things. And space is one of those things in a sufficiently complex universe, space, three dimensional position of things emerged. Yes, and we have this, we saw how this happens in detail in some models, both computationally and analytically. Okay, so connected to space is the idea of locality. Yes. So we've talked about realism. So I live in this world that like sports. Locality is a thing that you can affect things close to you and don't have an effect on things that are far away. It's the thing that bothers me about gravity in general or action at a distance. Same thing that probably bothered Newton, or at least he said a little bit about it. Okay, so what do you think about locality? Is it just a construct? Is it us humans just like this idea and are connected to it because we exist in it, we need it for our survival, but it's not fundamental? I mean, it seems crazy for it not to be a fundamental aspect of our reality. It does. Can you comfort me on a sort of as a therapist, like how do I? I'm not a good therapist, but I'll do my best. Okay. There are several different definitions of locality when you come to talk about locality in physics. In quantum field theory, which is a mixture of special relativity and quantum mechanics, there is a precise definition of locality. Field operators corresponding to events in space time, which are space like separated, commute with each other as operators. So in quantum mechanics, you think about the nature of reality as fields and things that are close in a field have an impact on each other more than farther away. That's, yes. That's very comforting. That makes sense. So that's a property of quantum field theory and it's well tested. Unfortunately, there's another definition of local, which was expressed by Einstein and expressed more precisely by John Bell, which has been tested experimentally and found to fail. And this set up is you take two particles. So one thing that's really weird about quantum mechanics is a property called entanglement. You can have two particles interact and then share a property without it being a property of either one of the two particles. And if you take such a system and then you make a measurement on particle A, which is over here on my right side, and particle B, which is over here. Somebody else makes a measurement of particle B. You can ask that whatever is the real reality of particle B, it not be affected by the choice the observer at particle A makes about what to measure, not the outcome, just the choice of the different things they might measure. And that's a notion of locality because it assumes that these things are very far spaced like separated. And it's gonna take a while for any information about the choice made by the people here at A to affect the reality at B. But you make that assumption, that's called Bell locality. And you derive a certain inequality that some correlations, functions of correlations have to satisfy. And then you can test that pretty directly in experiments which create pairs of photons or other particles. And it's wrong by many sigma. In experiment, it doesn't match. So what does that mean? That means that that definition of locality I stated is false. The one that Einstein was playing with. Yeah, and the one that I stated, that is it's not true that whatever is real about particle B is unaffected by the choice that the observer makes as to what to measure in particle A. No matter how long they've been propagating at almost the speed of light or the speed of light away from each other, it's no matter. So like the distance between them. Well, it's been tested, of course, if you want to have hope for quantum mechanics being incomplete or wrong and corrected by something that changes this. It's been tested over a number of kilometers. I don't remember whether it's 25 kilometers or a hundred and something kilometers, but. So in trying to solve the unsolved revolution, in trying to come up with the theory for everything, is causality fundamental and breaking away from locality? Absolutely. A crucial step. So in your book, essentially, those are the two things we really need to think about as a community. Especially the physics community has to think about this. I guess my question is, how do we solve? How do we finish the unfinished revolution? Well, that's, I can only tell you what I'm trying to do and what I've abandoned as not working. As one ant, smart ant in an ant colony. Yep. Or maybe dumb, that's why, who knows? But anyway, my view of the, we've had some nice theories invented. There's a bunch of different ones. Both relate to quantum mechanics, relate to quantum gravity. There's a lot to admire in many of these different approaches. But to my understanding, they, none of them completely solve the problems that I care about. And so we're in a situation which is either terrifying for a student or full of opportunity for the right student, in which we've got more than a dozen attempts. And I never thought, I don't think anybody anticipated it would work out this way. Which work partly and then at some point, they have an issue that nobody can figure out how to go around or how to solve. And that's the situation we're in. My reaction to that is twofold. One of them is to try to bring people, we evolved into this unfortunate sociological situation in which there are communities around some of these approaches. And to borrow again, a metaphor from Eric, they sit on top of hills in the landscape of theories and throw rocks at each other. And as Eric says, we need two things. We need people to get off their hills and come down into the valleys and party and talk and become friendly and learn to say, not no but, but yes and yes. Your idea goes this far, but maybe if we put it together with my idea, we can go further. Yes. So in that spirit, I've talked several times with Sean Carroll, who's also written an excellent book recently. And he kind of, he plays around, is a big fan of the many worlds interpretation of quantum mechanics. So I'm a troublemaker. So let me ask, what's your sense of Sean and the idea of many worlds interpretation? I've read many the commentary back and forth. You guys are friendly, respect each other, but have a lot of fun debating. I love Sean and he, no, I really, he's articulate and he's a great representative or ambassador of science to the public and for different fields of science to each other. He also, like I do, takes philosophy seriously. And unlike what I do in all cases, he has really done the homework. He's read a lot, he knows the people, he talks to them, he exposes his arguments to them. And I, there's this mysterious thing that we so often end up on the opposite sides of one of these issues. It's fun though. It's fun and I'd love to have a conversation about that, but I would want to include him. I see, about many worlds, well. No, I can tell you what I think about many worlds. I'd love to, but actually on that, let me pause. Sean has a podcast. You should definitely figure out how to talk to Sean. I would, I actually told Sean, I would love to hear you guys just going back and forth. So I hope you can make that happen eventually, you and Sean. I won't tell you what it is, but there's something that Sean said to me in June of 2016 that changed my whole approach to a problem. But I'll have to tell him first. Yes, and that, that'll be great to tell him on his podcast. So. I can't invite myself to his podcast. But I told him, yeah, okay, we'll make it happen. So many worlds. Anyway. What's your view? Many worlds, we talk about nonlocality. Many worlds is also a very uncomfortable idea or beautiful depending on your perspective. It's very nice in terms of, I mean, there's a realist aspect to it. I think you called it magical realism. Yeah. It's just a beautiful line. But at the same time, it's very difficult to far limited human minds to comprehend. So what are your thoughts about it? Let me start with the easy and obvious and then go to the scientific. Okay. It doesn't appeal to me. It doesn't answer the questions that I want answered. And it does so to such a strong case that when Roberto Mangueber Anger and I began looking for principles, and I want to come back and talk about the use of principles in science, because that's the other thing I was going to say, and I don't want to lose that. When we started looking for principles, we made our first principle, there is just one world and it happens once. But so it's not helpful to my personal approach, to my personal agenda, but of course I'm part of a community. And my sense of the many worlds interpretation, I have thought a lot about it and struggled a lot with it, is the following. First of all, there's Everett himself, there's what's in Everett. And there are several issues there connected with the derivation of the Born Rule, which is the rule that gives probabilities to events. And the reasons why there is a problem with probability is that I mentioned the two ways that physical systems can evolve. The many worlds interpretation cuts off, one, the one having to do with measurement, and just has the other one, the Schrodinger evolution, which is this smooth evolution of the quantum state. But the notion of probability is only in the second rule, which we've thrown away. So where does probability come from? And you have to answer the question because experimentalists use probabilities to check the theory. Now, at first sight, you get very confused because there seems to be a real problem because in the many worlds interpretation, this talk about branches is not quite precise, but I'll use it. There's a branch in which everything that might happen does happen with probability one in that branch. You might think you could count the number of branches in which things do and don't happen and get numbers that you can define as something like frequentist probabilities. And Everett did have an argument in that direction, but the argument gets very subtle when there are an infinite number of possibilities, as is the case in most quantum systems. And my understanding, although I'm not as much of an expert as some other people, is that Everett's own proposal failed, did not work. There are then, but it doesn't stop there. There is an important idea that Everett didn't know about, which is decoherence, and it is a phenomenon that might be very much relevant. And so a number of people post Everett have tried to make versions of what you might call many worlds quantum mechanics. And this is a big area and it's subtle, and it's not the kind of thing that I do well. So I consulted, that's why there's two chapters on this in the book I wrote. Chapter 10, which is about Everett's version, chapter 11, there's a very good group of philosophers of physics in Oxford, Simon Saunders, David Wallace, Harvey Brown, and a number of others. And of course there's David Deutsch, who is there. And those people have developed and put a lot of work into a very sophisticated set of ideas designed to come back and answer that question. They have the flavor of there are really no probabilities, we admit that, but imagine if the Everett story was true and you were living in that multiverse, how would you make bets? And so they use decision theory from the theory of probability and gambling and so forth to shape a story of how you would bet if you were inside an Everett in the universe and you knew that. And there's a debate among those experts as to whether they or somebody else has really succeeded. And when I checked in as I was finishing the book with some of those people, like Simon, who's a good friend of mine, and David Wallace, they told me that they weren't sure that any of them was yet correct. So that's what I put in my book. Now, to add to that, Sean has his own approach to that problem in what's called self referencing or self locating observers. And it doesn't, I tried to read it and it didn't make sense to me, but I didn't study it hard, I didn't communicate with Sean, I didn't do the things that I would do, so I had nothing to say about it in the book. I don't know whether it's right or not. Let's talk a little bit about science. You mentioned the use of principles in science. What does it mean to have a principle and why is that important? When I feel very frustrated about quantum gravity, I like to go back and read history. And of course, Einstein, his achievements are a huge lesson and hopefully something like a role model. And it's very clear that Einstein thought that the first job when you wanna enter a new domain of theoretical physics is to discover and invent principles and then make models of how those principles might be applied in some experimental situation, which is where the mathematics comes in. So for Einstein, there was no unified space and time. Minkowski invented this idea of space time. For Einstein, it was a model of his principles or his postulates. And I've taken the view that we don't know the principles of quantum gravity. I can think about candidates and I have some papers where I discuss different candidates and I'm happy to discuss them. But my belief now is that those partially successful approaches are all models, which might describe indeed some quantum gravity physics in some domain, in some aspect, but ultimately would be important because they model the principles and the first job is to tie down those principles. So that's the approach that I'm taking. So speaking of principles, in your 2006 book, The Trouble with Physics, you criticized a bit string theory for taking us away from the rigors of the scientific method or whatever you would call it. But what's the trouble with physics today and how do we fix it? Can I say how I read that book? Sure. Because I, and I'm not, this of course has to be my fault because you can't as an author claim after all the work you put in that you are misread. But I will say that many of the reviewers who are not personally involved and even many who were working on string theory or some other approach to quantum gravity told me, communicated with me and told me they thought that I was fair and balance was the word that was usually used. So let me tell you what my purpose was in writing that book, which clearly got diverted by, because there was already a rather hot argument going on. And this is. On which topic? On string theory specifically? Or in general in physics? No, more specifically than string theory. So since we're in Cambridge, can I say that? We're doing this in Cambridge. Yeah, yeah, of course. Cambridge, just to be clear, Massachusetts. And on Harvard campus. Right, so Andy Straminger is a good friend of mine and has been for many, many years. And Andy, so originally there was this beautiful idea that there were five string theories and maybe they would be unified into one. And we would discover a way to break that symmetries of one of those string theories and discover the standard model and predict all the properties of standard model particles, like their masses and charges and so forth, coupling constants. And then there was a bunch of solutions to string theory found, which led each of them to a different version of particle physics with a different phenomenology. These are called the Calabi Yao manifolds, named after Yao, who is also here. Not, certainly we've been friends at some time in the past anyway. And then there were, nobody was sure, but hundreds of thousands of different versions of string theory. And then Andy found there was a way to put a certain kind of mathematical curvature called torsion into the solutions. And he wrote a paper, String Theory with Torsion, in which he discovered there was, and not formally uncountable, but he was unable to invent any way to count the number of solutions or classify the diverse solutions. And he wrote that this is worrying because doing phenomenology the old fashioned way by solving the theory is not gonna work because there's gonna be loads of solutions for every proposed phenomenology for anything the experiments discovered. And it hasn't quite worked out that way. But nonetheless, he took that worry to me. We spoke at least once, maybe two or three times about that. And I got seriously worried about that. And this is just a little. So it's like an anecdote that inspired your worry about string theory in general? Well, I tried to solve the problem and I tried to solve the problem. I was reading at that time, a lot of biology, a lot of evolutionary theory, like Linmar Gullis and Steve Gould and so forth. And I could take your time to go through the things, but it occurred to me, maybe physics was like evolutionary biology and maybe the laws evolved. And there was, the biologists talk about a landscape, a fitness landscape of DNA sequences or protein sequences or species or something like that. And I took their concept and the word landscape from theoretical biology and made a scenario about how the universe as a whole could evolve to discover the parameters of the standard model. And I'm happy to discuss, that's called cosmological natural selection. Cosmological natural selection. Yeah. Wow, so the parameters of the standard model, so the laws of physics are changing. This idea would say that the laws of physics are changing in some way that echoes that of natural selection, or just it adjusts in some way towards some goal. Yes. And I published that, I wrote the paper in 1888 or 89, the paper was published in 92. My first book in 1997, The Life of the Cosmos was explicitly about that. And I was very clear that what was important is that because you would develop an ensemble of universes, but they were related by descent to natural selection, almost every universe would share the property that it was, its fitness was maximized to some extent, or at least close to maximum. And I could deduce predictions that could be tested from that. And I worked all of that out and I compared it to the anthropic principle where you weren't able to make tests or make falsifications. All of this was in the late 80s and early 90s. That's a really compelling notion, but how does that help you arrive? I'm coming to where the book came from. Yes. So what got me, I worked on string theory. I also worked on loop quantum gravity. And I was one of the inventors of loop quantum gravity. And because of my strong belief in some other principles, which led to this notion of wanting a quantum theory of gravity to be what we call relational or background independent, I tried very hard to make string theory background independent. And it ended up developing a bunch of tools which then could apply directly to general relativity and that became loop quantum gravity. So the things were very closely related and have always been very closely related in my mind. The idea that there were two communities, one devoted to strings and one devoted to loops is nuts and has always been nuts. Okay, so anyway, there's this nuts community of loops and strings that are all beautiful and compelling and mathematically speaking. And what's the trouble with all that? Why is that such a problem? So I was interested in developing that notion of how science works based on a community and ethics that I told you about. And I wrote a draft of a book about that, which had several chapters on methodology of science. And it was a rather academically oriented book. And those chapters were the first part of the book, the first third of it. And you didn't find their remnants in what's now the last part of the trouble with physics. And then I described a number of test cases, case studies. And one of them, which I knew was the search for quantum gravity and string theory and so forth. And I wasn't able to get that book published. So somebody made the suggestion of flipping it around and starting with a story of string theory, which was already controversial. This was 2004, 2005. But I was very careful to be detailed, to criticize papers and not people. You won't find me criticizing individuals. You'll find me criticizing certain writing. But in any case, here's what I regret. Let me make your program worthwhile. Yes. As far as I know, with the exception of not understanding how large the applications to condensed matter, say ADS CFT would get, I think largely my diagnosis of string theory as it was then has stood up since 2006. What I regret is that the same critique, I was using string theory as an example, and the same critique applies to many other communities in science and all of, including, and this is where I regret my own community, that is a community of people working on quantum gravity. Not science string theory. But, and I considered saying that explicitly. But to say that explicitly, since it's a small, intimate community, I would be telling stories and naming names and making a kind of history that I have no right to write. So I stayed away from that, but was misunderstood. But if I may ask, is there a hopeful message for theoretical physics that we can take from that book, sort of that looks at the community, not just your own work on, now with causality and nonlocality, but just broadly in understanding the fundamental nature of our reality, what's your hope for the 21st century in physics? Well, that we solve the problem. That we solve the unfinished problem of Einstein's. That's certainly the thing that I care about most in. Hope for most. Let me say one thing. Among the young people that I work with, I hear very often and sense a total disinterest in these arguments that we older scientists have. And an interest in what each other is doing. And this is starting to appear in conferences where the young people interested in quantum gravity make a conference, they invite loops and strings and causal dynamical triangulations and causal set people. And we're having a conference like this next week, a small workshop at perimeter. And I guess I'm advertising this. And then in the summer, we're having a big full on conference, which is just quantum gravity. It's not strings, it's not loops. But the organizers and the speakers will be from all the different communities. And this to me is very helpful. That the different ideas are coming together. At least people are expressing an interest in that. It's a huge honor talking to you, Lee. Thanks so much for your time today. Thank you. Thanks for listening to this conversation. And thank you to our presenting sponsor, Cash App. Download it, use code LexPodcast. You'll get $10 and $10 will go to FIRST, an organization that inspires and educates young minds to become science and technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter at Lex Friedman. And now let me leave you with some words from Lee Smolin. One possibility is God is nothing but the power of the universe to organize itself. Thanks for listening and hope to see you next time.\n",
      "## End of Transcript\n",
      "\n",
      "## Summary\n",
      "The podcast explores various topics related to science, philosophy, and technology. It covers concepts such as first principles, the constructed nature of reality, scientific methodology, ethics, quantum theory, and non-locality. The podcast emphasizes the importance of rigorous scientific methods, ethical principles, and open-mindedness. It also highlights the limitations of human understanding and the need for humility and openness to new perspectives. Overall, the podcast provides a wide-ranging exploration of key ideas and concepts, encompassing diverse fields of study and inviting further exploration.\n",
      "## End of Summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completeness evaluation score for episode 79\n",
      "[{'text': 'Not | The summary does not accurately capture the depth and breadth of the original document. The original document covers a wide range of topics, including discussions on quantum gravity, string theory, causality, nonlocality, and the nature of reality. The summary fails to convey the complexity and nuance of these discussions, and instead provides a very high-level overview of the podcast content.'}]\n",
      "##############################################\n",
      "##############################################\n",
      "\n",
      "#######  Episode 94\n",
      "## Transcript\n",
      "The following is a conversation with Ilya Sotskever, cofounder and chief scientist of OpenAI, one of the most cited computer scientists in history with over 165,000 citations, and to me, one of the most brilliant and insightful minds ever in the field of deep learning. There are very few people in this world who I would rather talk to and brainstorm with about deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor and a pleasure. This conversation was recorded before the outbreak of the pandemic. For everyone feeling the medical, psychological, and financial burden of this crisis, I'm sending love your way. Stay strong, we're in this together, we'll beat this thing. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, review it with five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter at lexfriedman, spelled F R I D M A N. As usual, I'll do a few minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEXPODCAST. Cash App lets you send money to friends, buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating. I recommend Ascent of Money as a great book on this history. Both the book and audio book are great. Debits and credits on ledgers started around 30,000 years ago. The US dollar created over 200 years ago, and Bitcoin, the first decentralized cryptocurrency, released just over 10 years ago. So given that history, cryptocurrency is still very much in its early days of development, but it's still aiming to and just might redefine the nature of money. So again, if you get Cash App from the App Store or Google Play and use the code LEXPODCAST, you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance robotics and STEM education for young people around the world. And now here's my conversation with Ilya Satsgever. You were one of the three authors with Alex Kaszewski, Geoff Hinton of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that launched the deep learning revolution. At that time, take us back to that time, what was your intuition about neural networks, about the representational power of neural networks? And maybe you could mention how did that evolve over the next few years up to today, over the 10 years? Yeah, I can answer that question. At some point in about 2010 or 2011, I connected two facts in my mind. Basically, the realization was this, at some point we realized that we can train very large, I shouldn't say very, tiny by today's standards, but large and deep neural networks end to end with backpropagation. At some point, different people obtained this result. I obtained this result. The first moment in which I realized that deep neural networks are powerful was when James Martens invented the Hessian free optimizer in 2010. And he trained a 10 layer neural network end to end without pre training from scratch. And when that happened, I thought this is it. Because if you can train a big neural network, a big neural network can represent very complicated function. Because if you have a neural network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds. Neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times. So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any object. So I thought, so I already had the idea then that we need to train a very big neural network on lots of supervised data. And then it must succeed because we can find the best neural network. And then there's also theory that if you have more data than parameters, you won't overfit. Today, we know that actually this theory is very incomplete and you won't overfit even if you have less data than parameters, but definitely, if you have more data than parameters, you won't overfit. So the fact that neural networks were heavily overparameterized wasn't discouraging to you? So you were thinking about the theory that the number of parameters, the fact that there's a huge number of parameters is okay? Is it gonna be okay? I mean, there was some evidence before that it was okayish, but the theory was most, the theory was that if you had a big data set and a big neural net, it was going to work. The overparameterization just didn't really figure much as a problem. I thought, well, with images, you're just gonna add some data augmentation and it's gonna be okay. So where was any doubt coming from? The main doubt was, can we train a bigger, will we have enough computer train a big enough neural net? With backpropagation. Backpropagation I thought would work. The thing which wasn't clear was whether there would be enough compute to get a very convincing result. And then at some point, Alex Kerchevsky wrote these insanely fast CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get image in it and it's gonna be the greatest thing. Was your intuition, most of your intuition from empirical results by you and by others? So like just actually demonstrating that a piece of program can train a 10 layer neural network? Or was there some pen and paper or marker and whiteboard thinking intuition? Like, cause you just connected a 10 layer large neural network to the brain. So you just mentioned the brain. So in your intuition about neural networks does the human brain come into play as a intuition builder? Definitely. I mean, you gotta be precise with these analogies between artificial neural networks and the brain. But there is no question that the brain is a huge source of intuition and inspiration for deep learning researchers since all the way from Rosenblatt in the 60s. Like if you look at the whole idea of a neural network is directly inspired by the brain. You had people like McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently learned about the computer and automata. Can we use some ideas from the computer and automata to design some kind of computational object that's going to be simple, computational and kind of like the brain and they invented the neuron. So they were inspired by it back then. Then you had the convolutional neural network from Fukushima and then later Yann LeCun who said, hey, if you limit the receptive fields of a neural network, it's going to be especially suitable for images as it turned out to be true. So there was a very small number of examples where analogies to the brain were successful. And I thought, well, probably an artificial neuron is not that different from the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So now we're not at a time where deep learning is very successful. So let us squint less and say, let's open our eyes and say, what do you use an interesting difference between the human brain? Now, I know you're probably not an expert neither in your scientists and your biologists, but loosely speaking, what's the difference between the human brain and artificial neural networks? That's interesting to you for the next decade or two. That's a good question to ask. What is an interesting difference between the neurons between the brain and our artificial neural networks? So I feel like today, artificial neural networks, so we all agree that there are certain dimensions in which the human brain vastly outperforms our models. But I also think that there are some ways in which our artificial neural networks have a number of very important advantages over the brain. Looking at the advantages versus disadvantages is a good way to figure out what is the important difference. So the brain uses spikes, which may or may not be important. Yeah, it's a really interesting question. Do you think it's important or not? That's one big architectural difference between artificial neural networks. It's hard to tell, but my prior is not very high and I can say why. There are people who are interested in spiking neural networks. And basically what they figured out is that they need to simulate the non spiking neural networks in spikes. And that's how they're gonna make them work. If you don't simulate the non spiking neural networks in spikes, it's not going to work because the question is why should it work? And that connects to questions around back propagation and questions around deep learning. You've got this giant neural network. Why should it work at all? Why should the learning rule work at all? It's not a self evident question, especially if you, let's say if you were just starting in the field and you read the very early papers, you can say, hey, people are saying, let's build neural networks. That's a great idea because the brain is a neural network. So it would be useful to build neural networks. Now let's figure out how to train them. It should be possible to train them probably, but how? And so the big idea is the cost function. That's the big idea. The cost function is a way of measuring the performance of the system according to some measure. By the way, that is a big, actually let me think, is that one, a difficult idea to arrive at and how big of an idea is that? That there's a single cost function. Sorry, let me take a pause. Is supervised learning a difficult concept to come to? I don't know. All concepts are very easy in retrospect. Yeah, that's what it seems trivial now, but I, because the reason I asked that, and we'll talk about it, is there other things? Is there things that don't necessarily have a cost function, maybe have many cost functions or maybe have dynamic cost functions or maybe a totally different kind of architectures? Because we have to think like that in order to arrive at something new, right? So the only, so the good examples of things which don't have clear cost functions are GANs. Right. And a GAN, you have a game. So instead of thinking of a cost function, where you wanna optimize, where you know that you have an algorithm gradient descent, which will optimize the cost function, and then you can reason about the behavior of your system in terms of what it optimizes. With a GAN, you say, I have a game and I'll reason about the behavior of the system in terms of the equilibrium of the game. But it's all about coming up with these mathematical objects that help us reason about the behavior of our system. Right, that's really interesting. Yeah, so GAN is the only one, it's kind of a, the cost function is emergent from the comparison. It's, I don't know if it has a cost function. I don't know if it's meaningful to talk about the cost function of a GAN. It's kind of like the cost function of biological evolution or the cost function of the economy. It's, you can talk about regions to which it will go towards, but I don't think, I don't think the cost function analogy is the most useful. So if evolution doesn't, that's really interesting. So if evolution doesn't really have a cost function, like a cost function based on its, something akin to our mathematical conception of a cost function, then do you think cost functions in deep learning are holding us back? Yeah, so you just kind of mentioned that cost function is a nice first profound idea. Do you think that's a good idea? Do you think it's an idea we'll go past? So self play starts to touch on that a little bit in reinforcement learning systems. That's right. Self play and also ideas around exploration where you're trying to take action that surprise a predictor. I'm a big fan of cost functions. I think cost functions are great and they serve us really well. And I think that whenever we can do things with cost functions, we should. And you know, maybe there is a chance that we will come up with some, yet another profound way of looking at things that will involve cost functions in a less central way. But I don't know, I think cost functions are, I mean, I would not bet against cost functions. Is there other things about the brain that pop into your mind that might be different and interesting for us to consider in designing artificial neural networks? So we talked about spiking a little bit. I mean, one thing which may potentially be useful, I think people, neuroscientists have figured out something about the learning rule of the brain or I'm talking about spike time independent plasticity and it would be nice if some people would just study that in simulation. Wait, sorry, spike time independent plasticity? Yeah, that's right. What's that? STD. It's a particular learning rule that uses spike timing to figure out how to determine how to update the synapses. So it's kind of like if a synapse fires into the neuron before the neuron fires, then it strengthens the synapse, and if the synapse fires into the neurons shortly after the neuron fired, then it weakens the synapse. Something along this line. I'm 90% sure it's right, so if I said something wrong here, don't get too angry. But you sounded brilliant while saying it. But the timing, that's one thing that's missing. The temporal dynamics is not captured. I think that's like a fundamental property of the brain is the timing of the timing of the timing of the signals. Well, you have recurrent neural networks. But you think of that as this, I mean, that's a very crude, simplified, what's that called? There's a clock, I guess, to recurrent neural networks. It's, this seems like the brain is the general, the continuous version of that, the generalization where all possible timings are possible, and then within those timings is contained some information. You think recurrent neural networks, the recurrence in recurrent neural networks can capture the same kind of phenomena as the timing that seems to be important for the brain, in the firing of neurons in the brain? I mean, I think recurrent neural networks are amazing, and they can do, I think they can do anything we'd want them to, we'd want a system to do. Right now, recurrent neural networks have been superseded by transformers, but maybe one day they'll make a comeback, maybe they'll be back, we'll see. Let me, on a small tangent, say, do you think they'll be back? So, so much of the breakthroughs recently that we'll talk about on natural language processing and language modeling has been with transformers that don't emphasize recurrence. Do you think recurrence will make a comeback? Well, some kind of recurrence, I think very likely. Recurrent neural networks, as they're typically thought of for processing sequences, I think it's also possible. What is, to you, a recurrent neural network? In generally speaking, I guess, what is a recurrent neural network? You have a neural network which maintains a high dimensional hidden state, and then when an observation arrives, it updates its high dimensional hidden state through its connections in some way. So do you think, that's what expert systems did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state, which is its knowledge base, and is growing it by sequential processing. Do you think of it more generally in that way, or is it simply, is it the more constrained form of a hidden state with certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden state is technically what you described there, the hidden state that goes inside the LSTM or the RNN or something like this. But then what should be contained, if you want to make the expert system analogy, I'm not, I mean, you could say that the knowledge is stored in the connections, and then the short term processing is done in the hidden state. Yes, could you say that? So sort of, do you think there's a future of building large scale knowledge bases within the neural networks? Definitely. So we're gonna pause on that confidence, because I want to explore that. Well, let me zoom back out and ask, back to the history of ImageNet. Neural networks have been around for many decades, as you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment and beyond, the success in the past 10 years? Okay, so the question is, to make sure I didn't miss anything, the key ideas that led to the success of deep learning over the past 10 years. Exactly, even though the fundamental thing behind deep learning has been around for much longer. So the key idea about deep learning, or rather the key fact about deep learning before deep learning started to be successful, is that it was underestimated. People who worked in machine learning simply didn't think that neural networks could do much. People didn't believe that large neural networks could be trained. People thought that, well, there was lots of, there was a lot of debate going on in machine learning about what are the right methods and so on. And people were arguing because there were no, there was no way to get hard facts. And by that, I mean, there were no benchmarks which were truly hard that if you do really well on them, then you can say, look, here's my system. That's when you switch from, that's when this field becomes a little bit more of an engineering field. So in terms of deep learning, to answer the question directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot of compute. Once you have a lot of supervised data and a lot of compute, then there is a third thing which is needed as well. And that is conviction. Conviction that if you take the right stuff, which already exists, and apply and mix it with a lot of data and a lot of compute, that it will in fact work. And so that was the missing piece. It was, you had the, you needed the data, you needed the compute, which showed up in terms of GPUs, and you needed the conviction to realize that you need to mix them together. So that's really interesting. So I guess the presence of compute and the presence of supervised data allowed the empirical evidence to do the convincing of the majority of the computer science community. So I guess there's a key moment with Jitendra Malik and Alex Alyosha Efros who were very skeptical, right? And then there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment. And I think ImageNet had served as that moment. That's right. And they represented this kind of, were the big pillars of computer vision community, kind of the wizards got together, and then all of a sudden there was a shift. And it's not enough for the ideas to all be there and the compute to be there, it's for it to convince the cynicism that existed. It's interesting that people just didn't believe for a couple of decades. Yeah, well, but it's more than that. It's kind of, when put this way, it sounds like, well, those silly people who didn't believe, what were they missing? But in reality, things were confusing because neural networks really did not work on anything. And they were not the best method on pretty much anything as well. And it was pretty rational to say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard tasks which produce undeniable evidence. And that's how we make progress. And that's why the field is making progress today because we have these hard benchmarks which represent true progress. And so, and this is why we are able to avoid endless debate. So incredibly you've contributed some of the biggest recent ideas in AI in computer vision, language, natural language processing, reinforcement learning, sort of everything in between, maybe not GANs. But there may not be a topic you haven't touched. And of course, the fundamental science of deep learning. What is the difference to you between vision, language, and as in reinforcement learning, action, as learning problems? And what are the commonalities? Do you see them as all interconnected? Are they fundamentally different domains that require different approaches? Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of unity. In fact. What do you mean by unity? Like overlap of ideas? Overlap of ideas, overlap of principles. In fact, there's only one or two or three principles which are very, very simple. And then they apply in almost the same way, in almost the same way to the different modalities, to the different problems. And that's why today, when someone writes a paper on improving optimization of deep learning and vision, it improves the different NLP applications and it improves the different reinforcement learning applications. Reinforcement learning. So I would say that computer vision and NLP are very similar to each other. Today they differ in that they have slightly different architectures. We use transformers in NLP and we use convolutional neural networks in vision. But it's also possible that one day this will change and everything will be unified with a single architecture. Because if you go back a few years ago in natural language processing, there were a huge number of architectures for every different tiny problem had its own architecture. Today, there's just one transformer for all those different tasks. And if you go back in time even more, you had even more and more fragmentation and every little problem in AI had its own little subspecialization and sub, you know, little set of collection of skills, people who would know how to engineer the features. Now it's all been subsumed by deep learning. We have this unification. And so I expect vision to become unified with natural language as well. Or rather, I shouldn't say expect, I think it's possible. I don't wanna be too sure because I think on the convolutional neural net is very computationally efficient. RL is different. RL does require slightly different techniques because you really do need to take action. You really need to do something about exploration. Your variance is much higher. But I think there is a lot of unity even there. And I would expect, for example, that at some point there will be some broader unification between RL and supervised learning where somehow the RL will be making decisions to make the supervised learning go better. And it will be, I imagine, one big black box and you just throw, you know, you shovel things into it and it just figures out what to do with whatever you shovel at it. I mean, reinforcement learning has some aspects of language and vision combined almost. There's elements of a long term memory that you should be utilizing and there's elements of a really rich sensory space. So it seems like the union of the two or something like that. I'd say something slightly differently. I'd say that reinforcement learning is neither, but it naturally interfaces and integrates with the two of them. Do you think action is fundamentally different? So yeah, what is interesting about, what is unique about policy of learning to act? Well, so one example, for instance, is that when you learn to act, you are fundamentally in a non stationary world because as your actions change, the things you see start changing. You experience the world in a different way. And this is not the case for the more traditional static problem where you have some distribution and you just apply a model to that distribution. You think it's a fundamentally different problem or is it just a more difficult generalization of the problem of understanding? I mean, it's a question of definitions almost. There is a huge amount of commonality for sure. You take gradients, you try, you take gradients. We try to approximate gradients in both cases. In the case of reinforcement learning, you have some tools to reduce the variance of the gradients. You do that. There's lots of commonality. Use the same neural net in both cases. You compute the gradient, you apply Adam in both cases. So, I mean, there's lots in common for sure, but there are some small differences which are not completely insignificant. It's really just a matter of your point of view, what frame of reference, how much do you wanna zoom in or out as you look at these problems? Which problem do you think is harder? So people like Noam Chomsky believe that language is fundamental to everything. So it underlies everything. Do you think language understanding is harder than visual scene understanding or vice versa? I think that asking if a problem is hard is slightly wrong. I think the question is a little bit wrong and I wanna explain why. So what does it mean for a problem to be hard? Okay, the non interesting dumb answer to that is there's a benchmark and there's a human level performance on that benchmark and how is the effort required to reach the human level benchmark. So from the perspective of how much until we get to human level on a very good benchmark. Yeah, I understand what you mean by that. So what I was going to say that a lot of it depends on, once you solve a problem, it stops being hard and that's always true. And so whether something is hard or not depends on what our tools can do today. So you say today through human level, language understanding and visual perception are hard in the sense that there is no way of solving the problem completely in the next three months. So I agree with that statement. Beyond that, my guess would be as good as yours, I don't know. Oh, okay, so you don't have a fundamental intuition about how hard language understanding is. I think, I know I changed my mind. I'd say language is probably going to be harder. I mean, it depends on how you define it. Like if you mean absolute top notch, 100% language understanding, I'll go with language. But then if I show you a piece of paper with letters on it, is that, you see what I mean? You have a vision system, you say it's the best human level vision system. I show you, I open a book and I show you letters. Will it understand how these letters form into word and sentences and meaning? Is this part of the vision problem? Where does vision end and language begin? Yeah, so Chomsky would say it starts at language. So vision is just a little example of the kind of a structure and fundamental hierarchy of ideas that's already represented in our brains somehow that's represented through language. But where does vision stop and language begin? That's a really interesting question. So one possibility is that it's impossible to achieve really deep understanding in either images or language without basically using the same kind of system. So you're going to get the other for free. I think it's pretty likely that yes, if we can get one, our machine learning is probably that good that we can get the other. But I'm not 100% sure. And also, I think a lot of it really does depend on your definitions. Definitions of? Of like perfect vision. Because reading is vision, but should it count? Yeah, to me, so my definition is if a system looked at an image and then a system looked at a piece of text and then told me something about that and I was really impressed. That's relative. You'll be impressed for half an hour and then you're gonna say, well, I mean, all the systems do that, but here's the thing they don't do. Yeah, but I don't have that with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of monogamy. So I like the idea of marrying somebody, being with them for several decades. So I believe in the fact that yes, it's possible to have somebody continuously giving you pleasurable, interesting, witty new ideas, friends. Yeah, I think so. They continue to surprise you. The surprise, it's that injection of randomness. It seems to be a nice source of, yeah, continued inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test, but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah, I feel like I misunderstood what you meant by impressing you. I thought you meant to impress you with its intelligence, with how well it understands an image. I thought you meant something like, I'm gonna show it a really complicated image and it's gonna get it right. And you're gonna say, wow, that's really cool. Our systems of January 2020 have not been doing that. Yeah, no, I think it all boils down to like the reason people click like on stuff on the internet, which is like, it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that as well. So forgive the romanticized question, but looking back to you, what is the most beautiful or surprising idea in deep learning or AI in general you've come across? So I think the most beautiful thing about deep learning is that it actually works. And I mean it, because you got these ideas, you got the little neural network, you got the back propagation algorithm. And then you've got some theories as to, this is kind of like the brain. So maybe if you make it large, if you make the neural network large and you train it on a lot of data, then it will do the same function that the brain does. And it turns out to be true, that's crazy. And now we just train these neural networks and you make them larger and they keep getting better. And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks works. Have you built up an intuition of why? Are there a lot of bits and pieces of intuitions, of insights of why this whole thing works? I mean, some, definitely. While we know that optimization, we now have good, we've had lots of empirical, huge amounts of empirical reasons to believe that optimization should work on most problems we care about. Do you have insights of why? So you just said empirical evidence. Is most of your sort of empirical evidence kind of convinces you? It's like evolution is empirical. It shows you that, look, this evolutionary process seems to be a good way to design organisms that survive in their environment, but it doesn't really get you to the insights of how the whole thing works. I think a good analogy is physics. You know how you say, hey, let's do some physics calculation and come up with some new physics theory and make some prediction. But then you got around the experiment. You know, you got around the experiment, it's important. So it's a bit the same here, except that maybe sometimes the experiment came before the theory. But it still is the case. You know, you have some data and you come up with some prediction. You say, yeah, let's make a big neural network. Let's train it. And it's going to work much better than anything before it. And it will in fact continue to get better as you make it larger. And it turns out to be true. That's amazing when a theory is validated like this. It's not a mathematical theory. It's more of a biological theory almost. So I think there are not terrible analogies between deep learning and biology. I would say it's like the geometric mean of biology and physics. That's deep learning. The geometric mean of biology and physics. I think I'm going to need a few hours to wrap my head around that. Because just to find the geometric, just to find the set of what biology represents. Well, in biology, things are really complicated. Theories are really, really, it's really hard to have good predictive theory. And in physics, the theories are too good. In physics, people make these super precise theories which make these amazing predictions. And in machine learning, we're kind of in between. Kind of in between, but it'd be nice if machine learning somehow helped us discover the unification of the two as opposed to sort of the in between. But you're right. That's, you're kind of trying to juggle both. So do you think there are still beautiful and mysterious properties in neural networks that are yet to be discovered? Definitely. I think that we are still massively underestimating deep learning. What do you think it will look like? Like what, if I knew, I would have done it, you know? So, but if you look at all the progress from the past 10 years, I would say most of it, I would say there've been a few cases where some were things that felt like really new ideas showed up, but by and large it was every year we thought, okay, deep learning goes this far. Nope, it actually goes further. And then the next year, okay, now this is peak deep learning. We are really done. Nope, it goes further. It just keeps going further each year. So that means that we keep underestimating, we keep not understanding it. It has surprising properties all the time. Do you think it's getting harder and harder? To make progress? Need to make progress? It depends on what you mean. I think the field will continue to make very robust progress for quite a while. I think for individual researchers, especially people who are doing research, it can be harder because there is a very large number of researchers right now. I think that if you have a lot of compute, then you can make a lot of very interesting discoveries, but then you have to deal with the challenge of managing a huge compute cluster to run your experiments. It's a little bit harder. So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest people I know, so I'm gonna keep asking. So let's imagine all the breakthroughs that happen in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by one person with one computer? Sort of in the space of breakthroughs, do you think compute will be, compute and large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely. I think it's pretty unlikely. I think that there are many, the stack of deep learning is starting to be quite deep. If you look at it, you've got all the way from the ideas, the systems to build the data sets, the distributed programming, the building the actual cluster, the GPU programming, putting it all together. So now the stack is getting really deep and I think it becomes, it can be quite hard for a single person to become, to be world class in every single layer of the stack. What about the, what like Vlad and Ravapnik really insist on is taking MNIST and trying to learn from very few examples. So being able to learn more efficiently. Do you think that's, there'll be breakthroughs in that space that would, may not need the huge compute? I think there will be a large number of breakthroughs in general that will not need a huge amount of compute. So maybe I should clarify that. I think that some breakthroughs will require a lot of compute and I think building systems which actually do things will require a huge amount of compute. That one is pretty obvious. If you want to do X and X requires a huge neural net, you gotta get a huge neural net. But I think there will be lots of, I think there is lots of room for very important work being done by small groups and individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one of the recent papers that you released, the Deep Double Descent, where bigger models and more data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah, definitely. So what happened is that some, over the years, some small number of researchers noticed that it is kind of weird that when you make the neural network larger, it works better and it seems to go in contradiction with statistical ideas. And then some people made an analysis showing that actually you got this double descent bump. And what we've done was to show that double descent occurs for pretty much all practical deep learning systems. And that it'll be also, so can you step back? What's the X axis and the Y axis of a double descent plot? Okay, great. So you can look, you can do things like, you can take your neural network and you can start increasing its size slowly while keeping your data set fixed. So if you increase the size of the neural network slowly, and if you don't do early stopping, that's a pretty important detail, then when the neural network is really small, you make it larger, you get a very rapid increase in performance. Then you continue to make it larger. And at some point performance will get worse. And it gets the worst exactly at the point at which it achieves zero training error, precisely zero training loss. And then as you make it larger, it starts to get better again. And it's kind of counterintuitive because you'd expect deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs in the case of linear classifiers. And the intuition basically boils down to the following. When you have a large data set and a small model, then small, tiny random, so basically what is overfitting? Overfitting is when your model is somehow very sensitive to the small random unimportant stuff in your data set. In the training data. In the training data set, precisely. So if you have a small model and you have a big data set, and there may be some random thing, some training cases are randomly in the data set and others may not be there, but the small model is kind of insensitive to this randomness because it's the same, there is pretty much no uncertainty about the model when the data set is large. So, okay. So at the very basic level to me, it is the most surprising thing that neural networks don't overfit every time very quickly before ever being able to learn anything. The huge number of parameters. So here is, so there is one way, okay. So maybe, so let me try to give the explanation and maybe that will be, that will work. So you've got a huge neural network. Let's suppose you've got, you have a huge neural network, you have a huge number of parameters. And now let's pretend everything is linear, which is not, let's just pretend. Then there is this big subspace where your neural network achieves zero error. And SGD is going to find approximately the point. That's right. Approximately the point with the smallest norm in that subspace. Okay. And that can also be proven to be insensitive to the small randomness in the data when the dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of the model, then there is a one to one correspondence between all the data sets and the models. So small changes in the data set actually lead to large changes in the model. And that's why performance gets worse. So this is the best explanation more or less. So then it would be good for the model to have more parameters, so to be bigger than the data. That's right. But only if you don't early stop. If you introduce early stop in your regularization, you can make the double descent bump almost completely disappear. What is early stop? Early stopping is when you train your model and you monitor your validation performance. And then if at some point validation performance starts to get worse, you say, okay, let's stop training. We are good enough. So the magic happens after that moment. So you don't want to do the early stopping. Well, if you don't do the early stopping, you get the very pronounced double descent. Do you have any intuition why this happens? Double descent? Oh, sorry, early stopping? No, the double descent. So the... Well, yeah, so I try... Let's see. The intuition is basically is this, that when the data set has as many degrees of freedom as the model, then there is a one to one correspondence between them. And so small changes to the data set lead to noticeable changes in the model. So your model is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that when you have a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. Oh, so it's able to, let's nicely put, discard the small changes, the randomness. The randomness, exactly. The spurious correlation which you don't want. Jeff Hinton suggested we need to throw back propagation. We already kind of talked about this a little bit, but he suggested that we need to throw away back propagation and start over. I mean, of course some of that is a little bit wit and humor, but what do you think? What could be an alternative method of training neural networks? Well, the thing that he said precisely is that to the extent that you can't find back propagation in the brain, it's worth seeing if we can learn something from how the brain learns. But back propagation is very useful and we should keep using it. Oh, you're saying that once we discover the mechanism of learning in the brain, or any aspects of that mechanism, we should also try to implement that in neural networks? If it turns out that we can't find back propagation in the brain. If we can't find back propagation in the brain. Well, so I guess your answer to that is back propagation is pretty damn useful. So why are we complaining? I mean, I personally am a big fan of back propagation. I think it's a great algorithm because it solves an extremely fundamental problem, which is finding a neural circuit subject to some constraints. And I don't see that problem going away. So that's why I really, I think it's pretty unlikely that we'll have anything which is going to be dramatically different. It could happen, but I wouldn't bet on it right now. So let me ask a sort of big picture question. Do you think neural networks can be made to reason? Why not? Well, if you look, for example, at AlphaGo or AlphaZero, the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning, better than 99.9% of all humans. Just the neural network, without the search, just the neural network itself. Doesn't that give us an existence proof that neural networks can reason? To push back and disagree a little bit, we all agree that Go is reasoning. I think I agree, I don't think it's a trivial, so obviously reasoning like intelligence is a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some of the same elements of reasoning. Reasoning is almost like akin to search, right? There's a sequential element of reasoning of stepwise consideration of possibilities and sort of building on top of those possibilities in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of like that. And when you have a single neural network doing that without search, it's kind of like that. So there's an existence proof in a particular constrained environment that a process akin to what many people call reasoning exists, but more general kind of reasoning. So off the board. There is one other existence proof. Oh boy, which one? Us humans? Yes. Okay, all right, so do you think the architecture that will allow neural networks to reason will look similar to the neural network architectures we have today? I think it will. I think, well, I don't wanna make two overly definitive statements. I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs of the future will be very similar to the architectures that exist today. Maybe a little bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful. Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks? So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning? So it's not a fundamentally different process. Again, this is stuff nobody knows the answer to. So when it comes to our neural networks, the thing which I would say is that neural networks are capable of reasoning. But if you train a neural network on a task which doesn't require reasoning, it's not going to reason. This is a well known effect where the neural network will solve the problem that you pose in front of it in the easiest way possible. Right, that takes us to one of the brilliant sort of ways you've described neural networks, which is you've referred to neural networks as the search for small circuits and maybe general intelligence as the search for small programs, which I found as a metaphor very compelling. Can you elaborate on that difference? Yeah, so the thing which I said precisely was that if you can find the shortest program that outputs the data at your disposal, then you will be able to use it to make the best prediction possible. And that's a theoretical statement which can be proved mathematically. Now, you can also prove mathematically that finding the shortest program which generates some data is not a computable operation. No finite amount of compute can do this. So then with neural networks, neural networks are the next best thing that actually works in practice. We are not able to find the best, the shortest program which generates our data, but we are able to find a small, but now that statement should be amended, even a large circuit which fits our data in some way. Well, I think what you meant by the small circuit is the smallest needed circuit. Well, the thing which I would change now, back then I really haven't fully internalized the overparameterized results. The things we know about overparameterized neural nets, now I would phrase it as a large circuit whose weights contain a small amount of information, which I think is what's going on. If you imagine the training process of a neural network as you slowly transmit entropy from the dataset to the parameters, then somehow the amount of information in the weights ends up being not very large, which would explain why they generalize so well. So the large circuit might be one that's helpful for the generalization. Yeah, something like this. But do you see it important to be able to try to learn something like programs? I mean, if we can, definitely. I think it's kind of, the answer is kind of yes, if we can do it, we should do things that we can do it. It's the reason we are pushing on deep learning, the fundamental reason, the root cause is that we are able to train them. So in other words, training comes first. We've got our pillar, which is the training pillar. And now we're trying to contort our neural networks around the training pillar. We gotta stay trainable. This is an invariant we cannot violate. And so being trainable means starting from scratch, knowing nothing, you can actually pretty quickly converge towards knowing a lot. Or even slowly. But it means that given the resources at your disposal, you can train the neural net and get it to achieve useful performance. Yeah, that's a pillar we can't move away from. That's right. Because if you say, hey, let's find the shortest program, well, we can't do that. So it doesn't matter how useful that would be. We can't do it. So we won't. So do you think, you kind of mentioned that the neural networks are good at finding small circuits or large circuits. Do you think then the matter of finding small programs is just the data? No. So the, sorry, not the size or the type of data. Sort of ask, giving it programs. Well, I think the thing is that right now, finding, there are no good precedents of people successfully finding programs really well. And so the way you'd find programs is you'd train a deep neural network to do it basically. Right. Which is the right way to go about it. But there's not good illustrations of that. It hasn't been done yet. But in principle, it should be possible. Can you elaborate a little bit, what's your answer in principle? Put another way, you don't see why it's not possible. Well, it's kind of like more, it's more a statement of, I think that it's, I think that it's unwise to bet against deep learning. And if it's a cognitive function that humans seem to be able to do, then it doesn't take too long for some deep neural net to pop up that can do it too. Yeah, I'm there with you. I've stopped betting against neural networks at this point because they continue to surprise us. What about long term memory? Can neural networks have long term memory? Something like knowledge bases. So being able to aggregate important information over long periods of time that would then serve as useful sort of representations of state that you can make decisions by, so have a long term context based on which you're making the decision. So in some sense, the parameters already do that. The parameters are an aggregation of the neural, of the entirety of the neural nets experience, and so they count as long term knowledge. And people have trained various neural nets to act as knowledge bases and, you know, investigated with, people have investigated language models as knowledge bases. So there is work there. Yeah, but in some sense, do you think in every sense, do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been mechanisms that do remember really long term information. What do you mean by that precisely? Precisely, I like the word precisely. So I'm thinking of the kind of compression of information the knowledge bases represent. Sort of creating a, now I apologize for my sort of human centric thinking about what knowledge is, because neural networks aren't interpretable necessarily with the kind of knowledge they have discovered. But a good example for me is knowledge bases, being able to build up over time something like the knowledge that Wikipedia represents. It's a really compressed, structured knowledge base. Obviously not the actual Wikipedia or the language, but like a semantic web, the dream that semantic web represented, so it's a really nice compressed knowledge base or something akin to that in the noninterpretable sense as neural networks would have. Well, the neural networks would be noninterpretable if you look at their weights, but their outputs should be very interpretable. Okay, so yeah, how do you make very smart neural networks like language models interpretable? Well, you ask them to generate some text and the text will generally be interpretable. Do you find that the epitome of interpretability, like can you do better? Like can you add, because you can't, okay, I'd like to know what does it know and what doesn't it know? I would like the neural network to come up with examples where it's completely dumb and examples where it's completely brilliant. And the only way I know how to do that now is to generate a lot of examples and use my human judgment. But it would be nice if a neural network had some self awareness about it. Yeah, 100%, I'm a big believer in self awareness and I think that, I think neural net self awareness will allow for things like the capabilities, like the ones you described, like for them to know what they know and what they don't know and for them to know where to invest to increase their skills most optimally. And to your question of interpretability, there are actually two answers to that question. One answer is, you know, we have the neural net so we can analyze the neurons and we can try to understand what the different neurons and different layers mean. And you can actually do that and OpenAI has done some work on that. But there is a different answer, which is that, I would say that's the human centric answer where you say, you know, you look at a human being, you can't read, how do you know what a human being is thinking? You ask them, you say, hey, what do you think about this? What do you think about that? And you get some answers. The answers you get are sticky in the sense you already have a mental model. You already have a mental model of that human being. You already have an understanding of like a big conception of that human being, how they think, what they know, how they see the world and then everything you ask, you're adding onto that. And that stickiness seems to be, that's one of the really interesting qualities of the human being is that information is sticky. You don't, you seem to remember the useful stuff, aggregate it well and forget most of the information that's not useful, that process. But that's also pretty similar to the process that neural networks do. It's just that neural networks are much crappier at this time. It doesn't seem to be fundamentally that different. But just to stick on reasoning for a little longer, you said, why not? Why can't I reason? What's a good impressive feat, benchmark to you of reasoning that you'll be impressed by if neural networks were able to do? Is that something you already have in mind? Well, I think writing really good code, I think proving really hard theorems, solving open ended problems with out of the box solutions. And sort of theorem type, mathematical problems. Yeah, I think those ones are a very natural example as well. If you can prove an unproven theorem, then it's hard to argue you don't reason. And so by the way, and this comes back to the point about the hard results, if you have machine learning, deep learning as a field is very fortunate because we have the ability to sometimes produce these unambiguous results. And when they happen, the debate changes, the conversation changes. It's a converse, we have the ability to produce conversation changing results. Conversation, and then of course, just like you said, people kind of take that for granted and say that wasn't actually a hard problem. Well, I mean, at some point we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of a sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the fascinating things in your entire body of work, but also the work at OpenAI recently, one of the conversation changes has been in the world of language models. Can you briefly kind of try to describe the recent history of using neural networks in the domain of language and text? Well, there's been lots of history. I think the Elman network was a small, tiny recurrent neural network applied to language back in the 80s. So the history is really, you know, fairly long at least. And the thing that started, the thing that changed the trajectory of neural networks and language is the thing that changed the trajectory of all deep learning and that's data and compute. So suddenly you move from small language models, which learn a little bit, and with language models in particular, there's a very clear explanation for why they need to be large to be good, because they're trying to predict the next word. So when you don't know anything, you'll notice very, very broad strokes, surface level patterns, like sometimes there are characters and there is a space between those characters. You'll notice this pattern. And you'll notice that sometimes there is a comma and then the next character is a capital letter. You'll notice that pattern. Eventually you may start to notice that there are certain words occur often. You may notice that spellings are a thing. You may notice syntax. And when you get really good at all these, you start to notice the semantics. You start to notice the facts. But for that to happen, the language model needs to be larger. So that's, let's linger on that, because that's where you and Noam Chomsky disagree. So you think we're actually taking incremental steps, a sort of larger network, larger compute will be able to get to the semantics, to be able to understand language without what Noam likes to sort of think of as a fundamental understandings of the structure of language, like imposing your theory of language onto the learning mechanism. So you're saying the learning, you can learn from raw data, the mechanism that underlies language. Well, I think it's pretty likely, but I also want to say that I don't really know precisely what Chomsky means when he talks about him. You said something about imposing your structural language. I'm not 100% sure what he means, but empirically it seems that when you inspect those larger language models, they exhibit signs of understanding the semantics whereas the smaller language models do not. We've seen that a few years ago when we did work on the sentiment neuron. We trained a small, you know, smallish LSTM to predict the next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from 500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment of the article, sorry, of the review. Now, why is that? Sentiment is a pretty semantic attribute. It's not a syntactic attribute. And for people who might not know, I don't know if that's a standard term, but sentiment is whether it's a positive or a negative review. That's right. Is the person happy with something or is the person unhappy with something? And so here we had very clear evidence that a small neural net does not capture sentiment while a large neural net does. And why is that? Well, our theory is that at some point you run out of syntax to models, you start to gotta focus on something else. And with size, you quickly run out of syntax to model and then you really start to focus on the semantics would be the idea. That's right. And so I don't wanna imply that our models have complete semantic understanding because that's not true, but they definitely are showing signs of semantic understanding, partial semantic understanding, but the smaller models do not show those signs. Can you take a step back and say, what is GPT2, which is one of the big language models that was the conversation changer in the past couple of years? Yeah, so GPT2 is a transformer with one and a half billion parameters that was trained on about 40 billion tokens of text which were obtained from web pages that were linked to from Reddit articles with more than three outputs. And what's a transformer? The transformer, it's the most important advance in neural network architectures in recent history. What is attention maybe too? Cause I think that's an interesting idea, not necessarily sort of technically speaking, but the idea of attention versus maybe what recurrent neural networks represent. Yeah, so the thing is the transformer is a combination of multiple ideas simultaneously of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key. The transformer is successful because it is the simultaneous combination of multiple ideas. And if you were to remove either idea, it would be much less successful. So the transformer uses a lot of attention, but attention existed for a few years. So that can't be the main innovation. The transformer is designed in such a way that it runs really fast on the GPU. And that makes a huge amount of difference. This is one thing. The second thing is that transformer is not recurrent. And that is really important too, because it is more shallow and therefore much easier to optimize. So in other words, users attention, it is a really great fit to the GPU and it is not recurrent, so therefore less deep and easier to optimize. And the combination of those factors make it successful. So now it makes great use of your GPU. It allows you to achieve better results for the same amount of compute. And that's why it's successful. Were you surprised how well transformers worked and GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came about in language. So you got to see the whole set of revolutions before and after. Were you surprised? Yeah, a little. A little? I mean, it's hard to remember because you adapt really quickly, but it definitely was surprising. It definitely was. In fact, you know what? I'll retract my statement. It was pretty amazing. It was just amazing to see generate this text of this. And you know, you gotta keep in mind that at that time we've seen all this progress in GANs in improving the samples produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that much. And suddenly we moved from, you know, whatever GANs were in 2015 to the best, most amazing GANs in one step. And that was really stunning. Even though theory predicted, yeah, you train a big language model, of course you should get this, but then to see it with your own eyes, it's something else. And yet we adapt really quickly. And now there's sort of some cognitive scientists write articles saying that GPT2 models don't truly understand language. So we adapt quickly to how amazing the fact that they're able to model the language so well is. So what do you think is the bar? For what? For impressing us that it... I don't know. Do you think that bar will continuously be moved? Definitely. I think when you start to see really dramatic economic impact, that's when I think that's in some sense the next barrier. Because right now, if you think about the work in AI, it's really confusing. It's really hard to know what to make of all these advances. It's kind of like, okay, you got an advance and now you can do more things and you've got another improvement and you've got another cool demo. At some point, I think people who are outside of AI, they can no longer distinguish this progress anymore. So we were talking offline about translating Russian to English and how there's a lot of brilliant work in Russian that the rest of the world doesn't know about. That's true for Chinese, it's true for a lot of scientists and just artistic work in general. Do you think translation is the place where we're going to see sort of economic big impact? I don't know. I think there is a huge number of... I mean, first of all, I wanna point out that translation already today is huge. I think billions of people interact with big chunks of the internet primarily through translation. So translation is already huge and it's hugely positive too. I think self driving is going to be hugely impactful and that's, it's unknown exactly when it happens, but again, I would not bet against deep learning, so I... So there's deep learning in general, but you think this... Deep learning for self driving. Yes, deep learning for self driving. But I was talking about sort of language models. I see. Just to check. Beard off a little bit. Just to check, you're not seeing a connection between driving and language. No, no. Okay. Or rather both use neural nets. That'd be a poetic connection. I think there might be some, like you said, there might be some kind of unification towards a kind of multitask transformers that can take on both language and vision tasks. That'd be an interesting unification. Now let's see, what can I ask about GPT two more? It's simple. There's not much to ask. It's, you take a transform, you make it bigger, you give it more data, and suddenly it does all those amazing things. Yeah, one of the beautiful things is that GPT, the transformers are fundamentally simple to explain, to train. Do you think bigger will continue to show better results in language? Probably. Sort of like what are the next steps with GPT two, do you think? I mean, I think for sure seeing what larger versions can do is one direction. Also, I mean, there are many questions. There's one question which I'm curious about and that's the following. So right now GPT two, so we feed it all this data from the internet, which means that it needs to memorize all those random facts about everything in the internet. And it would be nice if the model could somehow use its own intelligence to decide what data it wants to accept and what data it wants to reject. Just like people. People don't learn all data indiscriminately. We are super selective about what we learn. And I think this kind of active learning, I think would be very nice to have. Yeah, listen, I love active learning. So let me ask, does the selection of data, can you just elaborate that a little bit more? Do you think the selection of data is, like I have this kind of sense that the optimization of how you select data, so the active learning process is going to be a place for a lot of breakthroughs, even in the near future? Because there hasn't been many breakthroughs there that are public. I feel like there might be private breakthroughs that companies keep to themselves because the fundamental problem has to be solved if you want to solve self driving, if you want to solve a particular task. What do you think about the space in general? Yeah, so I think that for something like active learning, or in fact, for any kind of capability, like active learning, the thing that it really needs is a problem. It needs a problem that requires it. It's very hard to do research about the capability if you don't have a task, because then what's going to happen is that you will come up with an artificial task, get good results, but not really convince anyone. Right, like we're now past the stage where getting a result on MNIST, some clever formulation of MNIST will convince people. That's right, in fact, you could quite easily come up with a simple active learning scheme on MNIST and get a 10x speed up, but then, so what? And I think that with active learning, the need, active learning will naturally arise as problems that require it pop up. That's how I would, that's my take on it. There's another interesting thing that OpenAI has brought up with GPT2, which is when you create a powerful artificial intelligence system, and it was unclear what kind of detrimental, once you release GPT2, what kind of detrimental effect it will have. Because if you have a model that can generate a pretty realistic text, you can start to imagine that it would be used by bots in some way that we can't even imagine. So there's this nervousness about what is possible to do. So you did a really kind of brave and I think profound thing, which is start a conversation about this. How do we release powerful artificial intelligence models to the public? If we do it all, how do we privately discuss with other, even competitors, about how we manage the use of the systems and so on? So from this whole experience, you released a report on it, but in general, are there any insights that you've gathered from just thinking about this, about how you release models like this? I mean, I think that my take on this is that the field of AI has been in a state of childhood. And now it's exiting that state and it's entering a state of maturity. What that means is that AI is very successful and also very impactful. And its impact is not only large, but it's also growing. And so for that reason, it seems wise to start thinking about the impact of our systems before releasing them, maybe a little bit too soon, rather than a little bit too late. And with the case of GPT2, like I mentioned earlier, the results really were stunning. And it seemed plausible, it didn't seem certain, it seemed plausible that something like GPT2 could easily use to reduce the cost of this information. And so there was a question of what's the best way to release it, and a staged release seemed logical. A small model was released, and there was time to see the, many people use these models in lots of cool ways. There've been lots of really cool applications. There haven't been any negative application to be known of. And so eventually it was released, but also other people replicated similar models. That's an interesting question though that we know of. So in your view, staged release, is at least part of the answer to the question of how do we, what do we do once we create a system like this? It's part of the answer, yes. Is there any other insights? Like say you don't wanna release the model at all, because it's useful to you for whatever the business is. Well, plenty of people don't release models already. Right, of course, but is there some moral, ethical responsibility when you have a very powerful model to sort of communicate? Like, just as you said, when you had GPT2, it was unclear how much it could be used for misinformation. It's an open question, and getting an answer to that might require that you talk to other really smart people that are outside of your particular group. Have you, please tell me there's some optimistic pathway for people to be able to use this model for people across the world to collaborate on these kinds of cases? Or is it still really difficult from one company to talk to another company? So it's definitely possible. It's definitely possible to discuss these kind of models with colleagues elsewhere, and to get their take on what to do. How hard is it though? I mean. Do you see that happening? I think that's a place where it's important to gradually build trust between companies. Because ultimately, all the AI developers are building technology which is going to be increasingly more powerful. And so it's, the way to think about it is that ultimately we're all in it together. Yeah, I tend to believe in the better angels of our nature, but I do hope that when you build a really powerful AI system in a particular domain, that you also think about the potential negative consequences of, yeah. It's an interesting and scary possibility that there will be a race for AI development that would push people to close that development, and not share ideas with others. I don't love this. I've been a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting. What do you think it takes to, let's talk about AGI a little bit. What do you think it takes to build a system of human level intelligence? We talked about reasoning, we talked about long term memory, but in general, what does it take, do you think? Well, I can't be sure. But I think the deep learning, plus maybe another, plus maybe another small idea. Do you think self play will be involved? So you've spoken about the powerful mechanism of self play where systems learn by sort of exploring the world in a competitive setting against other entities that are similarly skilled as them, and so incrementally improve in this way. Do you think self play will be a component of building an AGI system? Yeah, so what I would say, to build AGI, I think it's going to be deep learning plus some ideas. And I think self play will be one of those ideas. I think that that is a very, self play has this amazing property that it can surprise us in truly novel ways. For example, like we, I mean, pretty much every self play system, both are Dota bot. I don't know if, OpenAI had a release about multi agent where you had two little agents who were playing hide and seek. And of course, also alpha zero. They were all produced surprising behaviors. They all produce behaviors that we didn't expect. They are creative solutions to problems. And that seems like an important part of AGI that our systems don't exhibit routinely right now. And so that's why I like this area. I like this direction because of its ability to surprise us. To surprise us. And an AGI system would surprise us fundamentally. Yes. And to be precise, not just a random surprise, but to find the surprising solution to a problem that's also useful. Right. Now, a lot of the self play mechanisms have been used in the game context or at least in the simulation context. How far along the path to AGI do you think will be done in simulation? How much faith, promise do you have in simulation versus having to have a system that operates in the real world? Whether it's the real world of digital real world data or real world like actual physical world of robotics. I don't think it's an easy or. I think simulation is a tool and it helps. It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand that. That's true, but one of the criticisms of self play, one of the criticisms of reinforcement learning is one of the, its current power, its current results, while amazing, have been demonstrated in a simulated environments or very constrained physical environments. Do you think it's possible to escape them, escape the simulator environments and be able to learn in non simulator environments? Or do you think it's possible to also just simulate in a photo realistic and physics realistic way, the real world in a way that we can solve real problems with self play in simulation? So I think that transfer from simulation to the real world is definitely possible and has been exhibited many times by many different groups. It's been especially successful in vision. Also open AI in the summer has demonstrated a robot hand which was trained entirely in simulation in a certain way that allowed for seem to real transfer to occur. Is this for the Rubik's cube? Yeah, that's right. I wasn't aware that was trained in simulation. It was trained in simulation entirely. Really, so it wasn't in the physical, the hand wasn't trained? No, 100% of the training was done in simulation and the policy that was learned in simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the heck it was, those weren't, were those part of the simulation? Well, the simulation was generally, so the simulation was trained to be robust to many different things, but not the kind of perturbations we've had in the video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe. So in theory, these are novel perturbations. Correct, it's not in theory, in practice. Those are novel perturbations? Well, that's okay. That's a clean, small scale, but clean example of a transfer from the simulated world to the physical world. Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in general. And the better the transfer capabilities are, the more useful simulation will become. Because then you could take, you could experience something in simulation and then learn a moral of the story, which you could then carry with you to the real world. As humans do all the time when they play computer games. So let me ask sort of a embodied question, staying on AGI for a sec. Do you think AGI system would need to have a body? We need to have some of those human elements of self awareness, consciousness, sort of fear of mortality, sort of self preservation in the physical space, which comes with having a body. I think having a body will be useful. I don't think it's necessary, but I think it's very useful to have a body for sure, because you can learn a whole new, you can learn things which cannot be learned without a body. But at the same time, I think that if you don't have a body, you could compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For example, there are many people who were born deaf and blind and they were able to compensate for the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able to physically interact with the world, and if you're not able to, I mean, I actually was getting at, maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not, but the idea of consciousness and a more constrained version of that is self awareness. Do you think an AGI system should have consciousness? We can't define, whatever the heck you think consciousness is. Yeah, hard question to answer, given how hard it is to define it. Do you think it's useful to think about? I mean, it's definitely interesting. It's fascinating. I think it's definitely possible that our systems will be conscious. Do you think that's an emergent thing that just comes from, do you think consciousness could emerge from the representation that's stored within neural networks? So like that it naturally just emerges when you become more and more, you're able to represent more and more of the world? Well, I'd say I'd make the following argument, which is humans are conscious. And if you believe that artificial neural nets are sufficiently similar to the brain, then there should at least exist artificial neural nets you should be conscious too. You're leaning on that existence proof pretty heavily. Okay, so that's the best answer I can give. No, I know, I know, I know. There's still an open question if there's not some magic in the brain that we're not, I mean, I don't mean a non materialistic magic, but that the brain might be a lot more complicated and interesting than we give it credit for. If that's the case, then it should show up. And at some point we will find out that we can't continue to make progress. But I think it's unlikely. So we talk about consciousness, but let me talk about another poorly defined concept of intelligence. Again, we've talked about reasoning, we've talked about memory. What do you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing formulated with the imitation game with natural language? Is there something in your mind that you will be deeply impressed by if a system was able to do? I mean, lots of things. There's a certain frontier of capabilities today. And there exist things outside of that frontier. And I would be impressed by any such thing. For example, I would be impressed by a deep learning system which solves a very pedestrian task, like machine translation or computer vision task or something which never makes mistake a human wouldn't make under any circumstances. I think that is something which have not yet been demonstrated and I would find it very impressive. Yeah, so right now they make mistakes in different, they might be more accurate than human beings, but they still, they make a different set of mistakes. So my, I would guess that a lot of the skepticism that some people have about deep learning is when they look at their mistakes and they say, well, those mistakes, they make no sense. Like if you understood the concept, you wouldn't make that mistake. And I think that changing that would be, that would inspire me. That would be, yes, this is progress. Yeah, that's a really nice way to put it. But I also just don't like that human instinct to criticize a model is not intelligent. That's the same instinct as we do when we criticize any group of creatures as the other. Because it's very possible that GPT2 is much smarter than human beings at many things. That's definitely true. It has a lot more breadth of knowledge. Yes, breadth of knowledge and even perhaps depth on certain topics. It's kind of hard to judge what depth means, but there's definitely a sense in which humans don't make mistakes that these models do. The same is applied to autonomous vehicles. The same is probably gonna continue being applied to a lot of artificial intelligence systems. We find, this is the annoying thing. This is the process of, in the 21st century, the process of analyzing the progress of AI is the search for one case where the system fails in a big way where humans would not. And then many people writing articles about it. And then broadly, the public generally gets convinced that the system is not intelligent. And we pacify ourselves by thinking it's not intelligent because of this one anecdotal case. And this seems to continue happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also extremely impressed by the system that exists today. But I think this connects to the earlier point we discussed that it's just confusing to judge progress in AI. Yeah. And you have a new robot demonstrating something. How impressed should you be? And I think that people will start to be impressed once AI starts to really move the needle on the GDP. So you're one of the people that might be able to create an AGI system here. Not you, but you and OpenAI. If you do create an AGI system and you get to spend sort of the evening with it, him, her, what would you talk about, do you think? The very first time? First time. Well, the first time I would just ask all kinds of questions and try to get it to make a mistake. And I would be amazed that it doesn't make mistakes and just keep asking broad questions. What kind of questions do you think? Would they be factual or would they be personal, emotional, psychological? What do you think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the people that might be in the room where this happens. So let me ask sort of a profound question about, I've just talked to a Stalin historian. I've been talking to a lot of people who are studying power. Abraham Lincoln said, \"'Nearly all men can stand adversity, \"'but if you want to test a man's character, give him power.'\" I would say the power of the 21st century, maybe the 22nd, but hopefully the 21st, would be the creation of an AGI system and the people who have control, direct possession and control of the AGI system. So what do you think, after spending that evening having a discussion with the AGI system, what do you think you would do? Well, the ideal world I'd like to imagine is one where humanity, I like, the board members of a company where the AGI is the CEO. So it would be, I would like, the picture which I would imagine is you have some kind of different entities, different countries or cities, and the people that leave their vote for what the AGI that represents them should do, and the AGI that represents them goes and does it. I think a picture like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city, for a country, and there would be multiple AGI's, for a city, for a country, and there would be, it would be trying to, in effect, take the democratic process to the next level. And the board can always fire the CEO. Essentially, press the reset button, say. Press the reset button. Rerandomize the parameters. But let me sort of, that's actually, okay, that's a beautiful vision, I think, as long as it's possible to press the reset button. Do you think it will always be possible to press the reset button? So I think that it definitely will be possible to build. So you're talking, so the question that I really understand from you is, will humans or humans people have control over the AI systems that they build? Yes. And my answer is, it's definitely possible to build AI systems which will want to be controlled by their humans. Wow, that's part of their, so it's not that just they can't help but be controlled, but that's the, they exist, the one of the objectives of their existence is to be controlled. In the same way that human parents generally want to help their children, they want their children to succeed. It's not a burden for them. They are excited to help children and to feed them and to dress them and to take care of them. And I believe with high conviction that the same will be possible for an AGI. It will be possible to program an AGI, to design it in such a way that it will have a similar deep drive that it will be delighted to fulfill. And the drive will be to help humans flourish. But let me take a step back to that moment where you create the AGI system. I think this is a really crucial moment. And between that moment and the Democratic board members with the AGI at the head, there has to be a relinquishing of power. So as George Washington, despite all the bad things he did, one of the big things he did is he relinquished power. He, first of all, didn't want to be president. And even when he became president, he gave, he didn't keep just serving as most dictators do for indefinitely. Do you see yourself being able to relinquish control over an AGI system, given how much power you can have over the world, at first financial, just make a lot of money, right? And then control by having possession as AGI system. I'd find it trivial to do that. I'd find it trivial to relinquish this kind of power. I mean, the kind of scenario you are describing sounds terrifying to me. That's all. I would absolutely not want to be in that position. Do you think you represent the majority or the minority of people in the AI community? Well, I mean. Say open question, an important one. Are most people good is another way to ask it. So I don't know if most people are good, but I think that when it really counts, people can be better than we think. That's beautifully put, yeah. Are there specific mechanism you can think of of aligning AI values to human values? Is that, do you think about these problems of continued alignment as we develop the AI systems? Yeah, definitely. In some sense, the kind of question which you are asking is, so if I were to translate the question to today's terms, it would be a question about how to get an RL agent that's optimizing a value function which itself is learned. And if you look at humans, humans are like that because the reward function, the value function of humans is not external, it is internal. That's right. And there are definite ideas of how to train a value function. Basically an objective, you know, and as objective as possible perception system that will be trained separately to recognize, to internalize human judgments on different situations. And then that component would then be integrated as the base value function for some more capable RL system. You could imagine a process like this. I'm not saying this is the process, I'm saying this is an example of the kind of thing you could do. So on that topic of the objective functions of human existence, what do you think is the objective function that's implicit in human existence? What's the meaning of life? Oh. I think the question is wrong in some way. I think that the question implies that there is an objective answer which is an external answer, you know, your meaning of life is X. I think what's going on is that we exist and that's amazing. And we should try to make the most of it and try to maximize our own value and enjoyment of a very short time while we do exist. It's funny, because action does require an objective function is definitely there in some form, but it's difficult to make it explicit and maybe impossible to make it explicit, I guess is what you're getting at. And that's an interesting fact of an RL environment. Well, but I was making a slightly different point is that humans want things and their wants create the drives that cause them to, you know, our wants are our objective functions, our individual objective functions. We can later decide that we want to change, that what we wanted before is no longer good and we want something else. Yeah, but they're so dynamic, there's gotta be some underlying sort of Freud, there's things, there's like sexual stuff, there's people who think it's the fear of death and there's also the desire for knowledge and you know, all these kinds of things, procreation, sort of all the evolutionary arguments, it seems to be, there might be some kind of fundamental objective function from which everything else emerges, but it seems like it's very difficult to make it explicit. I think that probably is an evolutionary objective function which is to survive and procreate and make sure you make your children succeed. That would be my guess, but it doesn't give an answer to the question of what's the meaning of life. I think you can see how humans are part of this big process, this ancient process. We exist on a small planet and that's it. So given that we exist, try to make the most of it and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life. One, do you have regrets? Moments that if you went back, you would do differently. And two, are there moments that you're especially proud of that made you truly happy? So I can answer that, I can answer both questions. Of course, there's a huge number of choices and decisions that I've made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret, but I try to take solace in the knowledge that at the time I did the best I could. And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of and they made me happy for some time, but I don't think that that is the source of happiness. So your academic accomplishments, all the papers, you're one of the most cited people in the world. All of the breakthroughs I mentioned in computer vision and language and so on, what is the source of happiness and pride for you? I mean, all those things are a source of pride for sure. I'm very grateful for having done all those things and it was very fun to do them. But happiness comes, but you know, happiness, well, my current view is that happiness comes from our, to a very large degree, from the way we look at things. You know, you can have a simple meal and be quite happy as a result, or you can talk to someone and be happy as a result as well. Or conversely, you can have a meal and be disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that, but I'm not sure, I don't want to be too confident. Being humble in the face of the uncertainty seems to be also a part of this whole happiness thing. Well, I don't think there's a better way to end it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it and thanks for talking today. Yeah, thanks for stopping by, I really enjoyed it. Thanks for listening to this conversation with Ilya Setskever and thank you to our presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App and using the code LEXPodcast. If you enjoy this podcast, subscribe on YouTube, review it with five stars on Apple Podcast, support on Patreon, or simply connect with me on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child? If this were then subjected to an appropriate course of education, one would obtain the adult brain. Thank you for listening and hope to see you next time.\n",
      "## End of Transcript\n",
      "\n",
      "## Summary\n",
      "The text explores various topics related to neural networks, deep learning, and artificial intelligence. It covers neural networks, generative adversarial networks (GANs), language representation, object recognition, overparameterization, and training large neural networks. The text also discusses the similarities between human and artificial neural networks, as well as the potential of spiking neural networks.\n",
      "\n",
      "The text highlights the advancements in neural network technology and its impact on various fields. It explores the potential of deep learning in unifying physics and neuroscience, as well as its potential benefits and challenges in various other fields. The text also discusses the ethical implications of AI and its potential impact on society.\n",
      "\n",
      "Overall, the text provides a concise overview of key concepts and advancements in the field of neural networks and deep learning. It emphasizes the power of deep learning and its potential to revolutionize various fields.\n",
      "## End of Summary\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19841 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m map_llm_chain_input \u001b[38;5;241m=\u001b[39m eval_input_data\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Run the input through the LLM chain (works in parallel)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m map_llm_chain_results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_llm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_llm_chain_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleteness evaluation score for episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain/chains/llm.py:227\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    229\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain/chains/llm.py:224\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    219\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    220\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    221\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:439\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    434\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    438\u001b[0m }\n\u001b[0;32m--> 439\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:356\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19841 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in episodes:\n",
    "\n",
    "    with open(f\"./predicted/podcast_summaries_ollama_gemma_{episode}.json\") as f: \n",
    "        json_data = json.load(f)\n",
    "        summarized_content = json_data['final_summary']\n",
    "\n",
    "\n",
    "    baseline_transcript = baseline_podcasts[str(episode)]['transcript']\n",
    "\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'document': baseline_transcript,\n",
    "            'summary': summarized_content,        \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"#######  Episode {episode}\")\n",
    "    print(\"## Transcript\")\n",
    "    print(baseline_transcript)\n",
    "    print(\"## End of Transcript\")\n",
    "    \n",
    "    print()\n",
    "    print(\"## Summary\")\n",
    "    print(summarized_content)\n",
    "    print(\"## End of Summary\")\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\", \"document\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "\n",
    "    print(f\"Completeness evaluation score for episode {episode}\")\n",
    "    print(map_llm_chain_results)\n",
    "    print(\"##############################################\")\n",
    "    print(\"##############################################\")\n",
    "    print()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
