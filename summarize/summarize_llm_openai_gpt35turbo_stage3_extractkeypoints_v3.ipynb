{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v3\"\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=200\n",
    "MAX_NUM_TOPICS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f905a9be810>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-24 09:11:50.367515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-24 09:17:36.904384\n",
      "chunks_text len: 81\n",
      "extract_keypoints start time: 2024-03-24 09:17:36.904534\n",
      "extract_keypoints done time 2024-03-24 09:20:34.366283\n",
      "Start time: 2024-03-24 09:20:34.366480\n",
      "Stage 1 done time 2024-03-24 09:25:36.756660\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow in the Tech Industry ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Impact of Open Sourcing TensorFlow ', 'text': 'The decision to open source TensorFlow is a definitive moment in the tech industry. Open innovation can be successful and inspire many companies to open source their code. The conversation is part of the Artificial Intelligence podcast. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library turned into TensorFlow in 2014, the open source library. The idea of deep learning was interesting back then.'}, {'title': 'The Progress of Deep Learning ', 'text': 'The idea of deep learning was interesting and intriguing in some ways. It had shown some very promising and early results. Scaling the compute and data resulted in better performance. The first year or two focused on proving out the belief. Early wins were achieved in the first year.'}, {'title': 'The Early Successes of Google Brain ', 'text': 'The early wins included collaboration with the speech research team and the success of the \"cat paper\" on images. The birth of Google Brain was focused on neural networks and deep learning from the beginning. The mission in the early days was to scale the technology.'}, {'title': 'The Rise of Machine Learning in Industry and Academia ', 'text': 'Machine learning is now at the core of the entire company and is growing in that direction. Google has been doing machine learning for a long time, and as they scaled it up, they showed that it was possible and would impact many things. Real products started to use machine learning, such as speech and image recognition. Academia also started to show interest in deep learning and pushed for more research in the field.'}, {'title': 'The Rise of Deep Learning and Open Source Innovation in 2014 ', 'text': \"Academia and external push for deep learning in 2014. Decision to open source TensorFlow seen as a big moment in software engineering. Google's decision to lead the world in open innovation.\"}, {'title': 'The Impact of Open Source on Research and Innovation ', 'text': 'The decision to go open source with their IP was a significant moment in time. The initial idea came from Jeff, who was a big proponent of open innovation. The research group was focused on sharing their research and pushing the state of the art forward. Sharing research has contributed to the rapid growth of deep learning and machine learning. The next step was to consider how software could help with sharing research. There were existing libraries such as Tiano and Torch that were considered in this process.'}, {'title': 'The Importance of Software for Specific Tasks ', 'text': 'The need for software to help with a specific task was identified. Existing libraries such as Tiano and Torch were found, but they were done by academia and at a different level. Google had developed lots of internal software and published papers, leading to successful open source projects. Hadoop had come from technology developed by the company, and they believed their technology was better for various reasons. Google Cloud was not providing their technology, but instead offering H base APIs on top of their original Bigtable technology.'}, {'title': \"Google's Contributions to Bigtable and TensorFlow \", 'text': 'Google is providing H base APIs on top of Bigtable to help the community and push a good standard forward. TensorFlow is an open source library that can be used anywhere. Google Cloud ensures lots of integrations with everything else and works really well with TensorFlow. The interviewee is leading the TensorFlow effort. The history and timeline of the TensorFlow project in terms of major milestones is requested.'}, {'title': 'The Evolution of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. The incredible ecosystem surrounding TensorFlow. The timeline of TensorFlow development, from summer 2014 to open sourcing in November 2015. The decision to open source TensorFlow made in late 2014. The fast pace of development in deep learning.'}, {'title': 'Title ', 'text': 'The Decision to Open Source TensorFlow TechnologyText '}, {'title': 'Machine Learning Algorithms on Mobile Phones ', 'text': 'Machine learning algorithms were being run on mobile phones. There were already products using customized handcrafted code or internal libraries for running machine learning on phones. The use of Theano and Caffe at Google influenced design decisions for building machine learning models for mobile phones.'}, {'title': 'Influences on Design Decisions in Software Development ', 'text': 'The design decisions were influenced by the parallel development of internal systems and external libraries such as Theano, Torch, Lua, Caffe, and possibly JNR. The team considered whether to have a graph or not in their design. The key decisions were driven by limitations in their prior systems and the fast pace of research.'}, {'title': 'Key Decisions for Transitioning to TensorFlow 2.0 and Eager Execution ', 'text': 'The key decisions included flexibility in research and hardware changes. Moving towards TensorFlow 2.0 and eager execution were important graph decisions. The choice was influenced by the need for flexibility in expressing various ideas. The decision was also influenced by the limitations of prior disbelief and the need for a more intuitive development process. The previous disbelief had a graph-like structure, but it was simpler and more linear.'}, {'title': 'Title ', 'text': 'Factors Influencing the Adoption of a Graph for Production DeploymentText '}, {'title': 'The Impact of Open Sourcing on Deep Learning Project ', 'text': 'The unexpected popularity of the project with 41 million downloads. The recognition of the need for the project from a research perspective and early days of deep learning. The potential future growth and enabling more people to use the project. The growth of deep learning after open sourcing and the opportunity to leverage that growth. The change in attention and interest from a global population of developers after open sourcing.'}, {'title': 'The Impact of Open Sourcing on a Deep Learning Project ', 'text': 'Open sourcing the project led to an incredible amount of attention from a global population of developers. The project started changing with the development of good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The project became very community-driven. The documentation provided was a huge step up from academic projects, which did not prioritize documentation. Deep learning shifted from being a research thing to something more accessible and practical.'}, {'title': 'Shift of Deep Learning from Research to Enterprise Support ', 'text': 'Deep learning shifted from being a research thing to something that developers could use for interesting projects. The focus shifted from just researchers to also include stability and deployment for users. Planning for version 1.0 involved considering the needs for stability, deployment, documentation, and designs. The release of version 1.0 attracted more enterprises to get involved and support the project. Post 1.0, there was a focus on attracting more enterprise support and development.'}, {'title': 'Enterprise Adoption of Product Post 1.0 Release ', 'text': 'The excitement around the enterprise adoption of the product post 1.0 release. The initial interest from researchers, hobbyists, and early adopters before 1.0 release. The pressure for stability from enterprises before the 1.0 release. The importance of understanding what enterprises want in the midst of product development.'}, {'title': 'The Continued Use of Inception and ResNet 50 ', 'text': 'Inception and ResNet 50 are still widely used by many people, even though they are several years old. Some users prioritize stability and simplicity over the latest performance or quality improvements. Providing stability and simplicity allows more people to access the technology. The research crowd is interested in pushing the boundaries with new and advanced models like RNNs and transformers.'}, {'title': 'Advancements in AI Technology ', 'text': 'Deep learning models are evolving from RNNs to transformers, and now need to combine with RL and GANs. The boundary of technology is shifting and pushing the state of the art in AI. Older technology is still very usable and stable, making it easier for many people to access and utilize. The advancement in AI is reflected in the development of apps and phones.'}, {'title': 'Common Data Analysis Methods for Hobbyists and Enterprises ', 'text': 'The most common case for hobbyists is to use apps and phones for data analysis. Enterprises are more focused on using data for making predictions. Enterprises typically use regression models, linear models, and gradient booster trees for data analysis. Deep learning is still beneficial for some enterprises, but structured data is the bread and butter for them.'}, {'title': 'Perspectives on Structured Data, Deep Learning, and TensorFlow Extended ', 'text': \"The audience's perspective on structured data and deep learning can vary. Enterprise with a large dataset can benefit from deep learning. The importance of stability and simplicity in the entire pipeline of TensorFlow Extended. The need for repetitive model training in various industries such as immigration and insurance.\"}, {'title': 'The Rise of Machine Learning in the Legal Field ', 'text': 'Machine learning entering the legal realm is a common trend in various disciplines such as immigration and insurance. Companies often struggle with organizing and digitizing their data, which hinders the implementation of machine learning in legal processes. There are various questions and challenges related to implementing machine learning in the legal field, ranging from basic requirements to specific expertise and support.'}, {'title': 'The Importance of Organized Data and Starting with Basic Models in Machine Learning ', 'text': 'Machine learning requires organized data for automation and prediction. TensorFlow ecosystem provides data sets and pre-trained models for easier use. Start with basic models and improve them, rather than aiming for the newest and fanciest models.'}, {'title': 'The Evolution of Keras and its Integration with TensorFlow ', 'text': 'Start with the basics and then improve. Keras made TensorFlow more accessible. Francois started the Keras project before he was at Google. Keras was initially on top of Tiano and then on top of TensorFlow. There were enough similarities between Keras and TensorFlow that Francois decided to integrate Keras with TensorFlow.'}, {'title': 'The Influence of Tiano on the Creation of the TensorFlow Interface ', 'text': \"The creation of the interface for TensorFlow was influenced by Tiano. Tiano started working on the interface before joining Google. Tiano was initially working on research and Keras as a side project. Tiano's research work was well-received and he eventually joined the TensorFlow team. Tiano's decision to create the interface was independent and relevant to the community.\"}, {'title': 'The Integration of Keras into TensorFlow 2.0 ', 'text': \"Keras was integrated into TensorFlow in a deep way. Keras is the recommended way for a beginner to interact with TensorFlow 2.0. The integration of Keras into TensorFlow was a result of a researcher's good work and the positive reception of the API. The researcher initially joined the team for a quarter but has been fully committed for two years. The decision to integrate Keras into TensorFlow was the result of careful consideration and evaluation of various APIs.\"}, {'title': 'The Decision to Focus on Keras for API Integration ', 'text': 'The decision to focus on Keras was a bold one after considering multiple APIs. The goal was to integrate Keras as much as possible and simplify the process for users. The community was confused about which API to use, leading to the need for a standard choice. The decision to focus on Keras 2.0 was based on simplifying and unifying the API choices. Keras was chosen based on user preference and community feedback.'}, {'title': 'Choosing Keras for Version 2.0 ', 'text': \"The decision to simplify and pick Keras as the main framework for version 2.0. Keras was chosen because it was popular and had many great features. The integration of Keras into TensorFlow was surprising but ultimately empowering. The team's focus on making things easier for developers. The importance of Python and Guido van Rossum in the development process.\"}, {'title': 'Open Source Projects and Key Figures ', 'text': 'Python has Guido van Rossum, who held the position of benevolent dictator for life. TensorFlow is a huge successful open source project. There are lots of different new features being incorporated in TensorFlow. Martin Wick has driven a lot of open source stuff and APIs. There are a number of people involved in key design directions.'}, {'title': 'Open Source Development and Community Growth ', 'text': 'Martin Wick has driven a lot of open source stuff and APIs. Regular design reviews are conducted. Efforts have been made to open up to the community and add transparency. More processes are being set in place, such as RFCs and special interest groups, to grow the community and scale it. The ecosystem is at a scale where a lone decision maker is not sufficient. Andrej Karpathy first did ComNetJS, allowing training of neural networks in the browser using JavaScript. TensorFlow.js is making training neural networks in the browser a serious and legitimate thing.'}, {'title': 'TensorFlow.js: Training and Networking in the Browser ', 'text': 'TensorFlow.js allows training and networking in the browser using JavaScript. TensorFlow.js is becoming a serious and legitimate way to operate, whether in the backend or front end. TensorFlow Extended and TensorFlow Lite for mobile are part of the convergence towards saving models in a consistent way. There is cohesiveness in being able to train on the desktop and then move models to mobile. The goal is to enable machine learning in multiple ways, including support for deep learning and other exciting developments in ML.'}, {'title': 'Advancements in Machine Learning Research and Integration ', 'text': 'The goal is to enable machine learning. Exciting developments in ML, including support for various algorithms. Focus on pushing the state of the art in research, such as with the release of BERT. Integration of research into real products to have a real impact on people. Consideration of the large number of compute devices across the world for ML and training.'}, {'title': 'The Expansion of Machine Learning to Mobile and Embedded Devices ', 'text': 'ML and training are no longer limited to workstations, data centers, or the cloud. Machine learning is now running on phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning has grown to cover more devices over time. There is a focus on pushing the boundaries and building more tooling to help with machine learning.'}, {'title': 'Exploring TensorFlow Libraries for Research and Production ', 'text': 'TensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended. There are lots of libraries being built on top of TensorFlow for research and production purposes. Some libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production. Libraries have come from within Google and from the community to address different needs and goals. The goal is to enable the community to build and use different pieces of technology.'}, {'title': 'Enhancing Community Collaboration and Model Sharing in TensorFlow 2.0 ', 'text': 'The goal is to enable different parts of the community to build the things they care about. The focus is on making different pieces work well together in TensorFlow 2.0. The core format and sharing of models through save model and TensorFlow hub are key areas of focus. There was initial skepticism about TensorFlow.js (or deep learning JS) when it first came out.'}, {'title': 'Challenges and Progress in Integrating TensorFlow.js ', 'text': 'The project of integrating TensorFlow.js into the ecosystem was technically very difficult. There are still many challenges to be overcome in the technical side. The team has iterated over the last few years and learned a lot. Despite the challenges, the goal is to make it easy for the end user. There are still challenges ahead, especially with more devices coming on board.'}, {'title': 'Challenges in Scaling and Evolving TensorFlow ', 'text': 'The end user experience should be easy, but there are many complexities behind the scenes. Challenges ahead include the increasing number of devices and the need for improvements in APIs and compiler tools. TensorFlow started as a monolithic system and scaling it out requires breaking it apart with clearer interfaces. The system is rapidly evolving and it is difficult to change and modify.'}, {'title': 'Challenges of Maintaining Compatibility in TensorFlow ', 'text': 'The system is four years old and still rapidly evolving, making it hard to change and modify. Many people rely on TensorFlow in their applications, creating technical debt and responsibility for previous versions to still work. It is challenging to innovate while maintaining compatibility with previous versions. TensorFlow 2.0 breaks some backward compatibility, but the conversion seems pretty straightforward. The importance of maintaining compatibility is still questioned given the rapid pace of deep learning.'}, {'title': 'The Impact of TensorFlow 2.0 on Backward Compatibility ', 'text': '2.0 breaks some back compatibility, but not too much. The conversion to 2.0 seems pretty straightforward. It is important to keep compatibility for production systems that rely on TensorFlow. There is a trade off in making new changes, as it may slow certain things down but bring overall bigger value.'}, {'title': 'Importance of Designing with a Clean Slate in Mind ', 'text': \"Designing with a clean slate in mind is important when starting new things. Making compromises occasionally is necessary, but designing with a clean slate is crucial. The overall value of bringing in new changes is much bigger, even if it may slow certain things down. It's important to consider the impact on future team members and not just focus on the present. Responsibility in the idea stage is crucial for success in implementing new ideas.\"}, {'title': 'The Importance of TensorFlow in Research ', 'text': \"It's important to have a clean slate and not worry about past ideas when thinking of new ones. The speaker has recently switched their research group to TensorFlow and believes it is leading in many ways. The speaker wishes everyone would use the same thing, and sees TensorFlow as the closest to that ideal. There is competition between TensorFlow and PyTorch, but the speaker sees it as a way to get different ideas and improve their own platform.\"}, {'title': 'Comparison of TensorFlow and PyTorch ', 'text': \"TensorFlow focused on both research and production, while PyTorch focused solely on research. PyTorch prioritized ease of use over speed, while TensorFlow prioritized speed and production readiness. Both TensorFlow and PyTorch learned from each other's approaches and made improvements based on previous technologies. PyTorch's approach was influenced by the desire to make research easier, while TensorFlow's approach was influenced by the need for production readiness.\"}, {'title': 'Exploring Different Spaces and Eager Execution ', 'text': 'The team had the benefit of seeing what had come before and exploring different kinds of spaces. They built on things like JNR and faced competition, which was interesting. They had thought about the area early on and revisited it multiple times before deciding to try it again. They started pushing on eager execution, which finally came together in 2.0 after some time and effort.'}, {'title': 'Advancements in TensorFlow 2.0 ', 'text': \"TensorFlow is doing some incredible work in the last couple of years. The ecosystem is being improved, making it easily accessible to Keras and eager execution. TensorFlow 2.0 is a significant development. There are lots of other things that TensorFlow 2.0 enables us to do and that we're excited about. It sets us up for clean APIs.\"}, {'title': 'Clean APIs and Their Performance and Optimization Benefits ', 'text': 'The clean APIs enable a lot of performance and optimization for users. It allows for exploration of other spaces behind the scenes after 2.0 in the future. It enables both single machine and distributed operations. The surface for what the users want has been cleaned up. It allows for a lot of things to be done behind the scenes once ready with 2.0.'}, {'title': 'Title ', 'text': 'Exploring Future Spaces with TensorFlowText '}, {'title': 'TensorFlow: Capabilities and Potential for Custom Clusters ', 'text': 'TensorFlow has the execution engine and key backends for CPUs and GPUs. It has the capability to do distributed computing. All these components work together in a single library or binary. There are some interfaces for splitting them apart, but they are not very clean. Clean interfaces for running on custom clusters with custom networking would be ideal. The clean separation of components would help in scaling and enable independent evolution and innovation. This would benefit individual developers and organizations involved in the ecosystem.'}, {'title': 'TensorFlow: A Tool for Independent Scaling and Corporate Adoption ', 'text': 'TensorFlow allows for independent scaling. Major corporations like Pepsi are already using TensorFlow. Some companies, like hardware vendors and IBM, are involved in special interest groups for TensorFlow. Autonomous vehicle companies are also interested in TensorFlow.'}, {'title': 'The Growth and Impact of TensorFlow ', 'text': 'TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. There is a focus on optimizing for the needs of special interest groups, such as autonomous vehicle companies. The growth of TensorFlow is not just due to it being a good tool, but also because it meets the needs of users.'}, {'title': 'Importance of Timing, Growth, and Community Engagement in Open Source Projects ', 'text': 'Timing and growth are important factors in the development of tools like TensorFlow. Listening to the community and being open to external contributions is crucial for the success of an open source project. Transparency and communication with the community are essential for the success of an open source project.'}, {'title': 'The Importance of Community in Project Development ', 'text': 'Community aspects play a significant role in the development process. Small projects may find it easier to implement processes and documentation. The use of tools and documentation becomes more important as a project grows. The growth of TensorFlow is fueled by people building and sharing projects on GitHub.'}, {'title': 'Transitioning to Newer Versions of TensorFlow ', 'text': \"There may be a partitioning like there is with Python 2 and 3, with older versions of TensorFlow not being as compatible easily. Working hard to make the transition to newer versions very easy. Lots of tooling discussed at the developer summit and continued investment in that tooling. Pushing hard to make the transition to newer versions very smooth. People want to move to the new thing because they see the value, not just because it's new. Expecting to see a shift towards the new version as people start to see the value.\"}, {'title': 'The Future of Deep Learning ', 'text': 'Most people want a really good thing. The value of the field will be seen over the next few months. The field is moving rapidly, which will help in doing more things. New things will clearly happen in 2.x, giving people good reasons to move. Change is expected to happen, but some basics of deep learning, like convolution models, will probably stick around in some form.'}, {'title': 'The Future of Deep Learning and TensorFlow ', 'text': 'The basics of deep learning, such as convolution models, will likely still be around in some form in five years. Reinforcement learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current progress. New developments in the field are hard to predict, but there is potential for new advancements. Combining eager execution and graphs to make programming more natural is a promising direction for the future. Swift for TensorFlow is taking a ground-up approach, which seems to be the right direction for the future. The future of hardware accelerators and the ability to train with four bits instead of. is uncertain.'}, {'title': 'Advancements in Hardware Accelerators and TensorFlow Evolution ', 'text': 'In five years, there is an expectation to see more in the area of hardware accelerators and training with four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. Efforts have been made to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to take some image model without caring about the specific type.'}, {'title': 'Importance of Simple Image Models for Beginners ', 'text': \"['Beginners want to be able to use simple image models for training or transfer learning.', 'Providing simple models in hub or similar platforms is important for beginners.', 'Different levels of users have different needs, from simple pre-trained models to custom layers and loops.', 'Pre-trained models decrease the time needed to start training.']\"}, {'title': 'The Impact of TensorFlow on Beginners and Professionals ', 'text': 'Providing pre-trained models decreases the time to start and achieve what is needed. TensorFlow has delivered trivial tools for beginners. High schoolers are doing amazing things with TensorFlow, which is both amazing and terrifying. There is a technical and management aspect to working with TensorFlow, leading a large number of developers and people.'}, {'title': 'The Importance of Team Cohesion and Collaboration in TensorFlow ', 'text': 'TensorFlow is a cutting edge technology. Cohesion across the team is important for delivering well. The product of what the team generates is larger than the individual contributions. Collaboration is essential for achieving more than what an individual can do.'}, {'title': 'The Importance of Team Culture and Vision ', 'text': \"The product of what the team generates is way larger than the whole or the individual put together. The culture of the team itself is important. Hiring good people is important. People have to care about what they're building and be motivated for the right kind of things. Having a somewhat unified vision of where the team wants to go is important. Google is a very bottom up organization in some sense, also research even more so. It's important to combine the bottom-up approach with a mix of direction.\"}, {'title': 'The Importance of Research and Individual Superstars in the Work Environment ', 'text': \"Research is important for the product and ecosystem. It's important to have a clear direction and not be all over the place. There are tensions and complexities in the work environment. Individual superstars play a significant role in the work, but can also create tensions within a team. The mission of superstars is important, even at Google.\"}, {'title': 'Challenges of Superstars in Team Dynamics ', 'text': 'Superstars can sometimes be against the dynamic of a team and cause tensions. The mission of the project in TensorFlow is beautiful, making it easier to work with people challenges. Google values getting people who care and have the same culture. The project allows for lots of people to do different things and grow, making the problem easier. It is important for superstars to work well with the team.'}, {'title': 'The Hiring Process and Productivity at Google ', 'text': 'Productivity at Google is about the team, not just individual superstars. Hiring process at Google has been refined over the last 20 years. Core technical skills are important, but motivation also matters in hiring engineers at Google.'}, {'title': 'The Importance of Motivation in the Hiring Process ', 'text': \"The hiring process has been helpful in maintaining consistency. Motivation is important in addition to technical skills for long term success. Alignment of motivation with the team's goals is crucial for success. Motivation is important at every level, not just for senior positions.\"}, {'title': 'Importance of Puzzle Solving and Culture Fit in the Hiring Process ', 'text': 'Puzzle solving ability and problem solving ability are important skills. The determining factor for hiring is whether the person has a strong internal drive and passion for the work. Culture fit is a significant part of the interview process at Google. Different projects and teams may have varying cultures and requirements. Technical skills are important, but culture fit is also considered in the hiring process.'}, {'title': 'Understanding the Culture and Development Pace at Google ', 'text': 'There are various kinds of projects and different kinds of things. TensorFlow has always been a fast moving project and requires people who are comfortable with that. Balancing the need for fast-moving development with the need for a full-fledged product is important. Finding the right fit for different projects and teams is important. The core culture at Google includes engineering excellence and a fun environment. There are differences in culture across projects, teams, and product areas at Google.'}, {'title': 'Striking a Balance in Engineering Excellence and Problem Solving ', 'text': 'Engineering excellence and problem solving are fun and hard at the same time. Striking a balance across different aspects is key to success. Making decisions about speed vs perfection, community involvement, and saying no to certain things are hard but important. Quick decisions and thoughtful decisions both have their challenges.'}, {'title': 'Making Hard Decisions and Balancing Deadlines ', 'text': 'Making hard decisions quickly due to lack of time. Making hard decisions with time to think about them. The Dev Summit came together incredibly. Balancing the value of deadlines and urgency with getting the right things together. Focusing on getting something good and works well rather than perfect. The team did a great job in putting things together.'}, {'title': 'Importance of Prioritizing and Iterating in Development ', 'text': \"Urgency to get the right things together. Focus on key things that are important and figure out how much of it's important. Developing in the open, both internally and externally, everything's available to everybody. Regular releases at a regular cadence. Iterating and improving on things, even if they aren't fully ready.\"}, {'title': 'Importance of Quick Iteration and Improvement in TensorFlow 2.0 Release ', 'text': \"Quick iteration and improvement is important. It's okay to release experimental versions for feedback. Pressure to make TensorFlow 2.0 stable. Comparison with WordPress 5.0 updates. Stability of TensorFlow 2.0 release.\"}, {'title': 'Status of NodeX Development ', 'text': 'The release candidate and final version will be stable. Every API in NodeX will remain in work. Changes can still be made under the covers and new things can be added. There is still more work to be done and more releases to come. The product is aimed to be great. TensorFlow already has 41 million downloads for 1.0 X. Many features are being polished and put in place.'}, {'title': 'App Development Update ', 'text': 'The app has 41 million downloads for 1.0 X. The focus is on polishing and putting together features. The release is planned for the next few months or next quarter. Spring is considered a relative concept in terms of timing. The speaker previously led a team at Google on search ads. Ads can connect people to what they want and need, but at their worst, they can be annoying.'}, {'title': 'The Power of Ads and Machine Learning in Personalized User Experience ', 'text': 'Ads can connect people to the things they want and need, but at their worst, they can be annoying and ruin the user experience. Machine learning has the opportunity to shine in connecting users to personalized data and mapping to their actual wants and needs. Search ads are an extension of what search is trying to do, which is to make information and make the user experience better.'}, {'title': 'The Importance of Quality Search Ads ', 'text': \"Search ads are an extension of what search is trying to do, which is to make information and the world's information accessible. It's important for search ads to align with what the users need. There is a minimum quality level for search ads before they are shown. Advertising is a key part and has been adapted to the web and became a core part of search and many other search engines across the world.\"}, {'title': 'The Importance of Ad Monetization on the Web ', 'text': 'The ad model has been adapted to the web and is a core part of search engines. There is a need to strike a balance between showing valuable ads and annoying ads. Monetization is necessary for services like search and websites to continue providing their service. The ad model is a significant revenue stream for businesses like Google. The internet is limited by the fact that nobody wants to pay for content.'}, {'title': 'The Evolution of Revenue Streams in the Internet Age ', 'text': 'The model funds businesses like Google, which is a significant revenue stream. The internet has a mix of paid and free services, with a transition towards more paid services in recent years. Advertisements, when done well, can be useful and not annoying. People are willing to pay for services on the internet if they see the value, as seen with examples like Netflix and paid apps.'}, {'title': 'Shifting Trends in Content Consumption ', 'text': 'People are willing to pay for content they see value in. Examples like Netflix and YouTube show that people are willing to pay for content. More people are willing to pay for newspaper and news website content. There is a shift towards a mixed model where content can be tried for free with ads, but also has a clear revenue model.'}, {'title': 'Empowering Students with TensorFlow Open Source ', 'text': 'The ability to run TensorFlow on desktops, which are constantly becoming more powerful. The increasing power of smartphones, which can also be used for training. The ease of getting started with Cloud platforms like Colab, without the need for installation.'}, {'title': 'Getting Started with Machine Learning and TensorFlow using Colab ', 'text': 'Colab makes it easy to get started with machine learning and TensorFlow. No installation is needed to use Colab, making it very convenient. Colab is a free service, making it great for beginners to play and explore. However, with free services, there are limitations on what you can do. Beginners should start by visiting the TensorFlow website and exploring tutorials and guides. Colab allows users to get started with machine learning without any installation needed.'}, {'title': 'No Installation Needed for Instant Start ', 'text': 'No installation needed, you can get started right there. Rajit is thanked for talking today. Lex is thanked for the conversation.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 16\n",
      "get topics 2024-03-24 09:25:39.037357 ...\n",
      "Best SD: inf, Best iteration: 0\n",
      "done get topics 2024-03-24 09:25:40.281424.\n",
      "Stage 2 start time 2024-03-24 09:25:40.281444\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow and Open Source Innovation in the Tech Industry\n",
      "2. Google's Contributions to Bigtable, TensorFlow, and Software Evolution\n",
      "3. Machine Learning Algorithms on Mobile Phones and Transitioning to TensorFlow 2.0\n",
      "4. The Impact of Open Sourcing on Deep Learning Projects and Enterprise Adoption\n",
      "5. Advancements in AI Technology, Data Analysis Methods, and the Rise of Machine Learning in the Legal Field\n",
      "6. The Evolution and Integration of Keras with TensorFlow, Open Source Projects and Key Figures\n",
      "7. Advancements in Machine Learning Research, Integration, and Community Growth\n",
      "8. Exploring TensorFlow Libraries, Community Collaboration, and Challenges in Scaling\n",
      "9. Challenges and Importance of TensorFlow in Research and Production, Comparison with PyTorch\n",
      "10. Exploring Different Spaces, Eager Execution, and Clean APIs in TensorFlow 2.0\n",
      "11. TensorFlow: Capabilities, Growth, and Importance of Community Engagement\n",
      "12. Transitioning to Newer Versions of TensorFlow, Future of Deep Learning, and Impact on Beginners and Professionals\n",
      "13. Importance of Hiring Process, Team Culture, and Productivity at Google\n",
      "14. Importance of Quick Iteration and NodeX Development Status\n",
      "15. App Development, Ads, Machine Learning, and Shifting Trends in Content Consumption\n",
      "16. Instant Start with TensorFlow Open Source, Empowering Students with Colab for Machine Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "Rajat Manga, director of Google's TensorFlow team, discusses the open source library's evolution and impact on deep learning in a podcast. He highlights the decision to open source TensorFlow as a pivotal moment in the tech industry, inspiring other companies to do the same. The conversation delves into the early days of Google Brain, the mission to scale technology, and the integration of machine learning into real products. The push for open innovation and the decision to share research have contributed to the rapid growth of deep learning and machine learning. The podcast also explores the development of TensorFlow 2.0 and the emphasis on building a passionate community of developers. Overall, the discussion provides insight into the significance of open sourcing TensorFlow and its role in advancing the field of artificial intelligence.\n",
      "\n",
      "The interviewee, who leads the TensorFlow effort, talks about the history and timeline of the project, from its development in summer 2014 to its open sourcing in November 2015. The decision to open source TensorFlow was made in late 2014, and the speaker highlights the fast pace of development in deep learning. The interviewee also emphasizes the incredible ecosystem surrounding TensorFlow and Google Cloud's integrations with the library. Overall, the podcast provides insight into Google's commitment to pushing forward a good standard in the community and the success of their open source projects.\n",
      "\n",
      "The podcast discusses the use of machine learning algorithms on mobile phones, influenced by the use of Theano and Caffe at Google. The design decisions for building machine learning models for mobile phones were driven by limitations in prior systems and the fast pace of research. The team considered whether to have a graph in their design, with key decisions including flexibility in research and hardware changes. Moving towards TensorFlow 2.0 and eager execution were important graph decisions, influenced by the need for flexibility in expressing various ideas and a more intuitive development process.\n",
      "\n",
      "The podcast discusses the impact of open sourcing a project, which led to a surge in global developer attention and a shift towards community-driven development. The project's focus shifted from research to practical use, with an emphasis on documentation, stability, and deployment for users. The release of version 1.0 attracted enterprise support, and there was a continued focus on attracting more enterprise involvement post-release. The project unexpectedly gained 41 million downloads, highlighting its popularity and potential for future growth. The podcast also explores the initial interest from researchers and early adopters, as well as the importance of understanding enterprise needs during product development. Overall, open sourcing the project led to a significant increase in attention and interest from developers worldwide, enabling more people to leverage the growth of deep learning.\n",
      "\n",
      "The podcast discusses the integration of Keras into TensorFlow, making it more accessible for beginners. Francois started the Keras project before joining Google and eventually integrated it with TensorFlow. Tiano, who initially worked on research and Keras as a side project, joined the TensorFlow team and played a key role in creating the interface for TensorFlow. The decision to focus on Keras was based on user preference and community feedback, with the goal of simplifying and unifying the API choices. The integration of Keras into TensorFlow was a result of careful consideration and evaluation of various APIs, ultimately empowering developers and making the process easier.\n",
      "\n",
      "TensorFlow.js allows for training and networking in the browser using JavaScript, making it a serious and legitimate tool for machine learning in both the backend and front end. It is part of the convergence towards saving models in a consistent way, allowing for training on the desktop and then moving models to mobile. The goal is to enable machine learning in multiple ways, including support for deep learning and other exciting developments in ML. The ecosystem for machine learning has grown to cover more devices over time, with a focus on pushing boundaries and building more tooling to help with machine learning. Efforts have been made to open up to the community and add transparency, with processes such as RFCs and special interest groups being set in place to grow the community and scale it. TensorFlow.js is making training neural networks in the browser a serious and legitimate thing, with a focus on pushing the state of the art in research and integrating it into real products to have a real impact on people.\n",
      "\n",
      "TensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended. There are lots of libraries being built on top of TensorFlow for research and production purposes. Some libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production. Libraries have come from within Google and from the community to address different needs and goals. The goal is to enable the community to build and use different pieces of technology. The focus is on making different pieces work well together in TensorFlow 2.0. The project of integrating TensorFlow.js into the ecosystem was technically very difficult. There are still many challenges to be overcome in the technical side. The team has iterated over the last few years and learned a lot. Despite the challenges, the goal is to make it easy for the end user. Challenges ahead include the increasing number of devices and the need for improvements in APIs and compiler tools. TensorFlow started as a monolithic system and scaling it out requires breaking it apart with clearer interfaces.\n",
      "\n",
      "The podcast discusses the challenges of maintaining compatibility with previous versions of TensorFlow while innovating and making new changes. It emphasizes the importance of designing with a clean slate in mind and making compromises when necessary. The speaker believes that TensorFlow is leading in many ways and wishes everyone would use it, but acknowledges the competition with PyTorch as a way to improve their own platform. The differences between TensorFlow and PyTorch are highlighted, with TensorFlow focusing on both research and production, and PyTorch prioritizing ease of use over speed. Both platforms have learned from each other's approaches and made improvements based on previous technologies. The speaker recently switched their research group to TensorFlow and sees it as the closest to the ideal platform. Overall, the podcast emphasizes the importance of responsibility in the idea stage and the value of bringing in new changes, even if it may slow certain things down.\n",
      "\n",
      "The podcast discusses the development and improvements of TensorFlow, particularly the release of TensorFlow 2.0. The team had the advantage of learning from previous versions and exploring different spaces, ultimately leading to the significant development of TensorFlow 2.0. They faced competition and revisited the area multiple times before pushing on eager execution, which finally came together after time and effort. The ecosystem has been improved, making it easily accessible to Keras and eager execution. TensorFlow 2.0 enables clean APIs, allowing for performance and optimization for users. It also enables exploration of other spaces behind the scenes in the future, as well as both single machine and distributed operations. The release of TensorFlow 2.0 has cleaned up the surface for what users want and allows for a lot of things to be done behind the scenes once ready with 2.0. Overall, the podcast highlights the exciting developments and possibilities that TensorFlow 2.0 brings to the table.\n",
      "\n",
      "TensorFlow is a powerful tool for distributed computing, with clean interfaces for running on custom clusters. It allows for independent scaling and has been downloaded 41 million times, with major corporations and autonomous vehicle companies already using it. The growth of TensorFlow is attributed to its ability to meet the needs of users and its focus on optimizing for special interest groups. Transparency, communication, and community involvement are crucial for the success of open source projects like TensorFlow. As the project grows, the use of tools and documentation becomes more important. The growth of TensorFlow is fueled by people building and sharing projects on GitHub.\n",
      "\n",
      "The podcast discusses the potential partitioning of older versions of TensorFlow, similar to Python 2 and 3, and the efforts being made to make the transition to newer versions smooth. It emphasizes the value of the new versions and the expectation of a shift towards them in the near future. The basics of deep learning, such as convolution models, are expected to remain relevant, while reinforcement learning and generative adversarial networks are likely to stay based on their progress. The future of hardware accelerators and training with four bits instead of 32 bits is uncertain, but there is potential for advancements in these areas. The podcast also highlights the efforts to make TensorFlow accessible and easy to use for beginners, with a focus on providing pre-trained models to decrease the time needed to start training. It also mentions the technical and management aspects of working with TensorFlow, and the impact it is having on developers and people in the field.\n",
      "\n",
      "The podcast discusses the hiring process at Google, emphasizing the importance of motivation and culture fit in addition to technical skills. It highlights the need for alignment of motivation with team goals, puzzle solving and problem-solving abilities, and a strong internal drive and passion for the work. The podcast also touches on the different cultures and requirements of various projects and teams at Google, as well as the balance between fast-moving development and a full-fledged product. It emphasizes the importance of engineering excellence, collaboration, and a unified vision within the team. The podcast also addresses the challenges of balancing speed and perfection, making hard decisions, and the role of individual superstars in the team dynamic. Overall, it emphasizes the value of hiring people who care about their work and have a unified vision, and the importance of the team in productivity at Google.\n",
      "\n",
      "The podcast discusses the importance of quick iteration and improvement, emphasizing the value of releasing experimental versions for feedback. There is pressure to make TensorFlow 2.0 stable, with a comparison to WordPress 5.0 updates. The stability of the TensorFlow 2.0 release is highlighted, with the assurance that the release candidate and final version will be stable. It is noted that every API in NodeX will remain in work, allowing for changes to be made and new features to be added. The podcast also mentions that there is still more work to be done and more releases to come, with the aim of creating a great product. It is noted that TensorFlow has already had 41 million downloads for 1.0 X, and that many features are being polished and put in place.\n",
      "\n",
      "The podcast discusses the importance of balancing valuable and annoying ads in the online world. The speaker, who previously led a team at Google on search ads, emphasizes the need for ads to connect users to personalized data and align with their needs. The ad model is a significant revenue stream for businesses like Google, as the internet has a mix of paid and free services. The speaker highlights the shift towards a mixed model where content can be tried for free with ads, but also has a clear revenue model. Additionally, the podcast emphasizes the willingness of people to pay for content they see value in, as seen with examples like Netflix and YouTube. Overall, the focus is on the importance of ads in connecting people to what they want and need, while also ensuring a positive user experience.\n",
      "\n",
      "The podcast discusses the ease of getting started with machine learning and TensorFlow, highlighting the ability to run TensorFlow on powerful desktops and smartphones without the need for installation. The conversation thanks Rajit for discussing the topic and Lex for the conversation. It emphasizes the convenience of using Cloud platforms like Colab, which is free and requires no installation. However, it also mentions the limitations of free services and recommends beginners to visit the TensorFlow website for tutorials and guides. Overall, the podcast emphasizes the accessibility and convenience of using Colab to get started with machine learning without the need for installation.\n",
      "RRR rewritten summary\n",
      "[{'text': \"Rajat Manga, director of Google's TensorFlow team, shares insights into the evolution and impact of the open source library on deep learning. The decision to open source TensorFlow is highlighted as a pivotal moment in the tech industry, inspiring other companies to do the same. The conversation delves into the early days of Google Brain, the mission to scale technology, and the integration of machine learning into real products. The push for open innovation and the decision to share research have contributed to the rapid growth of deep learning and machine learning. The discussion also explores the development of TensorFlow 2.0 and the emphasis on building a passionate community of developers. Overall, the significance of open sourcing TensorFlow and its role in advancing the field of artificial intelligence is emphasized.\\n\\nThe interviewee, who leads the TensorFlow effort, talks about the history and timeline of the project, from its development in summer 2014 to its open sourcing in November 2015. The decision to open source TensorFlow was made in late 2014, and the speaker highlights the fast pace of development in deep learning. The interviewee also emphasizes the incredible ecosystem surrounding TensorFlow and Google Cloud's integrations with the library. The podcast provides insight into Google's commitment to pushing forward a good standard in the community and the success of their open source projects.\\n\\nThe podcast discusses the use of machine learning algorithms on mobile phones, influenced by the use of Theano and Caffe at Google. The design decisions for building machine learning models for mobile phones were driven by limitations in prior systems and the fast pace of research. The team considered whether to have a graph in their design, with key decisions including flexibility in research and hardware changes. Moving towards TensorFlow 2.0 and eager execution were important graph decisions, influenced by the need for flexibility in expressing various ideas and a more intuitive development process.\\n\\nThe podcast discusses the impact of open sourcing a project, which led to a surge in global developer attention and a shift towards community-driven development. The project's focus shifted from research to practical use, with an emphasis on documentation, stability, and deployment for users. The release of version 1.0 attracted enterprise support, and there was a continued focus on attracting more enterprise involvement post-release. The project unexpectedly gained 41 million downloads, highlighting its popularity and potential for future growth. The podcast also explores the initial interest from researchers and early adopters, as well as the importance of understanding enterprise needs during product development.\\n\\nThe podcast discusses the integration of Keras into TensorFlow, making it more accessible for beginners. Francois started the Keras project before joining Google and eventually integrated it with TensorFlow. Tiano, who initially worked on research and Keras as a side project, joined the TensorFlow team and played a key role in creating the interface for TensorFlow. The decision to focus on Keras was based on user preference and community feedback, with the goal of simplifying and unifying the API choices. The integration of Keras into TensorFlow was a result of careful consideration and evaluation of various APIs, ultimately empowering developers and making the process easier.\\n\\nTensorFlow.js allows for training and networking in the browser using JavaScript, making it a serious and legitimate tool for machine learning in both the backend and front end. It is part of the convergence towards saving models in a consistent way, allowing for training on the desktop and then moving models to mobile. The goal is to enable machine learning in multiple ways, including support for deep learning and other exciting developments in ML. The ecosystem for machine learning has grown to cover more devices over time, with a focus on pushing boundaries and building more tooling to help with machine learning. Efforts have been made to open up to the community and add transparency, with processes such as RFCs and special interest groups being set in place to grow the community and scale it. TensorFlow.js is making training neural networks in the browser a serious and legitimate thing, with a focus on pushing the state of the art in research and integrating it into real products to have a real impact on people.\\n\\nTensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended. There are lots of libraries being built on top of TensorFlow for research and production purposes. Some libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production. Libraries have come from within Google and from the community to address different needs and goals. The goal is to enable the community to build and use different pieces of technology. The focus is on making different pieces work well together in TensorFlow 2.0. The project of integrating TensorFlow.js into the ecosystem was technically very difficult. There are still many challenges to be overcome in the technical side. The team has iterated over the last few years and learned a lot. Despite the challenges, the goal is to make it easy for the end user. Challenges ahead include the increasing number of devices and the need for improvements in APIs and compiler tools. TensorFlow started as a monolithic system and scaling it out requires breaking it apart with clearer interfaces.\\n\\nThe podcast discusses the challenges of maintaining compatibility with previous versions of TensorFlow while innovating and making new changes. It emphasizes the importance of designing with a clean slate in mind and making compromises when necessary. The speaker believes that TensorFlow is leading in many ways and wishes everyone would use it, but acknowledges the competition with PyTorch as a way to improve their own platform. The differences between TensorFlow and PyTorch are highlighted, with TensorFlow focusing on both research and production, and PyTorch prioritizing ease of use over speed. Both platforms have learned from each other's approaches and made improvements based on previous technologies. The speaker recently switched their research group to TensorFlow and sees it as the closest to the ideal platform. Overall, the podcast emphasizes the importance of responsibility in the idea stage and the value of bringing in new changes, even if it may slow certain things down.\\n\\nThe podcast discusses the development and improvements of TensorFlow, particularly the release of TensorFlow 2.0. The team had the advantage of learning from previous versions and exploring different spaces, ultimately leading to the significant development of TensorFlow 2.0. They faced competition and revisited the area multiple times before pushing on eager execution, which finally came together after time and effort. The ecosystem has been improved, making it easily accessible to Keras and eager execution. TensorFlow 2.0 enables clean APIs, allowing for performance and optimization for users. It also enables exploration of other spaces behind the scenes in the future, as well as both single machine and distributed operations. The release of TensorFlow 2.0 has cleaned up the surface for what users want and allows for a lot of things to be done behind the scenes once ready with 2.0. Overall, the podcast highlights the exciting developments and possibilities that TensorFlow 2.0 brings to the table.\\n\\nTensorFlow is a powerful tool for distributed computing, with clean interfaces for running on custom clusters. It allows for independent scaling and has been downloaded 41 million times, with major corporations and autonomous vehicle companies already using it. The growth of TensorFlow is attributed to its ability to meet the needs of users and its focus on optimizing for special interest groups. Transparency, communication, and community involvement are crucial for the success of open source projects like TensorFlow. As the project grows, the use of tools and documentation becomes more important. The growth of TensorFlow is fueled by people building and sharing projects on GitHub.\\n\\nThe podcast discusses the potential partitioning of older versions of TensorFlow, similar to Python 2 and 3, and the efforts being made to make the transition to newer versions smooth. It emphasizes the value of the new versions and the expectation of a shift towards them in the near future. The basics of deep learning, such as convolution models, are expected to remain relevant, while reinforcement learning and generative adversarial networks are likely to stay based on their progress. The future of hardware accelerators and training with four bits instead of 32 bits is uncertain, but there is potential for advancements in these areas. The podcast also highlights the efforts to make TensorFlow accessible and easy to use for beginners, with a focus on providing pre-trained models to decrease the time needed to start training. It also mentions the technical and management aspects of working with TensorFlow, and the impact it is having on developers and people in the field.\\n\\nThe podcast discusses the hiring process at Google, emphasizing the importance of motivation and culture fit in addition to technical skills. It highlights the need for alignment of motivation with team goals, puzzle solving and problem-solving abilities, and a strong internal drive and passion for the work. The podcast also touches on the different cultures and requirements of various projects and teams at Google, as well as the balance between fast-moving development and a full-fledged product. It emphasizes the importance of engineering excellence, collaboration, and a unified vision within the team. The podcast also addresses the challenges of balancing speed and perfection, making hard decisions, and the role of individual superstars in the team dynamic. Overall, it emphasizes the value of hiring people who care about their work and have a unified vision, and the importance of the team in productivity at Google.\\n\\nThe podcast discusses the importance of quick iteration and improvement, emphasizing the value of releasing experimental versions for feedback. There is pressure to make TensorFlow 2.0 stable, with a comparison to WordPress 5.0 updates. The stability of the TensorFlow 2.0 release is highlighted, with the assurance that the release candidate and final version will be stable. It is noted that every API in NodeX will remain in work, allowing for changes to be made and new features to be added. The podcast also mentions that there is still more work to be done and more releases to come, with the aim of creating a great product. It is noted that TensorFlow has already had 41 million downloads for 1.0 X, and that many features are being polished and put in place.\\n\\nThe podcast discusses the importance of balancing valuable and annoying ads in the online world. The speaker, who previously led a team at Google on search ads, emphasizes the need for ads to connect users to personalized data and align with their needs. The ad model is a significant revenue stream for businesses like Google, as the internet has a mix of paid and free services. The speaker highlights the shift towards a mixed model where content can be tried for free with ads, but also has a clear revenue model. Additionally, the podcast emphasizes the willingness of people to pay for content they see value in, as seen with examples like Netflix and YouTube. Overall, the focus is on the importance of ads in connecting people to what they want and need, while also ensuring a positive user experience.\\n\\nThe podcast discusses the ease of getting started with machine learning and TensorFlow, highlighting the ability to run TensorFlow on powerful desktops and smartphones without the need for installation. The conversation thanks Rajit for discussing the topic and Lex for the conversation. It emphasizes the convenience of using Cloud platforms like Colab, which is free and requires no installation. However, it also mentions the limitations of free services and recommends beginners to visit the TensorFlow website for tutorials and guides. Overall, the podcast emphasizes the accessibility and convenience of using Colab to get started with machine learning without the need for installation.\"}]\n",
      "Stage 2 done time 2024-03-24 09:28:36.058867\n",
      "stage_2_titles: len: 16\n",
      "['1. The Impact of TensorFlow and Open Source Innovation in the Tech Industry', \"2. Google's Contributions to Bigtable, TensorFlow, and Software Evolution\", '3. Machine Learning Algorithms on Mobile Phones and Transitioning to TensorFlow 2.0', '4. The Impact of Open Sourcing on Deep Learning Projects and Enterprise Adoption', '5. Advancements in AI Technology, Data Analysis Methods, and the Rise of Machine Learning in the Legal Field', '6. The Evolution and Integration of Keras with TensorFlow, Open Source Projects and Key Figures', '7. Advancements in Machine Learning Research, Integration, and Community Growth', '8. Exploring TensorFlow Libraries, Community Collaboration, and Challenges in Scaling', '9. Challenges and Importance of TensorFlow in Research and Production, Comparison with PyTorch', '10. Exploring Different Spaces, Eager Execution, and Clean APIs in TensorFlow 2.0', '11. TensorFlow: Capabilities, Growth, and Importance of Community Engagement', '12. Transitioning to Newer Versions of TensorFlow, Future of Deep Learning, and Impact on Beginners and Professionals', '13. Importance of Hiring Process, Team Culture, and Productivity at Google', '14. Importance of Quick Iteration and NodeX Development Status', '15. App Development, Ads, Machine Learning, and Shifting Trends in Content Consumption', '16. Instant Start with TensorFlow Open Source, Empowering Students with Colab for Machine Learning']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), MAX_NUM_TOPICS)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_v3_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
