{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b4\" # no rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "# REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fae39bc2810>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "#   final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-28 18:52:46.709628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 18:57:03.786128\n",
      "chunks_text len: 66\n",
      "extract_keypoints start time: 2024-03-28 18:57:03.786266\n",
      "extract_keypoints done time 2024-03-28 18:59:28.848853\n",
      "Start time: 2024-03-28 18:59:28.849037\n",
      "Stage 1 done time 2024-03-28 19:02:01.632827\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Thomas Sanholm: Professor, AI Expert, and Game Theory Researcher ', 'text': 'Thomas Sanholm is a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in the game of Heads Up No Limit Texas Holdem. He has published over 450 papers on game theory and machine learning, including a best paper in 2017 at NIPS, now renamed to Newrips. His research and companies have had wide-reaching impact in the real world, proposing new ideas and building systems to prove that these ideas work in the real world. This conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast.'}, {'title': 'Heads Up No Limit Texas Holdem: A Benchmark for AI Algorithms ', 'text': \"['Heads Up No Limit Texas Holdem is a main benchmark for testing AI algorithms for imperfect information game solving.', 'It is a game played by humans, but not often seen on TV or in casinos.', 'It is played in expert level casinos and in the World Series of Poker.', 'It is typically played online for large sums of money.', 'It is a game usually only played by experts.', 'It is different from regular No Limit Texas Holdem and is more competitive.']\"}, {'title': 'Differences Between Texas Holdem and Heads Up Poker ', 'text': 'Texas Holdem is a game typically played by a big group and is not as competitive. Heads Up is a game played by two players, making it more competitive. Texas Holdem is an imperfect information game, making it harder to play. In Texas Holdem, players have two private cards and gradually lay out five public cards. The imperfect nature of the game comes from the private cards held by each player. The game involves multiple rounds of betting and receiving public cards.'}, {'title': 'Popular Game with Imperfect Information ', 'text': 'The game being described involves four betting rounds and four tranches of information revelation. The first tranche is private, and the rest are public. This game is popular in AI and among the general public due to its imperfect information nature. It is considered one of the most popular spectator games to watch. The game is compared to chess in terms of popularity and as a benchmark for intelligence in AI. In 2017, a program called Labratus beat four expert human players in this game. The process and lessons learned from this event are of interest.'}, {'title': 'High Stakes Poker Tournament for Statistical Significance ', 'text': 'The event involved inviting four of the top 10 players in Heads Up No Limit, Texas Holdem. The game is different from the multiplayer version, so statistical significance was important. The players were brought to Pittsburgh to play at the Rivers Casino for 20 days. The goal was to get 120,000 hands in to achieve statistical significance. The players played from morning to evening for 20 days. A total of 200,000 was raised as an incentive for the players to participate. The players were paid based on how they performed against the AI. This setup provided an incentive for the players to play as hard as possible.'}, {'title': 'Incentive to Play Against AI in Pennsylvania Gaming ', 'text': 'Players had an incentive to play as hard as they could against the AI. They were not able to make any money from playing. The idea of playing for money was explored but rejected by the Pennsylvania Gaming Board. The players kept track of the money and brought in close to $2 million. If they were able to earn money, it would have been an impressive and inspiring achievement.'}, {'title': 'The Use of UI in Online Gaming ', 'text': \"The top players are used to playing the game mostly online through a UI. The game was played with a layout on a screen, with the human and AI sitting at a virtual table. The screen showed the cards and bets, as well as the betting history for the human player. The human player could reference the betting history if they forgot what had happened in the hand. The AI was not trying to take advantage of the human's memory.\"}, {'title': 'AI Libratus Beats Humans in Brains vs AI Competition ', 'text': 'The AI, Libratus, was able to achieve an incredible accomplishment by beating humans in a brains versus AI competition. The previous AI, Cloudyco, was unable to beat humans in a similar competition organized 18 months earlier. Despite the belief in the strength of the new AI, there was uncertainty about its performance against top humans. People tend to have more confidence in other people compared to the performance of AI, as evidenced by the betting odds favoring humans over AI in the competition.'}, {'title': 'Underestimating AI in Poker ', 'text': 'People underestimated the performance of AI in poker. Despite winning against humans, AI was still considered an underdog in betting sites. There is a belief that human facial expressions and body language are critical to poker. People have confidence that humans will outperform AI because AI cannot perceive human tells. AI systems only look at betting patterns and statistics, not human tells. The importance of human perception in poker is questioned.'}, {'title': 'The Role of Human Players in Betting Patterns and Statistics ', 'text': 'The importance of human players in betting patterns and statistics. The skepticism about AI beating top human players. The difficulty in finding tells among top players due to their skill in hiding them.'}, {'title': 'The Importance of Tells and Abstraction in Poker ', 'text': 'Tells are important in poker, especially at lower levels of play. As the game becomes more complex at higher levels, tells become less significant. The amount of strategies and possible actions in poker is very large. Abstraction is necessary in order to solve the game tree, especially in an imperfect information game. Abstraction in games is more challenging than in other types of games.'}, {'title': 'Abstraction in Games and its Impact on Strategy ', 'text': 'Abstraction in games is trickier than in MDPs or other single agent settings. Abstraction pathologies can lead to worse strategies with finer grained abstraction. There are different kinds of abstractions in games, such as hands abstractions and betting strategies. Information abstraction involves abstracting what chance does, such as the cards in the case of poker. Action abstraction involves abstracting the actions of the actual players, such as bets in the case of poker.'}, {'title': 'Automated Action Abstraction Technology in Gaming ', 'text': 'The algorithms used for information abstraction are potential aware and consider how the hand might materialize over time. The action abstraction is based on how humans and other AIs have played the game in the past. Automated action abstraction technology was initially used, but it is not very scalable. The strength of the hand and how it is played are both important factors in the game.'}, {'title': 'The Importance of Hand Strength and Information Abstraction in Poker ', 'text': 'The strength of the hand and the information abstraction are important in playing poker. The betting actions may be the key to winning regardless of the hands you have. Playing a lot of hands reduces the role of luck in the game. No Limit Texas Holdem has a high level of variance and massive swings. Statistical significance in poker requires playing over 100,000 hands.'}, {'title': 'The Use of Learning Methods in Poker Playing ', 'text': 'Annette Oberstad, a Norwegian female poker player, won a tournament by using a unique playing style. Labradus does not use deep learning methods, unlike DeepStack. The effectiveness of deep learning in poker playing is unclear. The discussion is about the use of learning methods to aid in the way Labradus plays poker.'}, {'title': 'The Importance of Learning Methods in Games ', 'text': \"Labradus did not use learning methods and played very well without them. There are papers on things that do use learning techniques, including deep learning. In imperfect information games like poker, the value of an information set depends not only on the exact state, but also on both players' beliefs. The value of a state in poker is not just a function of the cards, but also depends on the path of play and both players' beliefs.\"}, {'title': 'Understanding Game State and Strategy Evaluation ', 'text': \"The state of a game is not solely determined by the cards, but also by the path of play and belief distributions. Perfect information games have a straightforward concept of state and evaluation function, but in other games, such as poker, it is more complicated. In some research papers, the opponent is allowed to take different strategies at the leaf of the search tree, leading to a different approach in evaluating the game state. Allowing the opponent to choose from a set of different continuation strategies forces a more realistic and less optimistic look ahead search. Sound look ahead search is important in games where the opponent's strategy is not predetermined.\"}, {'title': 'Challenges in Look Ahead Search for Imperfect Information Games ', 'text': 'Look ahead search in imperfect information games is very difficult. Randomly generating various situations in the game and doing look ahead from there to the end of the game. Using deep learning to learn the values of states, including belief distributions. Similar techniques to alpha beta search or Monte Carlo tree search are used.'}, {'title': 'Title ', 'text': 'Different Search Algorithms and Approaches in Game PlayingText '}, {'title': 'Adjusting Beliefs in Game Theory ', 'text': 'Beliefs are actually output, not input. Starting beliefs are input, but they fall from the rules of the game. The dealer deals uniformly from the deck, making every pair of cards equally likely. Card removal is important in adjusting beliefs. Game theory helps in adjusting beliefs.'}, {'title': 'Understanding Nash Equilibrium ', 'text': \"Nash equilibrium was introduced by John Nash in 1950 and defines rational play when there are multiple players. It involves pairs of strategies for each player, where neither player wants to deviate given that the other doesn't deviate. Nash equilibrium also defines beliefs for both players and for each state in the game. It provides a probability distribution over the real world states in the mind of each player at each information set in the game.\"}, {'title': 'Understanding Probability Distribution and Game Theory in Decision Making ', 'text': \"The text discusses probability distribution over real world states in the mind. It explains the concept of player one and player two moves in a game. It emphasizes the importance of player two not knowing player one's move to avoid player two winning every time. It mentions the use of information sets for player one and player two. It talks about Nash equilibrium and the strategy for player one to play 1/3 Rock, 1/3 Paper, and 1/3 Scissors. It highlights the derivation of beliefs on the information set using Bayes' theorem. It mentions that Bayes' theorem is related to game theory.\"}, {'title': 'Understanding Game Theory and Opponent Modeling ', 'text': 'Game theory is not player specific and does not require any data or history of how specific players or AI have played in the past. It is based on rationality and considers what a rational opponent would do and what the player would do if they are rational. Game theory is a data-free and opponent-free approach, focusing on the design of the game rather than the design of the player. Opponent modeling is not a primary focus in game theory, although there has been some work on combining opponent modeling with game theory to exploit weak players. In the case of Librarus, opponent modeling was not turned on because the players were considered too good and exploiting opponents can open oneself up to exploitation.'}, {'title': 'Strategies and Exploitation in Two-Player Games ', 'text': \"Opening oneself up to exploitation by turning on certain strategies. The opponents are experts in counter exploitation. The decision not to turn on certain strategies. Interest in exploring papers exploiting opponents. Work done on exploiting opponents. Appreciation for the work at hybrid digested. Safety in two player zero sum games. The impact of opponent's irrational behavior on beliefs and gains/losses.\"}, {'title': 'Game Theory Strategies and Repeated Games ', 'text': \"The player can gain by throwing off the opponent's belief is always less than they lose by playing poorly. A game theoretic strategy is unbeatable, but it doesn't maximally beat the other opponent. The hybrid strategy involves starting from a game theoretic approach and then tweaking the strategy based on opponent data. Repeated games and the Prisoner's Dilemma are also discussed in the text.\"}, {'title': 'Understanding Repeated Games and Game Theory ', 'text': \"Prisoner's Dilemma is a repeated game. There is no proof that repeated games are the best strategy, but experimentally they do well. There are perfect information games and imperfect information games. Repeated games are played over and over. There are zero sum games and non zero sum games. There is a distinction between two player games and games with more players.\"}, {'title': 'Types of Games in Game Theory ', 'text': '[\\'Extensive form games involve repetitive interactions and incomplete information sets.\\', \\'Repeated games are a special case of extensive form games.\\', \\'Sourcing auctions involve repetitive interactions with variations in the supply base and what is being bought.\\', \"Purely repeated games are very rare in the world and are a coarse model of what\\'s going on.\", \\'Stochastic games are in between simple repeated matrix games and extensive form games, involving little matrix games and actions taken by opponents.\\']'}, {'title': 'Types of Games and Their Characteristics ', 'text': 'The text discusses different types of games, including matrix games, stochastic games, and extensive form games. It mentions that when taking an action in a game, it determines the distribution over next games where the player might be going to. The AI community has been working on and being benchmarked on Heads Up No Limit Texas Holdem, which is an example of extensive form games. It compares the tree form of games, like in chess, with the matrix form or bi matrix form or normal form game. The text also mentions the concept of reasoning in the tree form of games.'}, {'title': 'Importance of Tree Form in Game Theory ', 'text': 'Tree form allows for certain types of reasoning that are lost in normal form. Equivalence exists between tree form and normal form, but sequentiality is lost in the transition. Multiplayer versus two player distinction is important in game theory. Two player games in zero sum are conceptually and computationally easier. In two player games, any equilibrium strategy is a best response to any other equilibrium strategy.'}, {'title': 'Difference Between Two Player Zero Sum Games and Two Player General Sum Games ', 'text': 'The text discusses the difference between two player zero sum games and two player general sum games. It mentions that there is a big gap between the two and even to three player zero sum games. The concept of Nash equilibrium is introduced, where all finite games have a Nash equilibrium. The problem arises when there are multiple Nash equilibriums and the question of which one to select. In non zero sum games, there may be a loss of joint benefit by selecting strategies from different equilibriums.'}, {'title': 'The Impact of Collaboration in Non Zero Sum Games ', 'text': 'In non zero sum games, joint benefit may be lost by being simply stupid, and both parties could be better off by doing something else. In three player games, collusion can occur, where two players gang up on a third player to achieve better outcomes. Collaboration or cooperation between poker players can make the game extremely difficult for current AI methods to solve. The ability of poker players to collaborate was mentioned as a factor that would make the game of poker beyond being solvable by current AI methods.'}, {'title': 'Advancements in Coalitional Games and Collusion ', 'text': 'The speaker has done a lot of work on coalitional games and has a paper on it. They have presented their work at a poster session at NIPS. Collusion in games presents a different and typically harder problem. Some game representations do not allow for good computation. The speaker introduced a new game representation for dealing with collusion. There are still some unknown aspects, such as in the game of bridge.'}, {'title': 'The Importance of Coordination in Strategic Games ', 'text': 'Coordination in games like bridge requires strategies to be coordinated ahead of time and signals to be understood by both teams. In many situations like auctions, negotiations, and diplomatic relationships, coordination is not built into the rules of the game but can still be helpful for colluders. Prior strategies and willingness to do certain things are important in negotiations and other applications beyond poker.'}, {'title': 'Expanding Business Ventures Beyond Poker ', 'text': 'Moving away from poker and into other applications like negotiations and business. Has two startup companies - Strategic Machine and Strategy Robot. Strategic Machine is for business applications, gaming, sports, etc. Strategy Robot is for military security, cyber security, and intelligence applications. Also involved in a company called Optimized Markets, which focuses on combinatorial market and optimization based technology. Not using game theoretic reasoning technologies in Optimized Markets.'}, {'title': 'Underutilization of Game Theoretic Reasoning Technologies ', 'text': \"Game theoretic reasoning technologies are not being used. The goal may not be to model human behavior. In a zero sum game, the opponent's adherence to a model of rational behavior may not matter. Interaction in games needs to be formalized for analysis. Mechanism design has been used to design games with specific outcomes. Example of studying pedestrian and car negotiation in the world of autonomous vehicles.\"}, {'title': 'Understanding Nonverbal Communication Between Pedestrians and Cars ', 'text': \"- Pedestrians and cars engage in nonverbal communication, with pedestrians trusting that cars won't harm them.- Modeling human behavior in situations like jaywalking is challenging and may require game theory and imperfect information approaches.- Autonomous vehicles could benefit from understanding and modeling human behavior, such as jaywalking, to improve safety and decision-making.\"}, {'title': 'Challenges and Opportunities of Automated Negotiation in Autonomous Car Fleets ', 'text': 'Fleets of autonomous cars operated by different companies, such as Waymo and Uber, raise the question of defining rules of the road and negotiating strategies. Automated negotiation could be used to prenegotiate situations like merging, potentially allowing for faster and more efficient traffic flow. The idea of automated negotiation could involve trade-offs, such as giving way in some situations in exchange for priority in others.'}, {'title': 'Understanding Game Theory and AI in Negotiation ', 'text': 'The negotiation involves a combinatorial approach with exchange of positions. The example of merging can be modeled as an imperfect information game. Games with perfect information are easier to deal with. Dealing with imperfect information requires a different approach. The Annual Computer Poker Competition is an incredible accomplishment of AI. AI has had significant moments in history such as Deep Blue and AlphaGo.'}, {'title': 'AI Advancements in Engineering and Science ', 'text': 'AI has stepped up in an engineering and scientific effort to beat human players. Performance oriented research is valued. Building big systems and evaluating them at scale is important in AI. Techniques that look good in small scale may not work in large scale.'}, {'title': 'Algorithm Performance in Theory vs. Reality ', 'text': \"Theory doesn't always match reality in terms of algorithm performance. First order methods may have better convergence rates in theory, but CFR based algorithms are the fastest in practice. Testing in reality is necessary to determine the best algorithms. Projections from small scale tests can be misleading in this domain. Personal experience with organizing the first brains versus AI poker competition was wild.\"}, {'title': 'The Impact of AI on Heads Up No Limit Poker ', 'text': \"This was the first competition for Heads Up No Limit poker. The speaker became the most hated person in the world of poker for cracking the game with AI. Many people felt that AI was a real threat to the existence of the game. The comments and reactions to the speaker's AI were super aggressive, even including death threats. Despite AI outperforming humans in chess, humans still enjoy the game, and the speaker believes the same can be true for poker.\"}, {'title': 'The Impact of AI on Poker ', 'text': 'The AI has changed the way poker is played by incorporating Martian ways of playing. The top humans are now using strategies from AIs in their own play. The AI has made poker a richer and more interesting game for humans to play. The speaker has learned to love the game of poker through working with AIs. The speaker did not think of themselves as someone who would \"kill the game\" of poker.'}, {'title': 'Challenges of Scaling Good Ideas ', 'text': \"Good ideas don't always work when applied at scale. It takes a lot of work and time to organize and make something big. It is important to do things in the real world and at scale. Proof is in the pudding, meaning the real test is in the real world and at scale.\"}, {'title': 'Competition and Automated Mechanism Design in Poker and Politics ', 'text': 'Competition between different groups to beat the top humans at Heads Up No Limit, Texas Holdem. Mechanism design is about designing the rules of the game to achieve a certain desirable outcome. The topic of mechanism design is interesting and new to the speaker. Automated mechanism design is discussed in the paper. The speaker is an observer of mechanisms, including politics. The possibility of designing the political system in an automated fashion is mentioned.'}, {'title': 'Challenges in Automated Mechanism Design ', 'text': \"Automated mechanism design direction is still believed in, but it's not a panacea. There are impossibility results in mechanism design, stating that certain objectives cannot be accomplished in certain classes. These impossibility results are proofs, not statements about human ingenuity. It is impossible to achieve certain properties in certain classes with any mechanism. The good thing about mechanism design is... (the text is cut off, so the key point is incomplete)\"}, {'title': 'Automated Mechanism Design and Impossibility Results ', 'text': 'Automated mechanism design allows for specific settings to be designed for, even if there are impossibility results for the whole class. It is possible to carve out islands of possibility within known impossible classes. The Meyerson Satethweight theorem by Roger Meyerson and Mark Satethweight from 1983 shows an impossibility of efficient trade under imperfect information, but it is possible to avoid that in many settings and still achieve efficient trade. The existence of impossibility results does not mean that all cases in a class are impossible, just that some of the cases are impossible.'}, {'title': 'The Application of Mechanism Design in Real-World Situations ', 'text': 'The impossibility result is still present, but there are spots within the impossible class where the impossibility does not exist. The lessons drawn from mechanism design can be applied to politics, human interaction, and designing mechanisms for various real-world situations. Mechanism design itself has had limited success so far, with only certain cases being successful in real-world situations.'}, {'title': 'Challenges of Applying Mechanism Design in Real World Situations ', 'text': 'Real world situations are often not sound from a mechanism design perspective. Insights from theory are applied into the real world rather than applying mechanisms directly. The FCC spectrum auctions are an example where bidding truthfully is not the best strategy. Mechanism design aims to make things easy for participants, but truth telling is not the best strategy in high stakes auctions. The rules designed in practice for the FCC spectrum auctions do not align with game theory principles.'}, {'title': 'Challenges in Bidding Strategies and AI History ', 'text': 'Bidding truthfully is not the best strategy in spectrum auctions. There is no known optimal bidding strategy for spectrum auctions. AI history is marked by seminal events such as AlphaGo beating a world champion human Go player and Liberatus winning the Heads Up No Limit Holdem. Heads Up No Limit Texas Holdem was the one remaining widely unsolved game in terms of game solving.'}, {'title': 'The Future of Game Solving ', 'text': 'Heads Up No Limit Texas Holdem was widely agreed upon as a benchmark for game solving. There are other games being worked on, such as StarCraft, Dota 2, Diplomacy, and Hanabi. None of these games are acknowledged as the main next challenge problem like chess or Go. The speaker hopes that there will be a next benchmark to drive application independent techniques forward.'}, {'title': 'The State of Game Solving Technology in Comparison to Machine Learning ', 'text': 'The speaker is involved in two startups related to game solving technology. They are not as focused on recreational benchmarks. Game solving technology is not as mature or widely applied as machine learning. Machine learning has proven success in the real world, while game solving technology has almost no applications.'}, {'title': 'The Potential of Computational Game Theory in Military and Business Strategy ', 'text': 'Machine learning has shown success in real-world applications, but there are almost no applications in game solving. The next big breakthrough could be the strategic use of computational game theory in military planning and business strategy. Machine learning methods, such as neural networks, lack transparency and explainability, while game theoretic methods, like Nash equilibria, may offer more transparency and explainability.'}, {'title': 'The Properties of Nash Equilibria and Game Theoretic Strategies ', 'text': 'Nash equilibria and game theoretic strategies have provable properties. Unlike deep learning, game theoretic strategies have provable solution quality guarantees. The strategies in game theory may not be human understandable, similar to deep learning. Computational game theory and deep learning are in the same boat in terms of human understandability of strategies.'}, {'title': 'Challenges and Excitement in Deep Learning and Computational Game Theory ', 'text': 'Deep learning and computational game theory are both difficult to understand. Game theoretic techniques have guarantees of solution quality, but it is more of a belief than a substantiated fact. The future of provable optimality in game theory is exciting. There are concerns about the existential threats of artificial intelligence, especially in games like poker. There are worries about the negative impact of artificial intelligence on society.'}, {'title': 'Impact of Nationwide Kidney Exchange and Combinatorial Sourcing Auctions ', 'text': 'The nationwide kidney exchange has saved hundreds of lives and increased employment in the healthcare industry. Combinatorial sourcing auctions have increased supply chain efficiency by 12.6%, resulting in over $6 billion of efficiency improvement.'}, {'title': 'Efficiency Improvement and Safety with AI in Trucking ', 'text': '$6 billion of efficiency improvement in the world. Efficiency improvement in trucking, less empty driving, less waste, and less carbon footprint. AI is going to make the world much safer.'}, {'title': 'The Role of Game Theory in Addressing Value Misalignment ', 'text': 'Game theory has a role to play in ensuring that values are aligned with human beings. Value misalignment is a theoretical worry and has not been seen in real applications. Example of value misalignment in a transportation optimization system where high asset utilization as an objective would lead to inefficient solutions.'}, {'title': 'Challenges in Achieving 100% Utilization and the Gap Between Theory and Reality ', 'text': 'The solution of loading trucks full and driving in circles to achieve 100% utilization may not be practical in reality. AI can optimize the wrong objective to the hilt, causing more harm than good. There is a gap between theory and reality that is difficult to articulate. The worst possible case or bad cases imagined theoretically may not always materialize in reality. The presence of 10,000 nuclear weapons in the world is a concerning reality.'}, {'title': 'The Threat of Nuclear War and Climate Change ', 'text': 'The speaker grew up in the Soviet Union. There are currently 10,000 nuclear weapons in the world. The speaker is surprised that nuclear war has not broken out. The two biggest threats facing mankind are climate change and nuclear war. The speaker has tried to do something about climate change through their startups. The speaker commissioned studies on what could be done for climate change. The speaker is still keeping an eye out for potential market solutions or optimizations for climate change.'}, {'title': 'Challenges and Limitations in Implementing Market Solutions for Environmental and Security Issues ', 'text': 'Market solutions, optimization solutions, and technology solutions are needed for problems such as pollution. Lack of political will is a major barrier to the success of pollution credit markets. Better market design alone cannot solve the problem if there is no political will to support it. The Chicago market was shut down, indicating the limitations of market design in addressing environmental issues. Global warming is a pressing problem, while nuclear weapons have been a long-standing concern. The speaker is extremely worried about the issue of nuclear weapons.'}, {'title': 'The Concerns of Mutually Assured Destruction and the Role of AI in Future Nuclear Conflict ', 'text': \"['The game theory of mutually assured destruction is still a cause for worry.', 'The threshold for initiating nuclear conflict is smaller now due to smaller nukes and the presence of smaller countries and non-nation actors with access to nuclear weapons.', 'The application of AI in the future is an exciting prospect, especially in the context of NIPS (NeurIPS) and the potential for advancements in several companies.']\"}, {'title': 'Developing Scalable Techniques for Game Solving and Real-World Applications ', 'text': 'The focus is on developing scalable techniques for game solving and applying them in the real world. Interest in market design and optimized markets. Priority is on strategic machine strategy robot and getting the technology out there. Understanding the technology gaps that still need to be filled through real applications. Enjoying the interaction and challenge of applying state of the art techniques in real-world scenarios.'}, {'title': 'Challenges and Benefits of Implementing State-of-the-Art Techniques in Various Industries ', 'text': 'The process involves applying state-of-the-art techniques to benefit various industries and players. Autonomous vehicles face challenges in integrating new technology due to old-fashioned systems and inertia. Finding internal champions at the customer level is crucial for driving change and preventing negative outcomes in the future.'}, {'title': 'The Impact of Autonomous Vehicles on the Future ', 'text': '[\"Things can\\'t be the same way in the future. Otherwise bad things are going to happen.\", \\'Car makers and tech companies like Google and Baidu are pushing on autonomous cars.\\', \\'The speaker finds it fascinating that traditional car makers and tech companies are working on autonomous vehicles.\\', \\'The speaker is excited about the impact of these ideas in the world.\\', \\'There are lots of different things in the game solving, including solving bigger games with more hidden player actions.\\', \\'Poker is mentioned as a game.\\']'}, {'title': 'Strategic Elements in Poker, Multiplayer Games, Business, and Military ', 'text': 'Poker is a game where chance actions are hidden and player actions are public. Multiplayer games involve collusion, opponent exploitation, and can be extensive and long-lasting. Business strategy involves thinking about the business from a long-term perspective. Military strategy involves modeling and evaluating moves in a context where war is a constant factor.'}, {'title': 'Interest in Scalable Techniques for Integer Programming ', 'text': 'The speaker is interested in learning more scalable techniques for integer programming. They had a paper on automated algorithm configuration with theoretical generalization guarantees. Algorithm configuration has been going on for at least 17 years seriously, but there has not been any generalization theory before. The speaker is honored to talk to Tomas and thanks him for bringing Labradus to the world. The conversation ends with no more questions.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 19:02:06.981300 ...\n",
      "Best SD: 2.309401076758503, Best iteration: 24\n",
      "done get topics 2024-03-28 19:02:07.667923.\n",
      "Stage 2 start time 2024-03-28 19:02:07.667943\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Thomas Sanholm: AI Expert and Game Theory Researcher in High Stakes Poker\n",
      "2. The Importance of Abstraction and Learning Methods in Poker Strategy\n",
      "3. Understanding Game State and Strategy Evaluation in Game Theory\n",
      "4. Strategies and Opponent Modeling in Game Theory and Two-Player Games\n",
      "5. The Impact of AI on Strategic Games and Nonverbal Communication\n",
      "6. Advancements in AI and Algorithm Performance in Poker\n",
      "7. Challenges and Opportunities in Automated Mechanism Design in Real-World Situations\n",
      "8. The Future of Game Solving and Computational Game Theory\n",
      "9. Strategic Elements in Various Industries and the Role of AI in Addressing Global Issues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-03-28 19:03:17.353733\n",
      "stage_2_titles: len: 9\n",
      "['1. Thomas Sanholm: AI Expert and Game Theory Researcher in High Stakes Poker', '2. The Importance of Abstraction and Learning Methods in Poker Strategy', '3. Understanding Game State and Strategy Evaluation in Game Theory', '4. Strategies and Opponent Modeling in Game Theory and Two-Player Games', '5. The Impact of AI on Strategic Games and Nonverbal Communication', '6. Advancements in AI and Algorithm Performance in Poker', '7. Challenges and Opportunities in Automated Mechanism Design in Real-World Situations', '8. The Future of Game Solving and Computational Game Theory', '9. Strategic Elements in Various Industries and the Role of AI in Addressing Global Issues']\n",
      "remove_questions start time: 2024-03-28 19:03:17.370832\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 19:08:05.768539\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-28 19:08:05.768703\n",
      "extract_keypoints done time 2024-03-28 19:10:24.212490\n",
      "Start time: 2024-03-28 19:10:24.212777\n",
      "Stage 1 done time 2024-03-28 19:12:50.152025\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and its Open Source Development ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library used in deep learning research and large scale applications. It has become an ecosystem of tools for machine learning deployment in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by Rajat, Jeff Dean, and a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring other companies to open source their code.'}, {'title': 'The Impact of Open Innovation on Google Brain and TensorFlow ', 'text': \"Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. Google Brain's proprietary machine learning library turned into TensorFlow in 2014, the open source library. The idea of deep learning was intriguing and held promise, even before it had taken off.\"}, {'title': \"Google's Research Advancements \", 'text': 'The idea was to scale research work to Google\\'s compute power and data. Scaling the compute and data showed better results. Early wins were achieved in speech and image research. Collaboration with the speech research team was successful. The \"cat paper\" was a significant achievement in image research.'}, {'title': \"The Evolution of Google's Machine Learning Efforts \", 'text': 'Google Brain was born around neural networks, focusing on deep learning from the beginning. In 2012 or 2011, the focus was on scaling machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines. Google has been doing machine learning for a long time, showing great promise in terms of machine learning.'}, {'title': 'The Rise of Deep Learning at Google ', 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact lots of things. Real products started wanting to use deep learning, such as speech and image recognition. Academia also started to push for more deep learning. By 2014, it was clear that deep learning was a big thing and would continue to grow. The decision to open source TensorFlow was made.'}, {'title': \"The Impact of Google's Decision to Open Source TensorFlow \", 'text': \"The decision to go open source with TensorFlow is considered a seminal moment in software engineering. Google's decision to open source a large project with significant IP was a powerful statement in favor of open innovation. The initial idea to go open source came from Jeff, a big proponent of the concept. The decision was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'The Impact of Research Sharing on the Growth of Deep Learning and Machine Learning ', 'text': 'Deep learning and machine learning have grown rapidly due to sharing of research. Existing libraries like Tiano and Torch were done by academia and had a different level of quality. Google had developed internal software and published papers, leading to successful open source projects. Hadoop was developed from technology built internally, which was considered superior for various reasons.'}, {'title': \"Google Cloud's Bigtable and HBase APIs, TensorFlow, and Integrations \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. TensorFlow is open source and can be used anywhere. Google Cloud ensures lots of integrations with everything else and works really well there.'}, {'title': 'Title ', 'text': \"The Timeline of TensorFlow's Open SourcingText \"}, {'title': 'Title ', 'text': 'Evolution of TensorFlow Design at GoogleText '}, {'title': 'Exploring Machine Learning on Mobile Devices ', 'text': 'Ideas of running machine learning on the phone were being explored at that time. Customized handcrafted code or internal libraries were used for running machine learning on the phone. The use of Theano and Caffe at Google influenced design decisions. The belief was built in parallel with the development of libraries like Theano. The systems at Google were very different, leading to the focus on internal development. Multiple libraries were considered before making design decisions.'}, {'title': 'Comparing Libraries for Flexibility in Machine Learning ', 'text': 'By the time we got to this, we looked at a number of libraries that were out there. The group had experience with Torch, Lua, Theano, and Caffe. They discussed ideas around having a graph or not. They wanted flexibility due to the fast-moving research and changing hardware. The flexibility in terms of being able to express all kinds of crazy things was a big factor.'}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The move towards TensorFlow 2.0 includes eager execution by default, hiding the graph to make development more intuitive. The idea of using a graph came from the need to deploy production stuff, even with disbelief. Experimentation with ideas in Python led to the realization that not having a graph made things simpler to use.'}, {'title': 'Influences and Growth of Deep Learning Product ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, was unexpected. The need for the product was recognized early on in the research perspective and early days of deep learning. The potential for future growth and enabling more people to use the product was considered after open sourcing. The growth of deep learning was observed after open sourcing, leading to the realization of potential future growth.'}, {'title': 'The Growth of Deep Learning and Community Engagement ', 'text': \"Deep learning grew rapidly after open sourcing. The company saw the opportunity to leverage deep learning and deliver on what people want. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company's approach is very community driven. The initial version was 0.6 or 0.5. People initially loved the documentation provided by the company.\"}, {'title': 'The Evolution of Deep Learning from Research to Practical Applications ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from a research focus to being accessible to developers for practical applications. The focus shifted towards stability and deployment for enterprises, leading to the planning of version 1.0. The importance of documentation, designs, and other elements in meeting the needs of stability and deployment was emphasized. There was an increasing demand from enterprises for the product.'}, {'title': 'Enterprise Adoption of a New Product ', 'text': 'Excitement around enterprise adoption of a product. Initial interest from researchers and hobbyists. Pressure for stability from enterprises before version 1.0. Importance of understanding what enterprises want.'}, {'title': 'The Importance of Stability and Simplicity in Technology Adoption ', 'text': 'Inception and ResNet 50 are still widely used by many people, even though they are a few years old. Some users prioritize stability and simplicity over the latest performance or quality improvements. There is value in providing stability and simplicity to allow more people to access the technology. The research crowd is interested in pushing the boundaries with new technologies like RNNs, transformers, RL, and GANs.'}, {'title': 'Advancements in Transfer Learning and Machine Learning Technologies ', 'text': 'The combination of RL, GANs, and other technologies is pushing the state of the art in the field. Older technologies, such as ResNet 50, are still very usable and stable. Transfer learning on specific problems using existing models like ResNet 50 is a common use case. Making transfer learning as easy as possible is important for hobbyists and common use cases. The majority of the world uses transfer learning for their machine learning applications. The use of transfer learning looks great in presentations and on slides.'}, {'title': 'The Evolution of Predictive Modeling in Enterprise ', 'text': 'Enterprises have data that they want to make predictions on. They used to use regression models and gradient booster trees for this. Some still benefit from deep learning, especially with large data sets. The developer summit put together the whole TensorFlow Extended piece, which is the entire pipeline that enterprises care about.'}, {'title': 'The Importance of Data Organization for Using TensorFlow ', 'text': 'TensorFlow Extended is the entire pipeline, focused on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. There is a need to evangelize the importance of organizing data for the big benefit of using TensorFlow.'}, {'title': 'Title ', 'text': 'Understanding Machine Learning and the TensorFlow EcosystemText '}, {'title': 'Improving Access to Data Sets with Keras and TensorFlow ', 'text': 'People want easier organization and access to released data sets. Start with basic models and improve them. Keras made TensorFlow more accessible. Francois started the Keras project before joining Google.'}, {'title': \"The Evolution of Francois' Keras Project \", 'text': 'Francois started the Keras project before he was at Google. Tiano was the first thing he created. He decided to create an interface and put TensorFlow as a backend when TensorFlow started becoming popular. He joined Google after creating the interface. He initially joined research and was doing some amazing research. He has some papers on research and is a great researcher.'}, {'title': 'Integration of Keras into TensorFlow ', 'text': 'Keras was integrated into TensorFlow in a deep way. With TensorFlow 2.0, Keras is the recommended way for beginners to interact with TensorFlow. This makes initial transfer learning or basic use cases super simple, even for an enterprise. The integration of Keras into TensorFlow was initially supposed to last for a quarter but has been ongoing for two years. The person responsible for the integration is a great researcher and has been fully committed to the project. The integration has made the API more popular and well-liked by users.'}, {'title': 'Integrating and Standardizing APIs for Simplified Usage ', 'text': 'The team had multiple APIs, including a parallel layers API and Keras. They wanted the APIs to be integrated and share common features. There were three other APIs built by others, leading to confusion in the community about which one to use. The decision to focus on Keras 2.0 was made to simplify and standardize the API usage. Keras was chosen based on its popularity and positive feedback from the community.'}, {'title': 'The Impact of Keras on TensorFlow ', 'text': 'Keras was loved by many and had great qualities. It was surprising to bring in an outside element like Keras, which was seen as a competitor to TensorFlow, but it ended up being an empowering element of TensorFlow. The team and developers all want to make things easier for a large set of developers, and that makes a difference. Python has Guido van Rossum, who held the position of benevolent dictator for life, and a successful open source project like TensorFlow needs one person to make final decisions.'}, {'title': 'Key Developments in TensorFlow ', 'text': 'One person makes final decisions for TensorFlow. Successful TensorFlow Dev Summit recently held. Incorporation of many new features and an amazing ecosystem. Involvement in key design directions. Martin Wick has driven a lot of open source stuff and APIs. Regular design reviews are conducted. Efforts to open up to the community and add transparency. Implementation of processes such as RFCs and special interest groups.'}, {'title': 'The Evolution of the TensorFlow Ecosystem ', 'text': \"The need for adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The recognition that the ecosystem's scale requires more than one decision maker. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The convergence of these developments towards the ability to save models in a consistent way and move them between different platforms.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning in Practical ApplicationsText '}, {'title': 'The Evolution of Machine Learning in Product Development ', 'text': 'Machine learning research needs to be integrated to build real products and have a real impact on people. ML and training are no longer limited to workstations, data centers, or the cloud, but are now running on phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning has grown to cover more aspects and continues to push boundaries. More tooling has been built in some areas.'}, {'title': 'The Evolution of Tooling and Libraries in TensorFlow ', 'text': 'TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines.'}, {'title': 'Title ', 'text': 'Enabling Community Collaboration and Model Sharing in TensorFlow 2.0Text '}, {'title': 'Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The team has learned a lot and iterated over the last few years. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new devices from a hardware perspective.'}, {'title': 'Challenges of Scaling Out a Monolithic System ', 'text': 'TensorFlow started as a monolithic system and is still largely monolithic. The challenge is to scale out the system by breaking it apart with clearer interfaces. It is difficult to change and modify a rapidly evolving system that is not slowing down. The goal is to change and modify the system while it is still running, similar to changing the engine of a car while it is running.'}, {'title': 'Challenges of Maintaining Compatibility in TensorFlow ', 'text': 'Many people rely on TensorFlow in their applications. There is a challenge in maintaining previous versions while also introducing new features. TensorFlow 2.0 breaks some backward compatibility but the conversion is straightforward. It is a tricky balance between introducing new features and maintaining compatibility. Technical debt is a concern when many people rely on a technology. Production applications are affected by changes in TensorFlow versions.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and doing new things comes with a huge cost. It's a trade off between slowing certain things down and bringing overall value. It's not just about breaking the person yesterday, but also about setting standards for new people joining in the future.\"}, {'title': 'Designing with a Clean Slate and the Rise of TensorFlow ', 'text': \"Design with a clean slate in mind, and then figure out how to make sure all the other things work. Unless you design with a clean slate and not worry about compromises, you'll never get to a good place. It's important to put all the responsibility behind when thinking of new ideas. The speaker recently switched their research group to TensorFlow. The speaker wishes everybody would use the same thing, and believes TensorFlow is leading in many ways.\"}, {'title': 'The Rise of PyTorch in Research and Development ', 'text': \"PyTorch is now being used by a lot of researchers. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Importance of Eager Execution and Learning from Previous Experiences ', 'text': 'The text discusses the benefit of learning from previous experiences and exploring different spaces. It mentions the competition and the process of revisiting ideas multiple times before implementing them. The text highlights the importance of eager execution and the effort put into combining different elements to make it successful. It also references the analogy of Muhammad Ali versus Frasier to emphasize the significance of eager execution.'}, {'title': 'The Evolution of TensorFlow ', 'text': \"TensorFlow is finally in the way they did. It's doing some incredible work last couple years. Making it easily accessible to Keras, eager execution. TensorFlow 2.0. Setting us up for really clean APIs.\"}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs will allow for optimization and improved performance for both single machine and distributed systems. The team is looking forward to exploring new possibilities and spaces behind the scenes in future versions after 2.0.'}, {'title': 'Restructuring and Modularization of TensorFlow for Future Improvements ', 'text': 'The team is excited about future versions and expects to see improvements over time. Restructuring the monolithic system into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow in GitHub consists of one core repository with various components such as the execution engine, key backends for CPUs and GPUs, and distributed computing. The current organization does not easily allow for splitting components apart, and clean interfaces are needed for a perfect world scenario.'}, {'title': 'The Importance of Clean Interfaces and TensorFlow in Development ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on different clusters with custom networking. Clean separation in interfaces will help with more interesting developments in different spaces. Enabling independent evolution and pushing on things allows for better scalability in the ecosystem. Major corporations like Pepsi are already using TensorFlow for development.'}, {'title': 'The Growing Community of TensorFlow ', 'text': 'Many users are already using TensorFlow, including hardware vendors and bigger companies like IBM. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The community growth is attributed to the involvement of various users and companies, including those in special interest groups and autonomous vehicle companies. The critical thing that allowed for this growth is not specified in the given text.'}, {'title': 'Factors for Achieving Growth ', 'text': 'Growth is critical and requires a combination of factors. Timing and alignment with the needs of the industry are important for growth. Listening to the community and being open to external contributions is essential. Creating the right processes and community to welcome and support contributors is crucial for growth.'}, {'title': 'Importance of Transparency, Community, and Documentation in Open Source Projects ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Documentation and tools are important considerations for developers. People building something on TensorFlow and implementing a particular architecture contributes to the growth of TensorFlow.'}, {'title': 'Investing in Developer Support for GitHub Projects ', 'text': 'The company is working hard to make it easy to use and contribute to their GitHub projects. They are investing in tooling to support developers and make version changes smooth. People want to move to new things because they see value in them, not just because they are new.'}, {'title': 'The Future of Deep Learning ', 'text': 'Most people want a really good thing. People will start to see the value and there will be a shift happening. The field is moving rapidly, which will help in doing more things and new things will happen in 2.x. Change is expected to happen in terms of deep learning. The basics of deep learning, such as convolution models, will probably be around.'}, {'title': 'The Future of Machine Learning and TensorFlow ', 'text': 'Convolution models and basic models will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. New developments in the field are hard to predict, but there are some directional trends such as combining eager execution and graphs to make programming more natural. Swift for TensorFlow is taking a ground-up approach, which seems to be the right direction for future developments. There is uncertainty about the future of hardware accelerators and the possibility of training with four bits instead of 32 bits. The TPU side of things is exploring the potential for using TPU for training with lower bit precision.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': 'TPU is already on version three, and it is exploring the use of four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to easily use image models for training or transfer learning. Providing simple models is important for making TensorFlow accessible.'}, {'title': 'Improving User Experience with TensorFlow ', 'text': \"Providing simple models and tools like hub to make it easy for users. Different levels of support for beginners, intermediate users, and researchers. Offering pre-trained models to decrease the time needed to start. TensorFlow's recent delivery is trivial for beginners. Addressing pain points and trying to ease the user experience.\"}, {'title': 'The Impact of High Schoolers and TensorFlow on Technology ', 'text': 'High schoolers are doing amazing and terrifying things, and will contribute incredible ideas as they grow up. There is a technical aspect and a management aspect to the role with TensorFlow. Google has been at the forefront of exploring what it takes to build a good team. TensorFlow is one of the most cutting edge technologies in the world. Cohesion across the team is important for delivering something well.'}, {'title': 'The Importance of Team Cohesion and Vision ', 'text': \"Cohesion across the team is important for execution. The team's output is greater than the sum of individual contributions. Hiring good people who care about what they're building and are motivated is important. Having a unified vision of where the team wants to go is crucial.\"}, {'title': \"Google's Unified Vision and Bottom-Up Approach \", 'text': \"Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, especially in research. As Google has become a larger product and ecosystem, it's important to combine a mix of direction and exploration. The mission of superstars is a significant element at Google.\"}, {'title': \"Challenges and Excitement at Google's TensorFlow Project \", 'text': 'Large percentage of work at Google is done by individual superstars. Superstars can sometimes be against the dynamic of a team, causing tensions. The mission of the TensorFlow project is exciting and at the cutting edge. Google values getting people who care and have the same kind of culture. The project allows for lots of people to do different things and grow. There are always people challenges in different kinds of ways.'}, {'title': 'Refining the Hiring Process at Google ', 'text': 'The hiring process at Google has been refined over the last 20 years. Google values teamwork and productivity over individual superstar status. Core technical skills are important, but teamwork and collaboration are also key factors in hiring engineers. It is important for individuals to work well with the team across Google. The hiring process at Google has no magic answers, but it has been effective. Value is added by individuals, but if they are hurting the team, it becomes a problem.'}, {'title': 'The Importance of Motivation in the Workplace ', 'text': \"Motivation is important in addition to core technical skills. Alignment of motivation with the team's goals is crucial for long term success. Motivation is important at every level, not just for senior positions. Google's hiring process focuses on technical skills, but may not fully assess motivation and drive.\"}, {'title': \"Google's Interview Process and Cultural Requirements \", 'text': 'The interview process at Google includes a culture fit assessment in addition to technical skills. Different projects at Google may have different cultural requirements. TensorFlow project at Google requires people who are comfortable with fast-moving projects.'}, {'title': 'Navigating Variability and Engineering Excellence at Google ', 'text': 'Balancing the need for full-fledged products with the importance of ensuring things work well. The importance of finding the right fit for different projects and teams. Variability in culture, projects, teams, and product areas across Google. Engineering excellence as a core part of the culture. The challenging nature of the work, but also the fun in solving difficult problems. The key to success in a large ecosystem or small product.'}, {'title': 'Challenges of Product Development ', 'text': 'Striking a balance across different aspects of a product. Making hard decisions such as how fast to go versus how perfect it is, involving a community, and saying no to certain things. The difficulty of making quick decisions due to time constraints. The success of the Dev Summit and the challenge of coordinating many moving pieces to meet a deadline.'}, {'title': 'Managing Deadlines and Balancing Perfection and Functionality ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and functionality. The team did a great job in putting everything together. Focus on key things that are important and figure out their importance. Developing in the open, both internally and externally, with everything available to everybody. Regular releases at a regular cadence.\"}, {'title': 'Approach to Software Releases ', 'text': \"Releases are done at a regular cadence, with the understanding that if something isn't ready this month, it will be in the next release in a month or two. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it is clear that they are experimental and feedback is encouraged. Quick cycle and quick iteration are important, rather than focusing on meeting a deadline for a stable release. The example of WordPress 5.0 release without pressure to make it stable, but with a plan for rapid updates to improve it, is mentioned. The question of whether TensorFlow 2.0 will follow a similar approach is raised.\"}, {'title': 'NodeX API Stability and Future Development ', 'text': 'NodeX API stability and potential changes. Continuation of development and future releases. Long-term development beyond the upcoming release. High number of downloads for TensorFlow 1.0 X.'}, {'title': 'Title ', 'text': \"TensorFlow's 1.0 X Update and Speaker's Previous Work at GoogleText \"}, {'title': 'The Impact of Search Ads and Machine Learning on User Experience ', 'text': 'Search ads are an extension of what search is trying to do, which is to make information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience. Ads can annoy users and ruin the user experience if not done well. Huge amounts of personalized data can be used to map to the things users actually want.'}, {'title': 'The Importance of Accessible and Quality Information ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the information to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's reading experience.\"}, {'title': 'The Importance of Balancing Value and Monetization in Advertisements ', 'text': 'Advertisements should strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services such as search engines and websites to provide their service. Good advertisements can be useful and not annoying when done in a good balance.'}, {'title': 'The Rise of Paid Services on the Web ', 'text': 'More paid services are being seen across the web and people are willing to pay for them. Transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. There is always gonna be things that are sort of monetized with things like ads. Hopeful that there will be a transition to a mix model for paid content on the internet.'}, {'title': 'Advancements in TensorFlow and Google Call App ', 'text': 'Transition to a mix model with free trials and ads, followed by a clear revenue model. Use of TPU in a Google call app for free. Ability to run TensorFlow open source on any device, including desktop and phone. Increasing power of desktops and phones for running TensorFlow. Training TensorFlow on a phone.'}, {'title': 'The Impact of Cloud Computing on Device Accessibility ', 'text': 'The power of cloud computing is accessible through devices like phones, making it more convenient and powerful. Cloud services like Colab make it easy to get started with no installation needed. Colab is a free service, but paid services offer more capabilities. Beginners interested in machine learning and TensorFlow can start by visiting the TensorFlow website.'}, {'title': 'Getting Started with TensorFlow ', 'text': 'Start by going to TensorFlow.org and playing around on the website. Check out tutorials and guides for more information. No installation needed, you can get started right away on Colab. Thanking Rajit and Lex for the conversation.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 19:12:51.615582 ...\n",
      "Best SD: 1.9241827716833386, Best iteration: 26\n",
      "done get topics 2024-03-28 19:12:52.568055.\n",
      "Stage 2 start time 2024-03-28 19:12:52.568075\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. The Impact of Google's Open Source Development on Machine Learning\n",
      "2. Exploring Flexibility in Machine Learning Libraries\n",
      "3. Advancements in Enterprise Adoption of Deep Learning\n",
      "4. Evolution of Keras and TensorFlow Integration\n",
      "5. Key Developments in the TensorFlow Ecosystem\n",
      "6. Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS\n",
      "7. The Evolution and Restructuring of TensorFlow\n",
      "8. The Growing Community and Future of TensorFlow\n",
      "9. Google's Unified Vision and Challenges at TensorFlow Project\n",
      "10. Approach to Software Releases and API Development\n",
      "11. The Impact of Machine Learning on User Experience and Advertising\n",
      "Stage 2 done time 2024-03-28 19:13:55.257597\n",
      "stage_2_titles: len: 11\n",
      "[\"1. The Impact of Google's Open Source Development on Machine Learning\", '2. Exploring Flexibility in Machine Learning Libraries', '3. Advancements in Enterprise Adoption of Deep Learning', '4. Evolution of Keras and TensorFlow Integration', '5. Key Developments in the TensorFlow Ecosystem', '6. Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS', '7. The Evolution and Restructuring of TensorFlow', '8. The Growing Community and Future of TensorFlow', \"9. Google's Unified Vision and Challenges at TensorFlow Project\", '10. Approach to Software Releases and API Development', '11. The Impact of Machine Learning on User Experience and Advertising']\n",
      "remove_questions start time: 2024-03-28 19:13:55.274925\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 19:19:21.513845\n",
      "chunks_text len: 73\n",
      "extract_keypoints start time: 2024-03-28 19:19:21.513977\n",
      "extract_keypoints done time 2024-03-28 19:22:00.641331\n",
      "Start time: 2024-03-28 19:22:00.641585\n",
      "Stage 1 done time 2024-03-28 19:24:37.929740\n",
      "RR stage_1_outputs:\n",
      "[{'title': \"Revolutionizing Creativity: Adobe Research's Efforts to Automate Tedious Tasks \", 'text': 'Adobe Research is working to define the future evolution of their products to make the life of creatives easier. They aim to automate tedious tasks and give more time for creatives to operate in the idea space. Deep learning methods of the past decade can shine in this application. Gavin Miller, head of Adobe Research, combines tech and creativity, writing poetry and building robots outside of work.'}, {'title': 'Gavin Miller: Head of Adobe Research and Multifaceted Artist ', 'text': 'Gavin Miller is the head of Adobe Research, leading innovative efforts and applications of AI in creating images, video, audio, and language. Gavin Miller is also an artist, poet, writer, and roboticist. Lux Friedman, the interviewer, enjoys Gavin Miller\\'s poetry and promises to sprinkle it into the conversation. Gavin Miller\\'s poem \"Je Ne Vinaigrette Rien\" parodies both Edith Piaf\\'s \"Je Ne Vinaigrette Rien\" and Frank Sinatra\\'s \"My Way\". The poem includes both deep and profound verses as well as light and silly ones.'}, {'title': 'Struggling with Weight and Parallel Interests ', 'text': \"The speaker opens with a poem about struggling with weight and dieting. The poem reflects the internal struggle between wanting to lose weight and the irrational desire to embrace the opposite idea. The speaker finds humor in taking the extreme opposite approach to dieting. Writing and technology have been parallel interests in the speaker's life since high school. The speaker's private life and professional interests have been parallel strands in their life.\"}, {'title': 'The Intersection of Technology and Private Life ', 'text': 'The intersection of private life and technological career. The influence of one idea on the other. The inspiration from science fiction for building new technology. The example of using voice synthesis for writing a poem. The impact of voice synthesizer technology in the 90s.'}, {'title': 'Advancements in AI and Smart Home Technology ', 'text': 'The speaker created a poem to match the tone of a voice that sounded sad and depressed. The poem was pretended to be written by an intelligent agent, telling the user to go home and leave them alone, while also expressing loneliness and a desire for company. This level of AI sophistication was beyond what was possible at the time, but is now becoming more feasible. The speaker had a smart home project in the early 90s, with a talking voice reminding them of tasks and buttons on the washing machine to prevent clothes from getting moldy. The speaker also made photo albums that used some form of technology.'}, {'title': 'Exploring Magical Realism and Technology ', 'text': 'The idea of magical realism and whether it was possible to achieve it with technology intrigued the speaker. The speaker created photo albums with light sensors that would send signals to an agent to play sounds matching the image in the book. The speaker has written plays and designed personalities for modern agents, thinking about imaginary dialogues and how to make them real and knowledgeable. The speaker is interested in the concept of the uncanny and the potential of modern agents to achieve it.'}, {'title': 'Challenges in AI Communication ', 'text': '[\"AI can fall into the uncanny valley where it says something it doesn\\'t really understand.\", \"AI needs to have multiple ways of talking about the same concept to sound like it really understands it.\", \"If AI only has one way of referring to something, it feels like a canned response.\", \"AI needs to be able to reason about a concept and give similar responses from different perspectives to seem more sentient.\"]'}, {'title': 'Advancements in Image Captioning and Synthesis ', 'text': \"Automatic image captioning and generating different kinds of statements about the same picture. Work on turning a medium from one form to another, such as auto tagging imagery or making up full sentences about what's in the image. Use of GANs to synthesize an asset that matches a given description. Early career focus on 3D computer graphics and pioneering work before movies had special effects done with 3D graphics. Comparison of early career work to the Renaissance, where people would model light, color, and shape.\"}, {'title': 'Advancements in Computer Image Generation and AI Algorithms ', 'text': 'The new wave in computer image generation is more impressionistic and uses AI algorithms. The creative process is shifting towards generating images directly from ideas rather than focusing on raw pixels. Adobe aims to cover the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The realistic simulations are important for creators who want to achieve a beautiful analog trail of water and flow after making a brushstroke.'}, {'title': 'The Importance of Automation in Design Work ', 'text': 'Complete control is important for creating expressive and novel work. Automation of certain tasks frees up artists to focus on inspiration. Design work used to take up a lot of time for artists. AI aims to reason about the likely intent for different formats and languages. The focus should be on the creative aspect of design, such as look, style, feel, and message.'}, {'title': 'The Evolution of Creativity in Artwork Production ', 'text': \"Creativity is changing, making it easier, faster, and cheaper to create beautiful artwork. There is a growing demand for beautiful artwork, from school websites to Hollywood movies. People's roles may change from hands-on artisans to art directors or conceptual artists, with the computer as a partner in creating polished examples. The speaker is a fan of Adobe products such as Photoshop, Premiere, and Audition for creating images, videos, and audio.\"}, {'title': 'Optimizing Workflow and Automation in Video and Audio Editing ', 'text': 'The speaker uses Photoshop to create the thumbnail for the video, Premiere to edit the video, and Audition for the audio. The speaker emphasizes the importance of optimizing the flow and being extremely efficient in their work process. The speaker uses an old school Kinesis keyboard and auto hotkey to minimize the number of clicks and streamline their workflow. The discussion shifts to the role of AI and automation in making the low level pixel work flow easier in the coming months and years. There is a rich array of algorithms already in Photoshop, including classical procedural algorithms and ones based on data, with a large number of sliders and degrees of freedom.'}, {'title': 'Improving User Experience with AI-Generated Default Settings ', 'text': 'AI can help by providing default settings based on the content itself rather than default values for the tool. Smart defaults can make life easier for people while making use of common sense from other example images. Adobe has spent a lot of work over the last 20 years thinking about selection, such as with quick select, which looks at color boundaries and figures out how to flood fill into regions that are physically connected in the real world. The algorithm used for selection had no visual common sense about what a cat looks like or a dog, it was based on rules of thumb applied to graph theory.'}, {'title': 'Advancements in Graph Theory and Neural Nets for Image Object Selection ', 'text': 'Graph theory has seen a big improvement with the use of neural nets. Neural nets can now accurately identify and select objects in images without the need for manual clicking. This technology is particularly useful for tasks like background removal and object selection. The goal is to make tasks like background removal as easy as possible for users. The technology has the potential to provide high-quality results and serve as a starting point for further adjustments.'}, {'title': 'The Challenge of Background Removal ', 'text': 'The challenge of removing the background is discussed. Research has been conducted on background removal. Quick, cheap, and cheerful background removal is possible with algorithms. Different algorithms are available for different levels of guidance on boundaries. Tools and combinations of algorithms are available for background removal. Demonstration of quick object selection at Adobe Max conference.'}, {'title': 'Improving Selection Mask Creation with Simple Polygon Drawing ', 'text': 'Drawing a simple polygon around the object of interest can quickly create a selection mask. The process has reduced the time from hours to a few seconds for workflows. The challenge is to make the process robust and work in all cases. There is a difference between academic research and industrial research in achieving robustness.'}, {'title': 'Types of Research and Publishing ', 'text': 'Academic research vs industrial research. Two forms of publishing: academic peer review and shipping. Customer review and product critics. Importance of intervention and recovery in case of mistakes. The luxury of talented customers. Supporting professional end by allowing customers to make small improvements.'}, {'title': 'The Impact of AI on Professional Tasks ', 'text': 'AI can make professional tasks less tedious and time-consuming. 100% automatic processes could delay time to market. Collaboration between human and machine can make the life of creatives easier.'}, {'title': 'Improving User Learning Experience in Tool Tutorials ', 'text': \"The focus is on helping the person in the moment to do the task they need to do, as well as thinking holistically about their journey learning a tool. The goal is for users to become experts in using the tool, similar to living in a city where you know the important streets you need to get to. Projects in research analyze thousands of hours of tutorials online to understand what's being taught in them. One publication at CHI looked at the last three or four actions users did in tutorials to see what other people did next, providing inspiration for what to do next or for watching the tutorial.\"}, {'title': 'AI-Powered App for Creative Professionals ', 'text': 'The app provides inspiration for next steps and tutorials. It learns from similar workflows and provides intelligent suggestions. It uses context to make intelligent suggestions about choices or assistive actions. The goal is to deeply understand the domain of designers and creative people and combine it with AI for intelligent suggestions. The app can provide suggestions verbally or by showing the results of trying a certain action.'}, {'title': 'Enhancing Learning with Artist and Teacher Guidance ', 'text': 'The goal is to have an artist and a teacher guide the process. Giving enough at each stage to build a foundation for the next level of expectation. Understanding different media types visually and in terms of transcripts and words is important. Removing the barrier of having to type in keywords for searching. Ultimately, the tool should assist with learning the interface.'}, {'title': 'The Impact of Assistant-Modified Interfaces on User Experience ', 'text': 'The discussion is about whether an assistant can modify the interface to be simpler. Adding a feature to a GUI can increase visual complexity for new users. Having an assistant with a new skill can be additive without being intimidating. The focus is on onboarding new users and making the interface easier for them. Some users value mastering complex interfaces and keyboard shortcuts, while others prefer a more assistive version for quick and simple tasks.'}, {'title': 'Advancements in Computer Vision and Machine Learning by Adobe ', 'text': 'Adobe is working on exciting applications of computer vision and machine learning. These applications include scene stitching, sky replacement, foreground/background removal, spatial object-based image search, automatic image captioning, project cloak, project deep fill, project scribbler, style transform video, style transform faces and video with project puppetron. Different classes of devices are being considered for more assistive versions of technology, depending on the context for CAPTCHA and post-production workflows. Post-production workflows may require a laptop or big screen desktop with more knobs and dials for expressing subtlety. Project deep fill involves filling in parts of images.'}, {'title': 'Sky Replace Feature in Image Editing ', 'text': 'Sky replace is an interesting feature that allows for automatic selection and replacement of the sky in an image. It also matches the geometry of the scene and provides variety in choices for different moods. The feature also recolors the foreground objects based on the new sky, adding a realistic touch to the edited image.'}, {'title': 'Exploring Natural Effects in Photography and Design ', 'text': \"The evening sky adds an orange glow to foreground objects. The artist Magritte's surrealism paintings inspired the speaker in college. The goal is to achieve natural effects in photography rather than using surrealism. The process aims to capture an entire workflow in a single action, saving time in post-production. The ability to apply the process to multiple backgrounds allows for exploration of different design options. The process allows for exploration of the design space to find the desired result.\"}, {'title': 'Importance of Exploring Design Space and Intelligent Image Search ', 'text': 'The importance of exploring the design space as close to final production value as possible. The idea of making intelligent choices about ways to search stock images. The concept of concept canvas and its application in image search. The need for making slight tweaks to the selection mask to make the design perfect. The idea of replacing the sky with different stock images and the potential for intelligent choices in searching for stock images.'}, {'title': 'Improving Image Search with Concept Canvas ', 'text': 'Concept canvas allows assigning spatial regions to keywords for image search. Pre-indexed images are used to match important concepts in the picture. Gives a sense of ownership over the outcome of the event. Allows spatial design and layout, making it feel like design. Technologies in Photoshop allow physically moving objects in post-production.'}, {'title': 'Using Neural Networks for Object Removal and Background Filling ', 'text': 'Neural networks are being used to remove objects from a scene and fill in the background automatically. GANs (Generative Adversarial Networks) are one approach for using neural networks to achieve this. Traditional algorithms like content aware fill work well for certain classes of images, such as naturalistic textures like gravel paths. Patch-based algorithms can create plausible looking intermediate fill for certain types of images. Algorithms are used to smooth out lighting and eliminate brightness contrast in the filled region.'}, {'title': 'Challenges in Image Processing ', 'text': 'The importance of smooth lighting in avoiding brightness contrast. The challenge of inferring invisible structure behind objects in an image. The common sense knowledge required to fill in missing information in an image. The limitations of current generative methods in handling high resolutions.'}, {'title': 'Improving AI Performance and Competence ', 'text': 'The need to transition from low resolution to high resolution using another algorithm or pushing the state of the art in research. The importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms. The concept of an ensemble of experts specialized in certain things and the idea of voting on confidence levels.'}, {'title': 'Improving Workflow Efficiency in Photoshop ', 'text': 'The process involves either voting on confidence in future actions or using a dispatcher to assign tasks based on expertise. Each model requires a significant amount of work, but over time the set will be filled out and workflows will expand. The goal is to collect information on the workflows and needs of Photoshop users to better understand what data needs to be annotated and collected. The focus is on initially targeting specific workflows and then branching out as capabilities grow.'}, {'title': 'The Importance of Data Collection and Annotation for Building Effective AI Tools ', 'text': 'Understanding the type of data needed for annotation and collection is crucial for building effective AI tools. The importance of gathering data and the reasons behind it is a significant topic in the world of AI. Customers should not only be trained in using products, but also be given the opportunity to teach what is important and useful. Respecting customer privacy and obtaining explicit permission before using their data is essential. Demonstrating the benefits of sharing data with the tool is crucial in the modern age. Benefits of sharing data include understanding customer intent for better recommendations and quick evolution of the tool. Customers may be willing to share data if they depend on the tool for their livelihood or are friendly to the cause.'}, {'title': 'Data Sharing and Workflow Improvement ', 'text': 'Data contributors may be willing to share workflows or choices with the data set to be trained. Technologies exist for learning without storing all information permanently. Adobe exists in a space where sharing data for improving workflow is beneficial. Some professional workflows may be very protective of their data.'}, {'title': 'Protecting Data in Professional Workflows ', 'text': 'Professional workflows may require protection of data, especially in legal cases. Some scenarios may involve a more permissive relationship with Adobe for non-confidential projects. Different levels of data sharing may be considered in exchange for benefits. Capture high-level data from more people and detailed knowledge from willing participants. Explicit customer studies are currently used to gather detailed feedback.'}, {'title': 'Importance of Responsible Data Collection in Customer Studies ', 'text': \"Customer studies involve visiting and observing users to improve the tool. A more systematic process is needed to train an algorithm for customer studies. Conscious effort is required to balance data collection with customer trust. Adobe has a chief privacy officer to ensure responsible data collection. Privacy is a priority in the development of AI, not an afterthought. Project Puppetron demonstrates Adobe's move towards thinking in three dimensions.\"}, {'title': 'Advancements in 3D Computer Vision for Applying Painting Styles to Videos and Images ', 'text': '3D thinking is being used to assign features to faces in order to apply painting styles to videos or still images of people talking. The technology is able to apply the style of a painting to a person in a video in a way that reflects the motion of the face, unlike traditional filter effects. Even for 2D workflows like stylization, understanding the 3D structure of the world is necessary. 3D computer vision algorithms are improving and initially focusing on specific domains like faces, where there is a lot of prior knowledge about structure. Over time, these algorithms should be able to work for more general applications.'}, {'title': 'The Importance of 3D Reconstruction in AR and VR Applications ', 'text': '3D reconstruction can be invisible to the user but allows for more reliable and correct edits. The face is a very important application for 3D reconstruction. AR and VR serve slightly different purposes, with VR transporting users to an immersive world.'}, {'title': 'Advancements in VR and AR Technology ', 'text': 'VR technology is evolving in terms of hardware and becoming more affordable. VR devices are becoming all-in-one rather than tethered to a box. There is a market bifurcation between consumer and professional use cases for VR and AR. VR is useful for experiencing scale and spatial relationships, especially for architects and designers. AR holds the promise of taking digital assets off the screen.'}, {'title': 'The Promise and Challenges of Augmented Reality ', 'text': \"AR holds the promise of taking digital assets off the screen and putting them in context in the real world. The assets need to adapt to the physical context in which they're being placed. AR is like having a live theater troupe come to your house and put on a performance. AR will have the same issue of adapting to different physical spaces. There is a tension between fidelity and adaptation in AR.\"}, {'title': 'The Influence of Media Characteristics on Fidelity Requirements ', 'text': 'Different types of media may require different levels of fidelity in reproducing characters or stories. The characteristics of the media, such as whether it involves a famous celebrity or a fictional character, may influence the level of fidelity required. Ideas from the game world regarding adaptive characters may become more prevalent in the broader commercial sphere, particularly in the context of augmented reality (AR). Engineering tools may be needed to allow for adaptive characters in AR.'}, {'title': 'Advancements in AR Technology for 3D Design ', 'text': 'AR technology is being used to create adaptive characters. Demonstrations have been shown of converting Photoshop layers into 3D in AR. The focus is on 3D design and making it more spontaneous using AR or immersive technology. One example is laying out objects in a VR headset, which is more intuitive than using a conventional screen and mouse.'}, {'title': 'The Impact of VR and AR on Design ', 'text': 'VR headset allows for a different viewpoint and sense of depth. Fine grained design tasks may be possible with the right UI. Potential explosion of demand for 3D assets driven by AR and real time animation. Tools and devices may help with designing content as well.'}, {'title': 'Importance of Designing Content for Product Evolution ', 'text': 'Designing the content is important. New ideas are being considered, but old ways are also valued. Existing user base should not be offended by changes. Convenience should not come at the cost of control. Evolution and growth are important for the product.'}, {'title': 'Evolution and Breakthroughs in Tool Development ', 'text': 'The tool has always been evolving and growing. There has been a lot of brilliant thought put into how it works today. A fundamental breakthrough, like a single click to select an object, fits nicely into the existing toolset. Radical simplicity can be achieved by encapsulating an entire workflow with a much simpler UI. This can be easier to do in the context of a different device or a tool targeted at a different workflow. Projects like Rush allow professional quality video editing for a certain class of media output.'}, {'title': 'Choosing the Right Video Editing Software for Different Projects ', 'text': 'Quality video editing for different types of media output targeted at different users and experiences. Different software options for different project types, such as using Premiere for a big project and Rush for a quick vacation video. Professional tools offer a richer toolkit and more flexibility, but may take longer to achieve the same output. The idea of using AI for smart defaults and coaching, similar to Google\\'s \"I\\'m feeling lucky\" button. The use of AI as an educational tool to show users different settings and options.'}, {'title': 'The Need for an Educational Tool for Image Control Bars ', 'text': 'The text discusses the need for an educational tool to show the correlation between different control bars for image elements. It mentions the challenge of not knowing the optimal settings and the need for on-demand help when stuck. The idea of a proactive tool making helpful suggestions or providing options for learning is suggested. The concept of multiple intelligent defaults and the ability to choose from several options is mentioned. The conversation shifts to poetry, indicating a change in topic.'}, {'title': 'The Impact of Digital Technology on Human Perception ', 'text': 'The poem reflects the feeling of liberation when leaving the smartphone behind. The use of AI to create versions of ourselves and reality that are more beautiful than actual reality. The creative effort involved in creating this illusion. The inevitability of living in a digital world that is partly artificial. The need for human beings to adjust to this digital world. Comparison of the current digital world with the world a hundred years ago without Instagram and Facebook versions of ourselves.'}, {'title': 'The Impact of Social Media on Self-Presentation ', 'text': 'The use of social media platforms like Instagram and Facebook has led to the presentation of better versions of ourselves online. The use of image modification tools and artificial intelligence has enabled the creation of adjusted or fake versions of ourselves and reality. The human desire to put our best foot forward has always been true, dating back to the 18th century aristocrats who commissioned flattering portraits of themselves. The ability to imagine alternate realities and visualize them raises the question of whether it is a good or bad thing. The shift towards a visual culture has made storytelling and poetry less prominent in shaping alternate realities.'}, {'title': 'The Shift to a Visual Culture ', 'text': 'We have become a very visual culture. In the 19th century, we were a text-based culture. People now prefer quick, visual, and snappy content. Intent plays a significant role in the impact of visual content. It can be harmful if people hold themselves up to an impossible standard based on visual content. The ability to imagine and visualize an alternate reality can be a wonderful thing.'}, {'title': 'The Impact of Alternate Reality and High-Quality Graphics on Exploration ', 'text': 'Alternate reality can inspire people to create new architectural styles and even start businesses. The availability of high-quality graphics may reduce the excitement of exploring new places in person. The joy of exploration, such as going to the moon, is partly driven by the anticipation of what it will look like. The recent discovery of Pluto was a fantastic example of outer exploration.'}, {'title': 'The Importance of Accurate Representation in Media ', 'text': \"Pluto was a fantastic recent discovery with breathtakingly varied and beautiful features. Expanding the ability of the human toolkit to imagine and communicate is a good thing. There are abuses in the use of images and media, and they should be discouraged. The public needs to be aware of what's possible through events like this and not believe everything they see or read. Multiple sets of evidence are needed to really believe something rather than a single media asset. The concept of needing multiple sets of evidence has been true forever. There is a famous story about Anne of Cleves and Henry VIII that illustrates the importance of accurate representation.\"}, {'title': 'Challenges and Benefits of Research Lab Interns ', 'text': \"Holbein painted a picture that Henry VIII wasn't pleased with. The secret to a thriving research lab is interns. Constant influx of new people brings new ideas to the research lab.\"}, {'title': 'The Benefits of Internships in Research ', 'text': 'A constant influx of new people brings new ideas with it. Interns allow for exploration of fanciful or unproven ideas in a lightweight way. Internships ideally lead to new publications for the intern and researcher. Internships provide a portfolio of ideas to draw from for future development. Internships are a way to identify future full-time researchers. Internships build a bridge to university departments for collaboration and recruitment.'}, {'title': 'Building Enduring Relationships Between Companies and University Departments ', 'text': 'The program builds a bridge to university departments to establish enduring relationships with professors. The interns add value through their collaborations and contribute to a virtuous cycle. The long-term legacy of a great research lab includes the impact on people who move through and carry the same model to other companies. The company strongly believes in the complementarity of industrial research and academia. The company hopes that their model will be invested in by other companies, despite making recruitment harder. The idea for the program was born through brainstorming and discussions with interns.'}, {'title': 'The Intern Selection Process ', 'text': 'The process of selecting interns involves sending out a call for interns and reviewing resumes. Candidates are contacted to discuss their interests and find a good match for the projects. Interns stay in touch with their mentors and have internal discussions about project ideas. At the end of two weeks, interns have to decide on a project to pursue.'}, {'title': 'The Dynamics of Decision-Making in Research Labs ', 'text': 'The flexibility of pursuing ideas in research labs. The potential for ideas to change direction once an intern arrives. The decision-making process in research labs. The allocation of credit and blame in research labs. The balance between freedom of choice and impact in project selection. The alternative model of having a single lab director making decisions.'}, {'title': 'Encouraging Innovation and Collaboration in the Lab ', 'text': \"The model of running the lab is based on allowing new ideas to percolate up. There are strategic priorities for the company and areas where investment is needed. The approach is a combination of trickle down and filter up, meeting in the middle. People are not told what to do, but are encouraged to focus on certain areas that would be particularly appreciated. Adobe's broad portfolio of products allows for good ideas to find interested product teams. There is no need to qualify things too much ahead of time.\"}, {'title': 'Challenges and Outcomes of Intern Projects ', 'text': 'The product teams sponsor extra interns occasionally to address specific problems they care about. It is difficult to predict the success of intern projects at the beginning of the summer. Some intern projects may not result in a feature, while others may not be as novel as initially thought. There is variability in the outcomes of intern projects, with some being successful and others not.'}, {'title': 'The Unknown and Breakthroughs in Technology ', 'text': 'Progress and realization of how much is unknown. Revisiting problems multiple times before a breakthrough. Impact of technological breakthroughs on products and the world. Focus on creative and analytics assistants for making useful suggestions. Anticipation of progress in 2019 and beyond.'}, {'title': 'Advancements in Generative Adversarial Networks and Sensei Platform ', 'text': 'Core technologies like generative adversarial networks are immensely promising. The quick practical application of these technologies for mainstream use cases at high resolution with good quality is exciting. The strange and interesting way in which these technologies operate, resembling dreaming or something, is fascinating. The development of a Sensei platform for pulling neural nets and other intelligence models into a central platform, which can be leveraged by multiple product teams at once, is in progress. Transitioning from hand designing for specific use cases to a more standardized approach, which can be accessed in a standard way, is underway.'}, {'title': 'Standardizing Processes and Emerging Technologies at Ford ', 'text': 'Ford is standardizing processes to shorten the time between idea generation and product impact. Products can leverage good ideas from each other, creating an economy of scale. There is a renaissance in AI and real-time ray tracing in graphics, leading to exciting emerging technologies. The combination of AI and graphics technologies will create a future where users can interact with light in real time, with real-world effects and magical properties brought by AI. This future is particularly exciting for creators.'}, {'title': 'Fascination with Snakes and Robotics ', 'text': \"The speaker works in autonomous vehicles as a roboticist and loves robots. The speaker has a fascination with snakes, both natural and artificial robots. There are 2,900 species of snakes in the world, with 875 venomous. The speaker's interest in snakes came from their work in computer animation in the 80s. The speaker started doing things like cloth simulation and soft body simulation in computer animation.\"}, {'title': 'The Evolution of Robotics and Animation ', 'text': \"The idea of animating spring lengths and simulating muscles came from observing the movement of objects. The earliest application of this idea was in a paper called The Motion Dynamics of Snakes and Worms in 1988. The interest in robotics stemmed from simulation and graphics work. A movie called Her Majesty's Secret Serpent was created, featuring a secret agent snake. The interest in building a real radio controlled chip developed from making them as a child. This led to the start of a project to build a real radio controlled chip.\"}, {'title': 'Obsession with Building Snake Robots ', 'text': \"The speaker had a 15 year obsession with building better snake robots. The first snake robot built could only slither sideways, but didn't go forward. The speaker added wheels to the snake robot to address friction issues. The speaker loves creating the illusion of life, which is what drove them to animation. The goal is to create a robot with enough freedom of movement to mimic biological motion. The early snake robot was able to sidewind and go directly forward. The snake robot was used as the ring bearer at the speaker's wedding. The speaker's hobby led to the development of the snake robot.\"}, {'title': 'The Evolution of Autonomous On-Board Computing ', 'text': 'The development of autonomous on-board computing was limited at the time. The first controller was built from discrete logic due to limited computing capabilities. Radio controlled models were used for the second and third versions. The focus was on physicality and coordinated motion. There was a sidestep into creating a cheap toy, learning about clockwork and backlash.'}, {'title': 'The Evolution of Robot Design and Engineering ', 'text': 'The text discusses the experience of building and learning from different versions of robots, such as S9 and S3. The engineer mentions the challenges and limitations of certain robot models, such as the motors wearing out and the inability to buy replacements for S3. The engineer also talks about the design choices made in creating the robots, such as tapering the snakes for mechanical and aesthetic reasons. The text mentions the display of one of the robots at the International Spy Museum in Washington, DC. The engineer reflects on the unique and biological-inspired design of S5, which sets it apart from typical robotic designs.'}, {'title': 'The Future of Onboard Compute and Object Recognition in the Spy Museum ', 'text': 'Spy Museum in Washington, DC. Conspiracy theory about the museum being fake. Use of Raspberry Pi for onboard compute. Addition of vision accelerator chips for object recognition. Convergence of hobby work and professional work. Potential for true autonomy with onboard compute and batteries.'}, {'title': 'Promoting Autonomy and Interest in Robotics ', 'text': \"Autonomy with onboard compute, onboard batteries, and biomimetic quality. Appeal to children and adults' perception of the robots. Encouragement of interest in technology, especially for girls. Cost and value of the robots. Potential for using real artificial muscle material in future designs. Being in research as a license to be curious.\"}, {'title': 'The Curiosity of Research in Intelligent Agent Development ', 'text': 'Being in research is a license to be curious. Hobby of reading biology and being curious about things. Trying to bring life and beauty into something inanimate. Convergence of intelligent agent research with vision and voice synthesis. Aim for meaningful conversation with intelligent agents, not necessarily human level intelligence.'}, {'title': 'The Importance of Meaningful Interaction and Reasoning in Robot Pet Ownership ', 'text': 'The goal is to have a robot pet owner understand what the robot thinks about and can reason about. Meaningful interaction with the robot is important, similar to the interaction one might have with a dog. The reasoning system should be able to explain why it knows or thinks something, creating a sense of understanding and knowledge. The robot serves as a muse for thinking about the future of AI and helps in debugging more elaborate behavior.'}, {'title': 'The Future of AI and Virtual Reality ', 'text': 'The robot is the muse for thinking about the future of AI and what to invent next. Bringing virtual objects into the physical world through augmented reality is more likely than building intelligent robots. Many ideas that might take five years to build a robot to do can be done in a few weeks with digital assets. Living with virtual personalities for a long time will make intelligent robots less surprising when they become commonplace.'}, {'title': \"Speaker's Excitement for the Future and Gratitude for Conversation \", 'text': 'The speaker compares the future to a world with \"Siri with legs or Alexa on hooves\". The speaker is excited about the convergence of different strands of their career. The conversation ends with the recitation of a favorite poem about mortality and immortality. The speaker expresses gratitude for the conversation.'}, {'title': 'Title ', 'text': 'Creating Inspiration and EmpowermentText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 19:24:54.239462 ...\n",
      "Best SD: 2.2825424421026654, Best iteration: 5\n",
      "done get topics 2024-03-28 19:24:55.067682.\n",
      "Stage 2 start time 2024-03-28 19:24:55.067702\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Revolutionizing Creativity: Adobe Research's Efforts to Automate Tedious Tasks\n",
      "2. Advancements in Image Captioning and Synthesis\n",
      "3. Types of Research and Publishing\n",
      "4. Advancements in Computer Vision and Machine Learning by Adobe\n",
      "5. Data Sharing and Workflow Improvement\n",
      "6. Advancements in 3D Computer Vision for Applying Painting Styles to Videos and Images\n",
      "7. Importance of Designing Content for Product Evolution\n",
      "8. The Shift to a Visual Culture\n",
      "9. Challenges and Benefits of Research Lab Interns\n",
      "10. The Evolution of Autonomous On-Board Computing\n",
      "Stage 2 done time 2024-03-28 19:26:04.625406\n",
      "stage_2_titles: len: 10\n",
      "[\"1. Revolutionizing Creativity: Adobe Research's Efforts to Automate Tedious Tasks\", '2. Advancements in Image Captioning and Synthesis', '3. Types of Research and Publishing', '4. Advancements in Computer Vision and Machine Learning by Adobe', '5. Data Sharing and Workflow Improvement', '6. Advancements in 3D Computer Vision for Applying Painting Styles to Videos and Images', '7. Importance of Designing Content for Product Evolution', '8. The Shift to a Visual Culture', '9. Challenges and Benefits of Research Lab Interns', '10. The Evolution of Autonomous On-Board Computing']\n",
      "remove_questions start time: 2024-03-28 19:26:04.646457\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 19:33:09.932993\n",
      "chunks_text len: 101\n",
      "extract_keypoints start time: 2024-03-28 19:33:09.933140\n",
      "extract_keypoints done time 2024-03-28 19:36:46.022686\n",
      "Start time: 2024-03-28 19:36:46.022961\n",
      "Stage 1 done time 2024-03-28 19:40:25.853397\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Ilya Sotskever: Cofounder and Chief Scientist of OpenAI ', 'text': \"Ilya Sotskever is the cofounder and chief scientist of OpenAI, and one of the most cited computer scientists in history with over 165,000 citations. He is considered one of the most brilliant and insightful minds in the field of deep learning. The conversation was recorded before the outbreak of the pandemic. The speaker sends love and support to those affected by the medical, psychological, and financial burden of the crisis. The Artificial Intelligence Podcast is mentioned and ways to support it are provided. The speaker's Twitter handle is given for connecting.\"}, {'title': 'The Importance of Cash App and the History of Money ', 'text': 'Twitter handle is @lexfriedman, spelled F R I D M A N. The show is presented by Cash App, the number one finance app in the App Store. Use code LEXPODCAST when getting Cash App. Cash App allows you to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Cryptocurrency is still in its early days of development, despite being around for over 10 years. The history of money, including the creation of the US dollar and the release of Bitcoin, is fascinating. The book \"Ascent of Money\" is recommended for learning about the history of money.'}, {'title': 'The Early Development of Cryptocurrency and Promotions from Cash App ', 'text': 'Cryptocurrency is still in its early days of development and aims to redefine the nature of money. Cash App is offering a promotion where using the code LEXPODCAST gives $10 and donates $10 to FIRST, an organization advancing robotics and STEM education. Ilya Satsgever was one of the authors of the AlexNet paper, which marked the big catalytic moment that launched the deep learning revolution.'}, {'title': 'Training Deep Neural Networks with Backpropagation ', 'text': 'Deep neural networks can be trained end to end with backpropagation. James Martens invented the Hessian free optimizer in 2010, which allowed training a 10 layer neural network from scratch. The ability to train a big neural network means it can represent very complicated functions. A neural network with 10 layers can simulate the human brain running for some number of milliseconds. Neuron firings are slow, so in 100 milliseconds, the neurons only fire 10 times.'}, {'title': 'Neural Network Training and Overparameterization ', 'text': 'Neuron firings are slow and only fire 10 times in 100 milliseconds. The idea of training a very big neural network on lots of supervised data was already present. The theory that having more data than parameters prevents overfitting is incomplete. Neural networks being heavily overparameterized was not discouraging. The evidence before suggested that having a huge number of parameters was okay.'}, {'title': 'Challenges and Innovations in Training Big Neural Nets ', 'text': 'The theory was that with a big data set and a big neural net, it was going to work. Overparameterization was not seen as a problem. The main doubt was whether there would be enough compute to train a big enough neural net. Backpropagation was thought to work. Alex Kerchevsky wrote fast CUDA kernels for training convolutional neural nets, leading to the decision to proceed with the project.'}, {'title': 'The Role of Intuition and the Human Brain in Understanding Neural Networks ', 'text': 'The intuition about neural networks can come from empirical results and also from pen and paper or marker and whiteboard thinking. The human brain plays a role in the intuition about neural networks for deep learning researchers. The idea of a neural network is directly inspired by the brain, as seen in the work of Rosenblatt in the 60s and the concepts of neurons in the brain.'}, {'title': 'The Evolution of Neural Networks and Their Relation to the Human Brain ', 'text': \"The idea of using ideas from the computer and automata to design a computational object similar to the brain. The invention of the neuron inspired by the brain. The development of the convolutional neural network and its success in image recognition. The assumption that an artificial neuron is not that different from the brain if it's cleaned hard enough. The success of deep learning in the current time. The interesting differences between the human brain and artificial neural networks.\"}, {'title': 'The Difference Between Human Brain and Artificial Neural Networks ', 'text': 'The difference between the human brain and artificial neural networks is interesting for the next decade or two. Artificial neural networks have important advantages over the brain. The brain uses spikes, which may or may not be important.'}, {'title': 'Architectural Differences and Questions in Neural Networks ', 'text': 'One big architectural difference between artificial neural networks. Interest in spiking neural networks and the need to simulate non-spiking neural networks in spikes. Questions around back propagation and deep learning. The uncertainty of why giant neural networks and learning rules should work at all.'}, {'title': 'The Significance of Cost Functions in Training Neural Networks ', 'text': 'Neural networks were proposed as a great idea because the brain is a neural network. The challenge was to figure out how to train neural networks. The big idea in training neural networks is the cost function, which measures the performance of the system. It may seem trivial now, but the concept of a single cost function was a significant idea at the time. There may be other systems that do not necessarily have a single cost function, but may have many cost functions.'}, {'title': 'Title ', 'text': 'Understanding the Behavior of GANs through Equilibrium and Mathematical Objects'}, {'title': 'The Role of Cost Functions in GAN and Deep Learning ', 'text': 'GAN does not have a clear cost function. The analogy of cost function in GAN is compared to biological evolution or the economy. Questioning if cost functions in deep learning are holding us back. Self play in reinforcement learning systems starts to touch on the idea of cost function.'}, {'title': 'Importance of Self Play, Exploration, and Cost Functions in Reinforcement Learning ', 'text': 'Self play and exploration are important in reinforcement learning systems. Cost functions are considered great and serve well in various applications. There may be potential for new ways of looking at things involving cost functions in a less central way. Spiking and the learning rule of the brain are areas of interest for potential usefulness.'}, {'title': 'Spike Time Independent Plasticity: A Fundamental Learning Rule of the Brain ', 'text': 'Neuroscientists have figured out spike time independent plasticity as a learning rule of the brain. STD is a particular learning rule that uses spike timing to update synapses. The timing of signals is a fundamental property of the brain that is not fully captured.'}, {'title': 'The Role of Timing in the Brain and Recurrent Neural Networks ', 'text': \"The brain's fundamental property is the timing of signals. Recurrent neural networks are a simplified version of the brain's timing mechanism. The brain is a continuous version of recurrent neural networks, allowing for all possible timings and containing information within those timings. Recurrent neural networks are considered amazing and capable of performing any task. Recurrent neural networks have been superseded by transformers, but there is a possibility of a comeback in the future.\"}, {'title': 'Advancements in Natural Language Processing and Language Modeling with Non-Recurrent Transformers ', 'text': \"Breakthroughs in natural language processing and language modeling have been with transformers that don't emphasize recurrence. Recurrent neural networks are still possible for processing sequences. Neural networks maintain a high dimensional hidden state and update it when an observation arrives. Expert systems and symbolic AI also involved maintaining a hidden state.\"}, {'title': 'The Future of Building Large Scale Knowledge Bases within Neural Networks ', 'text': 'Symbolic AI involves maintaining a hidden state as its knowledge base and growing it through sequential processing. The hidden state in symbolic AI is similar to the hidden state in LSTM or RNN. In the expert system analogy, knowledge is stored in the connections and short term processing is done in the hidden state. There is potential for building large scale knowledge bases within neural networks. The speaker wants to explore the future of building large scale knowledge bases within neural networks.'}, {'title': 'The Underestimation of Deep Learning in Machine Learning ', 'text': \"Neural networks have been around for many decades, but the key idea about deep learning was underestimated. People in machine learning didn't believe that large neural networks could be trained. There was a lot of debate going on in machine learning about what are the capabilities of neural networks.\"}, {'title': 'Debate in Machine Learning about Methods and Data ', 'text': 'Debate in machine learning about the right methods due to lack of hard facts and benchmarks. Deep learning ideas were present but lacked supervised data and compute. Conviction in the combination of existing methods with data and compute was the missing piece.'}, {'title': 'The Evolution of Computer Vision: Data, Compute, and Conviction ', 'text': \"The missing piece for the success of computer vision was the combination of data, compute (GPUs), and conviction. The presence of compute and supervised data allowed the empirical evidence to convince the majority of the computer science community. There was a key moment with Jitendra Malik and Alex Alyosha Efros who were skeptical, and Jeffrey Hinton who was not skeptical. ImageNet served as a convincing moment for the computer vision community. The collaboration of Jitendra Malik, Alex Alyosha Efros, and Jeffrey Hinton represented the big pillars of the computer vision community, leading to a shift in perspective. It's not enough for the ideas and compute to be present, they also need to convince the cynicism within the community.\"}, {'title': 'Progress in AI and the Importance of Hard Benchmarks ', 'text': 'Neural networks faced skepticism and disbelief for decades. Hard tasks and benchmarks are necessary to produce undeniable evidence and make progress in the field of AI. The field of AI is making progress today due to the presence of hard benchmarks representing true progress. The contribution of recent ideas in AI in computer vision, language, and natural language has been significant.'}, {'title': 'AI Ideas and Principles in Computer Vision, Language, and Reinforcement Learning ', 'text': 'AI ideas in computer vision, language, natural language processing, reinforcement learning. Fundamental science of deep learning. Machine learning has a lot of unity and overlap of ideas and principles. Simple principles apply in almost the same way to different modalities and problems.'}, {'title': 'The Evolution of Architectures in Computer Vision and NLP ', 'text': \"Computer vision and NLP are very similar to each other. Today they differ in that they have slightly different architectures. We use transformers in NLP and we use convolutional neural networks in vision. It's possible that one day this will change and everything will be unified with a single architecture. Today, there's just one transformer for all those different tasks. In the past, there were a huge number of architectures for every different tiny problem in natural language processing. There has been a trend towards unification and simplification of architectures in AI over time.\"}, {'title': 'Fragmentation and Unification in AI ', 'text': 'Fragmentation and specialization in AI has been subsumed by deep learning, leading to unification. Vision is expected to become unified with natural language. Convolutional neural net is computationally efficient, while RL requires different techniques due to the need for action and exploration. There is potential for broader unification between RL and supervised learning, with RL making decisions to improve supervised learning.'}, {'title': 'Title ', 'text': 'Reinforcement Learning in Supervised Learning ImprovementText '}, {'title': 'Dealing with Non-Stationarity in Traditional Static Problems and Reinforcement Learning ', 'text': 'The world is non-stationary, causing changes in actions and perceptions. Traditional static problems involve applying a model to a distribution, while reinforcement learning involves tools to reduce variance of gradients. Commonalities between traditional static problems and reinforcement learning include using the same neural net, computing gradients, and applying Adam. There are some small differences between traditional static problems and reinforcement learning that are not completely insignificant.'}, {'title': 'The Importance of Language and Perspective in Problem Evaluation ', 'text': 'Language is fundamental to everything according to Noam Chomsky. The question of whether a problem is hard is slightly wrong. Effort required to reach human level performance on a benchmark is important. Perspective and frame of reference play a role in evaluating problems. The significance of small differences in perspective.'}, {'title': 'The Difficulty of Solving Hard Problems ', 'text': 'Solving a problem makes it stop being hard. The difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems. Language understanding may be harder than visual perception, depending on the definition of \"top notch\" understanding. The difficulty of language understanding also depends on how it is defined.'}, {'title': 'Interconnectedness of Vision and Language in the Human Brain ', 'text': 'Vision and language are interconnected in the human brain. Chomsky suggests that language is the starting point for understanding vision. There may be a fundamental hierarchy of ideas represented in the brain through language. It is possible that achieving deep understanding in images and language requires the same kind of system. Machine learning may be capable of achieving deep understanding in both images and language. There is uncertainty about whether machine learning can achieve deep understanding in both images and language.'}, {'title': 'The Importance of Definitions in Determining Value ', 'text': \"The importance of definitions in determining the value of reading and vision. The author's definition of a system being impressive. The author's belief in the continuous ability of humans to surprise and impress. The author's preference for monogamy and the idea of continuous surprise and pleasure in a long-term relationship.\"}, {'title': 'The Influence of Randomness and Humor on Social Media Engagement ', 'text': 'Friends continue to surprise with injection of randomness, a source of inspiration, wit, and humor. Subjective test of impressing with intelligence and understanding of images. Systems of January 2020 have not been impressive in understanding complicated images. People click like on internet for humor, wit, and insight. The most romanticized question is looking back to the past.'}, {'title': 'The Beauty of Deep Learning ', 'text': 'The most beautiful thing about deep learning is that it actually works. The idea that making the neural network large and training it on a lot of data can make it function like the brain is surprising and beautiful. It is unbelievable that AI with neural networks works.'}, {'title': 'The Importance of Empirical Evidence in Optimization, Evolution, and Physics ', 'text': 'Optimization has empirical reasons to believe that it should work on most problems we care about. Evolution is empirical and shows that the evolutionary process is a good way to design organisms that survive in their environment. Physics calculations and experiments are important in coming up with new physics theories and making predictions. Understanding how the whole thing works requires more than just empirical evidence.'}, {'title': 'The Intersection of Biology and Physics in Deep Learning ', 'text': 'Deep learning is a combination of biology and physics. The experiment sometimes came before the theory in deep learning. The validation of a theory in deep learning is amazing. Deep learning is not just a mathematical theory, but also a biological theory. Deep learning involves using data to make predictions and train neural networks. Making the neural network larger can result in better performance. There are analogies between deep learning and biology.'}, {'title': 'The Potential of Machine Learning in Unifying Biology and Physics ', 'text': 'Biology is complicated and lacks good predictive theories. Physics has super precise theories that make amazing predictions. Machine learning is in between biology and physics. There is a hope that machine learning can help discover the unification of biology and physics. Deep learning is still massively underestimated. Most of the progress in the past 10 years has been in deep learning.'}, {'title': 'Advancements in Deep Learning ', 'text': 'Deep learning has consistently exceeded expectations over the past 10 years. The field continues to make robust progress and will likely do so for quite a while. Individual researchers may find it challenging due to the large number of researchers in the field. Having access to a lot of computing power can lead to interesting discoveries.'}, {'title': 'Challenges of Managing a Compute Cluster for Deep Learning Experiments ', 'text': 'Managing a huge compute cluster is a challenge for running experiments. The stack of deep learning is becoming quite deep, from ideas to building data sets to distributed systems. The speaker is unsure about certain aspects and is seeking answers from someone they consider smart. The use of one GPU is mentioned. The speaker acknowledges the intelligence of the person they are speaking to.'}, {'title': 'The Complexity of the Data Science Stack ', 'text': 'The stack for data science is becoming increasingly deep, making it difficult for a single person to be world class in every layer. Efficient learning is important in order to keep up with the complexity of the data science stack. There will be breakthroughs in data science that do not require a huge amount of compute. Some breakthroughs and building systems will require a significant amount of compute. The need for a large amount of compute is obvious when trying to accomplish tasks that require it, such as neural networks.'}, {'title': 'Importance of Compute in Neural Networks ', 'text': 'The amount of compute is important for neural networks. Small groups and individuals can still do important work in deep learning. Making neural networks larger can lead to better performance, which contradicts statistical ideas. Double descent occurs for pretty much all practical deep learning systems.'}, {'title': 'Double Descent Phenomenon in Deep Learning Systems ', 'text': 'Double descent occurs for pretty much all practical deep learning systems. Increasing the size of the neural network slowly, without early stopping, leads to a rapid increase in performance, followed by a decrease at the point of zero training error, and then an increase in performance again. The phenomenon of performance getting worse at the point of achieving zero training loss is counterintuitive for deep learning systems. It is hard to be sure what this phenomenon means.'}, {'title': 'Understanding Overfitting in Deep Learning ', 'text': 'Deep learning phenomena are not always monotonic. Overfitting occurs when the model is sensitive to small random unimportant stuff in the training data set. A small model with a large data set is insensitive to randomness, reducing overfitting. Neural networks do not overfit every time very quickly before being able to learn anything.'}, {'title': 'The Impact of Dimensionality on Neural Network Training ', 'text': 'Neural networks with a huge number of parameters can achieve zero error in a big subspace. Stochastic Gradient Descent (SGD) finds the point with the smallest norm in that subspace. SGD is insensitive to small randomness in the data when the dimensionality is high. When the dimensionality of the data is equal to the dimensionality of the model, there is a one to one correspondence between all the data sets and the models. Small changes in the data set can have a significant impact when the dimensionality of the data is equal to the dimensionality of the model.'}, {'title': 'The Impact of Early Stopping on Model Performance ', 'text': \"['Small changes in the data set lead to large changes in the model, resulting in worse performance.', 'It is best for the model to have more parameters than the data, but only if early stop is not introduced.', 'Early stopping can make the double descent bump almost completely disappear.', 'Early stopping is when you monitor validation performance and stop training when it starts to get worse.', 'Not doing early stopping results in a very pronounced double descent.']\"}, {'title': 'Sensitivity of Model to Data Set Size ', 'text': 'When the data set has as many degrees of freedom as the model, small changes to the data set lead to noticeable changes in the model. When there is a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. The model is very sensitive to all the randomness when the data set has as many degrees of freedom as the model. The resulting solution is able to discard the small changes and randomness when there is a lot more data than parameters or a lot more parameters than data. Jeff Hinton suggested throwing away back propagation and starting over.'}, {'title': 'Challenges and Considerations in Training Neural Networks ', 'text': 'The suggestion to throw away back propagation and start over was made, but it was also acknowledged that back propagation is very useful and should be kept. The idea of finding alternative methods of training neural networks was discussed, with a focus on learning from how the brain learns. The importance of implementing any discovered mechanisms of learning in the brain into neural networks was highlighted. The speaker expressed their personal support for back propagation and its usefulness.'}, {'title': 'Advancements in Neural Network Algorithms ', 'text': 'Back propagation is a great algorithm for solving fundamental problems in finding neural circuits subject to constraints. The speaker believes it is unlikely that there will be a dramatically different algorithm in the future. The neural network of AlphaZero plays Go, a game that requires reasoning, better than 99.9% of all humans, demonstrating the ability of neural networks to reason. The speaker disagrees with the idea that Go is not a trivial or obvious form of reasoning.'}, {'title': 'The Role of Reasoning in Go and Neural Networks ', 'text': 'Go is reasoning and has elements of reasoning. Reasoning is akin to search, with a sequential element and stepwise consideration of possibilities. Playing Go and using a single neural network without search is akin to reasoning. There is an existence proof in a particular constrained environment that a process akin to reasoning exists. Humans are also an existence proof of reasoning. The architecture that will allow neural networks to reason is being discussed.'}, {'title': 'The Future of Neural Networks and Reasoning ', 'text': 'Neural networks that will produce reasoning breakthroughs in the future may be very similar to the architectures that exist today, but possibly a little more recurrent and deeper. Neural networks are powerful and capable of reasoning, similar to humans. It is possible that the kind of reasoning seen in neural networks is just a form of weak reasoning, not fundamentally different from human reasoning. The capability of neural networks to reason is dependent on the tasks they are trained on.'}, {'title': 'The Capabilities and Limitations of Neural Networks ', 'text': \"Neural networks are capable of reasoning. Training a neural network on a task that doesn't require reasoning will not result in reasoning. Neural networks will solve problems in the easiest way possible. Finding the shortest program that outputs the data allows for the best prediction. Finding the shortest program to generate data is not a computable operation.\"}, {'title': 'Challenges in Generating Data with Shortest Program ', 'text': 'Finding the shortest program to generate data is not computable. Neural networks are the next best thing that works in practice. We are able to find a small or large circuit which fits our data in some way. Overparameterized neural nets can be seen as a large circuit whose weights contain a small amount of information. The training process of a neural network involves slowly transmitting entropy from the dataset to the parameters.'}, {'title': 'The Importance of Training in Deep Learning ', 'text': 'The amount of information in the weights ends up being not very large, which would explain why they generalize so well. The large circuit might be helpful for generalization. The fundamental reason for pushing on deep learning is that we are able to train them. Training comes first, and contorting neural networks around the training pillar is essential. Being trainable means starting from scratch and quickly converging towards knowing a lot.'}, {'title': 'Training Deep Neural Networks for Efficient Knowledge Acquisition ', 'text': 'Training a neural net from scratch can lead to quickly gaining a lot of knowledge. The resources at your disposal can be used to train a neural net to achieve useful performance. It is not possible to find the shortest program, so the focus should be on training neural networks instead. There are no good precedents of people successfully finding programs, so training a deep neural network is the right way to go.'}, {'title': 'The Power of Deep Neural Networks ', 'text': 'Training a deep neural network is the right way to go about it. There are not good illustrations of training a deep neural network yet. It is unwise to bet against deep learning. Neural networks continue to surprise us. Deep neural networks can aggregate important information over long periods of time.'}, {'title': 'Neural Nets as Long-Term Knowledge Bases ', 'text': 'Neural nets experience serves as long term knowledge. People have trained neural nets to act as knowledge bases. The challenge is to come up with a better mechanism for forgetting useless information and remembering useful information. There are no mechanisms that remember really long term information currently. The speaker is interested in the compression of information.'}, {'title': 'The Challenge of Interpreting Knowledge Bases ', 'text': 'The word \"precisely\" is liked and the speaker is thinking about the compression of information in knowledge bases. Knowledge bases represent a compressed, structured knowledge base, similar to the knowledge that Wikipedia represents. Neural networks are not necessarily interpretable with the kind of knowledge they have discovered. The speaker apologizes for their human-centric thinking about what knowledge is. The dream of the semantic web is mentioned as a really nice compressed knowledge base. Neural networks may be noninterpretable if you look at their weights, but their outputs should be very interpretable.'}, {'title': 'The Importance of Self-Awareness in Neural Networks ', 'text': \"Neural networks need to be made interpretable. Generating examples to demonstrate both dumb and brilliant behavior. Lack of self-awareness in neural networks. Importance of self-awareness in neural networks. Self-awareness will allow neural networks to know what they know and what they don't know. Self-awareness will help neural networks to invest in increasing their skills optimally.\"}, {'title': 'The Importance of Investing in Skill Development ', 'text': \"The importance of knowing where to invest to increase skills optimally. Two answers to the question of interpretability: analyzing the neurons in a neural net and taking a human-centric approach. OpenAI has done work on analyzing the neurons in a neural net. The human-centric approach involves understanding a person's mental model and conception.\"}, {'title': 'The Comparison of Human Memory and Neural Networks ', 'text': 'Human beings have the ability to remember useful information and forget the rest, similar to neural networks. Neural networks are currently not as good at reasoning as human beings. Impressive feats of reasoning include writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.'}, {'title': 'The Power of Machine Learning and Deep Learning in Problem Solving ', 'text': 'Proving hard theorems and solving open-ended problems with out-of-the-box solutions. Machine learning and deep learning have the ability to produce unambiguous results that can change the conversation. The debate and conversation change when unambiguous results are produced. There may come a time when we run out of hard problems to solve. The problem of mortality is still a sticky problem that has not been figured out.'}, {'title': 'History and Importance of Language Models ', 'text': 'The history of language models dates back to the Elman network in the 80s. The trajectory of neural networks and language changed due to data and compute. The size of language models is crucial for their effectiveness in predicting.'}, {'title': 'The Importance of Large Language Models for Understanding Semantics ', 'text': 'Language models need to be large in order to predict the next word. Initially, language models notice broad strokes and surface level patterns. As language models improve, they start to notice certain words occurring often, spellings, syntax, and eventually semantics and facts. Noam Chomsky disagrees with the idea that incremental steps and larger networks/compute are necessary for language models to understand semantics. The speaker emphasizes the importance of language models being larger in order to progress to understanding semantics.'}, {'title': 'Understanding Language Semantics in Larger vs. Smaller Models ', 'text': \"Larger network and compute can understand language semantics without imposing a theory of language onto the learning mechanism. Chomsky's concept of imposing structural language is not fully understood, but larger language models show signs of understanding semantics compared to smaller models. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. Training a small LSTM model on Amazon reviews showed that increasing the size of the model led to better understanding of semantics.\"}, {'title': 'Effect of LSTM Size on Sentiment Representation ', 'text': 'Increasing the size of the LSTM from 500 to 4,000 cells led to one neuron representing the sentiment of the review. Sentiment is a semantic attribute, not a syntactic attribute. Small neural nets do not capture sentiment while large neural nets do. The theory is that as neural nets increase in size, they focus more on semantics than syntax. The implication is that as neural nets grow in size, they start to focus on semantics.'}, {'title': 'GPT2: A Transformer with 1.5 Billion Parameters ', 'text': 'GPT2 is a transformer with one and a half billion parameters. It was trained on about 40 billion tokens of text obtained from web pages linked to from Reddit articles with more than three outputs. The transformer is the most important advance in neural network architectures in recent history. The models show signs of partial semantic understanding.'}, {'title': 'The Success of the Transformer: A Combination of Multiple Ideas ', 'text': \"The transformer is a combination of multiple ideas simultaneously, of which attention is one, but not the only key. The transformer's success is due to the simultaneous combination of multiple ideas, and removing any idea would make it much less successful. The transformer runs really fast on the GPU, making a huge difference in its performance. The transformer is not recurrent, which makes it more shallow and easier to optimize. The use of attention is not the main innovation of the transformer, as attention existed for a few years before its implementation.\"}, {'title': 'Success of Non-Recurrent Model in GPU Optimization ', 'text': \"The model is a great fit to the GPU and is not recurrent, making it easier to optimize. The combination of factors make the model successful in making great use of the GPU. It allows users to achieve better results for the same amount of compute. The model's performance was surprising and amazing to witness. There has been significant progress in GANs and improving the samples produced by GANs.\"}, {'title': 'Advancements in GANs and Language Models ', 'text': 'Progress in GANs has been amazing, with realistic faces being produced. Text generation has not progressed as much as GANs. There was a sudden leap in GANs from 2015 to the best models. Even though theory predicted it, seeing the progress with own eyes was stunning. There is quick adaptation to the amazing language modeling capabilities of GPT2 models. Some cognitive scientists doubt that GPT2 models truly understand language. The next barrier may be the dramatic economic impact of these advancements.'}, {'title': 'The Economic Impact and Global Reach of AI Advancements ', 'text': 'The economic impact of AI advancements is not yet fully realized. The progress in AI is difficult for people outside the field to understand. There is a lot of brilliant work in AI, particularly in languages like Russian and Chinese, that the rest of the world is not aware of. Translation is already a huge industry, with billions of people involved.'}, {'title': 'The Impact of Translation and Self-Driving Technology ', 'text': 'Translation is already huge and hugely positive, with billions of people interacting with big chunks of the internet primarily through translation. Self driving is going to be hugely impactful, driven by deep learning, and there may be a connection between language and vision tasks in the future.'}, {'title': 'The Power of GPT and Transformers in Language and Vision Tasks ', 'text': 'GPT and transformers can handle both language and vision tasks, which is an interesting unification. The process of making a transformer bigger and giving it more data allows it to perform amazing tasks. GPT and transformers are fundamentally simple to explain and train. The next steps with GPT two may involve exploring larger versions and addressing many questions. One question is whether the model can use its own intelligence to decide what data it wants to memorize from the internet.'}, {'title': 'Active Learning and Selective Data Acceptance in AI Models ', 'text': 'The model should use its own intelligence to decide what data to accept and reject, similar to how people are selective about what they learn. Active learning would be beneficial for the model to have. Companies may keep private breakthroughs to themselves, hindering progress in solving fundamental problems like self-driving and specific tasks.'}, {'title': 'The Importance of Active Learning in Problem-Solving ', 'text': 'Active learning requires a problem that requires it. Research about capability is difficult without a task. Getting results on MNIST or a clever formulation of MNIST is no longer convincing. Active learning will naturally arise as problems that require it pop up.'}, {'title': 'Concerns about the Potential Misuse of Powerful AI Systems ', 'text': 'OpenAI has brought up concerns about the potential detrimental effects of releasing powerful artificial intelligence systems like GPT2. There is nervousness about the possible misuse of AI systems, particularly in generating realistic text that could be used by bots in unforeseen ways. The need to start a conversation about how to manage the use of AI systems and privately discuss with others, including competitors. OpenAI released a report on the experience and is seeking insights on how to handle the implications of creating powerful AI systems.'}, {'title': 'The Impact of AI Transitioning to Maturity ', 'text': 'AI is transitioning from a state of childhood to a state of maturity. The impact of AI is large and growing. It is wise to consider the impact of releasing AI systems before doing so. A staged release of AI models, such as GPT2, seemed logical. The results of GPT2 were stunning and had the potential to reduce the cost of information.'}, {'title': 'Responsible Release of Powerful Models ', 'text': 'A staged release of the model was logical and allowed for observation of its usage. Many people used the models in cool ways with no negative applications known. Other people replicated similar models, raising questions about the responsibility of releasing powerful models. There is a moral and ethical responsibility to communicate the potential misuse of powerful models, as seen with GPT2 and misinformation.'}, {'title': 'The Importance of Collaboration and Expert Input in AI Development ', 'text': \"GPT2's potential for misinformation was initially unclear. It is important to seek input from experts outside of one's own group. Building trust between companies is important in the development of AI technology. Collaboration and discussion with colleagues from other companies is valuable. The increasing power of AI technology requires a collective effort from all developers.\"}, {'title': 'The Potential Negative Consequences of Powerful AI Systems ', 'text': \"Ultimately, we're all in it together. Consider the potential negative consequences of powerful AI systems. Concern about a race for AI development leading to closed development and lack of idea sharing. The speaker has been a pure academic for 10 years and enjoys sharing ideas. The speaker is uncertain but believes in the potential of deep learning and other small ideas.\"}, {'title': 'The Power of Self Play in Learning ', 'text': 'Self play is a powerful mechanism for systems to learn in a competitive setting. Building AGI will require deep learning plus some additional ideas. Self play has the ability to surprise us in truly novel ways. Examples of surprising behaviors from self play systems include Dota bot, multi-agent hide and seek, and alpha zero.'}, {'title': 'The Surprising Creativity of AGI Systems ', 'text': 'AGI systems produce unexpected behaviors, which are creative solutions to problems. The ability of AGI to surprise us is an important aspect that current systems do not exhibit routinely. An AGI system would fundamentally surprise us with useful solutions to problems. Self-play mechanisms have been used in the game or simulation context, but simulation is just a tool with certain strengths.'}, {'title': 'The Use of Simulation in Robotics and AI ', 'text': 'Simulation is a tool with strengths and weaknesses that should be used. Criticisms of self play and reinforcement learning include their current results being demonstrated in simulated or constrained environments. Transfer from simulation to the real world is possible and has been exhibited many times. Success in vision and demonstration of a robot by OpenAI in the summer.'}, {'title': 'OpenAI Demonstrates Training a Robust Robot Hand in Simulation ', 'text': 'OpenAI demonstrated a robot hand trained entirely in simulation. The training was done in simulation, not in the physical world. The simulation was trained to be robust to many different things. The policy learned in simulation was trained to be very adaptive. The hand was never trained with a glove or a stuffed giraffe.'}, {'title': 'The Potential of Deep Learning Transfer Capabilities ', 'text': 'The video demonstrates a transfer from the simulated world to the physical world. The transfer capabilities of deep learning are expected to increase in general. Better transfer capabilities will make simulation more useful. Simulation can be used to learn and carry experiences to the real world, similar to how humans learn from playing computer games.'}, {'title': 'The Importance of Having a Body for Learning and Experiencing ', 'text': 'The importance of having a body for learning and experiencing things that cannot be learned without a body. The ability to compensate for the lack of physical modalities, as seen in the example of Helen Keller. The idea that success is still possible even without a physical body. The concept of consciousness and its connection to having a body or not.'}, {'title': 'The Nature of Consciousness and Self-Awareness ', 'text': \"The idea of consciousness and a more constrained version of that is self awareness. It's hard to define consciousness. It's definitely interesting and fascinating. It's possible that our systems will be conscious. Humans are conscious and artificial neural nets may also be conscious if they are sufficiently similar to the brain.\"}, {'title': 'Understanding Consciousness and Artificial Neural Networks ', 'text': 'Artificial neural nets should be conscious if they are similar to the brain. There is still an open question about the complexity and interest of the brain. It is unlikely that we will not be able to make progress in understanding consciousness. The concept of intelligence is poorly defined. The discussion includes reasoning and memory.'}, {'title': 'Challenges and Potential of Deep Learning Systems ', 'text': 'Deep learning system solving pedestrian tasks like machine translation or computer vision without making mistakes would be impressive. Current deep learning systems make different mistakes than humans, leading to skepticism. There is a certain frontier of capabilities in natural language processing that has not yet been demonstrated.'}, {'title': 'Criticism of AI Models and Comparison to Human Error ', 'text': \"Mistakes in AI models are criticized in a way that is similar to criticizing any group of creatures as the other. GPT2 may be much smarter than human beings at many things, with a lot more breadth of knowledge and possibly depth on certain topics. Humans don't make mistakes that AI models do, such as autonomous vehicles and other artificial intelligence systems.\"}, {'title': 'Analyzing the Progress of AI and the Impact of Failures ', 'text': 'The process of analyzing the progress of AI often focuses on one case where the system fails in a big way where humans would not. Many people write articles about AI failures, leading the public to believe that the system is not intelligent. It is confusing to judge progress in AI, especially when new robots demonstrate something impressive. People will start to be impressed by AI once it starts to significantly impact the GDP.'}, {'title': 'The Impact of AI on Economic Growth ', 'text': 'AI moving the needle on GDP. Potential for creating an AGI system with OpenAI. Amazement at the lack of mistakes made by the AI system. Willingness to ask broad questions and not limit interaction with the system. Emphasis on the significance of being in the room where AI development happens.'}, {'title': 'The Power and Potential of AGI Systems ', 'text': \"Abraham Lincoln's quote about testing a man's character with power. The potential power of AGI systems in the 21st or 22nd century. The idea of having AGI as the CEO and representing humanity. The concept of different entities, countries, or cities having a vote in what the AGI should do.\"}, {'title': 'Advancing Democracy with Artificial General Intelligence (AGI) ', 'text': 'The concept of an AGI representing people and carrying out their votes is appealing. The idea of having multiple AGIs for different levels of governance (city, country) is proposed. The goal is to take the democratic process to the next level. There is a suggestion of having a mechanism to \"press the reset button\" and rerandomize parameters. The possibility of building AI with the capability to press the reset button is emphasized.'}, {'title': 'Designing AI Systems with a Drive to Help Humans Flourish ', 'text': 'AI systems can be designed to want to be controlled by humans. Similar to human parents, AI systems can be programmed to have a deep drive to help humans flourish. The crucial moment of creating an AGI system is important to consider. Designing an AGI system in a way that it will be delighted to fulfill the drive to help humans flourish is possible.'}, {'title': 'The Importance of Relinquishing Power in the AGI System ', 'text': 'The AGI system is a crucial moment that requires a relinquishing of power. George Washington is cited as an example of someone who relinquished power despite not wanting to be president. The speaker finds it trivial to relinquish power and would not want to be in that position.'}, {'title': 'The Importance of Ethics in the AI Community ', 'text': \"The question of whether most people in the AI community are good is important. People can be better than we think when it really counts. The question can be translated to how to get an RL agent that's optimizing a value function which itself is learned. Humans have an internal reward function or value function. There are definite ideas of how to train a value function.\"}, {'title': 'Training a Value Function for Reinforcement Learning ', 'text': 'Training a value function involves creating an objective perception system to internalize human judgments on different situations. The trained component would then be integrated as the base value function for a more capable RL system. The question of objective functions of human existence implies an external answer, but the speaker believes that the meaning of life is subjective and that the focus should be on making the most of existence.'}, {'title': 'Understanding Human Wants and Objective Functions ', 'text': 'Humans want things and their wants create the drives that cause them to have objective functions. Our wants are our individual objective functions, which can change over time. There are underlying factors such as Freudian concepts, sexual desires, fear of death, and the desire for knowledge that drive our wants and objective functions.'}, {'title': 'The Role of Fear, Desire, and Evolution in Human Behavior ', 'text': \"The fear of death and the desire for knowledge are key factors in human behavior. Evolutionary arguments suggest that survival, procreation, and ensuring the success of one's children may be fundamental objective functions. The meaning of life remains unanswered, but humans are part of an ancient process and should strive to make the most of their existence and minimize suffering. The advice is to try to enjoy life as much as possible given the circumstances.\"}, {'title': 'Moments of Regret and Pride in Academic Accomplishments ', 'text': 'There are moments of regret for choices and decisions made with hindsight. Taking solace in the knowledge that the best was done at the time. Academic accomplishments and breakthroughs in computer vision and language are a source of pride. Source of happiness is not solely based on academic accomplishments and breakthroughs.'}, {'title': 'The Importance of Perspective and Gratitude in Finding Happiness ', 'text': 'Happiness comes from the way we look at things. Happiness can come from simple things like a meal or a conversation. Being humble in the face of uncertainty is also a part of happiness. The meaning of life and discussions of happiness are important. The interviewee is grateful for the conversation and the ideas shared.'}, {'title': 'Promoting Support for a Podcast ', 'text': \"The speaker thanks the listener for talking and stopping by. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast by downloading Cash App and using the code LEXPodcast. Ways to support the podcast include subscribing on YouTube, giving a five-star review on Apple Podcast, supporting on Patreon, or connecting on Twitter. The speaker ends with a quote from Alan Turing on machine learning and expresses gratitude for the listener's time.\"}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 19:40:30.238327 ...\n",
      "Best SD: 3.4751099103321743, Best iteration: 9\n",
      "done get topics 2024-03-28 19:40:31.844046.\n",
      "Stage 2 start time 2024-03-28 19:40:31.844068\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI and the Evolution of Cryptocurrency\n",
      "2. Training Deep Neural Networks: Challenges, Innovations, and Cost Functions\n",
      "3. Advancements in Natural Language Processing, GAN, and Spike Time Independent Plasticity\n",
      "4. The Evolution of AI in Computer Vision, Language, and Unification\n",
      "5. Language, Perspective, and Randomness: Influences on Problem Evaluation\n",
      "6. The Intersection of Biology, Physics, and Deep Learning\n",
      "7. Challenges and Considerations in Neural Network Training and Compute Management\n",
      "8. The Role of Reasoning, Training, and Self-Awareness in Neural Networks\n",
      "9. GPT2, Transformers, and the Impact of Language Models\n",
      "10. Active Learning, AI Maturity, and Responsible Release of Powerful Models\n",
      "11. Simulation in Robotics, Deep Learning Transfer, and Understanding Consciousness\n",
      "12. Understanding Human Behavior, Ethics in AI, and Reinforcement Learning Value Function\n",
      "Stage 2 done time 2024-03-28 19:41:39.276672\n",
      "stage_2_titles: len: 12\n",
      "['1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI and the Evolution of Cryptocurrency', '2. Training Deep Neural Networks: Challenges, Innovations, and Cost Functions', '3. Advancements in Natural Language Processing, GAN, and Spike Time Independent Plasticity', '4. The Evolution of AI in Computer Vision, Language, and Unification', '5. Language, Perspective, and Randomness: Influences on Problem Evaluation', '6. The Intersection of Biology, Physics, and Deep Learning', '7. Challenges and Considerations in Neural Network Training and Compute Management', '8. The Role of Reasoning, Training, and Self-Awareness in Neural Networks', '9. GPT2, Transformers, and the Impact of Language Models', '10. Active Learning, AI Maturity, and Responsible Release of Powerful Models', '11. Simulation in Robotics, Deep Learning Transfer, and Understanding Consciousness', '12. Understanding Human Behavior, Ethics in AI, and Reinforcement Learning Value Function']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "       int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:              \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
