Deb:
Hello, I'm Deb Donig, the host of Technically Human, a podcast that delves into the intersection of ethics and technology. Each week, I have the privilege of speaking with industry leaders, thinkers, writers, and technologists to explore what it truly means to be human in the age of tech. Together, we aim to understand the relationship between humans and the technologies we create and work towards building a better vision for technology that aligns with our human values. Today, I am thrilled to have Doctor Tamara Neese, the project director of Data and Society's Algorithmic Impact Methods Lab, as my guest. Doctor Neese brings a wealth of experience and expertise, having worked in various prestigious roles and holding a PhD in media, culture, and communication. I am excited to delve into a thought-provoking discussion with her.

Deb:
Hi Tamara.

Tamara:
Hi Deb.

Deb:
So, I want to talk about the recent landmark bill introduced by Congress. This bill is a significant step towards developing standards to measure and report the environmental impact of AI. It also aims to create a voluntary framework for AI developers to report their environmental impacts. Additionally, the legislation requires an interagency study to investigate and measure both the positive and negative environmental impacts of AI. This is a crucial development in the regulation of AI and its environmental effects.

Tamara:
Yeah, so, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. One of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important.

Deb:
You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?

Tamara:
Measuring the environmental impact of something like AI is quite challenging because of the complex global supply chains involved in its production. It requires a lot of coordination across different parts of the supply chain, different companies, and different parts of the world. The bill is relying on not just the EPA, but also the National Institute of Standards and Technology to convene a group of people who can identify the methodologies and standards needed for this kind of measurement work.

Deb:
What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.

Tamara:
As we delve into the world of large language models and generative AI, it becomes clear that these technologies require specialized chips and a significant amount of power, energy, and water for efficient operation. The growing problem of energy and water consumption associated with training and deploying AI is particularly evident in the ICT sector, where data centers play a significant role. These data centers not only consume a tremendous amount of energy but also have the potential to pollute local areas and deplete water resources, leading to environmental justice issues and potential carbon emissions. It's important to recognize that AI is just one part of a larger problem that has been ongoing for a long time, and there is a need for robust research to address all aspects of AI's environmental impact. Looking back at the history of environmental issues related to technology, such as the contamination of the water supply in the Silicon Valley region in the 1980s, we see the importance of community action and regulation to prevent future environmental damage. The current AI boom presents an opportunity to focus on making regulatory changes and understanding the real environmental costs, but it's crucial to remember that this is not a new problem and is not unique to generative AI.

Deb:
I know that this question is problematic, but I want to ask it anyway. When we're talking about AI and generative AI, there's a whole range of problems, concerns, issues, and dangers cited. These include the automation of labor leading to job loss, questions around compensation for labor, copyright questions about creativity, misinformation using generative AI, changes to the structure of how we get and source information online, and even environmental impact. I am aware of the problematics of trying to rank or propose a hierarchy of problems, but where would environmental impact rank in the hierarchy of problems caused by AI?

Tamara:
I believe that the environmental impacts of AI are closely tied to labor problems. Focusing on the environmental impact of AI forces us to take a more holistic view of the potential harms it can cause. The entire AI supply chain involves different parts of the world and various forms of labor exploitation, which means we need to consider the trade-offs, such as the impact of data centers on communities. Data centers can have negative effects on health and the environment, and we must also consider the justice concerns and impacts on marginalized communities in relation to labor and ethics in AI. It's important to consider the effects on habitats, ecosystems, and other species to truly understand the full spectrum of AI impacts.

Deb:
So what is that full spectrum of impact? The broader question here is how do you study AI's environmental impact? How do you measure it? What issues or areas comprehensively do you look at when you assess an environmental impact?

Tamara:
Yeah, it's a great question. I think a lot of researchers have spent time focusing on measuring the carbon attached to training and deployment of AI. Some have pushed for a lifecycle analysis, looking at the potential e-waste from the hardware used in creating AI. Measuring impact requires considering both technical assessments and social impacts, including talking to impacted communities. It's incredibly difficult to measure, but we need to bridge the gap between technical assessments and the downstream effects on communities. At Intel, the focus was on creating software to help developers optimize technology for environmental impact. Aim lab is working to reconcile the technical and socio-technical perspectives on measuring and reporting the environmental impacts of technology.

Deb:
As someone who has worked across academia, industry, and public interest technology, I have noticed significant fractures in the way different sectors approach the benefits and harms of AI. There are gaps in knowledge between these sectors, leading to silos in knowledge and controversial conversations. It is important to address these tensions and work towards a more cohesive approach to AI across industries.

Tamara:
I believe that having a broad understanding of a problem and looking at it from different perspectives is crucial, especially in research. People define problems differently and work on different timelines, which can create challenges. In tech companies, demonstrating return on investment and real impact is essential for the value of research. Collaboration between socio technical researchers and the tech industry can provide valuable perspectives and reframing of problems. Understanding power dynamics and hierarchies in tech is also helpful in policy implementation. This is the kind of work we are trying to do at AIM Lab, where we aim to lend our expertise to the tech industry and provide different perspectives on problems.

Deb:
I am grappling with the tensions surrounding climate change, environmental harms, and AI. On one hand, leaders in the tech industry tout AI as a solution to mitigate climate change and environmental harm by reducing inefficiencies and creating new technologies. However, there are valid concerns about the energy requirements of data centers, e-waste, and environmental damage caused by mining for tech resources. Additionally, there is a debate about job creation and retraining efforts in the AI industry, viewed from both environmental and labor perspectives. The controversy raises questions about assessing the benefits against the harms and the need for impact assessment.

Tamara:
Yeah, it's a great question. I think if we take a step back, a lot of technology is being developed in the name of being climate friendly or furthering goals of climate justice. AI could be used to help farmers in drought-stricken areas and for deforestation mitigation and coral reef health. But the question is, does the tech actually do what you think it's going to do? Are the people who are expected to use it having a good experience? Are these technologies being built through conversations with the people they are expected to help? User experience is often an afterthought in tech development. There are concerns about whether climate-related AI products, despite good intentions, will actually be executed in a helpful way. This is similar to the negative reception of crypto and NFTs, and the criticism of tokenizing natural resources as a form of exploitation.

Deb:
I believe that algorithmic impact assessments are crucial in today's technological world. As I have mentioned before, when people and ecologies intersect with technical systems, there are always unanticipated consequences. It is essential for us to be able to identify and document a push back against these sometimes ambient harms.

Tamara:
I believe that it is crucial to get input from the people who will be impacted by a technology early in the process. Engaging the community can help us identify potential problems before deployment. It is important to document potential harms and raise red flags around privacy and bias, and encourage developers to fully assess potential harms and work on fixing issues before release. We should also question whether the technology should be developed and released at all, and consider if there are better ways to allocate resources. It's important to keep an open mind and not just push a product out there that could end up causing havoc.

Deb:
I want to delve deeper into the issue of identifying and assessing the harms of AI products. It's not just about recognizing the problems, but also about pushing back when the products don't work as intended. Take the example of AI poop picking up robots developed to address the issue of homelessness in San Francisco. While it may seem like a technical solution, the better alternative, as pointed out by Dan Lyons, is public restrooms. This brings to light the underlying economics of product development - the incentive for developers is often to create marketable products rather than truly solving the problem at hand. So, the question is, how do we push back and mobilize our understanding and assessments to make a real difference?

Tamara:
I'm often asked about the most important aspect of my work, and I have to say it's my favorite part - translating empirical findings into real changes in policy, legal strategies, and bargaining strategies. It's crucial to document harms and prove algorithmic wage discrimination within gig apps. However, demonstrating bias in automated employment decision tools is challenging, and existing laws have limitations in addressing this issue. That's why the Workers' Algorithm Observatory is conducting algorithmic inquiry with rideshare drivers to understand their working conditions and combat algorithmic wage discrimination. Ultimately, the research we do must be beneficial to the people we're advocating for.

Deb:
I want to delve deeper into the power differential between those who create AI and those who are subjected to it. We need to consider our options to push back on tech companies and AI companies, especially those who have accrued tremendous power and wealth. These companies have set norms and standards that maximize their ability to profit and limit their liability for the harms they cause. Their products have become deeply institutionalized in our environment, making it difficult for us to alter their effect or extract ourselves from them. Additionally, these companies have massive lobbying power and money to fight lawsuits, and their wealth and profit often allow them to pay fines without affecting their business practices. It's clear that the profit they gain from creating and distributing their products far exceeds any penalty they may accrue from violations.

Tamara:
I believe that the control of large scale AI by just a few companies is quite frightening. These companies have an outsized amount of control over people's lives, and we need radical reformulation of what we consider successful products and companies. The pressure for change should come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities. Ultimately, we need to collectively have more power over how technology is produced and used.

Deb:
I want to delve into the concept of harm caused by tech products. It's important to consider that harm can be intentional or unintended. There may be distinctions between different ways in which tech products cause harm. Some argue that so-called unintended consequences are actually predictable but overlooked in favor of profit or progress. The value system of tech production may disregard the harm caused by the product. The idea of harm caused by tech products can vary depending on the perspective of the developers and users. It's crucial to think about the unintended consequences when it comes to what we see in tech culture.

Tamara:
I believe that we really need more historians to be in tech spaces. Bringing in historians can help us understand the historical context of new innovations and the potential harms they may bring. For example, when I was at Intel, I brought in historians to talk about the history of queer and trans computing, which shed light on the connections between past events and current technologies. The excitement around the metaverse should be tempered with an understanding of past issues like sexual violence and harassment in virtual spaces. It's important to acknowledge and address societal issues like racism, sexism, and homophobia when creating new technologies. We should not ignore these larger histories, as they can help us anticipate and address potential problems.

Deb:
I just want to take a moment to encourage everyone listening to check out Mar's episode of this show. In our interview, she emphasized the significance of the present moment of technological production being shaped by the narratives and histories of the past. This is something that we should all consider as we move forward with AI technologies.
Speaking of AI, I recently read a piece in Wired about using generative AI to bring back the voices of the deceased. It raised some important concerns about the environmental impact of this technology and the human costs associated with it. It's a topic that we should all be thinking about as we continue to develop and utilize AI in our society.

Tamara:
Yeah, so, in terms of preserving the data of the dead, it kind of gets at this larger problem in the tech industry, where data is power. Individual users' data is collected and maintained, but Google recently announced deactivating accounts of inactive users due to the cost of maintaining data. The expansion of data centers for generative AI will have a massive environmental impact. Creating AI versions of the dead raises ethical questions about authority, control, and profit. Deepfake technology and the use of deceased celebrities without consent are concerning. The idea of monetizing a person's likeness after death creates labor-related problems. These are all thorny issues that need to be addressed.

Deb:
I am deeply interested in exploring the connections between labor, environmental harm, and the preservation of data, particularly data related to the deceased. My extensive writing has focused on how socio-technical systems, such as social media platforms and AI, are reshaping cultural rituals and attitudes towards death. My book, "Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond," delves into these topics. Additionally, I am currently involved in work with the AI impact lab at data and society, furthering my exploration of these themes.

Tamara:
Yeah, definitely. I really feel like the problem of death is a very old human problem and a fairly universal one. Despite what transhumanists say about uploading our brains, death is definitely coming for everyone. The treatment of the dead can reveal social hierarchies within a society, and technologies like social media were not originally built for memorialization or mourning. It's interesting how tech companies often overlook the impact of death in their products. Considering the impact of technology on social relations and networks is important, and death is a rich field for thinking about algorithmic impacts.

Deb:
I want to discuss the concept of techno solutionism and its cousin, techno utopianism. How do we view these ideologies and practices as we move forward in our society? What do they get right and where do they miss the mark? It's important to consider their impact on our society and what they truly represent.

Tamara:
I believe that techno solutionism can be useful for making small changes in technology to solve specific problems efficiently. However, the larger problem lies in using technology to fix social issues that may not require technological solutions. Building relationships and maintaining dialogue with communities impacted by technology is crucial, and it is important to focus on these relationships with techno solutionism rather than just the technology itself. Imagining alternative futures and reimagining technology is essential, and dismantling the current way technology is built and deployed is necessary for realizing these alternative futures. Personally, I am interested in the magical and mystical qualities of technology and its transcendent effects on people. I believe that using technology to communicate with the dead is a practice that has been done for a very long time, and it doesn't necessarily require a digital form of technology. Overall, I think it's important to dig into the other uses of technology that might expand its magical qualities.

Deb:
As a professor teaching a course on data and human values at UC Berkeley, I am aware that many students across college campuses and universities listen to this show. Transitioning from academia to data and society, I understand the importance of preparing students who are going to end up in the tech industry. I want them to know, think about, understand, and reconsider the impact of their work on human values and society as they move into their careers.

Tamara:
Yeah, it's a great question. When I talk to younger generations interested in tech, I see a strong awareness of potential problems and a hunger for training in critical technology studies. It's important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past. Building coalitions with people outside of tech and learning from their experiences is valuable. Engaging with different communities and their concerns, and volunteering or getting involved with local issues is also important. We can all learn from those who came before us and work together to make the tech industry more ethical and inclusive.

Deb:
Thank you so much, Tamara.

Tamara:
Thank you, Deb.

