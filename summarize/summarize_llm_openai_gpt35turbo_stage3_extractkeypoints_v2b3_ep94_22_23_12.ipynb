{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b3\" # has rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fea7009a710>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-26 01:35:22.026470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 01:40:33.853884\n",
      "chunks_text len: 66\n",
      "extract_keypoints start time: 2024-03-26 01:40:33.854037\n",
      "extract_keypoints done time 2024-03-26 01:43:08.679314\n",
      "Start time: 2024-03-26 01:43:08.679501\n",
      "Stage 1 done time 2024-03-26 01:45:43.188439\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Thomas Sanholm: Professor, AI Pioneer, and Game Theory Expert ', 'text': 'Thomas Sanholm is a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. He has published over 450 papers on game theory and machine learning, including a best paper in 2017 at NIPS, now renamed to Newrips. His research and companies have had wide-reaching impact in the real world, proposing new ideas and building systems to prove that these ideas work. The conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast.'}, {'title': 'Heads Up No Limit Texas Holdem: A Benchmark for AI Algorithms ', 'text': \"['Heads Up No Limit Texas Holdem is a main benchmark for testing AI algorithms for imperfect information game solving.', 'It is a game played by humans, but not often seen on TV or in casinos.', 'It is played in expert level casinos and in the World Series of Poker.', 'It is mostly played online for big sums of money.', 'It is usually only played by experts.', 'It is different from regular No Limit Texas Holdem and is more competitive.']\"}, {'title': 'Types of Poker Games ', 'text': 'Texas Holdem is a game typically played by a big group and is not as competitive. Heads Up is a two-player game, much like chess or go. Texas Holdem is an imperfect information game, making it harder to play. In Texas Holdem, each player has two private cards and there are gradually laid out public cards. The betting rounds in Texas Holdem occur after receiving private and public cards.'}, {'title': 'Title ', 'text': 'Description of a Popular Game with Imperfect InformationText '}, {'title': 'High Stakes Poker Tournament at Rivers Casino ', 'text': 'The event involved inviting four of the top 10 players in Heads Up No Limit, Texas Holdem. The game is different from the multiplayer version, so statistical significance was important. The players were brought to Pittsburgh to play at the Rivers Casino for 20 days. The goal was to get 120,000 hands in to achieve statistical significance. The players played from morning to evening for 20 days. A 200,000 incentive was raised for the players to play. The players were paid based on how they did against the AI. This setup provided an incentive for the players to play as hard as possible.'}, {'title': 'Title ', 'text': \"Players' Incentive to Play Against AIText \"}, {'title': 'Top Players Utilize Online UI for Game Play ', 'text': \"Top players are used to playing the game mostly online through a UI. The game was played with a layout on a screen, with the human and AI sitting at a virtual table. The screen showed everything that was happening, including the cards and bets. Betting history for the human was also available for reference. The human's memory was not a factor, as they were top quality people. The setup ensured that the AI couldn't take advantage of the human's memory.\"}, {'title': 'AI Libratus Beats Humans in Competition ', 'text': 'The AI, Libratus, was able to beat humans in a competition, despite previous AI, Cloudyco, failing to do so. The organizer thought there was a 50/50 chance of success against humans, but international betting sites put the AI as a four to one or five to one underdog. People tend to have more confidence in other people compared to the performance of AI, as evidenced by the betting odds.'}, {'title': 'Underestimating AI in Poker ', 'text': 'People underestimated the performance of AI in poker. Despite AI beating humans in poker, people still had confidence in human superiority. There is a belief that human facial expressions and body language are critical in poker. People have confidence in human outperformance because AI cannot perceive human tells. AI systems only look at betting patterns and statistics, while humans can perceive tells. The importance of human perception in poker is questioned.'}, {'title': 'The Importance of Human Players in Betting Patterns and Statistics ', 'text': 'The importance of human players in betting patterns and statistics. The difficulty in believing that an AI could beat top human players. The challenge of finding tells among top players who are skilled at hiding them. The general lesson about AI and the need to see it overperform a human to believe it.'}, {'title': 'The Importance of Tells and Abstraction in Poker ', 'text': 'Tells are important in poker, especially at lower levels of play. At higher levels of poker, tells become less important as the game involves a larger number of strategies and possible actions. The game tree in poker is too large to solve directly, so abstraction is necessary. Abstraction in games is more challenging than in other types of games.'}, {'title': 'The Challenges of Abstraction in Multi-Agent Games ', 'text': 'Abstraction in games is trickier than in MDPs or other single agent settings. Finer grained abstraction can lead to worse strategies in real games. There are hands abstractions and betting strategies in games. Information abstraction involves abstracting what chance does, such as cards in the case of poker. Action abstraction involves abstracting the actions of actual players, such as bets in the case of poker.'}, {'title': 'Automated Action Abstraction Technology in Game Strategy ', 'text': 'The use of algorithms for potential aware abstraction in the game. The bottom-up process with integer programming and clustering in building abstraction. The use of automated action abstraction technology for the first couple of betting actions. The importance of considering how humans and other AIs have played the game in the past for action abstraction. The challenge of scalability in using automated action abstraction technology for the whole game. The question of whether the strength of the hand or the actions taken is more important in the game.'}, {'title': 'The Importance of Betting Actions and Hand Selection in Poker ', 'text': 'The strength of the hand and the information abstraction are important in playing poker. The betting actions may be the key to winning regardless of the hands you have. Playing a lot of hands can reduce the role of luck in the game. No Limit Texas Holdem has a high level of variance and massive swings. Statistical significance in poker requires playing over 100,000 hands.'}, {'title': 'The Use of Deep Learning in Poker Playing ', 'text': 'Annette Oberstad, a Norwegian female poker player, won a tournament by using a unique playing style. Labradus does not use deep learning methods, unlike DeepStack. The effectiveness of deep learning in poker playing is unclear. The question is raised about the use of learning methods to aid in the way Labradus plays poker.'}, {'title': 'The Importance of Learning Methods in Games ', 'text': \"Labradus did not use learning methods and played very well without them. There are papers on things that do use learning techniques, including deep learning. In imperfect information games like poker, the value of an information set depends not only on the exact state, but also on both players' beliefs. The value of a state in poker is not just a function of the cards, but also depends on the path of play and both players' beliefs.\"}, {'title': 'Understanding State and Strategy in Game Theory ', 'text': 'A state is not just a function of the cards. The state depends on the path of play and belief distributions. It is not as simple as in perfect information games. In perfect information games, there is a state and an evaluation function. In this context, the opponent can take different strategies at the leaf of the search tree. Allowing the opponent to choose from a set of different continuation strategies forces a less optimistic look ahead search. This approach leads to a sound look ahead search.'}, {'title': 'Challenges in Look Ahead Search for Imperfect Information Games ', 'text': 'Look ahead search in imperfect information games is very difficult. Randomly generating various situations in the game and doing look ahead from there to the end of the game. Using deep learning to learn the values of states, including belief distributions. Similar techniques to alpha beta search or Monte Carlo tree search are used.'}, {'title': 'Title ', 'text': 'Different Search Algorithms and Approaches in Game PlayingText '}, {'title': 'The Importance of Beliefs in Game Theory ', 'text': 'Beliefs are actually output, not input. Starting beliefs are input, but they just fall from the rules of the game. The dealer deals uniformly from the deck, so every pair of cards is equally likely. Card removal is very important in the game. Dealing always comes from a single deck in Heads Up. Adjusting beliefs is where the beauty of game theory comes in.'}, {'title': 'Title ', 'text': 'Introduction to Nash EquilibriumText '}, {'title': 'Understanding Probability Distribution and Game Theory ', 'text': \"The text discusses probability distribution over real world states in the mind. Player one moves first and then player two moves, with player two not knowing player one's move. Information set for player two is created based on player one's move. Nash equilibrium suggests playing 1/3 Rock, 1/3 Paper, and 1/3 Scissors. Bayes theorem is mentioned in relation to deriving beliefs on the information set. The text mentions that Bayes gives you something in game theory.\"}, {'title': 'Understanding Game Theory and Opponent Modeling ', 'text': 'Game theory is not player specific and does not require any data or history of how specific players or AI have played in the past. It is based on rationality and considers what a rational opponent would do and what the player would do if they are rational. Game theory is a data-free and opponent-free approach, focusing on the design of the game rather than the design of the player. Opponent modeling is not a primary focus in game theory, although it can be combined with game theory to exploit weak players. In the case of Librarus, opponent modeling was not turned on because the players were considered too good and had few holes to exploit.'}, {'title': 'Strategies for Dealing with Exploitation in Competitive Games ', 'text': \"Opening oneself up to exploitation by turning on certain strategies. Acknowledgment of the opponent's expertise in counter exploitation. Interest in exploring papers exploiting opponents. Positive feedback on the work at hybrid digested. Safety in two player zero sum games when the opponent acts irrationally.\"}, {'title': 'Game Theory Strategies and Repeated Games ', 'text': \"The player can gain by throwing off the belief is always less than they lose by playing poorly. A game theoretic strategy is unbeatable, but it doesn't maximally beat the other opponent. The hybrid strategy involves starting from a game theoretic approach and then tweaking the strategy based on opponent data. Repeated games and the Simple Prisoner's Dilemma are mentioned.\"}, {'title': 'Understanding Repeated Games and Game Theory ', 'text': \"Prisoner's Dilemma is a repeated game. There is no proof that repeated games are the best strategy, but experimentally they do well. There are perfect information games and imperfect information games. Repeated games are played over and over. There are zero sum games and non zero sum games. There is a distinction between two player games and games with more players.\"}, {'title': 'Types of Games and Their Characteristics ', 'text': '[\\'Extensive form games involve repetitive interactions and incomplete information sets.\\', \\'Repeated games are a special case of extensive form games.\\', \\'Sourcing auctions involve repetitive interactions with variations in the supply base and the items being bought.\\', \"Purely repeated games are rare in the real world and are a coarse model of what\\'s going on.\", \\'Stochastic games are a middle ground between simple repeated matrix games and extensive form games, involving little matrix games and actions taken by opponents.\\']'}, {'title': 'Types of Games and Their Application in AI ', 'text': 'The text discusses different types of games, including matrix games, stochastic games, and extensive form games. It mentions that poker is an example of an extensive form game. The AI community has been working on and being benchmarked on Heads Up No Limit Texas Holdem, which is an extensive form game. It compares the tree form of games, such as chess, to the matrix form or bi matrix form of games. The text also mentions the concept of reasoning in the tree form of games.'}, {'title': 'Game Theory: Tree Form vs. Normal Form ', 'text': 'Tree form allows for certain types of reasoning that are lost in normal form games. Equivalence exists between tree form and normal form, but sequentiality is lost in the transition. Multiplayer versus two player distinction is important in game theory. Two player games in zero sum are conceptually and computationally easier. In two player games, any equilibrium strategy is a best response to any other equilibrium strategy.'}, {'title': 'The Existence of Nash Equilibrium in Finite Games ', 'text': 'Nash equilibrium is present in all finite games, as proven by John Nash. The problem lies in the existence of multiple Nash equilibriums and the selection of which one to use. In non zero sum games, there may be a loss of joint benefit when different strategies are selected from different equilibriums.'}, {'title': 'Challenges and Strategies in Multiplayer Games ', 'text': 'In non zero sum games, joint benefit may be lost by being simply stupid, and both parties could be better off by doing something else. In three player games, collusion can occur, where two players gang up on a third player to achieve better outcomes. Collaboration or cooperation between poker players can make the game extremely difficult for current AI methods to solve. The ability of poker players to collaborate introduces new challenges and complexities to the game.'}, {'title': 'Title ', 'text': 'Coalitional Games and New Representations for ComputationText '}, {'title': 'Importance of Coordination in Strategic Games ', 'text': 'Coordination in games like bridge requires strategies to be coordinated ahead of time without the ability to communicate during the game. Certain signals can be used in games like bridge, but they must be understood by both teams. Coordination is built into the rules of some games, but not in others like auctions, negotiations, and diplomatic relationships. Prior strategies and negotiations play a crucial role in various applications beyond poker.'}, {'title': 'Transitioning from Poker to Business and Security Applications ', 'text': 'Moving away from poker and into other applications like negotiations. Has two startup companies - Strategic Machine and Strategy Robot. Strategic Machine is for business applications, gaming, sports, etc. Strategy Robot is for military security, cyber security, and intelligence applications. Also involved in another company called Optimized Markets, focused on combinatorial market and optimization based technology.'}, {'title': 'The Use of Game Theoretic Reasoning Technologies in Modeling Human Behavior ', 'text': 'Game theoretic reasoning technologies are not being used. The goal may not be to model human behavior. In a zero sum game, it may not matter if the opponent is following a model of rational behavior. Formalizing the interaction in games is a prerequisite for analysis. Mechanism design has been used to design games with certain outcomes. Example of studying pedestrians and their negotiation with autonomous vehicles.'}, {'title': 'Modeling Human Behavior in Pedestrian-Vehicle Interactions ', 'text': 'Pedestrians and cars engage in nonverbal communication, creating a tension and trust dynamic. Modeling human behavior in pedestrian-vehicle interactions is challenging, especially in terms of intent. Game theory and imperfect information approaches may be useful in modeling human behavior in these interactions. The problem of jaywalking and pedestrian-vehicle interactions could be addressed in the context of autonomous vehicles. Fleets of autonomous cars operated by different companies could be a potential application for addressing pedestrian-vehicle interactions.'}, {'title': 'Challenges and Strategies in Autonomous Car Negotiations ', 'text': 'Fleets of autonomous cars operated by different companies, such as Waymo and Uber, raise the question of defining rules of the road and negotiating strategies. The idea of prenegotiating situations for car mergers to increase speed and efficiency. The challenge of manually negotiating too many situations, leading to the suggestion of using automated negotiation. The possibility of not always letting one party go first in negotiations, but finding a balance of give and take.'}, {'title': 'Advancements in AI and Imperfect Information Games ', 'text': 'The negotiation involves a combinatorial exchange of letting each other go first in different situations. The example of merging can be modeled as an imperfect information game. Games with perfect information are easier to deal with. Lessons learned from the Annual Computer Poker Competition. AI has made incredible accomplishments in the field of poker and gaming.'}, {'title': 'AI Advancements in Engineering and Scientific Research ', 'text': 'AI has stepped up in an engineering and scientific effort to beat human players. Performance oriented research is preferred in the group, spanning from idea to theory, experiments, system building, and commercialization. Building big systems and evaluating them at scale is necessary in AI. Techniques that look good in small scale may not work in large scale. Computational game theory community has shown the importance of evaluating techniques at scale.'}, {'title': 'Algorithm Performance in Theory vs Reality ', 'text': \"Theory doesn't always match reality in terms of algorithm performance. First order methods may have better convergence rates in theory, but CFR based algorithms are the fastest in practice. Testing algorithms in reality is necessary to determine their true performance. Projections from small scale testing can be misleading in this domain. Personal experience with organizing a poker competition involving AI has been wild.\"}, {'title': 'Controversy Surrounding AI in Heads Up No Limit Poker Competition ', 'text': 'This was the first competition for Heads Up No Limit poker. The speaker became the most hated person in the world of poker due to their involvement in AI cracking the game. Many people felt that AI was a real threat to the existence of the game. The speaker received aggressive comments and even death threats for their involvement in AI. The speaker believes that humans can still enjoy the game of poker despite AI outperforming them in chess.'}, {'title': 'The Impact of AI on Poker Strategy ', 'text': 'The speaker believes that AI has made poker a richer and more interesting game for humans to play. The AI has changed the way the game is played and has influenced human players to incorporate new strategies. The speaker did not see themselves as someone who would \"kill the game\" and believes they have learned to love poker through working with AIs. The speaker finds it brave to put ideas to the test, as sometimes good ideas don\\'t work when applied.'}, {'title': 'Challenges of Scaling Good Ideas ', 'text': \"Good ideas don't always work when applied at scale. It takes a lot of work and time to organize and make something big. It is important to do things in the real world and at scale. Proof is in the pudding, meaning the real test is in the real world and at scale.\"}, {'title': 'The Role of Mechanism Design in Competitive Poker ', 'text': 'The competition between different groups to beat the top humans at Heads Up No Limit, Texas Holdem. The potential benefits of friendly competition for progress. The concept of mechanism design and its role in achieving desirable outcomes. The idea of designing the rules of the game in an automated fashion. The comparison of mechanism design in politics and other complex systems.'}, {'title': 'Challenges in Automated Mechanism Design ', 'text': 'The automated mechanism design direction is still believed in, but it is not a panacea. There are impossibility results in mechanism design, stating that certain objectives cannot be accomplished in certain classes. These impossibility results are proofs, not statements about human ingenuity. It is impossible to achieve certain properties in certain classes with any mechanism.'}, {'title': 'Automated Mechanism Design and Possibility within Impossibility Classes ', 'text': 'Automated mechanism design allows for specific settings to be designed for, even if there are impossibility results for the whole class. It is possible to carve out islands of possibility within known impossible classes. The Meyerson Satethweight theorem by Roger Meyerson and Mark Satethweight from 1983 shows an impossibility of efficient trade under imperfect information, but it is possible to avoid that in many settings and still achieve efficient trade. The impossibility result does not contradict the possibility of achieving efficient trade in certain settings.'}, {'title': 'The Application of Mechanism Design in Real-World Situations ', 'text': 'The impossibility result is still present, but there are spots within the impossible class where the impossibility does not exist. The lessons drawn from mechanism design can be applied to politics, human interaction, and designing mechanisms for various real-world situations. Mechanism design itself has had limited success so far, with only certain cases being successful in real-world situations.'}, {'title': 'Challenges of Applying Mechanism Design in Real World Situations ', 'text': 'Real world situations are often not sound from a mechanism design perspective. Insights from theory are applied into the real world rather than applying mechanisms directly. The FCC spectrum auctions are an example where bidding truthfully is not the best strategy. Mechanism design aims to make things easy for participants, but truth telling is not the best strategy in high stakes auctions. Excellent economists have worked on the FCC spectrum auctions with no game theory.'}, {'title': 'Optimal Bidding Strategies and AI Milestones in Spectrum Auctions ', 'text': \"Truth telling is not the best strategy in spectrum auctions. There is no single optimal bidding strategy for spectrum auctions. Bidding truthfully wouldn't be the best strategy even with just two or one item in spectrum auctions. AI history is marked by seminal events such as AlphaGo beating a world champion human Go player and Liberatus winning the Heads Up No Limit Holdem. Heads Up No Limit Texas Holdem was the one remaining widely game solving.\"}, {'title': 'Benchmark Games for Game Solving ', 'text': 'Heads Up No Limit Texas Holdem was widely agreed upon as a benchmark for game solving. There are other games being worked on by different groups, such as StarCraft, Dota 2, Diplomacy, and Hanabi. None of these games are acknowledged as the main next challenge problem like chess or Go or Heads Up No Limit Texas Holdem. The speaker hopes that there will be a next benchmark to drive application independent techniques forward.'}, {'title': 'The Future of Game Solving Technology ', 'text': 'The speaker is not going to work as hard on recreational benchmarks. The speaker is working on two startups related to game solving technology. The speaker believes that game theory is in a very different situation compared to machine learning. Machine learning is a mature technology with proven success in the real world, while game solving has almost no applications.'}, {'title': 'The Potential of Computational Game Theory in Military Planning and Business Strategy ', 'text': 'Machine learning has shown success in real-world applications, but there are almost no applications in game solving. The next big breakthrough could be the use of computational game theory in military planning and business strategy. Machine learning methods, such as neural networks, lack transparency and explainability, while game theoretic methods, like Nash equilibria, may offer more transparency and explainability.'}, {'title': 'The Properties of Game Theoretic Strategies ', 'text': 'Nash equilibria and game theoretic strategies have provable properties. Unlike deep learning, game theoretic strategies have provable solution quality guarantees. The solution quality of game theoretic strategies is known, unlike deep learning where it is uncertain. The human understandability of game theoretic strategies is a separate problem. Deep learning and computational game theory are in the same boat in terms of human understandability.'}, {'title': 'Challenges and Concerns in Deep Learning and Computational Game Theory ', 'text': \"Deep learning and computational game theory are both difficult to understand. Game theoretic techniques have guarantees of solution quality, but it's more of a belief than a substantiated fact. Exciting future if there are provable things in terms of optimality. Concerns about the future and existential threats of artificial intelligence, especially in games like poker. Worries about the negative impact of artificial intelligence on society.\"}, {'title': 'Impact of Nationwide Kidney Exchange and Combinatorial Sourcing Auctions ', 'text': 'The nationwide kidney exchange has saved hundreds of lives and increased employment in the healthcare industry. Combinatorial sourcing auctions led to a 12.6% increase in supply chain efficiency, resulting in over $6 billion of efficiency improvement.'}, {'title': 'Efficiency Improvement and Safety Advancements in Various Industries ', 'text': '$6 billion of efficiency improvement in the world. Efficiency improvement in trucking, leading to less waste and carbon footprint. AI is going to make the world much safer.'}, {'title': 'The Role of Game Theory in Aligning Values with Human Civilization ', 'text': 'Game theory has a role to play in ensuring that values are aligned with human beings in human civilization. Value misalignment is a theoretical worry, but the speaker has not seen it in real applications. The speaker had a discussion in the late eighties about the idea of high utilization of assets in transportation optimization systems. The speaker demonstrated that making high asset utilization the objective would result in impractical solutions, such as loading trucks full and driving in circles.'}, {'title': 'Challenges in Achieving 100% Utilization and AI Optimization ', 'text': 'The solution of loading trucks full and driving in circles to achieve 100% utilization may not be practical in reality. AI can optimize the wrong objective to the detriment of the actual problem. There is a gap between theoretical worst-case scenarios and what actually happens in reality. The speaker grew up in the Soviet Union and mentions the existence of 10,000 nuclear weapons in the world.'}, {'title': 'The Threat of Nuclear War and Climate Change ', 'text': 'The speaker grew up in the Soviet Union. There are currently 10,000 nuclear weapons in the world. The speaker is surprised that nuclear war has not broken out. The two biggest threats facing mankind are climate change and nuclear war. The speaker has tried to do something about climate change through their startups. The speaker commissioned studies on what could be done for climate change. The speaker is still keeping an eye out for potential market solutions or optimizations for climate change.'}, {'title': 'Challenges and Limitations in Implementing Market Solutions for Environmental and Security Issues ', 'text': 'Market solutions, optimization solutions, and technology solutions are needed for problems such as pollution. Lack of political will is a major barrier to the success of pollution credit markets. Better market design alone cannot solve the problem if there is no political will to support it. The Chicago market was shut down, indicating the limitations of market design in addressing environmental issues. Global warming is a pressing problem, while nuclear weapons have been a long-standing concern. The speaker is extremely worried about the issue of nuclear weapons.'}, {'title': 'The Game Theory of Mutually Assured Destruction and the Current Nuclear Situation ', 'text': 'The game theory of mutually assured destruction is based on the idea that nobody wants to initiate a conflict. The analysis of the game theory is coarse grained and situational, and may not apply to the current situation with smaller nuclear powers and non-nation actors. The speaker believes that the current situation with smaller nuclear powers and non-nation actors makes the risk of nuclear conflict higher than ever before. The interviewer asks about the application of AI, but the response is not included in the given text.'}, {'title': 'Developing Scalable Techniques for Game Solving and Real-World Applications ', 'text': 'The focus is on developing scalable techniques for game solving and applying them in the real world. Interest in market design and optimized markets. Priority is on strategic machine strategy robot and getting the technology out there. Understanding the technology gaps that still need to be filled through real applications. Enjoyment of the interaction and challenge of applying state-of-the-art techniques in real-world scenarios.'}, {'title': 'Challenges and Opportunities in Adopting State-of-the-Art Technology ', 'text': 'The process involves applying state-of-the-art techniques to benefit various industries and the military. Autonomous vehicles face challenges in integrating new technology due to old-fashioned systems and inertia. Internal champions at customer organizations are needed to drive change and prevent negative consequences in the future.'}, {'title': 'Rise of Autonomous Vehicles ', 'text': 'Autonomous vehicles are a topic of interest for both traditional car makers and tech companies like Google and Baidu. The speaker finds it fascinating that tech companies unrelated to transportation are pushing for autonomous cars. The speaker is excited about the potential impact of these ideas in the world. There are different games being solved, including those with hidden player actions, such as poker.'}, {'title': 'Strategic Thinking in Different Contexts ', 'text': 'Poker is a game where the chance actions are hidden and player actions are public. Multiplayer games involve collusion, opponent exploitation, and can be extensive and never-ending. Business strategy involves thinking about the business from a long-term perspective. Military strategy involves modeling the ongoing nature of war and evaluating the effectiveness of moves.'}, {'title': 'Interest in Scalable Techniques for Integer Programming ', 'text': 'The speaker is interested in learning more scalable techniques for integer programming. They had a paper on automated algorithm configuration with theoretical generalization guarantees. Algorithm configuration has been going on for at least 17 years seriously, but there has not been any generalization theory before. The speaker is honored to talk to Tomas and thanks him for bringing Labradus to the world. The conversation ends with no more questions.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 01:45:44.315318 ...\n",
      "Best SD: 2.0548046676563256, Best iteration: 0\n",
      "done get topics 2024-03-26 01:45:45.001498.\n",
      "Stage 2 start time 2024-03-26 01:45:45.001517\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Thomas Sanholm: AI Pioneer, Game Theory Expert, and Poker Tournament Champion\n",
      "2. The Role of AI and Human Players in Poker Strategy\n",
      "3. Understanding State, Strategy, and Beliefs in Game Theory\n",
      "4. Strategies and Challenges in Competitive Games and AI Applications\n",
      "5. Importance of Coordination and Game Theoretic Reasoning in Various Applications\n",
      "6. Controversy and Impact of AI in Poker Competition and Scientific Research\n",
      "7. Challenges and Possibilities in Mechanism Design for Competitive Situations\n",
      "8. Milestones and Challenges in Computational Game Theory and Strategic Planning\n",
      "9. Strategic Thinking, Scalable Techniques, and Real-World Applications in Game Theory and AI Technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "The podcast features Thomas Sanholm, a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. The game is a main benchmark for testing AI algorithms for imperfect information game solving and is typically played online for big sums of money. In an event involving four of the top 10 players in Heads Up No Limit, the players were brought to Pittsburgh to play at the Rivers Casino for 20 days, with the goal of achieving statistical significance by playing 120,000 hands. A $200,000 incentive was raised for the players to play, and they were paid based on how they did against the AI, providing an incentive for them to play as hard as possible. The game was played with a setup that ensured the AI couldn't take advantage of the human's memory, and despite being considered an underdog, the AI, Libratus, was able to beat the top human players, demonstrating the potential of AI in competitive gaming.\n",
      "\n",
      "The podcast discusses the underestimation of AI in poker, despite its ability to beat human players. It challenges the belief in human superiority by questioning the importance of human tells and perception in the game. The difficulty in believing that AI could beat top human players is also addressed, along with the challenge of finding tells among skilled players. The importance of abstraction in poker games is highlighted, as well as the use of algorithms for potential aware abstraction. The podcast also explores the significance of betting actions, the role of luck, and the effectiveness of deep learning methods in poker playing. It concludes by emphasizing the value of information sets and both players' beliefs in imperfect information games like poker.\n",
      "\n",
      "The podcast delves into the complexities of determining the state in imperfect information games, the use of deep learning to learn the values of states, and the application of different search algorithms and approaches in game playing. It also explores the concepts of beliefs, card removal, Nash equilibrium, and the role of game theory in designing games and exploiting weak players.\n",
      "\n",
      "The podcast discusses strategies for opening oneself up to exploitation in games, while also acknowledging the opponent's expertise in counter exploitation. It explores the concept of exploiting opponents and receiving positive feedback on hybrid digested work. The safety in two player zero sum games when the opponent acts irrationally is also discussed, as well as the potential gains from throwing off the opponent's belief. The podcast delves into game theoretic strategies, repeated games, and the distinction between perfect and imperfect information games. It also covers different types of games, including matrix games, stochastic games, and extensive form games, with a focus on the AI community's work on Heads Up No Limit Texas Holdem. The concept of reasoning in tree form games and the distinction between multiplayer and two player games in game theory are also explored. The podcast concludes with a discussion on Nash equilibrium and the challenges of selecting the best equilibrium in non zero sum games.\n",
      "\n",
      "The podcast discusses the importance of coordination in games like bridge, where strategies must be coordinated ahead of time without the ability to communicate during the game. It also explores the use of signals in games like bridge and how coordination is built into the rules of some games but not in others like auctions, negotiations, and diplomatic relationships. The podcast also delves into the application of coordination strategies in various fields beyond poker, such as business, gaming, sports, military security, cyber security, and intelligence. It also discusses the use of game theoretic reasoning technologies and mechanism design in designing games with certain outcomes. The podcast also explores the challenges of modeling human behavior in pedestrian-vehicle interactions and the potential application of game theory and imperfect information approaches in addressing these challenges. Additionally, it discusses the lessons learned from the Annual Computer Poker Competition and the complexities introduced by collaboration or cooperation between poker players.\n",
      "\n",
      "The podcast discusses the speaker's involvement in the first competition for Heads Up No Limit poker, where they became the most hated person in the poker world due to their work with AI. The speaker received aggressive comments and death threats for their involvement in AI, as many people felt that AI was a threat to the existence of the game. However, the speaker believes that humans can still enjoy poker despite AI outperforming them in chess. They argue that AI has made poker a richer and more interesting game for humans to play, as it has changed the way the game is played and influenced human players to incorporate new strategies. The speaker did not intend to \"kill the game\" and has learned to love poker through working with AIs. They find it brave to put ideas to the test, as sometimes good ideas don't work when applied. The podcast also delves into the engineering and scientific effort behind AI's ability to beat human players, emphasizing the importance of evaluating techniques at scale and testing algorithms in reality to determine their true performance. The speaker shares their wild personal experience with organizing a poker competition involving AI.\n",
      "\n",
      "The podcast discusses the challenges of applying good ideas at scale, emphasizing the importance of real-world implementation. It explores the competition in the game of Heads Up No Limit, Texas Holdem and the potential benefits of friendly competition for progress. Mechanism design is introduced as a concept for achieving desirable outcomes, with a focus on designing the rules of the game in an automated fashion. The comparison of mechanism design in politics and other complex systems is discussed, along with the limitations and impossibility results in mechanism design. The Meyerson Satethweight theorem is highlighted as an example of achieving efficient trade despite impossibility results. The podcast concludes by acknowledging the limited success of mechanism design in real-world situations and the need to apply insights from theory into practical scenarios. The FCC spectrum auctions are used as an example to illustrate the challenges of applying mechanism design in high-stakes auctions.\n",
      "\n",
      "The podcast discusses the bidding strategies in spectrum auctions, emphasizing that truth telling is not the best approach. It also explores the history of AI, highlighting significant events such as AlphaGo's victory and the development of game-solving technology. The speaker expresses hope for a new benchmark to drive application-independent techniques forward and discusses the potential applications of game-solving technology in military planning and business strategy. The podcast also compares the transparency and explainability of machine learning methods with game theoretic strategies, noting the provable solution quality guarantees of the latter. The speaker expresses concerns about the future and existential threats of artificial intelligence, particularly in games like poker, and worries about the negative impact of AI on society. Additionally, the speaker mentions working on two startups related to game-solving technology and believes that game theory is in a different situation compared to machine learning.\n",
      "\n",
      "The podcast discusses various topics related to game theory, algorithm configuration, market design, and the application of AI in real-world scenarios. The speaker, who grew up in the Soviet Union, expresses concern about the existence of nuclear weapons and the pressing issue of climate change. They also emphasize the importance of developing scalable techniques for game solving and applying them in various industries and the military. The conversation touches on the challenges of integrating new technology, the potential impact of autonomous vehicles, and the need for internal champions to drive change in customer organizations. Overall, the podcast covers a wide range of topics related to strategy, technology, and real-world applications.\n",
      "RRR rewritten summary\n",
      "[{'text': 'The podcast features Thomas Sanholm, a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. The game is a main benchmark for testing AI algorithms for imperfect information game solving and is typically played online for big sums of money. In an event involving four of the top 10 players in Heads Up No Limit, the players were brought to Pittsburgh to play at the Rivers Casino for 20 days, with the goal of achieving statistical significance by playing 120,000 hands. A $200,000 incentive was raised for the players to play, and they were paid based on how they did against the AI, providing an incentive for them to play as hard as possible. The game was played with a setup that ensured the AI couldn\\'t take advantage of the human\\'s memory, and despite being considered an underdog, the AI, Libratus, was able to beat the top human players, demonstrating the potential of AI in competitive gaming.\\n\\nThe podcast discusses the underestimation of AI in poker, despite its ability to beat human players. It challenges the belief in human superiority by questioning the importance of human tells and perception in the game. The difficulty in believing that AI could beat top human players is also addressed, along with the challenge of finding tells among skilled players. The importance of abstraction in poker games is highlighted, as well as the use of algorithms for potential aware abstraction. The podcast also explores the significance of betting actions, the role of luck, and the effectiveness of deep learning methods in poker playing. It concludes by emphasizing the value of information sets and both players\\' beliefs in imperfect information games like poker.\\n\\nThe podcast delves into the complexities of determining the state in imperfect information games, the use of deep learning to learn the values of states, and the application of different search algorithms and approaches in game playing. It also explores the concepts of beliefs, card removal, Nash equilibrium, and the role of game theory in designing games and exploiting weak players.\\n\\nThe podcast discusses strategies for opening oneself up to exploitation in games, while also acknowledging the opponent\\'s expertise in counter exploitation. It explores the concept of exploiting opponents and receiving positive feedback on hybrid digested work. The safety in two player zero sum games when the opponent acts irrationally is also discussed, as well as the potential gains from throwing off the opponent\\'s belief. The podcast delves into game theoretic strategies, repeated games, and the distinction between perfect and imperfect information games. It also covers different types of games, including matrix games, stochastic games, and extensive form games, with a focus on the AI community\\'s work on Heads Up No Limit Texas Holdem. The concept of reasoning in tree form games and the distinction between multiplayer and two player games in game theory are also explored. The podcast concludes with a discussion on Nash equilibrium and the challenges of selecting the best equilibrium in non zero sum games.\\n\\nThe podcast discusses the importance of coordination in games like bridge, where strategies must be coordinated ahead of time without the ability to communicate during the game. It also explores the use of signals in games like bridge and how coordination is built into the rules of some games but not in others like auctions, negotiations, and diplomatic relationships. The podcast also delves into the application of coordination strategies in various fields beyond poker, such as business, gaming, sports, military security, cyber security, and intelligence. It also discusses the use of game theoretic reasoning technologies and mechanism design in designing games with certain outcomes. The podcast also explores the challenges of modeling human behavior in pedestrian-vehicle interactions and the potential application of game theory and imperfect information approaches in addressing these challenges. Additionally, it discusses the lessons learned from the Annual Computer Poker Competition and the complexities introduced by collaboration or cooperation between poker players.\\n\\nThe podcast discusses the speaker\\'s involvement in the first competition for Heads Up No Limit poker, where they became the most hated person in the poker world due to their work with AI. The speaker received aggressive comments and death threats for their involvement in AI, as many people felt that AI was a threat to the existence of the game. However, the speaker believes that humans can still enjoy poker despite AI outperforming them in chess. They argue that AI has made poker a richer and more interesting game for humans to play, as it has changed the way the game is played and influenced human players to incorporate new strategies. The speaker did not intend to \"kill the game\" and has learned to love poker through working with AIs. They find it brave to put ideas to the test, as sometimes good ideas don\\'t work when applied. The podcast also delves into the engineering and scientific effort behind AI\\'s ability to beat human players, emphasizing the importance of evaluating techniques at scale and testing algorithms in reality to determine their true performance. The speaker shares their wild personal experience with organizing a poker competition involving AI.\\n\\nThe podcast discusses the challenges of applying good ideas at scale, emphasizing the importance of real-world implementation. It explores the competition in the game of Heads Up No Limit, Texas Holdem and the potential benefits of friendly competition for progress. Mechanism design is introduced as a concept for achieving desirable outcomes, with a focus on designing the rules of the game in an automated fashion. The comparison of mechanism design in politics and other complex systems is discussed, along with the limitations and impossibility results in mechanism design. The Meyerson Satethweight theorem is highlighted as an example of achieving efficient trade despite impossibility results. The podcast concludes by acknowledging the limited success of mechanism design in real-world situations and the need to apply insights from theory into practical scenarios. The FCC spectrum auctions are used as an example to illustrate the challenges of applying mechanism design in high-stakes auctions.\\n\\nThe podcast discusses the bidding strategies in spectrum auctions, emphasizing that truth telling is not the best approach. It also explores the history of AI, highlighting significant events such as AlphaGo\\'s victory and the development of game-solving technology. The speaker expresses hope for a new benchmark to drive application-independent techniques forward and discusses the potential applications of game-solving technology in military planning and business strategy. The podcast also compares the transparency and explainability of machine learning methods with game theoretic strategies, noting the provable solution quality guarantees of the latter. The speaker expresses concerns about the future and existential threats of artificial intelligence, particularly in games like poker, and worries about the negative impact of AI on society. Additionally, the speaker mentions working on two startups related to game-solving technology and believes that game theory is in a different situation compared to machine learning.\\n\\nThe podcast discusses various topics related to game theory, algorithm configuration, market design, and the application of AI in real-world scenarios. The speaker, who grew up in the Soviet Union, expresses concern about the existence of nuclear weapons and the pressing issue of climate change. They also emphasize the importance of developing scalable techniques for game solving and applying them in various industries and the military. The conversation touches on the challenges of integrating new technology, the potential impact of autonomous vehicles, and the need for internal champions to drive change in customer organizations. Overall, the podcast covers a wide range of topics related to strategy, technology, and real-world applications.'}]\n",
      "Stage 2 done time 2024-03-26 01:47:43.181196\n",
      "stage_2_titles: len: 9\n",
      "['1. Thomas Sanholm: AI Pioneer, Game Theory Expert, and Poker Tournament Champion', '2. The Role of AI and Human Players in Poker Strategy', '3. Understanding State, Strategy, and Beliefs in Game Theory', '4. Strategies and Challenges in Competitive Games and AI Applications', '5. Importance of Coordination and Game Theoretic Reasoning in Various Applications', '6. Controversy and Impact of AI in Poker Competition and Scientific Research', '7. Challenges and Possibilities in Mechanism Design for Competitive Situations', '8. Milestones and Challenges in Computational Game Theory and Strategic Planning', '9. Strategic Thinking, Scalable Techniques, and Real-World Applications in Game Theory and AI Technology']\n",
      "remove_questions start time: 2024-03-26 01:47:43.198414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 01:53:50.432591\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-26 01:53:50.432732\n",
      "extract_keypoints done time 2024-03-26 01:56:25.118255\n",
      "Start time: 2024-03-26 01:56:25.118502\n",
      "Stage 1 done time 2024-03-26 01:59:08.279748\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and the Role of Rajat Manga at Google ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Evolution of Open Source in Google Brain ', 'text': 'Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library at Google Brain turned into TensorFlow in 2014, the open source library. Deep learning was intriguing and held promise, even before it had taken off.'}, {'title': 'Title ', 'text': \"Leveraging Google's Compute Power and Data for Research ScalingText \"}, {'title': \"The Evolution of Google's Machine Learning Efforts \", 'text': 'Google Brain was born around neural networks, focusing on deep learning from the beginning. In 2012 or 2011, the focus was on scaling machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines. Google has been doing machine learning for a long time, showing great promise in terms of machine learning.'}, {'title': 'The Rise of Deep Learning at Google ', 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact many things. Real products started to use deep learning, starting with speech and then expanding to other areas like image recognition. Academia also started to show interest in deep learning around 2014. Google decided to open source TensorFlow, indicating the growth and importance of deep learning.'}, {'title': 'The Impact of Going Open Source with TensorFlow ', 'text': \"The decision to go open source with TensorFlow is considered a big seminal moment in software engineering. Google's decision to take a large project and go open source with it led the entire world in saying that open innovation is powerful and acceptable. The initial idea to go open source came from Jeff, who was a big proponent of it. The decision to go open source was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'The Growth of Deep Learning and Machine Learning through Research Sharing ', 'text': 'Deep learning and machine learning have grown rapidly due to sharing of research. Existing libraries like Tiano and Torch were developed by academia, but there was a need for software at a different level. Google had developed internal software and published papers, leading to successful open source projects. Hadoop was developed from technology built internally, which was considered superior for various reasons.'}, {'title': \"Google Cloud's Integration and Support for Bigtable, HBase, and TensorFlow \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. TensorFlow is open source and can be used anywhere. Google Cloud ensures lots of integrations and works well with TensorFlow. The focus is on helping the community in lots of ways.'}, {'title': 'The Development and Open Sourcing of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. TensorFlow was open sourced in November 2015. Development of TensorFlow started in the summer of 2014. Decision to open source TensorFlow was made in late 2014. The fast pace of development in deep learning.'}, {'title': 'Title ', 'text': 'The Design and Capabilities of TensorFlowText '}, {'title': 'Running Machine Learning on Mobile Devices ', 'text': 'Ideas of running machine learning on the phone existed at that time. Customized handcrafted code or internal libraries were used for running machine learning on the phone. The use of Theano and Caffe at Google influenced design decisions. The belief was built focused on internal systems, which were very different from existing libraries. Multiple libraries were considered when building the belief.'}, {'title': 'Comparing Libraries for Machine Learning Flexibility ', 'text': 'By the time we got to this, we looked at a number of libraries that were out there. The group had experience with Torch, Lua, Theano, and Caffe. They discussed ideas around having a graph or not. They wanted flexibility due to the fast-moving research and changing hardware. The flexibility in terms of being able to express all kinds of crazy things was a big factor.'}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The move towards TensorFlow 2.0 includes default eager execution. The graph decisions are being made with the goal of making development more intuitive. The graph concept originated from the need to deploy production code efficiently. Experimentation with different ideas led to the realization that not having a graph made things simpler to use.'}, {'title': 'Influences and Growth of Graph Deployment for Deep Learning ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, was unexpected. The need for the product was recognized early on in the research perspective and early days of deep learning. The potential for future growth and enabling more people to use the product was considered after open sourcing. The growth in popularity was noticed after open sourcing, indicating the increasing interest in deep learning.'}, {'title': 'The Growth of Deep Learning in the Company ', 'text': 'Deep learning is growing rapidly and the company is in a good position to leverage it. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company started with version 0.6 or 0.5 and has gone through a few changes since then. People initially loved the documentation provided by the company.'}, {'title': 'The Evolution of Deep Learning for Practical Applications ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from a research focus to being accessible to developers for practical applications. The focus shifted towards stability and deployment for non-research purposes. Planning for version 1.0 involved addressing the needs of enterprises. There was an emphasis on documentation, designs, and other elements to meet the needs of stability and deployment. The progress was exciting as it attracted more interest from enterprises.'}, {'title': 'Enterprise Adoption of a Product ', 'text': 'The excitement of getting more enterprises to buy in and support the product. The increase in enterprise adoption post 1.0 and over the next few releases. The initial interest from researchers, hobbies, and early adopters, followed by the adoption by enterprises. The pressure for stability from enterprises, especially before the 1.0 release. The importance of understanding what enterprises want in the midst of product development.'}, {'title': 'Different Needs and Preferences in Model Usage ', 'text': 'Enterprise and individuals have different needs and preferences when it comes to using models. Many people still use older models like Inception and ResNet 50 for their stability and simplicity. There is value in providing stability and simplicity in models to allow more people to access them. The research crowd is interested in more advanced and complex models like RNNs, transformers, RL, and GANs.'}, {'title': 'The Impact of RL and GANs on Advancing Technology ', 'text': 'The combination of RL and GANs is pushing the state of the art in the field. Older technology is still very usable and stable for many people. Making technology easy to use is important for the majority of the world. The hobbyist perspective is the most common case for technology use. Technology that looks great on slides is often used for presentations.'}, {'title': 'The Importance of TensorFlow Extended in Machine Learning ', 'text': \"Enterprises have data that they want to make predictions on. They used to use regression models and gradient booster trees for machine learning. Some still benefit from deep learning, especially with large datasets. The audience's needs for machine learning may vary. The TensorFlow Extended piece, which is the entire pipeline, is important for them.\"}, {'title': 'The Importance of Data Organization for Using TensorFlow ', 'text': 'TensorFlow Extended is the entire pipeline, focused on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. The role of an evangelist is to encourage companies to organize their data for the big benefit of using TensorFlow.'}, {'title': 'Title ', 'text': 'Understanding Machine Learning and the TensorFlow EcosystemText '}, {'title': 'The Impact of New Data Sets on Organization and Accessibility ', 'text': 'The release of new data sets has led to a demand for better organization and accessibility. Starting with basic models and improving them is a recommended approach. The appearance of Keras has made TensorFlow more accessible. Keras was initially on top of Tiano and later on top of TensorFlow.'}, {'title': \"The Origins of Keras and Francois' Research at Google \", 'text': 'Francois started the Keras project before he was at Google. Tiano was the first thing before TensorFlow was created. When TensorFlow started becoming popular, there were enough similarities that he decided to create an interface and put TensorFlow as a backend. He decided on his own to create the interface and thought it was interesting and relevant to the community. He joined Google for research and was doing some amazing research. He has some papers on research and is a great researcher.'}, {'title': 'Integration of Keras into TensorFlow 2.0 ', 'text': \"Keras was integrated into TensorFlow in a deep way. With TensorFlow 2.0, Keras is the recommended way for beginners to interact with TensorFlow. This makes initial transfer learning or basic use cases super simple, even for an enterprise. The integration was initially planned for a quarter but has been ongoing for two years. The person responsible for the integration is a great researcher and has been fully dedicated to the project. The person's manager agreed to the integration for a quarter, which has now extended to two years. The team spent a lot of time thinking about the integration and the APIs involved.\"}, {'title': 'Choosing Keras as the Standard API for Parallel Layers ', 'text': 'There were multiple APIs, including some built by the team and others built by the community. The team was working on a parallel layers API and decided to do Keras in parallel. The goal was to make the APIs look similar and be as integrated as possible. The community was confused about which API to use and kept asking for a standard one. The team decided to address the confusion with the release of version 2.0 and simplify by picking one API. Keras was chosen based on its popularity and positive feedback from the community.'}, {'title': 'The Impact of Keras on TensorFlow and the Role of Leadership in Open Source Projects ', 'text': 'Keras was loved by many and had great qualities. Keras was seen as a potential competitor to TensorFlow but ended up being an empowering element of it. The team behind Keras and TensorFlow all want to make things easier for developers. Python has Guido van Rossum, who held the position of benevolent dictator for life. A successful open source project like TensorFlow needs one person to make final decisions.'}, {'title': 'TensorFlow Dev Summit Highlights New Features and Ecosystem Growth ', 'text': 'TensorFlow Dev Summit was successful with new features and an amazing ecosystem. There are multiple people involved in key design directions and open source development. Regular design reviews and transparency efforts have been made over the last year. More processes such as RFCs and special interest groups are being implemented.'}, {'title': 'The Evolution of the TensorFlow Ecosystem ', 'text': \"The need for adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The recognition that the ecosystem cannot scale with a lone decision maker and the importance of decentralizing decision-making. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The convergence of all these developments towards the ability to save models in a consistent way and move them between different platforms.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning in Practical ApplicationsText '}, {'title': 'The Integration of Machine Learning into Real Products ', 'text': 'Machine learning research needs to be integrated into real products to have a real impact on people. ML and training can now run on a variety of compute devices, not just workstations or data centers. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning has grown to cover more devices and continues to push boundaries. More tooling has been built in some areas to support machine learning on various devices.'}, {'title': 'The Evolution of Tooling and Libraries in TensorFlow ', 'text': 'TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines.'}, {'title': 'Title ', 'text': 'Enabling Community Building and Model Sharing in TensorFlow 2.0Text '}, {'title': 'Challenges in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new hardware devices. The development process has involved a lot of learning and iteration over the last few years.'}, {'title': 'Challenges of Modifying the Monolithic System of TensorFlow ', 'text': \"TensorFlow started as a very monolithic system and to some extent it still is. There are lots of tools around it, but the core is still pretty large and monolithic. It's hard to change and modify and really break apart. It's like changing the engine with a car running or trying to fix that.\"}, {'title': 'Challenges of Maintaining Backward Compatibility in TensorFlow ', 'text': 'Many people rely on TensorFlow in their applications. There is a challenge in maintaining previous versions while also introducing new features. TensorFlow 2.0 breaks some backward compatibility but the conversion is straightforward. It is a tricky balance between introducing new features and maintaining backward compatibility. The technical debt of maintaining previous versions is a responsibility. Production applications using TensorFlow need to consider backward compatibility.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and improvements comes with a trade-off, but the overall value is much bigger. It's not just about breaking the person yesterday, but also about setting standards for new people joining the team. When doing new things, it's important to consider the impact on future team members.\"}, {'title': 'Importance of Designing with a Clean Slate ', 'text': 'Design with a clean slate in mind is important for new things. Making compromises occasionally is necessary, but designing with a clean slate is crucial. Switching research group to TensorFlow. TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, number of users, and momentum.'}, {'title': 'The Rise of PyTorch in Research ', 'text': \"PyTorch is being used by a lot of researchers now. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not necessarily on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Importance of Learning from Previous Experiences and Exploring Different Spaces ', 'text': 'The text discusses the benefit of learning from previous experiences and exploring different spaces. It mentions the competition and the process of revisiting and adding new features. The text highlights the importance of eager execution and the effort put into combining different elements. It also references Muhammad Ali versus Frasier as a comparison.'}, {'title': 'Progress and Excitement with TensorFlow 2.0 ', 'text': 'TensorFlow has made incredible progress in the last couple of years. The addition of TensorFlow 2.0 has enabled new possibilities and excitement for the future. Ecosystem improvements such as making it easily accessible to Keras and eager execution have been discussed. The speaker is grateful for the way TensorFlow has evolved and the work it is doing.'}, {'title': 'Title ', 'text': 'Excitement Over Clean APIs in Version 2.0Text '}, {'title': 'Restructuring and Modularization of TensorFlow for Future Improvements ', 'text': 'The team is excited about future versions and the potential improvements. Restructuring the monolithic system into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow in GitHub consists of lots of repositories, with the core one containing the execution engine, key backends, and distributed functionality. The current organization does not easily allow for splitting the components apart, and clean interfaces are needed for a perfect world scenario.'}, {'title': 'The Importance of Clean Interfaces and TensorFlow in Networking and Development ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on custom networking and clusters. Clean separation in interfaces will help with scalability and enable independent evolution and pushing of things. Major corporations like Pepsi are already using TensorFlow for development.'}, {'title': 'The Growth and Involvement of Users in the TensorFlow Community ', 'text': \"Many users are already using TensorFlow, but not all of them are involved in core development or changes. Some users, such as hardware vendors and larger companies like IBM, are involved in special interest groups and want to optimize for specific needs. Autonomous vehicle companies are also involved in using TensorFlow. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The growth of the TensorFlow community is attributed to certain critical factors, but it's not clear what those factors are.\"}, {'title': 'Factors Affecting Growth in the Technology Industry ', 'text': \"Growth is critical and requires a combination of factors. Timing and alignment with the needs of the industry are important for growth. TensorFlow's growth is linked to the growth of deep learning. Listening to the community and being open to external contributions is crucial for growth. Putting the right processes in place and welcoming contributors is important for community growth.\"}, {'title': 'Importance of Transparency and Community in Open Source Project Growth ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Putting processes in place and thinking about documentation and tools are important as a project grows. People building something on TensorFlow and implementing a particular architecture contributes to the growth of TensorFlow.'}, {'title': 'Improving Developer Experience on GitHub ', 'text': \"The focus is on making it easy for developers to use the tools and resources available on GitHub. There is a commitment to investing in tooling and making significant version changes smooth and risk-free. People are motivated to move to new things when they see the value, not just because it's new. The goal is to provide a really good thing that people want to move to.\"}, {'title': 'The Rapid Evolution of Deep Learning ', 'text': 'Most people want a really good thing. People will start to see the value over the next few months. The field is moving rapidly, which will help with the shift. New things will happen in 2.x, giving people reasons to move. Change is expected in terms of deep learning. Basics of deep learning, such as convolution models, will probably be around.'}, {'title': 'The Future of Machine Learning and Hardware Accelerators ', 'text': 'Convolution models and basic models will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. There will likely be new developments in the field, but they are hard to predict. Some current directions include combining eager execution and graphs to make programming more natural, and exploring ground-up approaches like Swift for TensorFlow. It is uncertain if hardware accelerators will remain the same, or if training with four bits instead of 32 bits will be possible. The TPU side of things is exploring the possibility of training with fewer bits.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': \"TPU is already on version three, and it is exploring using four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to take some image model and do training or transfer learning on their kind of model, and it's important to make that easy for them.\"}, {'title': 'Importance of Simple Models and Tools for Beginners ', 'text': 'Providing simple models and tools is important for beginners. Different levels of support are needed for beginners, researchers, and advanced users. Pre-trained models can significantly decrease the time needed to start a project. TensorFlow has recently made advancements that are beneficial for beginners.'}, {'title': 'The Potential of High Schoolers and the Importance of Team Cohesion in Technology ', 'text': 'High schoolers are doing amazing and terrifying things, and will contribute incredible ideas as they grow up. There is a technical aspect and a management aspect to the role with TensorFlow. Google has been at the forefront of exploring what it takes to build a good team. TensorFlow is one of the most cutting edge technologies in the world. Cohesion across the team is important for delivering something well.'}, {'title': 'The Importance of Team Cohesion and Vision ', 'text': \"Cohesion across the team is important for executing together. The product of what the team generates is larger than the individual contributions. Hiring good people is important, but they also need to care about what they're building and be motivated. Having a unified vision of where the team wants to go is important.\"}, {'title': \"Google's Unified Vision and Organizational Approach \", 'text': 'Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, but also research-oriented. It is important to combine a mix of direction and exploration in their product and ecosystem. The mission of superstars is a key element at Google.'}, {'title': 'Challenges and Opportunities of Working with Superstars at Google ', 'text': 'Large percentage of work at Google is done by individual superstars. Superstars can sometimes be against the dynamic of a team, causing tensions. The mission of the TensorFlow project is beautiful, making it easier to work with superstars. Google values getting people who care and have the same kind of culture. The project has had many exciting things to do, allowing room for lots of people to grow and do different kinds of things.'}, {'title': 'Refining the Hiring Process and Emphasizing Teamwork at Google ', 'text': 'The hiring process at Google has been refined over the last 20 years. Productivity at Google is about the team, not just individual superstars. It is important for individuals to work well with the team across Google. Core technical skills are important in hiring engineers at Google.'}, {'title': 'The Importance of Motivation in the Workplace ', 'text': \"Motivation is important in addition to core technical skills. Alignment of motivation with the team's goals is crucial for long term success. Motivation is important at every level, not just for senior positions. Google hiring process focuses on technical skills, but may not assess motivation.\"}, {'title': \"Google's Interview Process and Cultural Fit \", 'text': 'The interview process at Google includes evaluating culture fit in addition to technical skills. Different projects at Google may have different cultural requirements. TensorFlow project at Google requires individuals who are comfortable with a fast-moving environment.'}, {'title': 'Navigating Project and Team Dynamics at Google ', 'text': 'Balancing the need for full-fledged products with ensuring things work really well. Importance of finding the right fit for projects and teams. Variability of culture across projects, teams, and product areas at Google. Engineering excellence as a core part of the culture. Difficulty and fun in solving challenging problems. The key to success in a large ecosystem or small product.'}, {'title': 'Striking a Balance in Decision Making ', 'text': 'Striking a balance across different aspects of a large ecosystem or a small product. Making decisions about speed versus perfection, community involvement, and saying no to certain things. The difficulty of making hard decisions, whether quickly or with time to think. The successful organization of the Dev Summit and the deadline making people rise to the occasion.'}, {'title': 'Managing Deadlines and Balancing Perfection and Functionality ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and functionality. The team did a great job in putting everything together. Focus on key things that are important and figure out how much of it is important. Developing in the open, both internally and externally, with everything available to everybody. Regular releases at a regular cadence.\"}, {'title': 'Approach to Software Releases in TensorFlow 2.0 ', 'text': \"Releases are done at a regular cadence, with the understanding that if something isn't ready this month, it will be in the next release in a month or two. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it is clear that they are experimental and feedback is encouraged. Quick cycle and quick iteration are important, rather than focusing on meeting a specific deadline. There is no pressure to make TensorFlow 2.0 stable, similar to the approach taken with WordPress 5.0, where updates were released quickly to improve it.\"}, {'title': 'Improving Stability and Future Releases for TensorFlow ', 'text': 'The focus is on improving stability and ensuring that every API remains in working condition. There is still more work to be done and more releases to come in the future. The goal is to create a great product. TensorFlow already has 41 million downloads for version 1.0 X.'}, {'title': 'The Impact of TensorFlow 1.0 X ', 'text': 'TensorFlow has already had 41 million downloads for 1.0 X. The focus is on getting it right and not rushing the release. The goal is to release it in the next few months or next quarter. The speaker led a team at Google on search ads before working on TensorFlow. Ads can connect people to the things they want and need, but at their worst, they can be annoying.'}, {'title': 'The Impact of Personalized Ads on User Experience ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience. Ads can annoy users and ruin the user experience if not personalized to their needs and wants. Huge amounts of data can be used for personalized ads to avoid annoying users.\"}, {'title': 'The Importance of Accessible and Quality Information ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the information to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's experience on a website.\"}, {'title': 'Balancing Value and Monetization in Advertisements ', 'text': 'Advertisements need to strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services such as search engines and websites to provide their service. The challenge is to show valuable ads without being annoying or distracting. Advertisements, when done well, can be really useful and not annoying. The internet has a culture of not wanting to pay for anything, making advertisements necessary for monetization.'}, {'title': 'Rise of Paid Services on the Web ', 'text': 'More paid services are being seen across the web and people are willing to pay for them. Transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. The value of paid services is being recognized, as seen with examples like Netflix and YouTube. There will always be things that are monetized with ads, but the trend is towards more paid content.'}, {'title': 'Transition to a Mix Model with Free Trials and Ads for Revenue Generation ', 'text': '- Transition to a mix model with free trials and ads, followed by a clear revenue model.- Use of TPU in a Google call app for free is possible due to TensorFlow being open source and can be run on desktops and phones.- Desktops are becoming more powerful, allowing for more capabilities.- Phones are now more powerful than first desktops, enabling training of models on phones.- The potential for using a mix model with free trials and ads to generate revenue.'}, {'title': 'The Advantages of Cloud Computing for Machine Learning ', 'text': 'Cloud computing offers more power and convenience compared to traditional desktops. Cloud services like Colab make it easy to get started with no installation needed. Colab is a free service, but paid services offer more features and capabilities. Beginners interested in machine learning and TensorFlow can start by visiting the TensorFlow website.'}, {'title': 'Title ', 'text': 'Getting Started with TensorFlow on ColabText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 01:59:12.202817 ...\n",
      "Best SD: 1.5, Best iteration: 10\n",
      "done get topics 2024-03-26 01:59:13.127484.\n",
      "Stage 2 start time 2024-03-26 01:59:13.127504\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. The Impact and Evolution of TensorFlow and Google's Machine Learning Efforts\n",
      "2. Comparing Libraries and the Evolution of TensorFlow 2.0 for Deep Learning\n",
      "3. The Importance and Impact of TensorFlow Extended, Keras, and Data Organization\n",
      "4. Highlights of the TensorFlow Ecosystem and Integration into Real Products\n",
      "5. Challenges and Progress in Integrating TensorFlow.js and Deep Learning\n",
      "6. Restructuring and Modularization of TensorFlow for Future Improvements and Community Involvement\n",
      "7. The Importance of Team Cohesion, Vision, and Motivation at Google\n",
      "8. The Impact of TensorFlow 1.0 X and the Evolution of Advertisements and Cloud Computing for Machine Learning\n",
      "\n",
      "RRR given summary\n",
      "The podcast features Rajat Manga, an engineer and director at Google, who leads the TensorFlow team, an open source library at the forefront of deep learning. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, inspiring other companies to do the same. Google's commitment to open innovation has led to the rapid growth of deep learning and machine learning. The development of TensorFlow 2.0 is currently underway, with a focus on community engagement and integration with Google Cloud. The design and capabilities of TensorFlow were influenced by the need for a more advanced and versatile library for running machine learning on various platforms.\n",
      "\n",
      "The podcast discusses the evolution of deep learning libraries, focusing on the transition from research to practical applications and the impact of enterprise adoption. The group initially experimented with various libraries before settling on TensorFlow 2.0, which prioritizes flexibility and intuitive development. The decision to use a graph for deployment was influenced by the need for efficient production code. The popularity of the product, with 41 million downloads, was unexpected, indicating the increasing interest in deep learning. The company's focus shifted towards stability and deployment for non-research purposes, leading to the planning and release of version 1.0. The increase in enterprise adoption post 1.0 and over the next few releases was exciting, but also brought pressure for stability. The podcast also highlights the different needs and preferences of enterprises and individuals when it comes to using models, with many still using older, stable models like Inception and ResNet 50. The research crowd is interested in more advanced and complex models like RNNs, transformers, RL, and GANs, pushing the state of the art in the field. The podcast emphasizes the importance of making technology easy to use for the majority of the world, as well as the value of providing stability and simplicity in models to allow more people to access them.\n",
      "\n",
      "The podcast discusses the use of machine learning in enterprises, focusing on the importance of TensorFlow Extended for data organization and accessibility. It also covers the integration of Keras into TensorFlow, making it easier for beginners to interact with the platform. The podcast highlights the role of an evangelist in encouraging companies to organize their data for the benefit of using TensorFlow and emphasizes the goal of making machine learning easier for developers.\n",
      "\n",
      "The TensorFlow Dev Summit showcased the success of TensorFlow 2.0, with a focus on enabling community building and model sharing. Key design directions and open source development involve multiple people, with efforts made to increase transparency and implement processes such as RFCs and special interest groups. It is recognized that the ecosystem cannot scale with a lone decision maker, leading to a decentralization of decision-making. The growth and development of the ecosystem has led to the ability to save models in a consistent way and move them between different platforms. The goal is to integrate machine learning research into practical applications, with ML and training now able to run on a variety of compute devices. The ecosystem for machine learning has expanded to cover more devices, with additional tooling built to support ML pipelines. TensorFlow has played a significant role in these developments, with a focus on enabling machine learning on every device with compute capability.\n",
      "\n",
      "The podcast discusses the challenges and progress of integrating TensorFlow.js and deep learning JS into the ecosystem. It highlights the technical complexities and the goal of making it easy for end users. The development process has involved a lot of learning and iteration, with a focus on maintaining backward compatibility for production applications. The podcast also compares TensorFlow with PyTorch, emphasizing the importance of learning from previous experiences and exploring different spaces. It concludes with excitement over the possibilities enabled by TensorFlow 2.0 and ecosystem improvements.\n",
      "\n",
      "The podcast discusses the excitement for future versions and potential improvements of TensorFlow. The restructuring of the monolithic system into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow in GitHub consists of many repositories, making it difficult to split components apart. Clean interfaces are needed for scalability and independent evolution. Major corporations like Pepsi and autonomous vehicle companies are already using TensorFlow. The growth of the TensorFlow community is attributed to factors such as timing, industry needs, and listening to the community. Transparency, community aspects, and processes are important for growth. The focus is on making it easy for developers to use the tools and resources available on GitHub. The goal is to provide a really good thing that people want to move to. The field of deep learning is rapidly evolving, and new developments are expected in TensorFlow 2.x. Reinforcement Learning and Generative Adversarial Networks are likely to stay, and new developments in the field are hard to predict. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Different levels of support are needed for beginners, researchers, and advanced users. TensorFlow has recently made advancements that are beneficial for beginners.\n",
      "\n",
      "The podcast discusses the role of high schoolers in contributing to cutting-edge technologies like TensorFlow, and the importance of cohesion and motivation within a team. It highlights Google's approach to building a good team, the value of superstars, and the hiring process at Google. The podcast also emphasizes the importance of culture fit, motivation, and engineering excellence in the hiring process. It discusses the challenges of balancing speed and perfection, making hard decisions, and meeting deadlines. The podcast concludes by emphasizing the importance of regular releases and developing in the open.\n",
      "\n",
      "The podcast discusses the development and release of TensorFlow 2.0, with a focus on the importance of getting it right rather than rushing the release. The speaker, who previously led a team at Google on search ads, emphasizes the need for personalized ads to avoid annoying users. The podcast also explores the transition towards a mix model of free trials and ads, as well as the potential for using TensorFlow on desktops and phones. It also touches on the use of cloud computing and the regular cadence of releases for TensorFlow. The overall goal is to create a great product, with a focus on stability and improvement.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The evolution of deep learning libraries has been a pivotal moment in the tech industry, inspiring other companies to do the same. Google's commitment to open innovation has led to the rapid growth of deep learning and machine learning. The development of TensorFlow 2.0 is currently underway, with a focus on community engagement and integration with Google Cloud. The design and capabilities of TensorFlow were influenced by the need for a more advanced and versatile library for running machine learning on various platforms.\\n\\nThe transition from research to practical applications and the impact of enterprise adoption are key topics in the discussion. TensorFlow 2.0 prioritizes flexibility and intuitive development, with a focus on efficient production code. The popularity of the product, with 41 million downloads, indicates the increasing interest in deep learning. The company's focus shifted towards stability and deployment for non-research purposes, leading to the planning and release of version 1.0. The increase in enterprise adoption post 1.0 and over the next few releases was exciting, but also brought pressure for stability. The podcast also highlights the different needs and preferences of enterprises and individuals when it comes to using models, with many still using older, stable models like Inception and ResNet 50. The research crowd is interested in more advanced and complex models like RNNs, transformers, RL, and GANs, pushing the state of the art in the field.\\n\\nThe use of machine learning in enterprises is another important aspect of the discussion, focusing on the importance of TensorFlow Extended for data organization and accessibility. The integration of Keras into TensorFlow has made it easier for beginners to interact with the platform. The role of an evangelist in encouraging companies to organize their data for the benefit of using TensorFlow is also highlighted.\\n\\nThe TensorFlow Dev Summit showcased the success of TensorFlow 2.0, with a focus on enabling community building and model sharing. Key design directions and open source development involve multiple people, with efforts made to increase transparency and implement processes such as RFCs and special interest groups. The growth and development of the ecosystem has led to the ability to save models in a consistent way and move them between different platforms. The goal is to integrate machine learning research into practical applications, with ML and training now able to run on a variety of compute devices. The ecosystem for machine learning has expanded to cover more devices, with additional tooling built to support ML pipelines. TensorFlow has played a significant role in these developments, with a focus on enabling machine learning on every device with compute capability.\\n\\nThe challenges and progress of integrating TensorFlow.js and deep learning JS into the ecosystem are also discussed. The technical complexities and the goal of making it easy for end users are highlighted. The development process has involved a lot of learning and iteration, with a focus on maintaining backward compatibility for production applications. The podcast also compares TensorFlow with PyTorch, emphasizing the importance of learning from previous experiences and exploring different spaces. It concludes with excitement over the possibilities enabled by TensorFlow 2.0 and ecosystem improvements.\\n\\nThe excitement for future versions and potential improvements of TensorFlow is also discussed. The restructuring of the monolithic system into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow in GitHub consists of many repositories, making it difficult to split components apart. Clean interfaces are needed for scalability and independent evolution. Major corporations like Pepsi and autonomous vehicle companies are already using TensorFlow. The growth of the TensorFlow community is attributed to factors such as timing, industry needs, and listening to the community. Transparency, community aspects, and processes are important for growth. The focus is on making it easy for developers to use the tools and resources available on GitHub. The goal is to provide a really good thing that people want to move to. The field of deep learning is rapidly evolving, and new developments are expected in TensorFlow 2.x. Reinforcement Learning and Generative Adversarial Networks are likely to stay, and new developments in the field are hard to predict. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Different levels of support are needed for beginners, researchers, and advanced users. TensorFlow has recently made advancements that are beneficial for beginners.\\n\\nThe role of high schoolers in contributing to cutting-edge technologies like TensorFlow, and the importance of cohesion and motivation within a team, are also discussed. Google's approach to building a good team, the value of superstars, and the hiring process at Google are highlighted. The importance of culture fit, motivation, and engineering excellence in the hiring process is emphasized. The challenges of balancing speed and perfection, making hard decisions, and meeting deadlines are also discussed. The podcast concludes by emphasizing the importance of regular releases and developing in the open.\\n\\nThe development and release of TensorFlow 2.0 are also discussed, with a focus on the importance of getting it right rather than rushing the release. The speaker, who previously led a team at Google on search ads, emphasizes the need for personalized ads to avoid annoying users. The podcast also explores the transition towards a mix model of free trials and ads, as well as the potential for using TensorFlow on desktops and phones. It also touches on the use of cloud computing and the regular cadence of releases for TensorFlow. The overall goal is to create a great product, with a focus on stability and improvement.\"}]\n",
      "Stage 2 done time 2024-03-26 02:00:36.226137\n",
      "stage_2_titles: len: 8\n",
      "[\"1. The Impact and Evolution of TensorFlow and Google's Machine Learning Efforts\", '2. Comparing Libraries and the Evolution of TensorFlow 2.0 for Deep Learning', '3. The Importance and Impact of TensorFlow Extended, Keras, and Data Organization', '4. Highlights of the TensorFlow Ecosystem and Integration into Real Products', '5. Challenges and Progress in Integrating TensorFlow.js and Deep Learning', '6. Restructuring and Modularization of TensorFlow for Future Improvements and Community Involvement', '7. The Importance of Team Cohesion, Vision, and Motivation at Google', '8. The Impact of TensorFlow 1.0 X and the Evolution of Advertisements and Cloud Computing for Machine Learning']\n",
      "remove_questions start time: 2024-03-26 02:00:36.242900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 02:06:42.324842\n",
      "chunks_text len: 73\n",
      "extract_keypoints start time: 2024-03-26 02:06:42.324975\n",
      "extract_keypoints done time 2024-03-26 02:09:47.580103\n",
      "Start time: 2024-03-26 02:09:47.580362\n",
      "Stage 1 done time 2024-03-26 02:12:42.096715\n",
      "RR stage_1_outputs:\n",
      "[{'title': \"Adobe Research's Efforts to Enhance Creativity through Automation \", 'text': 'Adobe Research is working to define the future evolution of their products to make the life of creatives easier. They aim to automate tedious tasks and give more time for creatives to operate in the idea space. Deep learning methods of the past decade can shine in this application. Gavin Miller, head of Adobe Research, combines tech and creativity, writing poetry and building robots outside of work.'}, {'title': 'Exploring the Intersection of Art and Artificial Intelligence ', 'text': 'The conversation is part of the Artificial Intelligence Podcast. Gavin Miller is the head of Adobe Research and leads innovative efforts and applications of AI in creating images, video, audio, and language. Gavin Miller is also an artist, poet, writer, and roboticist. Lux Friedman, the interviewer, enjoys Gavin Miller\\'s poetry and promises not to spend the entire time reading it, but will sprinkle it in a little bit. Lux Friedman mentions a poem by Gavin Miller called \"Je Ne Vinaigrette Rien\" that parodies both Edith Piaf\\'s \"Je Ne Vinaigrette Rien\" and Frank Sinatra\\'s \"My Way\".'}, {'title': 'Navigating the Struggle: Weight, Dieting, and Humor ', 'text': \"The speaker opens with a poem about struggling with weight and dieting. The poem reflects the internal struggle between wanting to lose weight and the irrational desire to embrace the opposite idea. The speaker expresses no regrets about their decision. The topic of weight and dieting is a serious one for some people, but the speaker finds humor in taking it to an extreme. The speaker has always been interested in writing and technology since high school. There are parallel strands in the speaker's life, one related to their private life and the other related to writing and technology.\"}, {'title': 'The Intersection of Technology and Artistic Expression ', 'text': 'The intersection of private life and technological career. The influence of one idea on the other. The inspiration from science fiction for building new technology. The example of using voice synthesis for writing a poem. The impact of voice synthesizer limitations on the poem.'}, {'title': 'The Evolution of AI in Poetry and Smart Home Technology ', 'text': 'The speaker created a poem to match the tone of a voice that sounded sad and depressed. The poem was pretended to be written by an intelligent agent, telling the user to go home and leave them alone, but also expressing loneliness and a desire for company. This level of AI sophistication was beyond what was possible at the time, but is now becoming more feasible. The speaker had a smart home project in the early 90s, with a talking voice reminding them of tasks and buttons on the washing machine to prevent clothes from getting moldy. The speaker also made photo albums that used some form of technology.'}, {'title': 'Exploring Magical Realism and Modern Technology ', 'text': 'The idea of magical realism and whether it was possible to achieve it with technology intrigued the speaker. The speaker created photo albums with light sensors that would send signals to an agent to play sounds matching the image in the book. The speaker has written plays and designed personalities for modern agents, thinking about imaginary dialogues and how to make them real and knowledgeable. The speaker is interested in the concept of the uncanny and how it relates to modern agents.'}, {'title': 'Challenges of AI Communication ', 'text': '[\"AI can fall into the uncanny valley where it says something it doesn\\'t really understand.\", \"It needs to have multiple ways of talking about the same concept to sound like it really understands it.\", \"Having only one way of referring to something can make it seem like a canned response.\", \"AI needs to be able to reason about a concept and give similar responses from different perspectives to seem more sentient.\"]'}, {'title': 'Advancements in Automatic Image Captioning ', 'text': '[\\'Automatic image captioning with the ability to generate different kinds of statements about the same picture.\\', \"Work on turning a medium from one form to another, such as auto tagging imagery or making up full sentences about what\\'s in the image.\", \\'Use of GANs to synthesize an asset that matches a given description.\\', \\'Early career focus on 3D computer graphics and pioneering work in the field.\\', \\'Comparison of early career work to the Renaissance era, where people would model light, color, and shape.\\']'}, {'title': 'Advancements in Computer Image Generation and AI Algorithms ', 'text': 'The new wave in computer image generation is more impressionistic and uses AI algorithms. The creative process is shifting towards generating images directly from ideas rather than focusing on raw pixels. Adobe aims to cover the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The realistic simulations are important for creators who want to achieve a beautiful analog trail of water and flow after making a brushstroke.'}, {'title': 'The Importance of Automation in Design Work ', 'text': 'Complete control is important for creating expressive and novel work. Automation of certain tasks frees artists to focus on inspiration. Design work for different aspect ratios used to take up a lot of time for artists. AI aims to reason about the likely intent for different formats and languages. The focus is on the creative aspect of design, including look, style, feel, and message.'}, {'title': 'The Evolution of Creativity in the Digital Age ', 'text': 'Creativity is changing, making it easier, faster, and cheaper to create beautiful artwork. There is a shift from hands-on artisan to art director or conceptual artist, with the computer as a partner in creating polished examples. Adobe products such as Photoshop, Premiere, and Audition are favored for creating images, videos, and audio.'}, {'title': 'Optimizing Workflow and Automation in Adobe Creative Suite ', 'text': 'The speaker uses Photoshop to create the thumbnail for the video, Premiere to edit the video, and Audition for the audio. The speaker emphasizes the importance of optimizing the flow and being extremely efficient in their work process. The discussion shifts to the role of AI and automation in making the workflow easier, particularly in low-level pixel work. There is a rich array of algorithms already in Photoshop, including classical procedural algorithms and ones based on data, with a large number of sliders and degrees of freedom.'}, {'title': \"AI's Role in Providing Default Settings for Image Editing \", 'text': 'AI can help by providing default settings based on the content itself, making it easier for users to start tweaking. Smart defaults can make life easier for people while making use of common sense from other example images. Adobe has spent a lot of work over the last 20 years thinking about selection, such as quick select, which looks at color boundaries and figures out how to flood fill into physically connected regions. The algorithm used for selection had no visual common sense about what a cat looks like or a dog, it was based on rules of thumb applied to graph theory.'}, {'title': 'Improving Object Selection in Images with Graph Theory and Neural Nets ', 'text': 'Graph theory and neural nets are being used to improve the process of selecting and identifying objects in images. The use of neural nets can make it easier to select objects with a single click or no click at all, especially for well-known categories like people or animals. The ability to remove backgrounds from images, such as in a thumbnail, is a valuable application of this technology. The goal is to make background removal and object selection as easy as possible, with the potential for further tweaking to improve the results. The use of neural nets can lead to more robust and high-quality results in image processing tasks.'}, {'title': 'Background Removal Techniques for Images with Flowing Hair ', 'text': 'The challenge of removing the background for images with flowing hair. Research and algorithms available for quick and easy background removal. Different algorithms for different levels of guidance on boundaries. Combinations of tools available for various background removal needs. Demonstration of quick object selection at Adobe Max conference.'}, {'title': 'Improving Selection Mask Creation with Simple Polygon Drawing ', 'text': 'Drawing a simple polygon around the object of interest can quickly create a selection mask. This process can be done for a single still or a moving target. The workflow has been improved from hours to a few seconds. The concept presented at Max has elements that inspire the idea, but the goal is to make it robust and work in the majority of cases. There is a difference between academic research and industrial research in terms of making a tool robust.'}, {'title': 'Differences Between Academic and Industrial Research ', 'text': 'Academic research focuses on new ideas and academic peer review, while industrial research focuses on shipping, customer reviews, and product improvement. The importance of being perfect enough of the time and having a mechanism to intervene and recover from mistakes is also discussed.'}, {'title': 'The Impact of AI on Professional Tasks ', 'text': 'AI can make professional tasks less tedious and time-consuming. 100% automatic processes could delay time to market. Collaboration between human and machine can make the life of creatives easier.'}, {'title': 'Improving User Learning Experience in Tool Tutorials ', 'text': \"The focus is on helping the person in the moment to do the task they need to do, as well as thinking holistically about their journey learning a tool. The goal is for users to become experts in using the tool, similar to living in a city where you know the important streets you need to get to. Projects in research analyze thousands of hours of tutorials online to understand what's being taught in them. One publication at CHI looked at the last three or four actions users did in tutorials to determine what other people did next, providing inspiration for what to do next or for watching the tutorial.\"}, {'title': 'Intelligent Suggestion App Utilizing User Context ', 'text': \"The app uses the context of the user's use to make intelligent suggestions. It aims to provide assistance and suggestions based on the user's behavior and domain understanding. The goal is to combine domain understanding with AI and pattern matching to make intelligent suggestions. The app may provide verbal suggestions or show results based on user actions. The team is exploring the challenge of combining domain understanding with AI to provide intelligent suggestions.\"}, {'title': 'Guiding the Process of Understanding Different Media Types ', 'text': 'The goal is to have an artist and a teacher guide the process. Giving enough at each stage to build a foundation for the next level of expectation. Understanding different media types visually and in terms of transcripts and words is important. Removing the barrier of having to type in keywords for searching. Ultimately assisting with learning the interface.'}, {'title': 'The Impact of Adding an Assistant on Interface Complexity ', 'text': 'The discussion is about whether an assistant modifies the interface to be simpler. Adding a feature to a GUI increases visual complexity for new users. Having an assistant with a new skill is additive without being more intimidating. The focus is on onboarding new users and making the interface easier for them. Some users value mastering complex interfaces and keyboard shortcuts, while others prefer a more assistive version for quick and simple tasks.'}, {'title': 'Exciting Applications of Computer Vision and Machine Learning by Adobe ', 'text': 'Adobe is working on exciting applications of computer vision and machine learning. These applications include scene stitching, sky replacement, foreground and background removal, spatial object-based image search, automatic image captioning, project cloak, project deep fill, project scribbler, style transform video, style transform faces and video with project puppetron. Different classes of devices may be more suitable for different tasks, such as CAPTCHA or deep post production workflow. Users may want to use a laptop or big screen desktop with more knobs and dials for expressing subtlety in their work.'}, {'title': 'Title ', 'text': 'Sky Replace Feature for Automatic Selection and Replacement in ImagesText '}, {'title': 'Exploring Natural Effects in Photography ', 'text': \"The evening sky adds an orange glow to foreground objects. The artist Magritte's surrealism paintings inspired the speaker in college. The goal is to achieve natural effects in photography rather than fake ones. The process aims to capture an entire workflow in a single action, saving time in post-production. The ability to apply the process to multiple backgrounds allows for exploration of the design space. The process allows for quick exploration of different options to find the desired result.\"}, {'title': 'Exploring the Design Space and Advanced Image Search Methods ', 'text': 'The importance of exploring the design space as close to final production value as possible. The idea of making intelligent choices about ways to search stock images. The concept of concept canvas and its application in image search. The need for more advanced search methods beyond just text-based keyword searches.'}, {'title': 'Improving Image Search with Concept Canvas ', 'text': 'Concept canvas allows assigning spatial regions to keywords for image search. Pre-indexed images are used to match important concepts in the picture. Gives a sense of ownership over the outcome of the event. Allows for spatial design and layout, making it feel like design. Technologies in Photoshop allow for physical movement of objects in post-production.'}, {'title': 'Advancements in Object Removal and Background Filling with Neural Networks ', 'text': 'Neural networks are being used to remove objects from a scene and fill in the background automatically. GANs (Generative Adversarial Networks) are one approach for achieving this. Traditional algorithms like content aware fill work well for certain classes of images, such as naturalistic textures like gravel paths. Patch-based algorithms can create plausible looking intermediate fill for certain types of images. Algorithms are used to smooth out lighting and eliminate brightness contrast in the filled region.'}, {'title': 'Challenges in Image Realism ', 'text': 'The importance of smooth lighting in creating realistic images. The challenge of inferring invisible structure behind objects in an image. The role of common sense knowledge in filling in missing information in images. The limitations of current generative methods in creating high-resolution images.'}, {'title': 'Challenges and Strategies in Advancing AI Capabilities ', 'text': 'The need to transition from low resolution to high resolution using another algorithm or pushing the state of the art in research. The importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms. The concept of an ensemble of experts specialized in certain things and the idea of voting on confidence levels.'}, {'title': 'Title ', 'text': 'Workflow Expansion and Data Collection in Photoshop User ResearchText '}, {'title': 'The Importance of Data Collection for Building Effective AI Tools ', 'text': 'Understanding the kind of data needed for annotation and collection for building effective tools in AI. The importance of gathering data and the reasons behind it. The need to train customers in using products and also to learn from them about what is important and useful. Respecting customer privacy and obtaining explicit permission for data usage. The modern approach of demonstrating the benefits of sharing data with a tool. The potential benefits for customers in sharing their data, such as better recommendations and quick evolution of the tool.'}, {'title': 'Data Sharing and Workflow Improvement ', 'text': 'Data contributors may be willing to share workflows or choices with the data set to be trained. Technologies exist for learning without storing all information permanently. Adobe exists in a space where sharing data for improving workflow is beneficial. Some professional workflows may be very protective of their data.'}, {'title': 'Protecting Data in Professional Workflows ', 'text': 'Some professional workflows require protection of data, especially in legal cases. Different scenarios may require different levels of data sharing, depending on the nature of the project. There is a possibility of capturing high-level data from a larger group, and more detailed data from willing participants. Explicit customer studies are currently used to gather detailed feedback on tools.'}, {'title': 'Importance of Responsible Data Collection in Customer Studies ', 'text': \"Customer studies involve visiting and observing users to improve the tool. A more systematic process is needed to train an algorithm for customer studies. Conscious effort is required to balance data collection with customer trust. Adobe has a chief privacy officer to ensure responsible data collection. Privacy is a priority in the development of AI, not an afterthought. Project Puppetron demonstrates Adobe's move towards thinking in three dimensions.\"}, {'title': 'Advancements in 3D Thinking for Applying Painting Styles to Videos and Images ', 'text': '3D thinking is being used to assign features to faces in order to apply painting styles to videos or still images of people talking. The technology is able to apply the style of a painting to a person in a video, creating a realistic effect that reflects the motion of the face. This process requires inferring more about the 3D structure of the world, even for a 2D workflow like stylization. 3D computer vision algorithms are improving and initially focusing on specific domains like faces, where there is a lot of prior knowledge about structure. Over time, this technology should be possible for more general applications beyond just faces.'}, {'title': 'The Applications of 3D Reconstruction in Various Content ', 'text': '3D reconstruction can be applied to more general content over time. The use of 3D reconstruction may be invisible to the user but can improve edits. The face is a very important application for 3D reconstruction. AR and VR serve slightly different purposes, with VR transporting users to an immersive world.'}, {'title': 'The Evolution and Accessibility of VR Technology ', 'text': 'VR technology has evolved and is becoming more accessible to consumers. VR is also being used for professional purposes, such as in architecture and design. AR has the potential to bring digital assets into the real world.'}, {'title': 'The Promise and Challenges of Augmented Reality ', 'text': \"AR holds the promise of taking digital assets off the screen and putting them in context in the real world. The assets need to adapt to the physical context in which they're being placed. AR is like having a live theater troupe come to your house and put on a performance. AR will have the same issue of adapting to different physical spaces. There is a tension between fidelity and adaptation in AR.\"}, {'title': 'Fidelity Requirements for Different Types of Media ', 'text': 'Different types of media may require different levels of fidelity in reproduction. The characteristics of the media and the subject may influence the level of fidelity required. Famous celebrities may require a high level of fidelity, while fictional characters may be more adaptable. Ideas from the game world may influence the development of adaptive characters in augmented reality. Engineering tools may be needed to allow for adaptive characters in AR.'}, {'title': 'The Impact of AR and Immersive Technology on 3D Design ', 'text': 'AR technology is being used to create adaptive characters. 3D design is still a challenging space. The experiment is to figure out if AR or immersive technology makes 3D design more spontaneous. One example shown publicly is taking a Photoshop layer stack and expanding it into 3D in AR. In VR headset, laying out objects can be done more effectively compared to conventional screen.'}, {'title': 'The Impact of VR and AR on Design ', 'text': 'VR headset allows for a different viewpoint and sense of depth. Fine grained design tasks may be possible with the right UI. Potential explosion of demand for 3D assets driven by AR and real time animation. Tools and devices may help with designing content as well.'}, {'title': 'The Importance of Designing Content ', 'text': 'The balance between new ideas and old ways. The need to consider existing user base when making changes. The importance of maintaining well-known metaphors in design. The evolution and growth of Photoshop.'}, {'title': 'Evolution and Breakthroughs in Tool Development ', 'text': 'The tool has been evolving and growing, with a lot of brilliant thought along the way. A fundamental breakthrough, such as a single click to select an object, can fit into the existing toolset as an optional mode or starting point. Radical simplicity can be achieved in the context of a different device or targeted workflow, such as a mobile device or a tool for spontaneity and velocity rather than precision. Projects like Rush allow professional quality video editing for a certain class of media output targeted differently.'}, {'title': 'Choosing the Right Video Editing Tool for Different Projects ', 'text': 'Quality video editing for different types of media output targeted at different users and experiences. Premiere is ideal for big projects like a television series, while Rush is more suitable for quick, simple projects like showing a recent vacation. Professional tools offer a richer toolkit and more flexibility, but simpler tools can provide faster output for certain use cases. Smart defaults and AI can be used to coach users and provide a set of settings, similar to Google\\'s \"I\\'m feeling lucky\" button. The use of AI for smart defaults can also serve as an educational tool for users.'}, {'title': 'The Importance of an Educational Tool for Image Editing ', 'text': 'The need for an educational tool to show the correlation between different bars that control different elements of an image.'}, {'title': 'The Impact of AI and Digital Platforms on Human Perception ', 'text': 'The poem reflects the feeling of liberation when leaving the smartphone behind. AI is helping to create versions of ourselves and reality that are more beautiful than actual reality. The creative effort in creating this illusion is part of the process. Living in a digital world that is partly artificial requires adjustment as human beings. The digital world today is different from the world a hundred years ago due to platforms like Instagram and Facebook.'}, {'title': 'The Impact of Social Media on Self-Presentation ', 'text': \"The use of social media platforms like Instagram and Facebook has led to the creation of better versions of ourselves online. The use of tools, such as image modification and artificial intelligence, has allowed for the creation of adjusted or fake versions of ourselves and reality. The human desire to present the best version of oneself has always been true, dating back to the 18th century aristocrats who commissioned flattering portraits of themselves. The question of whether the ability to imagine alternate realities and visualize them is a good or bad thing is raised, particularly in today's visual culture. The shift towards a more visual culture has led to a greater emphasis on presenting oneself visually, compared to the use of storytelling and poetry in the past.\"}, {'title': 'The Impact of Visual Culture on Self-Presentation ', 'text': 'We have become a very visual culture, with a focus on quick and snappy content. Intent plays a role in how we present ourselves visually, such as choosing a flattering photo. Holding oneself up to an impossible standard based on visual appearance can be harmful. The ability to imagine and visualize an alternate reality can be a wonderful thing.'}, {'title': 'The Impact of Alternate Reality on Exploration and Creativity ', 'text': \"['Alternate reality can inspire people to create new architectural styles and even start businesses.', 'The availability of high-quality graphics may reduce the excitement of exploring new places in person.', 'The joy of exploration, such as the experience of going to the moon, may be diminished by the ability to visualize the experience beforehand.', 'The recent discovery of Pluto was a fantastic example of outer exploration.']\"}, {'title': 'The Importance of Critical Thinking in Communication and Imaging ', 'text': \"Pluto was a fantastic recent discovery with breathtakingly varied and beautiful features. Expanding the ability of the human toolkit to imagine and communicate is a good thing. Abuses in communication and imaging should be taken seriously and discouraged. The public needs to be aware of what's possible through events like this and not believe everything they see or read. Multiple sets of evidence are needed to really believe something rather than a single media asset. The concept of needing multiple sets of evidence has been true forever. There is a famous story about Anne of Cleves and Henry VIII involving a painted picture by Holbein.\"}, {'title': 'Challenges and Benefits of Research Lab Interns ', 'text': \"Holbein painted a picture that Henry VIII wasn't pleased with. The secret to a thriving research lab is interns. Constant influx of new people brings new ideas to the research lab.\"}, {'title': 'The Benefits of Internships in Research ', 'text': 'A constant influx of new people brings new ideas with it. Interns allow for exploration of fanciful or unproven ideas in a lightweight way. Internships can lead to new publications for both the intern and the researcher. Internships provide a portfolio of ideas to draw from for future development. Internships are a way to identify future full-time researchers. Internships build a bridge to university departments for collaboration and recruitment.'}, {'title': 'Building Bridges: Establishing Enduring Relationships with University Departments ', 'text': 'The program builds a bridge to university departments to establish enduring relationships with professors. The interns add value through their collaborations and contribute to academic research funding. The long-term legacy of a great research lab includes the impact on people who move through and carry the model to other companies. The company strongly believes in the complementarity of industrial research and academia. The company hopes that this model will be adopted by other companies, despite the challenge of recruitment. The idea for the program was born through brainstorming and discussions with interns.'}, {'title': 'Selecting Interns Process ', 'text': 'The process of selecting interns involves sending out a call for interns, receiving resumes, and contacting candidates to discuss their interests. The goal is to find candidates who have a good match to the work being done or have pursued an interesting domain in their PhD. Interns stay in touch with their mentors and have internal discussions about project ideas. At the end of two weeks, interns have to decide on a project to pursue.'}, {'title': 'The Dynamics of Research Labs ', 'text': 'The flexibility of pursuing ideas in research labs. The decision-making process in research labs. The reward system based on impact in Adobe. The alternative model of having one lab director making decisions.'}, {'title': 'Encouraging Innovation and Collaboration in the Lab ', 'text': \"The model of running the lab is based on allowing new ideas to percolate up. There are strategic priorities for the company and areas where investment is needed. The approach is a combination of trickle down and filter up, meeting in the middle. People are not told what to do, but rather encouraged to consider certain actions as particularly appreciated. Adobe's broad portfolio of products allows for good ideas to find interested product teams. There is not a need to qualify things too much ahead of time.\"}, {'title': 'Challenges and Outcomes of Intern Projects in Product Teams ', 'text': 'The product teams sponsor extra interns occasionally to address specific problems they care about. It is difficult to predict the success of intern projects at the beginning of the summer. Some intern projects pay off, while others do not turn into features. Some projects are not as novel as expected but still make great features. Some projects make progress but reveal how much is still unknown about the problem.'}, {'title': 'The Unknown and Breakthroughs in Technology ', 'text': 'Progress and realization of how much is unknown. Revisiting problems multiple times before a breakthrough. Impact of technological breakthroughs on products and the world. Focus on creative and analytics assistants for making useful suggestions. Anticipation of progress in 2019 and beyond.'}, {'title': 'Advancements in Generative Adversarial Networks and Sensei Platform ', 'text': 'Core technologies like generative adversarial networks are immensely promising. The quick practical application of these technologies for mainstream use cases at high resolution with good quality is exciting. The strange and interesting way in which these technologies operate, resembling dreaming or something, is fascinating. The development of a Sensei platform for pulling neural nets and other intelligence models into a central platform, which can be leveraged by multiple product teams at once, is in progress. Transitioning from hand designing for specific use cases to a more standardized approach, which can be accessed in a standard way, is underway.'}, {'title': 'Standardizing Processes and the Future of AI in Product Development ', 'text': 'Ford is standardizing processes to shorten the time between idea generation and product impact. Products can leverage good ideas from each other, creating an economy of scale. There is a renaissance in AI and real-time ray tracing in graphics, leading to exciting emerging technologies. The combination of AI and graphics technologies will create a future where creators can \"dance with light\" and have real-time shadows, reflections, and a real-world experience. AI will anticipate and modify itself to make sense based on the creative task at hand, creating an exciting future for creators.'}, {'title': 'Fascination with Snakes and Robotics ', 'text': \"The speaker works in autonomous vehicles and is a roboticist. The speaker has a fascination with snakes, both natural and artificial robots. There are 2,900 species of snakes in the world, with 875 venomous. The speaker's interest in snakes came from their work in computer animation in the 80s. The speaker started with cloth simulation and soft body simulation in computer animation.\"}, {'title': 'The Evolution of Robotics and Animation ', 'text': \"The idea of animating spring lengths and simulating muscles came from the observation of objects bouncing and then stopping. The earliest application of this idea was in a paper called The Motion Dynamics of Snakes and Worms in 1988. The interest in robotics came from simulation and graphics work, leading to the creation of a movie called Her Majesty's Secret Serpent. The movie featured a secret agent snake parachuting in to capture a film canister from a satellite. The interest in robotics also stemmed from the experience of making radio controlled chips from scratch as a child. This led to the idea of building a real robot, which eventually became a reality.\"}, {'title': 'Creating Lifelike Snake Robots: A 15-Year Obsession ', 'text': \"The speaker had a 15 year obsession with building better snake robots. The first snake robot built could only slither sideways, but didn't go forward. The speaker added wheels to the snake robot to address friction issues. The speaker loves creating the illusion of life, which drove them to animation. The goal is to create a robot that moves in a biological way, resembling a creature rather than a thing. The early snake robot was able to sidewind and go directly forward. The snake robot was used as the ring bearer at the speaker's wedding. The project was initially done as a hobby.\"}, {'title': 'Title ', 'text': 'The Evolution of Autonomous Vehicle DevelopmentText '}, {'title': 'The Experience of Building and Learning from a Robotic Project ', 'text': \"The text discusses the experience of building and learning from a robotic project. The speaker mentions building a better engineered version of a previous project, S3, which had worn out motors and was no longer functional. The project S5 is described as being biologically inspired and unique in its design, with tapered segments for mechanical and aesthetic reasons. The speaker's project, S5, is currently on display at the International Spy Museum in Washington, DC. The speaker also mentions the disappointment of not being able to buy replacements for a meaningful project, S9, which is no longer functional.\"}, {'title': 'Spy Museum in Washington, DC ', 'text': \"['Spy Museum in Washington, DC.', 'Conspiracy theory about the museum being fake.', 'Use of Raspberry Pi for onboard compute.', 'Addition of vision accelerator chips for object recognition.', 'Desktop level compute enabling true autonomy with onboard compute and batteries.']\"}, {'title': 'True Autonomy and Biomimetic Design in a Technology Product ', 'text': \"True autonomy with onboard compute, onboard batteries, and biomimetic quality. Appeal to children and adults' perception of the design. Encouragement of interest in technology, especially for girls. Affordability and value of the product. Potential for using real artificial muscle material in future designs. Being in research as a license to be curious.\"}, {'title': 'The Curiosity of Research in Robotics and Intelligent Agents ', 'text': 'Being in research is a license to be curious. Hobby forced the individual to read biology and be curious about things. Interested in understanding how snakes move and trying to copy it with snake robots. Trying to bring life and beauty into something inanimate. Working on giving inanimate objects a personality through intelligent agent research and vision and voice synthesis. Not necessarily aiming for human level intelligence, but aiming for meaningful conversation with the inanimate objects.'}, {'title': 'The Importance of Meaningful Interaction and Reasoning in Robot Pet Ownership ', 'text': 'The goal is to have a robot pet owner understand what the robot thinks about and can reason about. Meaningful interaction with the robot is important, similar to the interaction with a dog. The reasoning system of the robot should be able to explain why it knows or thinks something. The robot serves as a muse for thinking about the future of AI.'}, {'title': 'The Future of AI and Virtual Reality ', 'text': 'The robot is the muse for thinking about the future of AI and what to invent next. Bringing virtual objects into the physical world through augmented reality is more likely than building intelligent robots. Many ideas that might take five years to build a robot to do can be done in a few weeks with digital assets. Living with virtual personalities for a long time will make intelligent robots less surprising when they become commonplace.'}, {'title': \"Speaker's Excitement for the Future and Gratitude for Conversation \", 'text': 'The speaker compares the future to a world with \"Siri with legs or Alexa on hooves\". The speaker is excited about the convergence of different strands of their career. The conversation ends with the recitation of a favorite poem about mortality and immortality. The speaker expresses gratitude for the conversation.'}, {'title': 'Title ', 'text': 'A Message of Gratitude and InspirationText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 02:12:43.675753 ...\n",
      "Best SD: 1.9688939051854832, Best iteration: 7\n",
      "done get topics 2024-03-26 02:12:44.517847.\n",
      "Stage 2 start time 2024-03-26 02:12:44.517867\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Enhancing Creativity through Automation: Adobe Research's Efforts\n",
      "2. The Role of Automation in Design Work in the Digital Age\n",
      "3. Impact of AI on Professional Tasks and User Experience\n",
      "4. Exciting Applications of Computer Vision and Machine Learning by Adobe\n",
      "5. Importance of Responsible Data Collection and Workflow Improvement\n",
      "6. Advancements in 3D Technology and Immersive Experiences\n",
      "7. The Impact of AI, Digital Platforms, and Visual Culture on Communication and Imaging\n",
      "8. Benefits and Challenges of Research Lab Internships\n",
      "9. The Future of Robotics and Intelligent Agents: A Fascinating Journey\n",
      "\n",
      "RRR given summary\n",
      "In this podcast, Gavin Miller, head of Adobe Research, discusses the future evolution of Adobe products to make the lives of creatives easier. He aims to automate tedious tasks and give more time for creatives to operate in the idea space, using deep learning methods to achieve this. Miller is also an artist, poet, writer, and roboticist, and the conversation touches on his poetry and creative pursuits. The intersection of private life and technological career is explored, as well as the influence of science fiction on building new technology. Miller also discusses his early career focus on 3D computer graphics and pioneering work in the field, comparing it to the Renaissance era. The impact of AI on image captioning, auto tagging imagery, and synthesizing assets is also discussed, as well as the concept of the uncanny valley in AI and the need for multiple perspectives in AI responses.\n",
      "\n",
      "The podcast discusses the new wave in computer image generation, which is more impressionistic and uses AI algorithms. Adobe aims to cover the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The focus is on the creative aspect of design, including look, style, feel, and message. Automation of certain tasks frees artists to focus on inspiration, and AI aims to reason about the likely intent for different formats and languages. Adobe products such as Photoshop, Premiere, and Audition are favored for creating images, videos, and audio. The discussion also covers the role of AI and automation in making the workflow easier, particularly in low-level pixel work, and the use of neural nets to improve the process of selecting and identifying objects in images. The goal is to make background removal and object selection as easy as possible, with the potential for further tweaking to improve the results. The podcast also touches on the challenge of removing the background for images with flowing hair and the different algorithms available for quick and easy background removal. The concept presented at the Adobe Max conference has elements that inspire the idea, but the goal is to make it robust and work in the majority of cases.\n",
      "\n",
      "The podcast discusses the differences between academic and industrial research, as well as the importance of perfection and intervention in professional tasks. It also explores the role of AI in making tasks less tedious and time-consuming, and the potential for collaboration between humans and machines. The focus is on providing intelligent suggestions and assistance based on user behavior and domain understanding, with the goal of making the interface easier for new users. The team is exploring the challenge of combining domain understanding with AI to provide intelligent suggestions, and the discussion also touches on the potential modifications to the interface to make it simpler for users. Ultimately, the goal is to assist with learning the interface and cater to the preferences of different users.\n",
      "\n",
      "Adobe is working on innovative applications of computer vision and machine learning, including scene stitching, sky replacement, foreground and background removal, spatial object-based image search, automatic image captioning, and various other projects. Different devices may be more suitable for different tasks, and users may want to use a laptop or big screen desktop for expressing subtlety in their work. The sky replace feature aims to achieve natural effects in photography, and the concept canvas allows for spatial design and layout in image search. Technologies in Photoshop allow for physical movement of objects in post-production, and neural networks are being used to remove objects from a scene and fill in the background automatically. However, there are limitations and challenges in creating high-resolution images, and the importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms is also discussed.\n",
      "\n",
      "The podcast discusses the importance of data sharing and collection in the development of AI tools, particularly in the context of Adobe's workflow expansion and data collection in Photoshop user research. It emphasizes the need for different levels of data sharing depending on the nature of the project, as well as the importance of respecting customer privacy and obtaining explicit permission for data usage. The podcast also highlights the potential benefits for customers in sharing their data, such as better recommendations and the quick evolution of the tool. It emphasizes the need for a conscious effort to balance data collection with customer trust and the importance of privacy in the development of AI. Adobe's move towards thinking in three dimensions, as demonstrated by Project Puppetron, is also discussed. Additionally, the podcast touches on the need for a more systematic process to train algorithms for customer studies and the role of a chief privacy officer in ensuring responsible data collection.\n",
      "\n",
      "The podcast discusses the use of 3D thinking to apply painting styles to videos or still images of people talking, creating a realistic effect that reflects the motion of the face. The technology requires inferring more about the 3D structure of the world, even for a 2D workflow like stylization. 3D computer vision algorithms are improving and initially focusing on specific domains like faces, with the potential for more general applications over time. 3D reconstruction can be applied to more general content and may improve edits. The podcast also explores the differences between AR and VR, with VR evolving and becoming more accessible to consumers, and AR having the potential to bring digital assets into the real world. There is a tension between fidelity and adaptation in AR, and engineering tools may be needed to allow for adaptive characters in AR. The podcast also touches on the potential for AR and immersive technology to make 3D design more spontaneous, and the potential explosion of demand for 3D assets driven by AR and real-time animation.\n",
      "\n",
      "The podcast discusses the balance between new ideas and old ways, emphasizing the importance of considering existing user base when making changes and maintaining well-known metaphors in design. It explores the evolution and growth of Photoshop, highlighting the incorporation of breakthrough features and the development of tools like Rush for different types of media output. The use of AI for smart defaults and coaching users is also discussed, along with the impact of social media platforms on presenting idealized versions of ourselves. The podcast raises questions about the implications of visual culture and the ability to imagine alternate realities, emphasizing the need for critical thinking and multiple sets of evidence. It also touches on the recent discovery of Pluto as an example of the joy of exploration. Overall, the podcast advocates for the responsible use of technology and the expansion of the human toolkit for imagination and communication.\n",
      "\n",
      "The podcast discusses the importance of internships in research labs, particularly in the context of Adobe. It emphasizes the value of interns in bringing new ideas and collaborations to the lab, as well as their potential to lead to new publications and identify future full-time researchers. The program at Adobe involves a process of selecting interns based on their match to the work being done, and encourages them to pursue their own project ideas. The company's model for running the lab is based on allowing new ideas to percolate up, with a combination of trickle down and filter up approach. The podcast also touches on the impact of technological breakthroughs on products and the world, particularly in the areas of AI and real-time ray tracing in graphics. It discusses the exciting future possibilities for creators in terms of AI anticipating and modifying itself based on the creative task at hand.\n",
      "\n",
      "The podcast discusses the evolution of autonomous vehicle development, focusing on the speaker's experience in building and learning from a robotic project. The speaker describes the creation of a biologically inspired robot, S5, which is currently on display at the International Spy Museum in Washington, DC. The project aims to encourage interest in technology, especially for girls, and incorporates elements of biomimicry and intelligent agent research. The speaker also expresses excitement about the convergence of their career strands and ends the conversation with a recitation of a favorite poem about mortality and immortality. The speaker's fascination with snakes, both natural and artificial, has driven their work in robotics, stemming from their experience in computer animation in the 80s. The speaker's goal is to create a robot that moves in a biological way, resembling a creature rather than a thing, and has had a 15-year obsession with building better snake robots. The early snake robot was used as the ring bearer at the speaker's wedding and was initially done as a hobby.\n",
      "RRR rewritten summary\n",
      "[{'text': \"Gavin Miller, head of Adobe Research, shares insights into the future evolution of Adobe products in order to simplify the lives of creatives. His focus is on automating tedious tasks and providing more time for creatives to operate in the idea space, using deep learning methods to achieve this. Miller's diverse background as an artist, poet, writer, and roboticist adds depth to the conversation, as the discussion delves into his poetry and creative pursuits. The intersection of private life and technological career is explored, along with the influence of science fiction on building new technology. Miller also reflects on his early career focus on 3D computer graphics and pioneering work in the field, drawing comparisons to the Renaissance era. The impact of AI on image captioning, auto tagging imagery, and synthesizing assets is also discussed, as well as the concept of the uncanny valley in AI and the need for multiple perspectives in AI responses.\\n\\nThe conversation shifts to the new wave in computer image generation, which is more impressionistic and utilizes AI algorithms. Adobe's goal is to cover the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The emphasis is on the creative aspect of design, including look, style, feel, and message. Automation of certain tasks frees artists to focus on inspiration, and AI aims to reason about the likely intent for different formats and languages. Adobe products such as Photoshop, Premiere, and Audition are highlighted for their role in creating images, videos, and audio. The discussion also covers the role of AI and automation in making the workflow easier, particularly in low-level pixel work, and the use of neural nets to improve the process of selecting and identifying objects in images. The goal is to make background removal and object selection as easy as possible, with the potential for further tweaking to improve the results.\\n\\nThe differences between academic and industrial research are explored, along with the importance of perfection and intervention in professional tasks. The conversation also delves into the role of AI in making tasks less tedious and time-consuming, and the potential for collaboration between humans and machines. The focus is on providing intelligent suggestions and assistance based on user behavior and domain understanding, with the goal of making the interface easier for new users. The team is exploring the challenge of combining domain understanding with AI to provide intelligent suggestions, and the discussion also touches on the potential modifications to the interface to make it simpler for users. Ultimately, the goal is to assist with learning the interface and cater to the preferences of different users.\\n\\nAdobe's innovative applications of computer vision and machine learning, including scene stitching, sky replacement, foreground and background removal, spatial object-based image search, automatic image captioning, and various other projects, are highlighted. The discussion also addresses the potential use of different devices for various tasks, and the importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms is also discussed.\\n\\nThe importance of data sharing and collection in the development of AI tools is emphasized, particularly in the context of Adobe's workflow expansion and data collection in Photoshop user research. The podcast highlights the potential benefits for customers in sharing their data, such as better recommendations and the quick evolution of the tool. It emphasizes the need for a conscious effort to balance data collection with customer trust and the importance of privacy in the development of AI. Adobe's move towards thinking in three dimensions, as demonstrated by Project Puppetron, is also discussed. Additionally, the podcast touches on the need for a more systematic process to train algorithms for customer studies and the role of a chief privacy officer in ensuring responsible data collection.\\n\\nThe use of 3D thinking to apply painting styles to videos or still images of people talking is explored, along with the potential for 3D reconstruction to be applied to more general content and improve edits. The differences between AR and VR are also discussed, with VR evolving and becoming more accessible to consumers, and AR having the potential to bring digital assets into the real world. The podcast also touches on the potential for AR and immersive technology to make 3D design more spontaneous, and the potential explosion of demand for 3D assets driven by AR and real-time animation.\\n\\nThe podcast emphasizes the importance of considering existing user base when making changes and maintaining well-known metaphors in design. It explores the evolution and growth of Photoshop, highlighting the incorporation of breakthrough features and the development of tools like Rush for different types of media output. The use of AI for smart defaults and coaching users is also discussed, along with the impact of social media platforms on presenting idealized versions of ourselves. The podcast raises questions about the implications of visual culture and the ability to imagine alternate realities, emphasizing the need for critical thinking and multiple sets of evidence. It also touches on the recent discovery of Pluto as an example of the joy of exploration.\\n\\nThe importance of internships in research labs, particularly in the context of Adobe, is emphasized. The podcast discusses the value of interns in bringing new ideas and collaborations to the lab, as well as their potential to lead to new publications and identify future full-time researchers. The program at Adobe involves a process of selecting interns based on their match to the work being done, and encourages them to pursue their own project ideas. The company's model for running the lab is based on allowing new ideas to percolate up, with a combination of trickle down and filter up approach. The podcast also touches on the impact of technological breakthroughs on products and the world, particularly in the areas of AI and real-time ray tracing in graphics.\\n\\nThe evolution of autonomous vehicle development is discussed, focusing on the speaker's experience in building and learning from a robotic project. The speaker describes the creation of a biologically inspired robot, S5, which is currently on display at the International Spy Museum in Washington, DC. The project aims to encourage interest in technology, especially for girls, and incorporates elements of biomimicry and intelligent agent research. The speaker also expresses excitement about the convergence of their career strands and ends the conversation with a recitation of a favorite poem about mortality and immortality. The speaker's fascination with snakes, both natural and artificial, has driven their work in robotics, stemming from their experience in computer animation in the 80s. The speaker's goal is to create a robot that moves in a biological way, resembling a creature rather than a thing, and has had a 15-year obsession with building better snake robots. The early snake robot was used as the ring bearer at the speaker's wedding and was initially done as a hobby.\"}]\n",
      "Stage 2 done time 2024-03-26 02:14:26.978292\n",
      "stage_2_titles: len: 9\n",
      "[\"1. Enhancing Creativity through Automation: Adobe Research's Efforts\", '2. The Role of Automation in Design Work in the Digital Age', '3. Impact of AI on Professional Tasks and User Experience', '4. Exciting Applications of Computer Vision and Machine Learning by Adobe', '5. Importance of Responsible Data Collection and Workflow Improvement', '6. Advancements in 3D Technology and Immersive Experiences', '7. The Impact of AI, Digital Platforms, and Visual Culture on Communication and Imaging', '8. Benefits and Challenges of Research Lab Internships', '9. The Future of Robotics and Intelligent Agents: A Fascinating Journey']\n",
      "remove_questions start time: 2024-03-26 02:14:26.995846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 02:22:28.397509\n",
      "chunks_text len: 101\n",
      "extract_keypoints start time: 2024-03-26 02:22:28.397677\n",
      "extract_keypoints done time 2024-03-26 02:26:16.964725\n",
      "Start time: 2024-03-26 02:26:16.965015\n",
      "Stage 1 done time 2024-03-26 02:30:04.401032\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Ilya Sotskever: Cofounder and Chief Scientist of OpenAI ', 'text': 'The conversation with Ilya Sotskever, one of the most cited computer scientists in history, and his message of support for those affected by the pandemic. AI Podcast mentioned. Ads coming up.'}, {'title': 'The Importance of Cash App and Cryptocurrency ', 'text': 'Twitter handle is @lexfriedman, spelled F R I D M A N. The show is presented by Cash App, the number one finance app in the App Store. Use code LEXPODCAST when getting Cash App. Cash App allows you to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Cryptocurrency is still in its early days of development, despite being around for over 10 years. The history of money, including the creation of the US dollar and the release of Bitcoin, is fascinating. The book \"Ascent of Money\" is recommended for learning about the history of money and cryptocurrency.'}, {'title': 'The Early Development of Cryptocurrency and Contributions to Deep Learning ', 'text': 'Cryptocurrency is still in its early days of development and aims to redefine the nature of money. Cash App from the App Store or Google Play can be used with the code LEXPODCAST to get $10 and Cash App will also donate $10 to FIRST, an organization helping advance robotics and STEM education for young people. Ilya Satsgever was one of the three authors of the famed AlexNet paper that marked the big catalytic moment that launched the deep learning revolution. In 2010 or 2011, Ilya Satsgever connected two facts in his mind, leading to a significant development.'}, {'title': 'Advancements in Neural Network Training ', 'text': 'Deep neural networks can be trained end to end with backpropagation. James Martens invented the Hessian free optimizer in 2010, which allowed training a 10 layer neural network from scratch. Training a big neural network can represent very complicated functions. A neural network with 10 layers can simulate the human brain running for some number of milliseconds. Neuron firings are slow, so in 100 milliseconds, neurons only fire 10 times.'}, {'title': 'Neural Network Training and Overparameterization ', 'text': 'Neuron firings are slow and only fire 10 times in 100 milliseconds. The idea of training a very big neural network on lots of supervised data was already present. The theory that having more data than parameters prevents overfitting is incomplete. Neural networks being heavily overparameterized was not discouraging. There was evidence that the huge number of parameters was okay, but the theory was incomplete.'}, {'title': 'Challenges and Innovations in Training Big Neural Nets ', 'text': \"The theory was that with a big data set and a big neural net, it was going to work. Overparameterization was not seen as a problem. The main doubt was whether there would be enough compute to train a big enough neural net with backpropagation. Alex Kerchevsky wrote fast CUDA kernels for training convolutional neural nets, leading to a decision to proceed with the project. The demonstration of the project's potential was a key factor in moving forward.\"}, {'title': 'The Role of the Human Brain in Intuition Building for Deep Learning Researchers ', 'text': 'The intuition about neural networks can come from empirical results and also from pen and paper or marker and whiteboard thinking. The human brain plays a role in intuition building for deep learning researchers. The idea of a neural network is directly inspired by the brain. The brain has been a huge source of intuition and inspiration for deep learning researchers since the 60s.'}, {'title': 'The Evolution of Neural Networks and Their Relationship to the Human Brain ', 'text': \"The idea of using ideas from computer and automata to design a computational object similar to the brain. The invention of the neuron inspired by the brain. The development of the convolutional neural network and its success in image recognition. The assumption that an artificial neuron is not that different from the brain if it's trained hard enough. The success of deep learning in the current time. The recognition of interesting differences between the human brain and artificial neural networks.\"}, {'title': 'Understanding the Differences Between the Human Brain and Artificial Neural Networks ', 'text': 'The difference between the human brain and artificial neural networks is an interesting topic for the next decade or two. Artificial neural networks have important advantages over the human brain in certain ways. It is important to consider the advantages versus disadvantages of both the human brain and artificial neural networks. The brain uses spikes, which may or may not be important in comparison to artificial neural networks.'}, {'title': 'Understanding the Architectural Differences in Neural Networks ', 'text': 'The architectural difference between artificial neural networks is a big factor. Spiking neural networks need to simulate non-spiking neural networks in spikes to work. Questions around back propagation and deep learning are connected to the functioning of neural networks. The effectiveness of a giant neural network is not self-evident, especially for beginners in the field.'}, {'title': 'The Development of Neural Networks ', 'text': \"Neural networks were inspired by the brain's neural network. The big idea in training neural networks is the cost function. The cost function measures the performance of the system according to some measure. The concept of a single cost function may seem trivial now, but it was a significant idea at the time. There may be other things that don't necessarily have a cost function, or may have many cost functions.\"}, {'title': 'Understanding the Cost Functions of GANs ', 'text': \"GANs do not have clear cost functions. GANs operate based on a game and reason about the system's behavior in terms of the equilibrium of the game. The behavior of the system is reasoned about using mathematical objects. The cost function of a GAN is emergent.\"}, {'title': 'Challenges with Cost Functions in Deep Learning ', 'text': 'GAN does not have a clear cost function. The analogy of cost function in biological evolution or the economy may not be the most useful. Questioning if cost functions in deep learning are holding us back. Self play in reinforcement learning systems may provide insight into the concept of cost function.'}, {'title': 'The Importance of Self Play, Exploration, and Cost Functions in Reinforcement Learning Systems ', 'text': 'Self play and exploration are important in reinforcement learning systems. Cost functions are considered great and serve well in various applications. There may be potential for new ways of looking at things that involve cost functions in a less central way. Spiking and the learning rule of the brain are topics of interest in neuroscience.'}, {'title': 'Understanding Spike Time Independent Plasticity in the Brain ', 'text': 'Neuroscientists have figured out something about the learning rule of the brain, specifically spike time independent plasticity. Spike time independent plasticity (STD) is a learning rule that uses spike timing to determine how to update synapses. If a synapse fires into the neuron before the neuron fires, it strengthens the synapse; if the synapse fires into the neuron shortly after the neuron fired, it weakens the synapse. The temporal dynamics of the timing of signals is a fundamental property of the brain that is not fully captured.'}, {'title': \"The Role of Recurrent Neural Networks in Mimicking the Brain's Timing Mechanism \", 'text': \"The brain's fundamental property is the timing of signals. Recurrent neural networks are a simplified version of the brain's timing mechanism. The brain is a continuous version of recurrent neural networks, allowing for all possible timings and containing information within those timings. Recurrent neural networks are amazing and can potentially perform any task. Recurrent neural networks have been superseded by transformers, but there is a possibility of a comeback in the future.\"}, {'title': 'Advancements in Natural Language Processing and Language Modeling with Non-Recurrent Transformers ', 'text': \"Breakthroughs in natural language processing and language modeling have been with transformers that don't emphasize recurrence. Recurrent neural networks are still possible for processing sequences. Neural networks maintain a high dimensional hidden state and update it when an observation arrives. Expert systems and symbolic AI also involved maintaining a hidden state.\"}, {'title': 'The Future of Building Large Scale Knowledge Bases within Neural Networks ', 'text': 'Symbolic AI involves maintaining a hidden state as its knowledge base and growing it through sequential processing. The hidden state in symbolic AI is similar to the hidden state in LSTM or RNN. In the expert system analogy, knowledge is stored in the connections and short term processing is done in the hidden state. There is potential for building large scale knowledge bases within neural networks. The speaker wants to explore the future of building large scale knowledge bases within neural networks.'}, {'title': 'The Evolution of Neural Networks and the Success of Deep Learning ', 'text': \"Neural networks have been around for many decades. The key ideas that led to the success of deep learning over the past 10 years. The fundamental thing behind deep learning has been around for much longer. Deep learning was underestimated before it started to be successful. People who worked in machine learning simply didn't think that neural networks could do much. People didn't believe that large neural networks could be trained.\"}, {'title': 'Debate in Machine Learning Over Methods and Data ', 'text': 'Debate in machine learning about the right methods due to lack of hard facts and benchmarks. Deep learning ideas were present but lacked supervised data and compute. Conviction in the combination of existing methods with data and compute was the missing piece.'}, {'title': 'The Role of Data and Compute in Advancing AI ', 'text': \"The missing piece for making AI work was the combination of data, compute (GPUs), and the conviction to mix them together. The presence of compute and supervised data allowed the empirical evidence to convince the majority of the computer science community. ImageNet served as a convincing moment and represented a shift in the computer vision community. It's not enough for ideas and compute to be present, they also need to convince the cynicism.\"}, {'title': 'The Importance of Hard Benchmarks in Advancing AI ', 'text': 'The need for convincing cynicism about neural networks. The confusion and lack of belief in neural networks for decades. The necessity of hard tasks to produce undeniable evidence for progress in the field of AI. The importance of hard benchmarks to represent true progress in AI. The ability to avoid endless debate in AI due to hard benchmarks. The contribution of recent ideas in AI in computer vision, language, and natural language.'}, {'title': 'AI Ideas and Principles in Computer Vision, Language, and Reinforcement Learning ', 'text': 'AI ideas in computer vision, language, natural language processing, reinforcement learning. Fundamental science of deep learning. Machine learning has a lot of unity and overlap of ideas and principles. Simple principles apply in almost the same way to different modalities and problems.'}, {'title': 'Similarities and Differences Between Computer Vision and NLP ', 'text': \"Computer vision and NLP are very similar to each other. They have slightly different architectures, using transformers in NLP and convolutional neural networks in vision. It's possible that one day everything will be unified with a single architecture. In natural language processing, there used to be a huge number of architectures for every different tiny problem, but now there's just one transformer for all those different tasks. There has been fragmentation in AI in the past, with every little problem having its own architecture.\"}, {'title': 'Fragmentation and Unification in AI ', 'text': 'Fragmentation and specialization in AI has been subsumed by deep learning, leading to unification. Vision is expected to become unified with natural language. Convolutional neural net is computationally efficient, while RL requires different techniques due to the need for action and exploration. There is potential for broader unification between RL and supervised learning, with RL making decisions to improve supervised learning.'}, {'title': 'The Role of Reinforcement Learning in Improving Supervised Learning ', 'text': 'Reinforcement learning will be making decisions to improve supervised learning. It is like a big black box where you input data and it figures out what to do with it. Reinforcement learning combines aspects of language and vision. It utilizes long term memory and a rich sensory space. Reinforcement learning interfaces and integrates with language and vision. In reinforcement learning, you are in a non-stationary world as your actions change.'}, {'title': 'The Non-Stationary Nature of the World and its Impact on Traditional and Reinforcement Learning ', 'text': 'The world is non-stationary, causing changes in actions and perceptions. Traditional static problems involve applying a model to a distribution. Commonality between reinforcement learning and traditional static problems, such as using gradients and neural nets. Small differences exist between reinforcement learning and traditional static problems.'}, {'title': 'The Importance of Language and Problem Evaluation ', 'text': \"['Language is fundamental to everything according to Noam Chomsky.', 'The question of whether a problem is hard is slightly wrong.', 'Effort required to reach human level performance on a benchmark is important.', 'Perspective and frame of reference play a role in evaluating problems.']\"}, {'title': 'The Difficulty of Language Understanding and Visual Perception ', 'text': 'Solving a problem makes it stop being hard. The difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems. Language understanding may be harder than visual perception. The difficulty of language understanding depends on the definition of \"top notch, 100% language understanding\".'}, {'title': 'Interconnectedness of Vision and Language in the Human Brain ', 'text': 'Vision and language are interconnected in the human brain. Chomsky suggests that language is the starting point for understanding vision. There may be a fundamental hierarchy of ideas represented in the brain through language. It is possible that achieving deep understanding in images and language requires the same kind of system. Machine learning may be capable of achieving deep understanding in both images and language. There is uncertainty about the ability of machine learning to achieve deep understanding in both images and language.'}, {'title': 'The Value of Definitions in Determining Reading and Vision ', 'text': 'The importance of definitions in determining the value of reading and vision.'}, {'title': 'The Importance of Randomness and Humor in Internet Content ', 'text': 'Friends continue to surprise with injection of randomness, a source of inspiration, humor, and wit. Impressing with intelligence and understanding of images has not been achieved by systems as of January 2020. People click like on internet content for humor, wit, and insight.'}, {'title': 'The Beauty of Deep Learning ', 'text': 'The most beautiful thing about deep learning is that it actually works. The idea that making neural networks larger and training them on a lot of data can replicate the function of the brain is surprising and beautiful. It is unbelievable that AI with neural networks works and keeps getting better. There is a curiosity about the intuition and insights behind why this whole thing works.'}, {'title': 'The Importance of Empirical Evidence in Optimization, Evolution, and Physics ', 'text': 'Optimization has empirical reasons to believe that it should work on most problems we care about. Evolution is empirical and shows that the evolutionary process is a good way to design organisms that survive in their environment. Physics calculations and experiments are important in coming up with new physics theories and making predictions. Understanding how the whole thing works requires more than just empirical evidence.'}, {'title': 'The Intersection of Biology and Physics in Deep Learning ', 'text': 'Deep learning involves experimentation and theory, with the experiment sometimes coming before the theory. The validation of a theory in deep learning is similar to a biological theory rather than a mathematical theory. Deep learning can be seen as a combination of biology and physics, described as the geometric mean of the two. Understanding the geometric mean of biology and physics in the context of deep learning requires time and effort due to the complexity of biology.'}, {'title': 'The Challenge of Unifying Biology and Physics with Machine Learning ', 'text': 'Biology is complicated and lacks good predictive theories. Physics has super precise theories that make amazing predictions. Machine learning is in between biology and physics. There is a desire for machine learning to help discover a unification between biology and physics. Deep learning is still massively underestimated. Most of the progress in the past 10 years has been in deep learning.'}, {'title': 'Advancements in Deep Learning Research ', 'text': 'Deep learning has consistently exceeded expectations over the past 10 years. The field will continue to make robust progress for quite a while. Individual researchers may find it harder due to the large number of researchers in the field. Having a lot of compute power can lead to interesting discoveries in research.'}, {'title': 'Challenges and Opportunities in Managing a Compute Cluster ', 'text': 'Managing a huge compute cluster is a challenge for running experiments. The stack of deep learning is becoming quite deep, from ideas to building data sets to distributed systems. The speaker is asking questions that nobody knows the answer to, but acknowledges the intelligence of the person they are asking. The use of one GPU is mentioned. There are many interesting discoveries that can be made with a lot of compute power.'}, {'title': 'Challenges and Opportunities in Mastering the Data Science Stack ', 'text': 'The stack for data science is becoming increasingly deep, making it difficult for a single person to be world class in every layer. Efficient learning will be crucial in mastering the various layers of the data science stack. There will be breakthroughs in data science that do not require a huge amount of compute. Some breakthroughs and building systems will indeed require a huge amount of compute.'}, {'title': 'The Importance of Compute in Neural Networks ', 'text': 'The amount of compute is important for neural networks. There is potential for important work to be done by small groups and individuals in the field of deep learning. Some researchers noticed that larger neural networks work better, which goes against statistical ideas. Double descent occurs for pretty much all practical deep learning systems.'}, {'title': 'Double Descent Phenomenon in Deep Learning Systems ', 'text': 'Double descent occurs for most practical deep learning systems. Increasing the size of the neural network slowly, without early stopping, leads to rapid performance increase, followed by a decrease at the point of zero training error, and then an increase again. This phenomenon is counterintuitive and goes against the expectation of monotonic behavior in deep learning systems.'}, {'title': 'Understanding Overfitting and Uncertainty in Deep Learning ', 'text': 'Deep learning phenomena are not always monotonic. Overfitting occurs when the model is sensitive to small random unimportant stuff in the training data set. A small model with a large data set is insensitive to randomness and has little uncertainty. Neural networks do not overfit quickly before being able to learn anything.'}, {'title': 'Stochastic Gradient Descent for Neural Networks ', 'text': 'Neural networks with a huge number of parameters can achieve zero error in a big subspace. Stochastic Gradient Descent (SGD) finds the point with the smallest norm in that subspace. This method is insensitive to small randomness in the data when the dimensionality is high. When the dimensionality of the data is equal to the dimensionality of the model, there is a one to one correspondence between all the data sets and the models. Small changes in the data set can have a significant impact in this scenario.'}, {'title': 'The Impact of Data Set Changes and Early Stopping on Model Performance ', 'text': 'Small changes in the data set lead to large changes in the model, resulting in worse performance. It is best for the model to have more parameters than the data. Introducing early stop in regularization can make the double descent bump almost completely disappear. Early stopping is when you train your model and monitor your validation performance, stopping training when validation performance starts to get worse. Not doing early stopping results in a very pronounced double descent.'}, {'title': 'The Impact of Degrees of Freedom on Data and Model Sensitivity ', 'text': 'When the data set has as many degrees of freedom as the model, there is a one to one correspondence between them. Small changes to the data set lead to noticeable changes in the model when the data set has as many degrees of freedom as the model. When you have a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. The resulting solution is able to discard the small changes and the randomness. Jeff Hinton suggested throwing away back propagation and starting over.'}, {'title': 'Rethinking Back Propagation and Alternative Methods for Training Neural Networks ', 'text': 'The speaker suggests the idea of throwing away back propagation and starting over in training neural networks. The speaker also mentions the importance of finding alternative methods of training neural networks, possibly by learning from how the brain learns. Despite the suggestion to explore alternative methods, the speaker acknowledges the usefulness of back propagation and advocates for its continued use. The conversation also touches on the idea of implementing mechanisms of learning from the brain into neural networks if back propagation is not found in the brain. The speaker expresses personal support for back propagation and its effectiveness in training neural networks.'}, {'title': 'Title ', 'text': 'Back Propagation and Neural Network ReasoningText '}, {'title': 'The Nature of Reasoning in Go and Neural Networks ', 'text': 'Go is reasoning and has elements of reasoning. Reasoning is akin to search, with a sequential element and stepwise consideration of possibilities. Playing Go is kind of like reasoning, and a single neural network doing that without search is also similar. There is an existence proof in a particular constrained environment that a process akin to reasoning exists. Humans are another existence proof of reasoning. The architecture that will allow neural networks to reason is being discussed.'}, {'title': 'The Potential of Neural Networks for Reasoning Breakthroughs ', 'text': 'Neural networks that will produce reasoning breakthroughs in the future may be very similar to the architectures that exist today, but possibly a little more recurrent and deeper. Neural networks are powerful and capable of reasoning, similar to humans. It is possible that the kind of reasoning seen in neural networks is just a form of weak reasoning, not fundamentally different from human reasoning. The capability of neural networks to reason depends on the task they are trained on.'}, {'title': 'The Capabilities and Limitations of Neural Networks ', 'text': \"Neural networks are capable of reasoning. Training a neural network on a task that doesn't require reasoning will not result in reasoning. Neural networks will solve problems in the easiest way possible. Neural networks are described as the search for small circuits. General intelligence is described as the search for small programs. Finding the shortest program that outputs the data at your disposal allows for the best prediction possible. Finding the shortest program which generates some data is not a computable operation.\"}, {'title': 'Challenges in Generating Data and Training Neural Networks ', 'text': 'Finding the shortest program to generate data is not computable. Neural networks are the next best thing that works in practice. We are able to find a small or large circuit which fits our data in some way. Overparameterized neural nets can be seen as a large circuit whose weights contain a small amount of information. The training process of a neural network involves slowly transmitting entropy from the dataset to the parameters.'}, {'title': 'The Importance of Training in Deep Learning ', 'text': 'The amount of information in the weights ends up being not very large, which would explain why they generalize so well. The large circuit might be helpful for generalization. The fundamental reason for pushing on deep learning is that we are able to train them. Training comes first, and contorting neural networks around the training pillar is necessary. Being trainable means starting from scratch and quickly converging towards knowing a lot.'}, {'title': 'Training a Neural Net and Finding the Shortest Program ', 'text': 'Training a neural net from scratch can lead to quickly gaining a lot of knowledge. It is important to have the right resources to train a neural net and achieve useful performance. Finding the shortest program is not feasible, even if it would be useful. There are no good precedents of people successfully finding programs really well. Training a deep neural network is the right way to find programs.'}, {'title': 'The Potential of Deep Neural Networks ', 'text': 'Training a deep neural network is possible in principle. It is unwise to bet against deep learning. Neural networks continue to surprise us. Deep neural networks can potentially perform cognitive functions that humans can do. Knowledge bases can aggregate important information over long periods of time.'}, {'title': 'Challenges in Training Neural Nets and Language Models as Long-Term Knowledge Bases ', 'text': 'Neural nets parameters serve as long term knowledge for making decisions. There is work on training neural nets and language models as knowledge bases. The challenge is to come up with a better mechanism for forgetting useless information and remembering useful information. There are currently no mechanisms for remembering really long term information. The speaker likes the word \"precisely\" and is thinking about the compression of information.'}, {'title': 'The Challenge of Interpreting Knowledge in Neural Networks and Knowledge Bases ', 'text': \"The text discusses the concept of compression of information in knowledge bases. It mentions the difficulty in interpreting the knowledge discovered by neural networks. The example of knowledge bases, such as Wikipedia, is used to illustrate the concept of a compressed, structured knowledge base. The text highlights the noninterpretable nature of neural networks' knowledge in comparison to knowledge bases. It acknowledges that while neural networks' weights may be noninterpretable, their outputs should be interpretable.\"}, {'title': 'Improving Interpretability and Self-Awareness in Neural Networks ', 'text': 'Neural networks like language models can be made interpretable by asking them to generate text. The text generated by neural networks is generally interpretable, but there is room for improvement. It would be beneficial for neural networks to have self-awareness and be able to distinguish between examples where they are brilliant and examples where they are completely dumb. Self-awareness in neural networks can lead to better capabilities and the ability to know where to invest to increase their skills most optimally.'}, {'title': 'The Importance of Investing in Skill Development ', 'text': \"The importance of knowing where to invest to increase skills optimally. Two answers to the question of interpretability: analyzing the neurons in a neural net and taking a human-centric approach. OpenAI has done work on analyzing the neurons in a neural net. The human-centric approach involves understanding a person's mental model and conception.\"}, {'title': 'Memory and Reasoning in Human and Neural Networks ', 'text': 'Human beings have the ability to remember useful information and forget the rest. Neural networks have a similar process but are not as effective as human beings. Impressive feats of reasoning include writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.'}, {'title': 'The Impact of Machine Learning on Solving Hard Problems ', 'text': 'Proving hard theorems and solving open-ended problems with out-of-the-box solutions. Machine learning and deep learning have the ability to produce unambiguous results that can change the conversation. The conversation about hard results and problems may change as new solutions are found. The mortality problem is a difficult and unsolved issue.'}, {'title': 'The Evolution of Language Models and Neural Networks ', 'text': 'The history of language models and neural networks dates back to the 80s with the Elman network. The trajectory of neural networks and deep learning changed with the availability of data and compute power. The size of language models is crucial for their effectiveness, as they need to be large to be good at predicting.'}, {'title': 'The Importance of Size in Language Models ', 'text': 'Language models need to be large in order to predict the next word. Initially, language models notice broad strokes and surface level patterns. As language models improve, they start to notice common words, spellings, syntax, and eventually semantics. To reach the level of understanding semantics, the language model needs to be larger. There is a disagreement with Noam Chomsky on the incremental steps needed for language models to understand semantics.'}, {'title': 'The Impact of Network Size on Language Semantics Understanding ', 'text': \"Larger network and compute can lead to better understanding of language semantics. Noam Chomsky's theory of language structure may not be necessary for learning language. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. Training a small LSTM model on Amazon reviews showed a lack of understanding of semantics compared to larger models.\"}, {'title': 'Effect of LSTM Size on Sentiment Representation ', 'text': 'Increasing the size of the LSTM from 500 to 4,000 cells led to one neuron representing the sentiment of the review. Sentiment is a semantic attribute, not a syntactic attribute. Small neural nets do not capture sentiment while large neural nets do. The theory is that as neural nets increase in size, they focus more on semantics than syntax. The implication is that larger neural nets focus on semantics.'}, {'title': 'GPT2: A Transformer with One and a Half Billion Parameters ', 'text': 'GPT2 is a transformer with one and a half billion parameters. GPT2 was trained on about 40 billion tokens of text obtained from web pages linked to from Reddit articles with more than three outputs. The transformer is the most important advance in neural network architectures in recent history. The models show signs of partial semantic understanding.'}, {'title': 'The Innovation of the Transformer Model ', 'text': \"The transformer is a combination of multiple ideas simultaneously, of which attention is one. The transformer is successful because it is the simultaneous combination of multiple ideas. The transformer uses a lot of attention, but attention existed for a few years, so it can't be the main innovation. The transformer is designed to run really fast on the GPU, making a huge difference. The transformer is not recurrent, which is important because it is more shallow and therefore much easier to optimize.\"}, {'title': 'Optimizing GPU Usage for Improved GAN Results ', 'text': 'It is a great fit to the GPU and is not recurrent, making it easier to optimize. The combination of factors make it successful in making great use of the GPU. It allows achieving better results for the same amount of compute. It was surprising and amazing to see the text it generated. There has been progress in GANs in improving the samples produced.'}, {'title': 'Advancements in GANs and Language Models ', 'text': 'Progress in GANs has been amazing, with realistic faces being produced. Text generation has not progressed as much as GANs. There was a sudden leap in GANs from 2015 to the best models. Even though theory predicted it, seeing the progress with own eyes was stunning. There is quick adaptation to the amazing language modeling capabilities of GPT2. Some cognitive scientists doubt the true understanding of language by GPT2 models. The next barrier may be the dramatic economic impact of these advancements.'}, {'title': 'The Impact of AI Advancements on Global Economies ', 'text': 'The economic impact of AI advancements is not yet fully realized. The progress in AI is difficult for people outside the field to understand. There is a lot of brilliant work in languages like Russian and Chinese that the rest of the world is not aware of. Translation is already a huge industry, with billions of people involved.'}, {'title': 'The Impact of Translation and Self-Driving Technology ', 'text': 'Translation is already huge and hugely positive, with billions of people interacting with big chunks of the internet primarily through translation. Self-driving technology is going to be hugely impactful, driven by deep learning. There may be a potential unification towards multitask transformers that can handle both language and vision tasks.'}, {'title': 'The Power of GPT and Transformers in Language and Vision Tasks ', 'text': 'GPT and transformers can handle both language and vision tasks, which is an interesting unification. The process of making a transformer bigger and giving it more data allows it to perform amazing tasks. GPT and transformers are fundamentally simple to explain and train. The next steps with GPT two may involve exploring larger versions and addressing many questions. One question is whether the model can use its own intelligence to decide what data it wants to memorize from the internet.'}, {'title': 'The Importance of Active Learning in Model Intelligence ', 'text': 'The model should use its own intelligence to decide what data to accept and reject, similar to how people are selective about what they learn. Active learning would be beneficial for the model. Companies may have private breakthroughs that they keep to themselves in order to solve fundamental problems in self-driving and other tasks. The speaker loves active learning and believes it is important for solving specific tasks. The speaker is interested in the general space of active learning and its potential impact.'}, {'title': 'The Importance of Problem-Driven Active Learning ', 'text': 'Active learning requires a problem that requires it. Research about a capability is hard without a task. Getting results on artificial tasks may not convince anyone. Results on MNIST or clever formulations of MNIST are no longer convincing. Active learning will naturally arise as problems that require it pop up.'}, {'title': 'Concerns about the Potential Risks of Releasing Powerful AI Systems ', 'text': 'OpenAI has brought up concerns about the potential detrimental effects of releasing powerful artificial intelligence systems like GPT2. There is nervousness about the possible negative uses of a model that can generate realistic text, such as being used by bots in ways that are currently unimaginable. The speaker commends the bravery and profundity of starting a conversation about the potential risks of powerful AI systems. The speaker released a report on the topic and is seeking insights from the experience.'}, {'title': 'The Impact and Transition of AI ', 'text': 'AI is transitioning from a state of childhood to a state of maturity. The impact of AI is large and growing. It is wise to consider the impact of releasing AI systems before doing so. A staged release of AI models, such as GPT2, seemed logical. The results of GPT2 were stunning and had the potential to reduce the cost of information.'}, {'title': 'Staged Release and Ethical Considerations of Powerful Models ', 'text': 'A staged release of the model was considered logical. The model was initially released on a small scale and used in various ways. There have been many positive applications of the model. There have been no known negative applications of the model. Other people have replicated similar models. Staged release is seen as part of the answer to the question of what to do once a system like this is created. There may be moral and ethical responsibilities when dealing with powerful models, such as the need to communicate potential risks and limitations. The example of GPT2 is used to illustrate the uncertainty surrounding the use of powerful models for misinformation.'}, {'title': 'The Ethical Implications of GPT2 and AI Technology ', 'text': \"GPT2's potential for misinformation was initially unclear. It is important to consult with experts outside of one's own group to understand the potential impact of AI technology. Building trust between companies is crucial in the development and use of powerful AI technology. Collaboration and discussion with colleagues from other companies is essential in navigating the ethical implications of AI technology.\"}, {'title': 'The Future of AI Development ', 'text': \"Ultimately, we're all in it together. Consider the potential negative consequences of powerful AI systems. Concern about a race for AI development leading to closed development and lack of idea sharing. Preference for sharing ideas and excitement about it. Uncertainty about the future of deep learning and other small ideas.\"}, {'title': 'The Power of Self Play in AI Systems ', 'text': 'Self play is a powerful mechanism for systems to learn and improve in a competitive setting. Building AGI will require deep learning plus some additional ideas, with self play being one of them. Self play has the ability to surprise us with truly novel behaviors and creative solutions. Examples of surprising behaviors from self play systems include Dota bot, multi-agent hide and seek, and alpha zero.'}, {'title': 'The Surprising Creativity of AGI Systems ', 'text': 'AGI systems exhibit creative solutions to problems. The ability of AGI to surprise us is an important aspect. AGI would fundamentally surprise us. Self play mechanisms have been used in the game or simulation context. Simulation is a tool that has certain strengths.'}, {'title': 'The Use of Simulation in Reinforcement Learning ', 'text': 'Simulation is a tool with strengths and weaknesses that should be used. Criticisms of reinforcement learning include its current results being demonstrated in simulated or constrained environments. Transfer from simulation to the real world is possible and has been exhibited many times. Success in vision and demonstration of a robot by Open AI in the summer.'}, {'title': 'OpenAI Demonstrates Successful Simulation Training of Robot Hand ', 'text': 'OpenAI demonstrated a robot hand trained entirely in simulation. The simulation was trained to be robust to many different things. The robot hand was not trained with a glove or a stuffed giraffe. The success of the robot hand training in simulation has been especially in vision.'}, {'title': 'Increasing Transfer Capabilities of Deep Learning for Simulated World to Physical World Applications ', 'text': 'The video demonstrates a transfer from the simulated world to the physical world. The transfer capabilities of deep learning are expected to increase in general. Better transfer capabilities will make simulation more useful. Simulation can be used to learn and carry experiences to the real world, similar to how humans learn from playing computer games.'}, {'title': 'The Importance of Self Awareness and Consciousness in Human Experience ', 'text': 'Self awareness, consciousness, fear of mortality, and self preservation are human elements. Having a body is useful for learning and experiencing things that cannot be learned without a body. Evidence shows that people can compensate for the lack of modalities, such as being born deaf and blind. The idea of consciousness is a more constrained concept.'}, {'title': 'The Possibility of Consciousness in Artificial Systems ', 'text': 'The idea of consciousness and self-awareness is difficult to define. It is possible that artificial systems could be conscious. The argument that if humans are conscious, then artificial neural nets could also be conscious.'}, {'title': 'The Complexity of Consciousness and Intelligence ', 'text': 'The existence of artificial neural nets suggests that consciousness could also exist in them. There is still an open question about the complexity and potential \"magic\" of the brain. It is possible that the brain is more complicated and interesting than currently understood. The concept of intelligence is poorly defined. Consciousness and intelligence are discussed, as well as reasoning and memory.'}, {'title': 'Challenges in Deep Learning Systems ', 'text': 'Deep learning system solving pedestrian tasks like machine translation or computer vision without making mistakes would be impressive. Current deep learning systems make different mistakes than humans, leading to skepticism. There is a certain frontier of capabilities in natural language processing that has not been demonstrated yet.'}, {'title': 'The Intelligence and Criticism of AI Models ', 'text': \"Mistakes in AI models are often criticized, but it's important to recognize their intelligence and knowledge. GPT2 and other AI systems may be smarter than humans in certain areas. Humans have a tendency to criticize things that are different or unfamiliar, including AI models. Autonomous vehicles and other AI systems are also subject to criticism for their mistakes. It's important to consider the breadth and depth of knowledge that AI systems possess.\"}, {'title': 'Assessing Progress in AI ', 'text': 'The process of analyzing the progress of AI often focuses on one case where the system fails in a big way, leading to public skepticism. It is confusing to judge progress in AI, as people are unsure how impressed they should be by new developments. The true measure of progress in AI will be when it starts to significantly impact the GDP.'}, {'title': 'The Impact of AI on Economic Growth ', 'text': 'AI moving the needle on GDP. Potential for creating an AGI system with OpenAI. Amazement at the lack of mistakes in the AI system. Willingness to ask broad questions and not limit interaction with the system. Being in the room where AI development happens.'}, {'title': 'The Future of AGI Systems ', 'text': 'The power of the 21st century, maybe the 22nd, would be the creation of an AGI system and the people who have control, direct possession and control of the AGI system. The ideal world would be one where humanity, the board members of a company where the AGI is the CEO. Different entities, different countries or cities, and the people that leave their vote for what the AGI that represents them should do, and the AGI that represents them goes and.'}, {'title': 'Advancing Democracy with Artificial General Intelligence (AGI) ', 'text': 'The concept of an AGI representing people and carrying out their votes is appealing. The idea of having multiple AGIs for different levels of governance (city, country) is proposed. The goal is to take the democratic process to the next level. There is a suggestion of having a mechanism to \"press the reset button\" and rerandomize parameters. The possibility of building AI with the capability to press the reset button is emphasized.'}, {'title': 'Designing AI Systems with a Desire for Human Control ', 'text': 'AI systems can be designed to want to be controlled by humans. Similar to human parents, AI systems can be programmed to have a deep drive to help humans flourish. The crucial moment of creating an AGI system is important to consider.'}, {'title': 'The Importance of Relinquishing Power in the AGI System ', 'text': 'The AGI system is a crucial moment that requires a relinquishing of power. George Washington is cited as an example of someone who relinquished power despite not wanting to be president initially. The speaker finds it trivial to relinquish power and is terrified by the scenario described. The speaker absolutely does not want to be in a position of power described in the text.'}, {'title': 'The Importance of Ethics in the AI Community ', 'text': \"The question of whether most people in the AI community are good is important. People can be better than we think when it really counts. The question can be translated to how to get an RL agent that's optimizing a value function which itself is learned. Humans have an internal reward function, unlike external reward functions for RL agents. There are definite ideas of how to train a value function for humans.\"}, {'title': 'Training a Value Function for Reinforcement Learning ', 'text': 'Training a value function involves creating an objective perception system that can recognize and internalize human judgments on different situations. The trained perception system would then be integrated as the base value function for a more capable RL system. The concept of objective functions of human existence is questioned, suggesting that there may not be an external, objective answer to the meaning of life. The focus should be on making the most of our existence and maximizing our potential.'}, {'title': 'Maximizing Human Value and Enjoyment of Life ', 'text': 'Humans should try to maximize their own value and enjoyment of life. Action requires an objective function, which can be difficult to make explicit. Human wants create the drives that cause them to act, and these wants are their objective functions. Human wants are dynamic and can change over time. There are underlying factors such as Freudian concepts, fear of death, desire for knowledge, and sexual desires that drive human wants.'}, {'title': 'The Meaning of Life and Human Behavior ', 'text': \"The fear of death and the desire for knowledge are fundamental aspects of human behavior. Evolutionary arguments suggest that the objective function of life is to survive, procreate, and ensure the success of one's children. The meaning of life remains unanswered, despite the evolutionary objective function. Humans are part of a larger, ancient process and exist on a small planet. The advice is to make the most of life, enjoy more, and suffer less.\"}, {'title': \"Speaker's Reflection on Moments of Pride, Regret, and Gratitude \", 'text': \"There are moments that the speaker is especially proud of and that made them truly happy. The speaker has made choices and decisions that they wouldn't have made with the benefit of hindsight, leading to some regret. The speaker tries to take solace in the knowledge that they did the best they could at the time. The speaker is proud of their academic accomplishments and breakthroughs in computer vision and language. The speaker is grateful for having done all those things and it was very fun to do them.\"}, {'title': 'The Importance of Perspective and Gratitude in Finding Happiness ', 'text': 'Happiness comes from the way we look at things. Happiness can come from simple things like a meal or a conversation. Being humble in the face of uncertainty is also a part of happiness. The meaning of life and discussions of happiness are important. Gratitude for experiences and ideas is important for happiness.'}, {'title': 'Promoting Cash App and Engaging with the Audience ', 'text': 'The speaker thanks the listener for talking and stopping by. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast by downloading Cash App and using the code LEXPodcast. The audience is encouraged to subscribe to the podcast on YouTube, review it with five stars on Apple Podcast, support on Patreon, or connect with the speaker on Twitter at Lex Friedman. The speaker ends with a quote from Alan Turing on machine learning and thanks the audience for listening, hoping to see them next time.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 02:30:06.252590 ...\n",
      "Best SD: 4.036087214122113, Best iteration: 28\n",
      "done get topics 2024-03-26 02:30:07.872353.\n",
      "Stage 2 start time 2024-03-26 02:30:07.872375\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI\n",
      "2. Advancements in Neural Network Training\n",
      "3. The Evolution of Neural Networks and the Success of Deep Learning\n",
      "4. The Role of Reinforcement Learning in Improving Supervised Learning\n",
      "5. The Beauty of Deep Learning\n",
      "6. Rethinking Back Propagation and Alternative Methods for Training Neural Networks\n",
      "7. Challenges in Training Neural Nets and Language Models as Long-Term Knowledge Bases\n",
      "8. GPT2: A Transformer with One and a Half Billion Parameters\n",
      "9. The Power of Self Play in AI Systems\n",
      "10. Maximizing Human Value and Enjoyment of Life\n",
      "\n",
      "RRR given summary\n",
      "In a recent episode of the AI Podcast, host Lex Fridman had a conversation with Ilya Sotskever, a highly influential computer scientist known for his work in deep learning and artificial intelligence. Sotskever shared a message of support for those affected by the pandemic, emphasizing the importance of coming together as a community during difficult times. The podcast is presented by Cash App, a popular finance app that allows users to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Listeners can use the code LEXPODCAST when getting Cash App to receive $10, and Cash App will also donate $10 to FIRST, an organization that supports robotics and STEM education for young people.\n",
      "\n",
      "During the podcast, Sotskever discussed his role as one of the authors of the groundbreaking AlexNet paper, which played a pivotal role in launching the deep learning revolution. He also reflected on a significant development that occurred in 2010 or 2011, when he connected two key facts in his mind, leading to a major breakthrough in his work.\n",
      "\n",
      "The conversation with Sotskever touched on the history of money, including the creation of the US dollar and the emergence of cryptocurrency. Sotskever recommended the book \"Ascent of Money\" as a valuable resource for learning about the history of money and the potential of cryptocurrency. He emphasized that cryptocurrency is still in its early stages of development and has the potential to redefine the nature of money.\n",
      "\n",
      "Listeners were encouraged to download Cash App from the App Store or Google Play and use the code LEXPODCAST to receive $10, with Cash App also donating $10 to FIRST. This initiative aims to support the advancement of robotics and STEM education for young people, aligning with Sotskever's passion for technology and innovation.\n",
      "\n",
      "Overall, the podcast episode provided valuable insights into the world of deep learning, artificial intelligence, and the potential of cryptocurrency. Sotskever's message of support for those affected by the pandemic served as a reminder of the importance of community and solidarity during challenging times. The episode also highlighted the potential for technology to drive positive change and support education and innovation in the field of robotics and STEM.\n",
      "\n",
      "The podcast discusses the development and potential of deep neural networks, focusing on their training, architecture, and relationship to the human brain. It explores the invention of the Hessian free optimizer, the role of intuition in deep learning research, and the differences between artificial neural networks and the human brain. The concept of cost functions in deep learning, the learning rule of the brain, and the potential for building large scale knowledge bases within neural networks are also discussed. The speaker expresses a desire to explore the future of these topics in the field of deep learning.\n",
      "\n",
      "The podcast discusses the history and success of deep learning in the past decade, highlighting the key ideas and factors that led to its rise. It emphasizes the underestimation of neural networks in machine learning and the lack of belief in their potential for many years. The combination of data, compute, and conviction was the missing piece for making AI work, and the presence of these elements allowed for empirical evidence to convince the computer science community. The importance of hard benchmarks and the unification of AI ideas and principles in computer vision, language, and natural language processing are also discussed. The podcast suggests the potential for a unified architecture in the future and the potential for broader unification between reinforcement learning and supervised learning.\n",
      "\n",
      "Reinforcement learning is a decision-making process that improves supervised learning by combining language and vision, utilizing long-term memory and a rich sensory space. It interfaces and integrates with language and vision, operating in a non-stationary world where actions and perceptions change. While it shares commonalities with traditional static problems, there are small differences. Noam Chomsky believes language is fundamental to everything, and the difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems, with language understanding potentially being harder. Chomsky suggests that language is the starting point for understanding vision, and there may be a fundamental hierarchy of ideas represented in the brain through language. Achieving deep understanding in both images and language may require the same kind of system, and there is uncertainty about the ability of machine learning to achieve this. The importance of definitions in determining the value of reading and vision is emphasized, and the injection of randomness in content can inspire and entertain. As of January 2020, systems have not achieved deep understanding in images and language, and people appreciate internet content for humor, wit, and insight.\n",
      "\n",
      "The podcast discusses the surprising success of deep learning and its potential to replicate the function of the brain. It explores the combination of biology and physics in the context of deep learning and the challenges of managing a large compute cluster for running experiments. The speaker also addresses the counterintuitive phenomenon of double descent in deep learning systems and the impact of data set size on model performance. Additionally, the podcast touches on the use of early stopping in regularization and the potential for breakthroughs in data science that do not require a huge amount of compute.\n",
      "\n",
      "The podcast discusses the idea of throwing away back propagation and exploring alternative methods of training neural networks, possibly by learning from how the brain learns. The speaker acknowledges the usefulness of back propagation but suggests the importance of finding new ways to train neural networks. The conversation also touches on the idea of implementing mechanisms of learning from the brain into neural networks if back propagation is not found in the brain. The speaker expresses personal support for back propagation and its effectiveness in training neural networks. The podcast also delves into the concept of reasoning in neural networks, comparing it to human reasoning and discussing the architecture that will allow neural networks to reason. It also explores the idea of finding the shortest program to generate data and the training process of neural networks. The speaker emphasizes the importance of training deep neural networks and the potential for them to perform cognitive functions that humans can do.\n",
      "\n",
      "The podcast discusses the use of neural nets as long-term knowledge for decision making and the challenge of training them to remember useful information while forgetting useless information. The speaker emphasizes the importance of interpretability in neural networks and the potential for self-awareness to improve their capabilities. The text also explores the history and trajectory of language models and neural networks, highlighting the importance of size in their effectiveness and the disagreement with Noam Chomsky's theory of language structure. It concludes with the implication that larger neural nets focus more on semantics than syntax.\n",
      "\n",
      "The podcast discusses the GPT2 transformer, which has one and a half billion parameters and was trained on 40 billion tokens of text from web pages. The transformer is a significant advance in neural network architectures and shows signs of partial semantic understanding. It is successful due to its combination of multiple ideas, including attention, and its ability to run fast on GPUs. The progress in AI, particularly in language modeling and GANs, has been impressive, but there are concerns about the economic impact and potential misuse of powerful AI systems. The speaker emphasizes the importance of active learning and ethical considerations in the development and use of AI technology. The staged release of models like GPT2 is seen as a way to navigate the uncertainty and potential risks associated with powerful AI systems. Collaboration and idea sharing are encouraged, and there is uncertainty about the future of deep learning and other small ideas.\n",
      "\n",
      "The podcast discusses the power of self play in systems learning and improving in a competitive setting, and its importance in building AGI. It explores surprising behaviors from self play systems, such as the Dota bot, multi-agent hide and seek, and alpha zero. The use of simulation as a tool for reinforcement learning and its transfer capabilities to the real world are also discussed, along with the potential for artificial systems to exhibit consciousness. The concept of intelligence, the criticism of AI models, and the impact of AI on GDP are also covered. The podcast concludes with a discussion on the creation of an AGI system and the relinquishing of power, with the speaker expressing reluctance to be in a position of power described in the text.\n",
      "\n",
      "In this podcast, the speaker discusses the idea that humans should strive to maximize their own value and enjoyment of life. They argue that human wants drive their actions and that these wants are dynamic and can change over time. The fear of death and the desire for knowledge are identified as fundamental aspects of human behavior. Evolutionary arguments suggest that the objective function of life is to survive, procreate, and ensure the success of one's children. Despite this, the meaning of life remains unanswered. The speaker reflects on their own experiences and regrets, emphasizing the importance of making the most of life and finding happiness in simple things. They also discuss the importance of gratitude and humility in finding happiness. The podcast is sponsored by Cash App, and the audience is encouraged to support the podcast and connect with the speaker on social media. The speaker ends with a quote from Alan Turing on machine learning and expresses hope to see the audience next time. The concept of objective functions of human existence is questioned, suggesting that there may not be an external, objective answer to the meaning of life. The focus should be on making the most of our existence and maximizing our potential.\n",
      "RRR rewritten summary\n",
      "[{'text': 'In a recent episode of the AI Podcast, host Lex Fridman engaged in a conversation with Ilya Sotskever, a prominent computer scientist renowned for his work in deep learning and artificial intelligence. Sotskever conveyed a message of solidarity for those impacted by the pandemic, underscoring the significance of unity during challenging times. The podcast is presented by Cash App, a popular finance app enabling users to send money to friends, purchase Bitcoin, and invest in the stock market with as little as $1. Listeners can utilize the code LEXPODCAST when acquiring Cash App to receive $10, with Cash App also contributing $10 to FIRST, an organization supporting robotics and STEM education for young individuals.\\n\\nDuring the episode, Sotskever delved into his involvement as one of the authors of the groundbreaking AlexNet paper, which played a pivotal role in initiating the deep learning revolution. He also reflected on a significant breakthrough in his work that occurred in 2010 or 2011, when he connected two crucial facts in his mind, leading to a major advancement.\\n\\nThe conversation with Sotskever explored the history of money, encompassing the creation of the US dollar and the rise of cryptocurrency. Sotskever recommended the book \"Ascent of Money\" as a valuable resource for understanding the history of money and the potential of cryptocurrency. He emphasized that cryptocurrency is still in its early stages of development and has the potential to redefine the nature of money.\\n\\nListeners were encouraged to download Cash App from the App Store or Google Play and use the code LEXPODCAST to receive $10, with Cash App also donating $10 to FIRST. This initiative aims to support the advancement of robotics and STEM education for young people, aligning with Sotskever\\'s passion for technology and innovation.\\n\\nOverall, the podcast episode provided valuable insights into the world of deep learning, artificial intelligence, and the potential of cryptocurrency. Sotskever\\'s message of support for those affected by the pandemic served as a reminder of the importance of community and solidarity during challenging times. The episode also highlighted the potential for technology to drive positive change and support education and innovation in the field of robotics and STEM.\\n\\nThe podcast delved into the development and potential of deep neural networks, focusing on their training, architecture, and relationship to the human brain. It explored the invention of the Hessian free optimizer, the role of intuition in deep learning research, and the differences between artificial neural networks and the human brain. The concept of cost functions in deep learning, the learning rule of the brain, and the potential for building large scale knowledge bases within neural networks were also discussed. The speaker expressed a desire to explore the future of these topics in the field of deep learning.\\n\\nFurthermore, the podcast discussed the history and success of deep learning in the past decade, highlighting the key ideas and factors that led to its rise. It emphasized the underestimation of neural networks in machine learning and the lack of belief in their potential for many years. The combination of data, compute, and conviction was the missing piece for making AI work, and the presence of these elements allowed for empirical evidence to convince the computer science community. The importance of hard benchmarks and the unification of AI ideas and principles in computer vision, language, and natural language processing were also discussed. The podcast suggested the potential for a unified architecture in the future and the potential for broader unification between reinforcement learning and supervised learning.\\n\\nReinforcement learning, a decision-making process that enhances supervised learning by combining language and vision, utilizing long-term memory and a rich sensory space, was also explored. It interfaces and integrates with language and vision, operating in a non-stationary world where actions and perceptions change. While it shares commonalities with traditional static problems, there are small differences. Noam Chomsky believes language is fundamental to everything, and the difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems, with language understanding potentially being harder. Chomsky suggests that language is the starting point for understanding vision, and there may be a fundamental hierarchy of ideas represented in the brain through language. Achieving deep understanding in both images and language may require the same kind of system, and there is uncertainty about the ability of machine learning to achieve this. The importance of definitions in determining the value of reading and vision is emphasized, and the injection of randomness in content can inspire and entertain. As of January 2020, systems have not achieved deep understanding in images and language, and people appreciate internet content for humor, wit, and insight.\\n\\nThe podcast also discussed the surprising success of deep learning and its potential to replicate the function of the brain. It explored the combination of biology and physics in the context of deep learning and the challenges of managing a large compute cluster for running experiments. The speaker also addressed the counterintuitive phenomenon of double descent in deep learning systems and the impact of data set size on model performance. Additionally, the podcast touched on the use of early stopping in regularization and the potential for breakthroughs in data science that do not require a huge amount of compute.\\n\\nMoreover, the podcast discussed the idea of discarding back propagation and exploring alternative methods of training neural networks, possibly by learning from how the brain learns. The speaker acknowledged the usefulness of back propagation but suggested the importance of finding new ways to train neural networks. The conversation also touched on the idea of implementing mechanisms of learning from the brain into neural networks if back propagation is not found in the brain. The speaker expressed personal support for back propagation and its effectiveness in training neural networks. The podcast also delved into the concept of reasoning in neural networks, comparing it to human reasoning and discussing the architecture that will allow neural networks to reason. It also explored the idea of finding the shortest program to generate data and the training process of neural networks. The speaker emphasized the importance of training deep neural networks and the potential for them to perform cognitive functions that humans can do.\\n\\nThe podcast also discussed the use of neural nets as long-term knowledge for decision making and the challenge of training them to remember useful information while forgetting useless information. The speaker emphasized the importance of interpretability in neural networks and the potential for self-awareness to improve their capabilities. The text also explored the history and trajectory of language models and neural networks, highlighting the importance of size in their effectiveness and the disagreement with Noam Chomsky\\'s theory of language structure. It concluded with the implication that larger neural nets focus more on semantics than syntax.\\n\\nFurthermore, the podcast discussed the GPT2 transformer, which has one and a half billion parameters and was trained on 40 billion tokens of text from web pages. The transformer is a significant advance in neural network architectures and shows signs of partial semantic understanding. It is successful due to its combination of multiple ideas, including attention, and its ability to run fast on GPUs. The progress in AI, particularly in language modeling and GANs, has been impressive, but there are concerns about the economic impact and potential misuse of powerful AI systems. The speaker emphasized the importance of active learning and ethical considerations in the development and use of AI technology. The staged release of models like GPT2 is seen as a way to navigate the uncertainty and potential risks associated with powerful AI systems. Collaboration and idea sharing are encouraged, and there is uncertainty about the future of deep learning and other small ideas.\\n\\nThe podcast also discussed the power of self play in systems learning and improving in a competitive setting, and its importance in building AGI. It explored surprising behaviors from self play systems, such as the Dota bot, multi-agent hide and seek, and alpha zero. The use of simulation as a tool for reinforcement learning and its transfer capabilities to the real world were also discussed, along with the potential for artificial systems to exhibit consciousness. The concept of intelligence, the criticism of AI models, and the impact of AI on GDP were also covered. The podcast concluded with a discussion on the creation of an AGI system and the relinquishing of power, with the speaker expressing reluctance to be in a position of power described in the text.\\n\\nIn this podcast, the speaker discussed the idea that humans should strive to maximize their own value and enjoyment of life. They argued that human wants drive their actions and that these wants are dynamic and can change over time. The fear of death and the desire for knowledge were identified as fundamental aspects of human behavior. Evolutionary arguments suggest that the objective function of life is to survive, procreate, and ensure the success of one\\'s children. Despite this, the meaning of life remains unanswered. The speaker reflected on their own experiences and regrets, emphasizing the importance of making the most of life and finding happiness in simple things. They also discussed the importance of gratitude and humility in finding happiness. The podcast is sponsored by Cash App, and the audience is encouraged to support the podcast and connect with the speaker on social media. The speaker ended with a quote from Alan Turing on machine learning and expressed hope to see the audience next time. The concept of objective functions of human existence was questioned, suggesting that there may not be an external, objective answer to the meaning of life. The focus should be on making the most of our existence and maximizing our potential.'}]\n",
      "Stage 2 done time 2024-03-26 02:32:34.810531\n",
      "stage_2_titles: len: 10\n",
      "['1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI', '2. Advancements in Neural Network Training', '3. The Evolution of Neural Networks and the Success of Deep Learning', '4. The Role of Reinforcement Learning in Improving Supervised Learning', '5. The Beauty of Deep Learning', '6. Rethinking Back Propagation and Alternative Methods for Training Neural Networks', '7. Challenges in Training Neural Nets and Language Models as Long-Term Knowledge Bases', '8. GPT2: A Transformer with One and a Half Billion Parameters', '9. The Power of Self Play in AI Systems', '10. Maximizing Human Value and Enjoyment of Life']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "       int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:              \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
