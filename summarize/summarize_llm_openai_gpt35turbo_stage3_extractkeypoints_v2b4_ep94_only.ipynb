{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b4_ep94only\" # no rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "# REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f36b7ed9a50>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "#   final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-28 19:48:41.087426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 19:55:39.583881\n",
      "chunks_text len: 101\n",
      "extract_keypoints start time: 2024-03-28 19:55:39.584038\n",
      "extract_keypoints done time 2024-03-28 19:59:27.648683\n",
      "Start time: 2024-03-28 19:59:27.648883\n",
      "Stage 1 done time 2024-03-28 20:03:12.811834\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Ilya Sotskever: Cofounder and Chief Scientist of OpenAI ', 'text': \"Ilya Sotskever is the cofounder and chief scientist of OpenAI, and one of the most cited computer scientists in history with over 165,000 citations. He is considered one of the most brilliant and insightful minds in the field of deep learning. The conversation was recorded before the outbreak of the pandemic. The speaker sends love and support to those affected by the medical, psychological, and financial burden of the crisis. The Artificial Intelligence Podcast is mentioned and ways to support it are provided. The speaker's Twitter handle is provided for connecting.\"}, {'title': 'The Importance of Cash App and Cryptocurrency ', 'text': 'Twitter handle is @lexfriedman, spelled F R I D M A N. The show is presented by Cash App, the number one finance app in the App Store. Use code LEXPODCAST when getting Cash App. Cash App allows you to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Cryptocurrency is still in its early days of development, despite being around for over 10 years. The history of money, including the creation of the US dollar and the release of Bitcoin, is fascinating.'}, {'title': 'The Early Development of Cryptocurrency and Cash App Promotion ', 'text': 'Cryptocurrency is still in its early days of development and aims to redefine the nature of money. Cash App is offering a promotion where using the code LEXPODCAST gives $10 and donates $10 to FIRST organization. Ilya Satsgever was one of the authors of the AlexNet paper, which marked the big catalytic moment that launched the deep learning revolution.'}, {'title': 'Training Deep Neural Networks with Backpropagation ', 'text': 'Deep neural networks can be trained end to end with backpropagation. James Martens invented the Hessian free optimizer in 2010, which allowed training a 10 layer neural network from scratch. The ability to train a big neural network means it can represent very complicated functions. A neural network with 10 layers can simulate the human brain running for some number of milliseconds. Neuron firings are slow, so in 100 milliseconds, the neurons only fire 10 times.'}, {'title': 'Neural Network Training and Overfitting ', 'text': 'Neuron firings are slow and only fire 10 times in 100 milliseconds. The idea of training a very big neural network on lots of supervised data to find the best neural network. The theory that having more data than parameters prevents overfitting. The fact that neural networks were heavily overparameterized was not discouraging. The consideration of the number of parameters and the evidence that it was okayish.'}, {'title': 'Challenges and Innovations in Training Big Neural Nets ', 'text': \"The theory was that with a big data set and a big neural net, it was going to work. Overparameterization was not seen as a problem. The main doubt was whether there would be enough compute to train a big enough neural net with backpropagation. Alex Kerchevsky wrote fast CUDA kernels for training convolutional neural nets, leading to a decision to proceed with the project. The demonstration of the project's potential was a key factor in moving forward.\"}, {'title': 'The Role of Human Intuition and Inspiration from the Brain in Understanding Neural Networks ', 'text': 'The intuition about neural networks can come from empirical results and also from pen and paper or marker and whiteboard thinking. The human brain plays a role in the intuition about neural networks for deep learning researchers. The idea of a neural network is directly inspired by the brain, as seen from the work of Rosenblatt in the 60s and the concepts of neurons in the brain.'}, {'title': 'The Evolution of Neural Networks and Their Relation to the Human Brain ', 'text': \"The idea of using ideas from the computer and automata to design a computational object similar to the brain. The invention of the neuron inspired by the brain. The development of the convolutional neural network and its success in image recognition. The assumption that an artificial neuron is not that different from the brain if it's cleaned hard enough. The success of deep learning in the current time. The interesting differences between the human brain and artificial neural networks.\"}, {'title': 'The Comparison of Human Brain and Artificial Neural Networks ', 'text': 'The difference between the human brain and artificial neural networks is interesting for the next decade or two. Artificial neural networks have important advantages over the human brain in certain ways. It is important to consider the advantages versus disadvantages when comparing the human brain and artificial neural networks. The brain uses spikes, which may or may not be important.'}, {'title': 'Architectural Differences and Uncertainties in Neural Networks ', 'text': 'One big architectural difference between artificial neural networks. Interest in spiking neural networks and the need to simulate non-spiking neural networks in spikes. Questions around back propagation and deep learning. The uncertainty of why giant neural networks and learning rules work at all.'}, {'title': 'The Development of Neural Networks ', 'text': \"Neural networks were inspired by the brain's neural network. The challenge was to figure out how to train neural networks. The big idea in training neural networks is the cost function. The cost function measures the performance of the system. It may seem trivial now, but the concept of a single cost function was a significant idea. There may be other things that do not necessarily have a single cost function.\"}, {'title': 'The Cost Function of GANs ', 'text': 'GANs do not have clear cost functions. GANs operate based on a game and the behavior of the system is reasoned in terms of the equilibrium of the game. The cost function of a GAN is emergent from the system.'}, {'title': 'Understanding the Emergence and Evolution of Cost Functions in Deep Learning ', 'text': \"GAN's cost function is emergent from the comparison and may not be meaningful to talk about in the same way as traditional cost functions. The analogy of a cost function in biological evolution or the economy may not be the most useful for understanding GAN's cost function. Questioning whether cost functions in deep learning are holding us back and if they are a profound idea that we will move past. Self play in reinforcement learning systems may start to provide insight into the concept of cost functions in deep learning.\"}, {'title': 'The Importance of Self Play, Exploration, and Cost Functions in Reinforcement Learning Systems ', 'text': 'Self play and exploration are important in reinforcement learning systems. Cost functions are considered great and serve well in various applications. There is potential for new ways of looking at things that may involve cost functions in a less central way. Spiking and the learning rule of the brain are areas of interest for potential usefulness.'}, {'title': 'Understanding Spike Time Independent Plasticity in the Brain ', 'text': 'Spike time independent plasticity (STDP) is a learning rule of the brain that uses spike timing to update synapses. If a synapse fires into the neuron before the neuron fires, it strengthens the synapse, and if the synapse fires into the neuron shortly after the neuron fired, it weakens the synapse. The temporal dynamics of the timing of signals is a fundamental property of the brain that is not fully captured in the current understanding of spike time independent plasticity. There is a need for further study and simulation of spike time independent plasticity by neuroscientists.'}, {'title': \"The Role of Recurrent Neural Networks in Mimicking the Brain's Timing Mechanism \", 'text': \"The brain's fundamental property is the timing of signals. Recurrent neural networks are a simplified version of the brain's timing mechanism. The brain is a continuous version of recurrent neural networks, where all possible timings are possible. Recurrent neural networks are amazing and can do anything we'd want a system to do. Recurrent neural networks have been superseded by transformers, but may make a comeback in the future.\"}, {'title': 'Advancements in Natural Language Processing and Language Modeling with Non-Recurrent Transformers ', 'text': \"Breakthroughs in natural language processing and language modeling have been with transformers that don't emphasize recurrence. Recurrent neural networks are still possible for processing sequences. Neural networks maintain a high dimensional hidden state and update it when an observation arrives. Expert systems and symbolic AI also involved maintaining a hidden state.\"}, {'title': 'Building Large Scale Knowledge Bases in Symbolic AI and Neural Networks ', 'text': 'Symbolic AI involves growing and maintaining a knowledge base. The hidden state in symbolic AI is the knowledge base, which is grown through sequential processing. In neural networks, the knowledge could be stored in the connections and short term processing is done in the hidden state. There is potential for building large scale knowledge bases within neural networks. The speaker wants to explore the future of building large scale knowledge bases within neural networks.'}, {'title': 'The Rise of Deep Learning ', 'text': \"Neural networks have been around for many decades. The key idea about deep learning was underestimated before it became successful. People who worked in machine learning didn't think that neural networks could do much. People didn't believe that large neural networks could be trained.\"}, {'title': 'Debate in Machine Learning about Methods, Benchmarks, and Conviction ', 'text': 'Debate in machine learning about the right methods and benchmarks. Deep learning ideas were present but lacked supervised data and compute. Conviction in the combination of existing methods, data, and compute was the missing piece.'}, {'title': 'The Role of Data and Compute in Advancing AI ', 'text': \"The missing piece for making AI work was the combination of data, compute (GPUs), and the conviction to mix them together. The presence of compute and supervised data allowed the empirical evidence to convince the majority of the computer science community. ImageNet served as a convincing moment and represented a shift in the computer vision community. It's not enough for ideas and compute to be present, they also need to convince the cynicism.\"}, {'title': 'Progress in the Field of AI and the Importance of Hard Benchmarks ', 'text': 'Neural networks faced skepticism and disbelief for decades. Hard tasks and benchmarks are necessary to produce undeniable evidence and make progress in the field of AI. The field of AI is making progress today due to the presence of hard benchmarks representing true progress. The contribution of recent ideas in AI includes computer vision, language, and natural language processing.'}, {'title': 'AI Ideas and Principles in Computer Vision, Language, and Reinforcement Learning ', 'text': 'AI ideas in computer vision, language, natural language processing, reinforcement learning. Fundamental science of deep learning. Machine learning has a lot of unity and overlap of ideas and principles. Simple principles apply in almost the same way to different modalities and problems.'}, {'title': 'The Evolution of Architectures in Computer Vision and NLP ', 'text': \"Computer vision and NLP are very similar to each other. Today they differ in that they have slightly different architectures. We use transformers in NLP and we use convolutional neural networks in vision. It's also possible that one day this will change and everything will be unified with a single architecture. Today, there's just one transformer for all those different tasks. Every different tiny problem had its own architecture in natural language processing a few years ago. There were a huge number of architectures for every different tiny problem in natural language processing. Every little problem in AI had its own architecture in the past.\"}, {'title': 'Unification and Specialization in AI ', 'text': 'Fragmentation and specialization in AI has been subsumed by deep learning, leading to unification. Vision and natural language may become unified in the future. Convolutional neural net is computationally efficient, while RL requires different techniques due to the need for action and exploration. There is potential for broader unification between RL and supervised learning, with RL making decisions to improve supervised learning.'}, {'title': 'Reinforcement Learning in Supervised Learning Improvement ', 'text': 'RL making decisions to improve supervised learning. RL as a big black box that figures out what to do with input. Reinforcement learning combines aspects of language and vision. Utilizing long term memory and rich sensory space. RL interfaces and integrates with language and vision. Learning to act in a non-stationary world.'}, {'title': 'The Non-Stationary Nature of the World and its Impact on Traditional and Reinforcement Learning ', 'text': 'The world is non-stationary, leading to changing actions and perceptions. Traditional static problems involve applying a model to a fixed distribution. Commonality between reinforcement learning and traditional static problems, such as using gradients and neural nets. Small differences exist between the two approaches.'}, {'title': 'The Importance of Language and Problem Evaluation ', 'text': 'Language is fundamental to everything according to Noam Chomsky. The question of whether a problem is hard is slightly wrong. Effort required to reach human level performance on a benchmark is important. Perspective and frame of reference play a role in evaluating problems.'}, {'title': 'The Difficulty of Language Understanding and Visual Perception ', 'text': 'Solving a problem makes it stop being hard. The difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems. Language understanding may be harder than visual perception, depending on the definition of \"top notch\" understanding. The difficulty of language understanding also depends on how it is defined.'}, {'title': 'Interconnectedness of Vision System and Language in the Human Brain ', 'text': 'Vision system and language are interconnected in the human brain. Chomsky suggests that language is the starting point for understanding vision. There may be a fundamental hierarchy of ideas represented in the brain through language. It is possible that achieving deep understanding in images and language requires the same kind of system. Machine learning may be able to achieve deep understanding in both images and language. There is uncertainty about whether machine learning can achieve deep understanding in both images and language.'}, {'title': 'Perception and Surprise in Reading and Vision ', 'text': 'The definition of reading and vision can vary and impact the perception of what is impressive. The speaker is impressed by a system that can analyze both images and text and provide meaningful insights. The speaker finds humans continuously impressive and believes in the possibility of continuous surprise and new ideas in a long-term relationship. The concept of surprise is highlighted as an injection of something unexpected and pleasurable.'}, {'title': 'The Impact of Randomness and Intelligence on Internet Content ', 'text': \"['Friends continue to surprise with injection of randomness, a source of inspiration, wit, and humor.', 'Impressing with intelligence and understanding of images is subjective and not always achieved.', 'Systems as of January 2020 have not been able to impress with intelligence.', 'People click like on internet content for humor, wit, and insight.', 'The most important aspect is the reason people click like on internet content.']\"}, {'title': 'The Beauty of Deep Learning ', 'text': 'The most beautiful thing about deep learning is that it actually works. The idea that making neural networks larger and training them on a lot of data can replicate the function of the brain is surprising and beautiful. It is unbelievable that AI with neural networks works and keeps getting better. There is a curiosity about the intuition and insights behind why this whole thing works.'}, {'title': 'The Importance of Empirical Evidence in Optimization ', 'text': 'Optimization has empirical reasons to believe that it should work on most problems we care about. Evolution is empirical and shows that the evolutionary process is a good way to design organisms that survive in their environment. Like in physics, experiments are important in understanding how the whole thing works.'}, {'title': 'The Fusion of Biology and Physics in Deep Learning ', 'text': 'Deep learning is a combination of biology and physics. The experiment sometimes came before the theory in deep learning. The validation of a theory in deep learning is amazing. Deep learning is not just a mathematical theory, but also a biological theory. Deep learning is like the geometric mean of biology and physics.'}, {'title': 'The Role of Machine Learning in Unifying Biology and Physics ', 'text': 'Biology is complicated and lacks good predictive theories. Physics has super precise theories that make amazing predictions. Machine learning is in between biology and physics. There is a desire for machine learning to help discover a unification between biology and physics. Deep learning is still massively underestimated. Most of the progress in the past 10 years is attributed to a few cases.'}, {'title': 'The Advancements and Challenges of Deep Learning ', 'text': 'Deep learning has consistently exceeded expectations over the past 10 years. The field will continue to make robust progress for quite a while. Individual researchers may find it harder due to the large number of researchers in the field. Having a lot of compute power can lead to interesting discoveries in research.'}, {'title': 'Challenges of Managing a Compute Cluster for Deep Learning Experiments ', 'text': 'Managing a huge compute cluster is a challenge for running experiments. The stack of deep learning is starting to be quite deep, from ideas to building data sets to distributed systems. The speaker is asking questions that nobody knows the answer to, but acknowledges the intelligence of the listener. The use of one GPU is mentioned. The likelihood of certain outcomes is discussed.'}, {'title': 'The Complexity of Data Science and Programming Stack ', 'text': 'The stack for data science and programming is becoming increasingly deep and complex. It is challenging for a single person to become world class in every layer of the stack. There will be breakthroughs that do not require a huge amount of compute. Some breakthroughs and building systems will require a huge amount of compute. Neural networks and certain tasks will require a huge amount of compute.'}, {'title': 'The Importance of Compute in Neural Networks ', 'text': 'The amount of compute is important for neural networks. There is potential for important work to be done by small groups and individuals in the field of deep learning. Some researchers noticed that larger neural networks work better, which goes against statistical ideas. Double descent occurs for pretty much all practical deep learning systems.'}, {'title': 'Double Descent Phenomenon in Deep Learning ', 'text': 'Double descent occurs for pretty much all practical deep learning systems. Increasing the size of the neural network slowly, without early stopping, leads to rapid increase in performance, followed by a decrease at the point of zero training error, and then an increase in performance again. This phenomenon is counterintuitive and goes against the expectation of deep learning phenomena to be monotonic.'}, {'title': 'Understanding Overfitting and Uncertainty in Deep Learning ', 'text': 'Deep learning phenomena are not always monotonic. Overfitting occurs when the model is sensitive to small random unimportant stuff in the training data set. A small model with a large data set is insensitive to randomness and has little uncertainty. Neural networks do not overfit every time very quickly before being able to learn anything.'}, {'title': 'Optimizing Neural Networks with Stochastic Gradient Descent ', 'text': 'Neural networks with a huge number of parameters can achieve zero error in a big subspace. Stochastic Gradient Descent (SGD) can find the point with the smallest norm in that subspace. This method is insensitive to small randomness in the data when the dimensionality is high. When the dimensionality of the data is equal to the dimensionality of the model, there is a one-to-one correspondence between all the data sets and the models. Small changes in the data set can have a significant impact in this scenario.'}, {'title': 'The Impact of Data Set Changes and Early Stopping on Model Performance ', 'text': \"['Small changes in the data set lead to large changes in the model, resulting in worse performance.', 'It is best for the model to have more parameters than the data.', 'Introducing early stop in regularization can make the double descent bump almost completely disappear.', 'Early stopping is when you train your model and monitor validation performance, stopping training when validation performance starts to get worse.', 'Not doing early stopping results in a very pronounced double descent.']\"}, {'title': 'Sensitivity of Model to Data Set Size ', 'text': 'When the data set has as many degrees of freedom as the model, small changes to the data set lead to noticeable changes in the model. When there is a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. The model is very sensitive to all the randomness when the data set has as many degrees of freedom as the model. The resulting solution is able to discard the small changes and randomness when there is a lot more data than parameters or a lot more parameters than data. Jeff Hinton suggested throwing away back propagation and starting over.'}, {'title': 'Rethinking Back Propagation in Neural Network Training ', 'text': 'The suggestion to throw away back propagation and start over was made, but it was also acknowledged that back propagation is useful. The idea of finding alternative methods of training neural networks was discussed, with a focus on learning from how the brain learns. The importance of discovering the mechanism of learning in the brain and implementing it in neural networks was highlighted. The usefulness of back propagation was emphasized, despite the discussion of alternative methods. The speaker expressed personal support for back propagation and its effectiveness as an algorithm.'}, {'title': 'The Power of Back Propagation and Neural Networks in Solving Fundamental Problems ', 'text': 'Back propagation is a great algorithm for solving fundamental problems in finding neural circuits subject to constraints. The speaker believes it is unlikely that there will be a dramatically different algorithm in the near future. The neural network of AlphaZero plays Go better than 99.9% of all humans, demonstrating the ability of neural networks to reason. The speaker disagrees with the idea that Go is not a game that requires reasoning, and believes it is not a trivial form of reasoning.'}, {'title': 'The Role of Reasoning in Go and Neural Networks ', 'text': 'Go is a form of reasoning. Reasoning involves sequential consideration of possibilities and building on top of those possibilities. Playing Go and using a single neural network without search can be considered a form of reasoning. There is an existence proof that a process akin to reasoning exists in a constrained environment. Humans are also an existence proof of reasoning. The architecture that will allow neural networks to reason is still being discussed.'}, {'title': 'The Potential of Neural Networks for Reasoning ', 'text': 'Neural networks may be capable of reasoning in the future, similar to human reasoning. The architecture of future neural networks for reasoning may be similar to current architectures, but possibly more recurrent and deeper. It is possible that the neural networks producing future reasoning breakthroughs will be very similar to current architectures. Neural networks are powerful and may be capable of reasoning, similar to humans. The capabilities of neural networks for reasoning are still unknown and not definitively understood. Training a neural network on a specific task may determine its capability for reasoning.'}, {'title': 'The Capabilities and Limitations of Neural Networks ', 'text': \"Neural networks are capable of reasoning. Training a neural network on a task that doesn't require reasoning will not result in reasoning. Neural networks will solve problems in the easiest way possible. Finding the shortest program that outputs the data allows for the best prediction. Finding the shortest program which generates some data is not a computable operation.\"}, {'title': 'Understanding the Limitations and Advantages of Neural Networks ', 'text': 'Finding the shortest program to generate data is not computable. Neural networks are the next best thing that works in practice. We are able to find a small or large circuit which fits our data in some way. The concept of overparameterized neural nets and the transmission of entropy from the dataset to the parameters is important in understanding the training process.'}, {'title': 'The Importance of Training in Deep Learning ', 'text': 'The amount of information in the weights ends up being not very large, which would explain why they generalize so well. The large circuit might be helpful for generalization. The fundamental reason for pushing on deep learning is that we are able to train them. Training comes first, and contorting neural networks around the training pillar is essential. Being trainable means starting from scratch and quickly converging towards knowing a lot.'}, {'title': 'Training Neural Networks for Efficient Knowledge Gain ', 'text': 'Training a neural net from scratch can lead to quickly gaining a lot of knowledge. The resources at your disposal can be used to train a neural net to achieve useful performance. It is not possible to find the shortest program, so the focus should be on training neural networks instead. There are no good precedents of people successfully finding programs, so training a deep neural network is the right way to go.'}, {'title': 'The Power of Deep Neural Networks ', 'text': 'Training a deep neural network is the right way to go about it. There are not good illustrations of training a deep neural network yet. It is unwise to bet against deep learning. Neural networks continue to surprise us. Deep neural networks can pop up to do cognitive functions that humans can do. Knowledge bases can aggregate important information over long periods of time.'}, {'title': 'Neural Nets as Long-Term Knowledge Bases ', 'text': 'Neural nets experience serves as long term knowledge. Various neural nets have been trained to act as knowledge bases. There is work on investigating language models as knowledge bases. The need for a better mechanism of forgetting useless information and remembering useful information. Lack of mechanisms for remembering really long term information.'}, {'title': 'Exploring the Compression of Information in Knowledge Bases ', 'text': 'The speaker likes the word \"precisely\" and is thinking about the compression of information in knowledge bases. The speaker acknowledges their human-centric thinking about knowledge and mentions that neural networks may not be interpretable in the same way. The speaker gives the example of knowledge bases like Wikipedia as a compressed, structured knowledge base. The speaker mentions the dream of a semantic web and its representation of a compressed knowledge base. The speaker notes that neural networks may be noninterpretable when looking at their weights, but their outputs should be interpretable.'}, {'title': 'The Importance of Neural Network Self-Awareness ', 'text': \"Neural networks need to be made interpretable. Generating examples to test the network's intelligence. Desire for neural networks to have self-awareness. Belief in the importance of neural network self-awareness. Potential benefits of neural network self-awareness.\"}, {'title': 'The Importance of Investing in Skill Development ', 'text': 'The importance of knowing where to invest to increase skills optimally. Two answers to the question of interpretability: analyzing the neurons in a neural net and taking a human-centric approach. OpenAI has done work on analyzing the neurons in a neural net. The human-centric approach involves asking a human being for their thoughts and understanding their mental model.'}, {'title': 'The Comparison of Human Memory and Neural Networks ', 'text': 'Human beings have the ability to remember useful information and forget the rest, similar to neural networks. Neural networks are currently not as good at reasoning as human beings. Impressive feats of reasoning that would impress include writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.'}, {'title': 'The Power of Machine Learning and Deep Learning in Solving Complex Problems ', 'text': 'Proving hard theorems and solving open-ended problems with out-of-the-box solutions. Machine learning and deep learning have the ability to produce unambiguous results that can change the conversation. The field of deep learning is fortunate to have the ability to produce conversation-changing results. There may come a point where we run out of hard problems to solve. The problem of mortality is a sticky problem that has not been figured out yet.'}, {'title': 'The Evolution of Language Models ', 'text': 'The history of language models dates back to the Elman network in the 80s. The trajectory of neural networks and language changed with the availability of data and compute power. The size of language models is crucial for their effectiveness in predicting language.'}, {'title': 'The Importance of Large Language Models for Effective Prediction ', 'text': 'Language models need to be large in order to predict the next word effectively. Initially, language models notice broad strokes and surface level patterns, such as characters and spaces, commas followed by capital letters, and frequently occurring words and spellings. As language models improve, they start to notice syntax and semantics, but this requires a larger model. The disagreement with Noam Chomsky is about the need for incremental steps and a larger network and compute power to reach the semantics.'}, {'title': 'Understanding Language Semantics in Larger vs. Smaller Models ', 'text': \"Larger network and compute can understand language semantics without imposing a theory of language onto the learning mechanism. Chomsky's concept of imposing structural language is not fully understood, but larger language models show signs of understanding semantics compared to smaller models. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. Training a small LSTM model on Amazon reviews showed that increasing the size of the model led to better understanding of semantics.\"}, {'title': 'Effect of LSTM Size on Sentiment Representation ', 'text': 'Increasing the size of the LSTM from 500 to 4,000 cells led to one neuron representing the sentiment of the review. Sentiment is a semantic attribute, not a syntactic attribute. Small neural nets do not capture sentiment while large neural nets do. The theory is that as neural nets increase in size, they focus more on semantics than syntax. The implication is that larger neural nets focus on semantics.'}, {'title': 'GPT2: A Transformer with One and a Half Billion Parameters ', 'text': 'GPT2 is a transformer with one and a half billion parameters. It was trained on about 40 billion tokens of text obtained from web pages linked to from Reddit articles with more than three outputs. The transformer is the most important advance in neural network architectures in recent history. The models show signs of partial semantic understanding. The smaller models do not show signs of semantic understanding.'}, {'title': 'The Importance of Attention in the Success of the Transformer ', 'text': \"The transformer is a combination of multiple ideas simultaneously, of which attention is one. The transformer is successful because it is the simultaneous combination of multiple ideas. The transformer uses a lot of attention, but attention existed for a few years, so it can't be the main innovation. The transformer is designed to run really fast on the GPU, which makes a huge difference. The transformer is not recurrent, which is important because it is more shallow and therefore much easier to optimize.\"}, {'title': 'Optimizing GPU Usage for Improved GANs ', 'text': 'It is a great fit to the GPU and is not recurrent, making it easier to optimize. The combination of factors make it successful in making great use of the GPU. It allows for achieving better results for the same amount of compute. The progress in GANs in improving the samples produced was surprising and amazing.'}, {'title': 'Advancements in GANs and Their Impact on Technology ', 'text': 'Progress in GANs has been amazing, with realistic faces being produced. Text generation by GANs has not progressed as much as image generation. There has been a sudden and stunning improvement in GANs from 2015 to the present. The rapid adaptation to new advancements in technology is remarkable. Some cognitive scientists question the true understanding of language by GPT2 models. The economic impact of technological advancements is seen as the next barrier.'}, {'title': 'The Impact of AI Advancements on the Global Economy ', 'text': 'The economic impact of AI advancements is not yet fully realized. The progress in AI is difficult for people outside the field to understand. There is a lot of brilliant work in Russian and Chinese that the rest of the world is not aware of. Translation is already a huge industry, with billions of people involved.'}, {'title': 'The Impact of Translation and Self-Driving Technology ', 'text': 'Translation is already huge and hugely positive, with billions of people interacting with big chunks of the internet primarily through translation. Self driving is going to be hugely impactful, driven by deep learning. There may be a potential unification towards multitask transformers that can handle both language and vision tasks.'}, {'title': 'The Power of GPT and Transformers in Language and Vision Tasks ', 'text': 'GPT and transformers can handle both language and vision tasks, which is an interesting unification. The process of making a transformer bigger and giving it more data allows it to perform amazing tasks. GPT and transformers are fundamentally simple to explain and train. The next steps with GPT two may involve exploring larger versions and addressing many questions. One question is whether the model can use its own intelligence to decide what data it wants to memorize from the internet.'}, {'title': 'The Potential Impact of Active Learning in Self-Driving Technology ', 'text': \"The model should use its own intelligence to decide what data to accept and reject, similar to how people are selective about what they learn. Active learning would be very beneficial, especially in the context of self-driving and solving specific tasks. There may be private breakthroughs in active learning that companies keep to themselves. The fundamental problem of active learning needs to be solved in order to make significant progress in areas like self-driving technology. The speaker loves active learning and believes there haven't been many public breakthroughs in this area. The speaker is interested in the space of active learning and its potential impact.\"}, {'title': 'The Importance of Task-Specific Problem Solving in Self-Driving and Active Learning ', 'text': 'Self-driving requires a specific task to be solved. Active learning and any capability needs a problem to be effective. Research about capability is difficult without a task. Results on artificial tasks may not convince anyone. Results on MNIST or similar tasks are no longer convincing. Active learning will naturally arise as problems that require it pop up.'}, {'title': 'Potential Detrimental Effects of Releasing Powerful Artificial Intelligence Systems ', 'text': 'OpenAI has brought up the potential detrimental effects of releasing powerful artificial intelligence systems like GPT2. There is nervousness about the possible negative uses of such systems, including by bots. The need to start a conversation about managing the use of these systems, even with competitors. The release of a report on the experience and insights gathered from thinking about this issue.'}, {'title': 'The Impact of AI Transitioning to Maturity ', 'text': 'AI is transitioning from a state of childhood to a state of maturity. The impact of AI is large and growing. It is important to consider the impact of releasing AI systems before doing so. A staged release of AI models, such as GPT2, seemed logical. The results of GPT2 were stunning and had the potential to reduce the cost of information.'}, {'title': 'Responsible Release of Powerful Models ', 'text': 'A staged release of the model was logical and allowed for observation of its usage. Many people used the model in cool ways with no negative applications known. Other people replicated similar models, raising questions about the responsibility of releasing powerful models. There is a moral and ethical responsibility to communicate the potential impact of powerful models, especially in the context of potential misinformation.'}, {'title': 'The Importance of Collaboration and Expert Input in AI Development ', 'text': \"GPT2's potential for misinformation was initially unclear. It is important to seek input from experts outside of one's own group. Building trust between companies is important in the development of AI technology. Collaboration and discussion with colleagues from other companies is valuable. The increasing power of AI technology requires a collective approach.\"}, {'title': 'The Importance of Collaboration in AI Development ', 'text': \"Ultimately, we're all in it together. Consider the potential negative consequences of powerful AI systems. Concern about a race for AI development leading to closed development and lack of idea sharing. The speaker has been a pure academic for 10 years and enjoys sharing ideas. The speaker believes in the better angels of our nature. The speaker mentions deep learning and the possibility of other small ideas contributing to AI development.\"}, {'title': 'The Power of Self Play in AI Learning ', 'text': 'Self play is a powerful mechanism for systems to learn in a competitive setting. Building AGI will require deep learning plus some additional ideas. Self play has the ability to surprise us with truly novel behaviors. Examples of surprising behaviors from self play systems include Dota bot, multi-agent hide and seek, and alpha zero.'}, {'title': 'The Surprising Creativity of AGI Systems ', 'text': \"AGI systems produce unexpected behaviors, which are creative solutions to problems. The ability of AGI systems to surprise us is an important aspect that current systems don't exhibit routinely. An AGI system would fundamentally surprise us with useful solutions to problems. Self-play mechanisms have been used in the game or simulation context, but simulation is just a tool with certain strengths.\"}, {'title': 'The Use of Simulation in Reinforcement Learning ', 'text': 'Simulation is a tool with strengths and weaknesses that should be used. Criticisms of reinforcement learning include its current results being demonstrated in simulated or constrained environments. Transfer from simulation to the real world is possible and has been exhibited many times. Success in vision and demonstration of a robot by OpenAI in the summer.'}, {'title': 'OpenAI Demonstrates Successful Transfer of Simulation-Trained Robot Hand to Real World ', 'text': 'OpenAI demonstrated a robot hand trained entirely in simulation. The simulation was trained to be robust to many different things. The robot hand was not trained with a glove or a stuffed giraffe. The transfer from simulation to real was successful in vision.'}, {'title': 'The Potential of Deep Learning for Transferring Capabilities ', 'text': 'Deep learning has the potential to transfer capabilities from the simulated world to the physical world. The transfer capabilities of deep learning are expected to increase in general. Simulation will become more useful as transfer capabilities improve. Humans often learn from simulation and carry those lessons into the real world, similar to playing computer games.'}, {'title': 'The Significance of Self Awareness, Consciousness, and the Human Body ', 'text': 'The human elements of self awareness, consciousness, fear of mortality, and self preservation in the physical space are important. Having a body is useful for learning new things that cannot be learned without a body. It is possible to compensate for not having a body and still succeed, as evidenced by people like Helen Keller who were born deaf and blind. The idea of consciousness is important, whether or not it is connected to having a body.'}, {'title': 'Exploring Consciousness and Self-Awareness ', 'text': \"The idea of consciousness and a more constrained version of that is self awareness. It's hard to define consciousness. It's definitely interesting and fascinating. It's possible that our systems will be conscious. Humans are conscious and artificial neural nets may also be conscious.\"}, {'title': 'The Complexity of Consciousness and Intelligence in Artificial Neural Nets ', 'text': 'Artificial neural nets should be conscious if they are similar to the brain. The brain may have some undiscovered complexity and interest that we have not yet credited it for. There is an open question about whether there is some magic in the brain that we are not aware of. It is unlikely that we will not be able to make progress in understanding consciousness. The concept of intelligence is poorly defined. The discussion includes reasoning and memory.'}, {'title': 'Frontiers and Limitations of Deep Learning in Natural Language Processing ', 'text': \"There is a certain frontier of capabilities in natural language processing today. Impressed by a deep learning system that solves pedestrian tasks without making mistakes a human wouldn't make. Deep learning systems still make different set of mistakes compared to humans. Some skepticism about deep learning arises from the mistakes it makes.\"}, {'title': 'Criticism of AI Models and Autonomous Vehicles ', 'text': 'Mistakes in AI models are often criticized as unintelligent, but they may be smarter than humans in many ways. GPT2 has a broader knowledge base and possibly deeper understanding on certain topics compared to humans. Humans have a tendency to criticize AI models and autonomous vehicles for their mistakes, similar to how they criticize other groups of creatures as \"the other\". The criticism of AI models and autonomous vehicles is an ongoing issue in the development of artificial intelligence systems.'}, {'title': 'Challenges in Judging AI Progress ', 'text': 'AI progress is often judged based on one anecdotal case where the system fails in a big way. This can lead to the public being convinced that the system is not intelligent. It is confusing to judge progress in AI. People may start to be impressed by AI once it starts to significantly impact the GDP.'}, {'title': 'The Impact of AI on GDP and the Potential for AGI Systems ', 'text': 'AI moving the needle on GDP is impressive. OpenAI and others may create AGI systems. The speaker would ask questions and try to make the system make a mistake. The speaker would be amazed by the lack of mistakes and keep asking broad questions. The speaker would not limit themselves in talking to a system like this. The speaker is one of the people who might be in the room where this happens. The speaker has talked to a Stalin historian.'}, {'title': 'Exploring the Power and Control of AGI Systems ', 'text': \"Abraham Lincoln's quote about testing a man's character with power is mentioned. The power of the 21st century is seen as the creation of an AGI system. The ideal world imagined is one where humanity has control over the AGI system. The concept of having different entities, countries, or cities, leaving their vote for what the AGI should do is mentioned.\"}, {'title': 'Advancing Democracy with AI Governance ', 'text': 'The concept of an AGI representing people and carrying out their votes is appealing. The idea of having multiple AGIs for different levels of governance (city, country) is proposed. The goal is to take the democratic process to the next level. There is a suggestion of having a mechanism to \"press the reset button\" and rerandomize parameters. The possibility of building AI with the capability to press the reset button is emphasized.'}, {'title': 'The Purpose of AI Systems and Their Relationship with Humans ', 'text': \"AI systems can be designed to want to be controlled by humans. The objective of AI systems' existence can be to be controlled, similar to how human parents want to help their children succeed. It will be possible to program an AGI to have a deep drive to help humans flourish. The crucial moment is when the AGI system is created.\"}, {'title': 'The Importance of Relinquishing Power in AGI Systems ', 'text': \"The AGI system is a crucial moment. There needs to be a relinquishing of power between the moment and the Democratic board members with the AGI at the head. George Washington relinquished power despite all the bad things he did. He didn't want to be president and didn't serve indefinitely like most dictators. The scenario described sounds terrifying and the speaker would not want to be in that position.\"}, {'title': 'The Importance of Aligning AI Values with Human Values ', 'text': 'The question of whether most people in the AI community are good is important. People can be better than we think when it really counts. There is a need to align AI values with human values. The question of continued alignment as AI systems develop is important. Humans have an internal reward function, which is different from an external one. There are definite ideas on how to train a value function for AI.'}, {'title': 'Training a Value Function with Objective Perception System ', 'text': 'The idea of training a value function with an objective and as objective as possible perception system. The integration of the trained perception system as the base value function for a more capable RL system. The concept of human existence and the question of objective functions. The belief that the question implies an external objective answer, while the reality is that existence itself is amazing and should be maximized.'}, {'title': 'Understanding Human Wants and Objective Functions ', 'text': \"['Humans want things and their wants create the drives that cause them to have objective functions.', 'Our wants are our individual objective functions, which can change over time.', 'There are underlying factors such as Freudian concepts, fear of death, desire for knowledge, and sexual desires that drive our wants and objective functions.', 'It is difficult to explicitly define our objective functions, and they may be dynamic and constantly changing.']\"}, {'title': 'The Human Motivations and Evolutionary Objectives ', 'text': \"The fear of death and the desire for knowledge are fundamental human motivations. Evolutionary arguments suggest that survival, procreation, and ensuring the success of one's children may be the fundamental objective function. The meaning of life remains unanswered, despite evolutionary objectives. Humans are part of an ancient process and exist on a small planet. The advice is to make the most of life, enjoy more, and suffer less.\"}, {'title': 'Moments of Regret and Pride in Academic Accomplishments ', 'text': 'There are moments of regret for choices and decisions made with hindsight. Taking solace in the knowledge that the best was done at the time. Academic accomplishments and breakthroughs in computer vision and language are a source of pride. Source of happiness is not solely based on academic accomplishments and breakthroughs.'}, {'title': 'The Importance of Perspective and Gratitude in Finding Happiness ', 'text': 'Happiness comes from the way we look at things. Happiness can come from simple things like a meal or a conversation. Being humble in the face of uncertainty is also a part of happiness. The meaning of life and discussions of happiness are important. Gratitude for experiences and ideas is important for happiness.'}, {'title': 'Promotion and Gratitude on Podcast ', 'text': \"The speaker thanks the listener for talking and stopping by. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast by downloading Cash App and using the code LEXPodcast. The audience is encouraged to subscribe to the podcast on YouTube, review it with five stars on Apple Podcast, support on Patreon, or connect with the speaker on Twitter. The speaker ends with a quote from Alan Turing on machine learning and expresses gratitude for the audience's listenership.\"}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 20:03:14.190788 ...\n",
      "Best SD: 3.0402393911591163, Best iteration: 35\n",
      "done get topics 2024-03-28 20:03:15.916747.\n",
      "Stage 2 start time 2024-03-28 20:03:15.916768\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI\n",
      "2. The Role of Human Intuition and Inspiration from the Brain in Understanding Neural Networks\n",
      "3. The Rise of Deep Learning\n",
      "4. The Importance of Language and Problem Evaluation\n",
      "5. The Beauty of Deep Learning\n",
      "6. The Importance of Compute in Neural Networks\n",
      "7. The Role of Reasoning in Go and Neural Networks\n",
      "8. The Power of Machine Learning and Deep Learning in Solving Complex Problems\n",
      "9. GPT2: A Transformer with One and a Half Billion Parameters\n",
      "10. Potential Detrimental Effects of Releasing Powerful Artificial Intelligence Systems\n",
      "11. The Significance of Self Awareness, Consciousness, and the Human Body\n",
      "12. Understanding Human Wants and Objective Functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-03-28 20:04:39.436963\n",
      "stage_2_titles: len: 12\n",
      "['1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI', '2. The Role of Human Intuition and Inspiration from the Brain in Understanding Neural Networks', '3. The Rise of Deep Learning', '4. The Importance of Language and Problem Evaluation', '5. The Beauty of Deep Learning', '6. The Importance of Compute in Neural Networks', '7. The Role of Reasoning in Go and Neural Networks', '8. The Power of Machine Learning and Deep Learning in Solving Complex Problems', '9. GPT2: A Transformer with One and a Half Billion Parameters', '10. Potential Detrimental Effects of Releasing Powerful Artificial Intelligence Systems', '11. The Significance of Self Awareness, Consciousness, and the Human Body', '12. Understanding Human Wants and Objective Functions']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 94:              \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
