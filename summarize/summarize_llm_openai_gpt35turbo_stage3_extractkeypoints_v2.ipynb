{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2\"\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=2000\n",
    "CHUNK_OVERLAP=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f2280d64290>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-23 21:39:34.199043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-23 21:44:43.890750\n",
      "chunks_text len: 40\n",
      "extract_keypoints start time: 2024-03-23 21:44:43.890869\n",
      "extract_keypoints done time 2024-03-23 21:46:15.335321\n",
      "Start time: 2024-03-23 21:46:15.335509\n",
      "Stage 1 done time 2024-03-23 21:47:45.447746\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of Rajat Manga and TensorFlow on Deep Learning ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry. Open innovation can be successful and inspire many companies to open source their code. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The idea of deep learning was interesting and intriguing in some ways back in 2011. The idea of deep learning had shown some very promising and early results.'}, {'title': 'The Advancements of Deep Learning ', 'text': 'The idea of deep learning was interesting and intriguing in some ways. Deep learning had shown some very promising and early results. Scaling the compute and data resulted in better performance. Early wins were achieved in speech and image recognition. The birth of Google Brain was around neural networks and deep learning. Scaling deep learning to hundreds and thousands of machines showed great promise.'}, {'title': 'Evolution of Machine Learning at Google ', 'text': 'Machine learning at Google has been ongoing for a long time, with deep learning being a new addition. The company successfully scaled machine learning to hundreds and thousands of machines, even up to 10,000 machines. Real products, such as speech and image recognition, started to use machine learning. Academia also started to show interest in deep learning, leading to the decision to open source TensorFlow. The decision to open source TensorFlow was considered a seminal moment in software engineering, as it demonstrated the power of open innovation. The decision to open source a project with significant intellectual property was a bold move by Google. The discussions around open source and its potential impact were significant at the time.'}, {'title': 'The Impact of Open Source on Innovation ', 'text': \"The decision to go open source with a lot of IP was a significant moment in time. The initial idea came from Jeff, who was a big proponent of open innovation. The research group's focus on sharing research and pushing the state of the art forward led to the growth of deep learning and machine learning. The existing libraries for software were done by academia and had a significantly different level. Google had previously published papers with open source projects that were successful. Google's decision to provide H base APIs on top of Bigtable aimed to provide something better and help push a good standard.\"}, {'title': 'Google Cloud and TensorFlow Integration ', 'text': 'Google Cloud is providing H base APIs on top of Bigtable to help push a good standard forward. TensorFlow is open source and can be used anywhere, with lots of integrations on Google Cloud. TensorFlow project was started in summer of 2014 and open sourced in November 2015. The project had seen different use cases at Google and needed to support running at large scale in the data center.'}, {'title': 'Title ', 'text': 'The Evolution of TensorFlow: From Decision to Open Source to Design ConsiderationsText '}, {'title': 'Choosing TensorFlow 2.0 for Building a New System ', 'text': \"The belief was built in parallel with the development of libraries like Theano, Torch, Lua, and Caffe. The team looked at a number of libraries before deciding on TensorFlow 2.0. The decision to use graphs in TensorFlow 2.0 was influenced by the team's prior experience with graph-like structures in their previous belief system. The team wanted flexibility and the ability to express a variety of concepts in their new system. The move towards eager execution in TensorFlow 2.0 was driven by the desire to make development more intuitive. The team had experience deploying production systems with their prior belief system, and this influenced their decisions in building the new system.\"}, {'title': 'Influence of Graph Usage on Project Success ', 'text': 'The decision to use a graph for production was influenced by the need for simplicity and ease of deployment. The popularity of the project exceeded the initial expectations, with 41 million downloads. The open sourcing of the project led to a significant increase in attention from developers globally. The project evolved to include good documentation, an ecosystem of tools, and a community.'}, {'title': 'The Evolution of Deep Learning Framework ', 'text': 'The project gained an incredible amount of attention from a global population of developers after it was open sourced. The project started changing with the development of good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. Deep learning transitioned from being a research thing to something that developers could use to do interesting things, even if they had no clue what machine learning was before. The focus shifted from just researchers to also include stability and the ability to deploy things, leading to the planning for version 1.0. Enterprise adoption started to take off after the initial release and over the next few releases. Between the initial release and 1.0, there was a lot of interest from researchers, hobbies, and early adopters.'}, {'title': 'Enterprise Adoption and Research Trends in Technology ', 'text': 'Enterprise adoption of the technology started to take off after the initial release and over the next few releases. Enterprises want stability and something that just works, as they often run models that are several years old. Providing stability and simplicity allows more people to access the technology. The research crowd wants to push the state of the art with new and complex models like RNNs, transformers, RL, and GANs.'}, {'title': 'Advancements in Deep Learning Models ', 'text': 'Deep learning models are evolving, with a shift towards using transformers, RL, and GANs. Older models and techniques are still widely usable and stable. Transfer learning, especially with models like ResNet 50, is a common use case for many people. Enterprises primarily focus on making predictions with their data, often using regression models or structured data. Deep learning shines when dealing with very large datasets. The developer summit is an important event for discussing the latest advancements in the field.'}, {'title': 'The Importance of Organizing Data for Enterprise Companies ', 'text': 'Enterprise companies have a very large data set, where deep learning can shine. The developer summit focused on the entire pipeline of TensorFlow Extended, emphasizing stability and simplicity. Many companies are old school in the way they organize their data, which is often not ready or digitized. There is a need to evangelize the importance of organizing data in order to benefit from TensorFlow. Common questions and challenges include not knowing where to start with machine learning, not having enough data, or not understanding how it can help. The key to automation and making predictions is often digitizing and organizing the data.'}, {'title': 'Machine Learning Beginners and the TensorFlow Ecosystem ', 'text': 'Machine learning beginners often struggle with where to start. Digitizing data is essential for automation and making predictions. TensorFlow ecosystem provides more data sets and pre-trained models. Starting with basic models and improving them is a good approach. Keras made TensorFlow more accessible as it was initially on top of Tiano and then on top of TensorFlow.'}, {'title': 'The Integration of Keras into TensorFlow ', 'text': 'Keras was created by Tiano and integrated into TensorFlow. Tiano initially worked on Keras as a nights and weekends project while doing research at Google. Keras is now the recommended way for beginners to interact with TensorFlow 2.0. There was a parallel layers API being built alongside Keras. The integration of Keras into TensorFlow has made transfer learning and basic use cases simple.'}, {'title': 'Integrating Keras with TensorFlow for Simplified API ', 'text': \"The decision to use Keras in parallel with the parallel layers API was a bold one. The goal was to simplify and integrate the APIs to address confusion in the community. Keras was chosen based on its popularity and positive feedback from the community. Keras was initially seen as a competitor to TensorFlow but became an empowering element of it. The team's focus is on making it easier for developers and aligning with the same goals. The need for a final decision maker in a successful open source project like TensorFlow is highlighted.\"}, {'title': 'The Role of Leadership in Open Source Projects ', 'text': 'Python has Guido van Rossum, who until recently held the position of benevolent dictator for life. Successful open source projects like TensorFlow need one person who makes a final decision. Regular design reviews and transparency have been added to the community. The ecosystem has grown with the involvement of multiple people, not just one decision maker. TensorFlow.js allows training and running neural networks in the browser using JavaScript. TensorFlow Extended, TensorFlow Lite for mobile, and other extensions have been developed to expand the ecosystem.'}, {'title': 'Advancements in Machine Learning with TensorFlow ', 'text': 'TensorFlow.js allows training and running neural networks in the browser using JavaScript. TensorFlow Extended and TensorFlow Lite are also available for different use cases. The goal is to enable machine learning in both research and real-world applications. There is a focus on pushing the state of the art in machine learning research. The aim is to integrate research into real products to have a positive impact on people. Machine learning is no longer limited to workstations or data centers, but can run on various devices including phones and tiny chips.'}, {'title': 'Machine Learning Expansion to Mobile and Embedded Devices ', 'text': 'ML and training are no longer limited to workstations, data centers, or the cloud. Machine learning is now running on phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem continues to grow and cover more devices. Tooling has been built to help with training and production of ML pipelines. There are many libraries being built on top, including for research and production. The goal is to enable others to build the things they care about in the ML community.'}, {'title': 'Title ', 'text': 'Enabling Community Building and Integration Challenges in TensorFlowText '}, {'title': 'Challenges in Scaling TensorFlow for End Users ', 'text': \"It should be easy for the end user, but there are lots of things that go behind that. Challenges ahead include more devices coming on board and the need for clearer interfaces. TensorFlow started as a monolithic system and one of the key challenges is to scale it out by breaking it apart with clearer interfaces. The downside of many people relying on TensorFlow is the responsibility for previous versions to still work, creating technical debt. It's a tricky balance between innovation and maintaining previous versions.\"}, {'title': 'The Impact of TensorFlow 2.0 on Backward Compatibility ', 'text': \"2.0 breaks some back compatibility, but not too much. Conversion to 2.0 seems pretty straightforward. There is a tricky balance between maintaining compatibility and making new changes. It is important to keep compatibility for production systems that rely on TensorFlow. There is a trade off between slowing certain things down and bringing overall value. When doing new things, it is important to start with a clean slate in mind. Designing with a clean slate is necessary to get to a good place. It's important to put old ideas behind when thinking of new things. The speaker is often asked about their opinion on PyTorch versus TensorFlow.\"}, {'title': 'The Benefits of Using TensorFlow in Research ', 'text': \"The speaker switched their research group to TensorFlow and believes it is leading in many ways. They wish everyone would use the same thing, and see TensorFlow as the closest to that ideal. They acknowledge that PyTorch is also being used by researchers and enjoy the competition. They value different ideas and approaches, and see the benefit of learning from PyTorch's focus on research. The speaker emphasizes the importance of considering production in addition to research, which influenced their choice of TensorFlow. They recognize the benefits of exploring different spaces and building on previous work, as seen in PyTorch's approach. Competition has made them revisit their approach and consider different perspectives multiple times.\"}, {'title': 'Developing TensorFlow 2.0: Building on Previous Work and Exploring New Spaces ', 'text': 'The team had the benefit of seeing what had come before and exploring different kinds of spaces. They built on previous work, such as JNR, and faced competition. They had considered the area of eager execution early on and revisited it multiple times before deciding to try it again. It took a while to get all the components together for eager execution, but it has been doing incredible work in the last couple of years. They also discussed making the ecosystem easily accessible to Keras and other things that 2.0 enables them to do. They are excited about the clean APIs and the potential for a whole bunch of stuff behind the scenes once they are ready with 2.0.'}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The clean APIs in TensorFlow 2.0 enable a lot of performance improvements without the need for manual tuning. The restructuring of TensorFlow into more modular pieces will be important for other organizations and ecosystem. TensorFlow 2.0 allows for exploration of other spaces behind the scenes in future versions. The monolithic structure of TensorFlow is being transformed into more modular pieces for easier use by other organizations. The clean interfaces in a perfect world would allow for easy separation of different components in TensorFlow.'}, {'title': 'Overview of TensorFlow and its Ecosystem ', 'text': 'TensorFlow has the execution engine, key backends for CPUs and GPUs, and support for distributed computing. There are some interfaces for splitting apart the core components, but they are not very clean. Clean separation of components would enable easier evolution and scaling of the ecosystem. Major corporations like Pepsi are already using TensorFlow, but they may not contribute to the core development. Various organizations, including hardware vendors and companies like IBM, are involved in special interest groups for TensorFlow. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors.'}, {'title': 'The Growth of TensorFlow as an Open Source Project ', 'text': 'TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The growth of TensorFlow is linked to the growth of deep learning itself. Listening to the community and being open to external contributions has been important for the growth of TensorFlow. Transparency has been a key aspect for the growth of TensorFlow as an open source project. Putting the right processes in place, such as documentation and community welcoming, has been crucial for the growth of TensorFlow.'}, {'title': 'The Role of Community in the Growth of TensorFlow ', 'text': 'Community aspects play a significant role in project growth and development. As a project grows, implementing processes, documentation, and tools becomes essential. The growth of TensorFlow is fueled by people building and sharing projects on GitHub. The developer summit discussed tooling and the importance of smooth version transitions. People are motivated to move to new technologies when they see the value, and this shift is expected to happen in the next few months. The rapid pace of the field will drive further advancements in TensorFlow 2.x.'}, {'title': 'The Future of Technology in Deep Learning ', 'text': 'People will start to see the value of the new technology over the next few months. The field is rapidly evolving, allowing for more advancements and new developments. Some basics of deep learning, such as convolution models, will likely still be around in five years. RL and GAN are likely to stay, while new developments are hard to predict. Projects are focusing on combining eager execution and graphs to make programming more natural. There is uncertainty about the future of hardware accelerators and the ability to train with four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving.'}, {'title': 'Advancements in Hardware Accelerators and Training with TensorFlow ', 'text': 'In five years, more advancements are expected in the area of hardware accelerators and training with four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and the community to achieve the biggest benefit. Efforts are being made to make TensorFlow accessible and easy to use, especially for beginners who want to train or do transfer learning on simple models. Different levels of support are being provided, from pre-trained models for beginners to custom layers and loops for researchers. Providing pre-trained models significantly decreases the time needed to start and achieve the desired results.'}, {'title': 'The Importance of Pre-Trained Models and Team Cohesion ', 'text': 'Providing pre-trained models decreases the time to start and achieve what is needed. TensorFlow has delivered something trivial for beginners. High schoolers are doing amazing and terrifying things with technology. Cohesion across the team is important for delivering something well. The product of what the team generates is larger than the individual contributions. Hiring good people and having a strong team culture is important for success.'}, {'title': 'The Importance of Team Culture and Hiring Good People ', 'text': \"The importance of team culture and hiring good people. The need for team members to care about what they're building and be motivated for the right kind of things. The importance of having a somewhat unified vision of where the team wants to go. The challenge of combining bottom-up organization with a unified direction. The difficulty in monitoring the health of the team and determining alignment. The presence of tensions and complexities within the team. The impact of individual superstars on team dynamics and the challenges they may pose. The potential struggles and challenges in managing people within a team.\"}, {'title': 'Challenges and Strategies in Managing Superstars within a Team ', 'text': \"Superstars can sometimes be against the dynamic of a team and cause tensions. The mission of the project in TensorFlow is exciting and at the cutting edge. Google values hiring people who care and have the same kind of culture. Room for lots of people to do different things and grow makes the problem of working with superstars a bit easier. Productivity is broadly about the team, regardless of whether someone is a superstar or not. Google has a refined hiring process that focuses on core technical skills and motivation. Alignment of motivation with the team's goals is important for long term success.\"}, {'title': 'Hiring Process Emphasizing Technical Skills, Motivation, and Culture Fit ', 'text': \"The hiring process at the company focuses on core technical skills and motivation. Motivation is important for long term success, especially for senior positions. The Google hiring process assesses puzzle solving and problem solving abilities, but may not fully determine the candidate's motivation. Culture fit is an important part of the interview process at the company. Different projects may require different cultural fits, such as the fast-moving nature of the TensorFlow project. The company values individuals who are comfortable with fast-moving projects and are motivated to contribute to the team's success.\"}, {'title': 'Cultural Diversity and Project Development at Google ', 'text': \"Different projects and teams at Google have varying cultures and requirements. Balancing speed and quality is important in project development. Engineering excellence is a core part of Google's culture. Solving difficult problems can be fun. Making hard decisions, such as saying no to certain things, is a part of the job. Striking a balance across different aspects of a project is crucial for success.\"}, {'title': 'Making Decisions and Meeting Deadlines ', 'text': \"Some decisions are made quickly due to lack of time, while others are thought through but are always hard. The Dev Summit came together well, despite the many moving pieces. Deadlines can bring a sense of urgency and help in getting the right things together, even if it's not perfect. The team focuses on key important things and develops in the open, with everything available to everybody. Releases are done at a regular cadence, and if something doesn't make it in one release, it will in the next one. It's okay to put out things that aren't fully ready, as long as it's clear that they are experimental and feedback is welcomed. Quick cycle and quick iteration are important, rather than focusing on strict deadlines.\"}, {'title': 'Importance of Iteration and Improvement in the Development of TensorFlow 2.0 ', 'text': \"Quick iteration and improvement is important. No external deadlines for TensorFlow 2.0, but internal deadlines may exist. Focus on making TensorFlow 2.0 a great product. No rush to release TensorFlow 2.0 just because it's already out there. Will continue to have more releases and updates even after TensorFlow 2.0 is released. Experimentation and feedback are valued in the development process. Pressure to make TensorFlow 2.0 stable, but willingness to release updates quickly to improve it. The focus is on getting it right rather than meeting a specific deadline.\"}, {'title': 'TensorFlow 1.0 X Reaches 41 Million Downloads ', 'text': \"TensorFlow has 41 million downloads for 1.0 X. The focus is on polishing and putting features together. They want to get it right and really focus on that. They are looking to release the product in the next few months or next quarter. Ads at their best connect people to the things they want and need. Machine learning has the opportunity to shine in connecting users to what they want and need. The goal of search ads is to make the world's information accessible, including products and other things people care about.\"}, {'title': 'The Importance of Search Ads ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. There is a minimum quality level for search ads before they are shown. Advertising is a key part of the web and has become a core part of search and many other search engines. It is important to strike a balance between showing valuable ads to the user and providing monetization to the service. Advertisements, when done well, can be really useful and not annoying. The internet has a limitation in that nobody wants to pay for anything, so advertisements are necessary for monetization. The goal is to continue growing and improving the effectiveness of advertisements.\"}, {'title': 'The Future of Monetization and Revenue Models on the Web ', 'text': 'The model that funds businesses like Google is a significant revenue stream. Advertisements, when coupled at their best, are actually really useful and not annoying. There has been a transition towards more paid services across the web, with people willing to pay for them because they see the value. The future of TensorFlow in terms of empowering a class of 300 students and enabling them to do their homework is a question that needs to be addressed. The future of training networks with TPUs, cloud services, and open source TensorFlow is a topic of interest. The potential for a mixed model where users can try something out for free with ads, but also have a clear revenue model beyond that, is hopeful. The question of how a person can use the TPU in a Google call app for free is raised. The future of monetization with things like ads and the potential for a mix model is discussed. The willingness of people to pay for newspaper content and good news websites across the web has increased in recent years.'}, {'title': 'Introduction to TensorFlow and Cloud Services ', 'text': 'TensorFlow is open source and can be run on desktops and phones. Cloud services like Colab make it easy to get started with TensorFlow. Colab is a free service for beginners to play and explore with TensorFlow. Paid services offer more features and capabilities for TensorFlow. Beginners interested in machine learning and TensorFlow should start by visiting TensorFlow.org and exploring tutorials and guides.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-23 21:47:47.535146 ...\n",
      "Best SD: 0.9558139185602919, Best iteration: 2\n",
      "done get topics 2024-03-23 21:47:47.818743.\n",
      "Stage 2 start time 2024-03-23 21:47:47.818761\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Impact of Rajat Manga and TensorFlow on Deep Learning\n",
      "2. The Evolution of Deep Learning Framework\n",
      "3. Enterprise Adoption and Research Trends in Technology\n",
      "4. Machine Learning Beginners and the TensorFlow Ecosystem\n",
      "5. Advancements in Machine Learning with TensorFlow\n",
      "6. Challenges in Scaling TensorFlow for End Users\n",
      "7. Overview of TensorFlow and its Ecosystem\n",
      "8. Hiring Process Emphasizing Technical Skills, Motivation, and Culture Fit\n",
      "9. Importance of Iteration and Improvement in the Development of TensorFlow 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "In this podcast, Rajat Manga, director of Google's TensorFlow team, discusses the evolution and impact of the open source library on deep learning. TensorFlow 2.0, currently in alpha, is being developed by a large team at Google Brain. The decision to open source TensorFlow was a pivotal moment in the tech industry, inspiring open innovation and leading to the growth of deep learning and machine learning. Google Cloud is providing H base APIs on top of Bigtable to help push a good standard forward. TensorFlow, open source since 2015, can be used anywhere and has many integrations on Google Cloud. The project, started in 2014, has seen different use cases at Google and needed to support running at large scale in the data center.\n",
      "\n",
      "The podcast discusses the evolution of TensorFlow, highlighting the development of good documentation, tools, a community, and a YouTube channel that transformed deep learning from a research concept to something accessible to developers. The focus shifted to include stability and deployment, leading to the planning for version 1.0. Enterprise adoption took off after the initial release, and there was significant interest from researchers, hobbyists, and early adopters. The decision to use graphs in TensorFlow 2.0 was influenced by the team's prior experience and the desire for flexibility. The move towards eager execution was driven by the goal of making development more intuitive. The popularity of the project exceeded expectations, with 41 million downloads, and the open sourcing led to a significant increase in global developer attention. The project evolved to include good documentation, tools, and a community, and the team's experience in deploying production systems influenced their decisions in building the new system.\n",
      "\n",
      "The podcast also discusses the increasing adoption of technology by enterprises, with a focus on the need for stability and simplicity. Deep learning is becoming more popular, especially with the use of transformers, RL, and GANs, and the developer summit is an important event for discussing advancements in the field. Many companies struggle with organizing their data, which is essential for benefiting from technologies like TensorFlow. Common challenges include not knowing where to start with machine learning, not having enough data, or not understanding how it can help. The key to automation and making predictions often lies in digitizing and organizing the data.\n",
      "\n",
      "The podcast highlights the TensorFlow ecosystem, which provides data sets and pre-trained models, and the accessibility of Keras as a beginner-friendly interface for TensorFlow 2.0. The integration of Keras into TensorFlow, alongside the development of a parallel layers API, aims to simplify and integrate the APIs to address confusion in the community. The decision to use Keras was based on its popularity and positive feedback from the community, and it has become an empowering element of TensorFlow. The podcast also emphasizes the need for a final decision maker in successful open source projects like TensorFlow, and the growth of the ecosystem with the involvement of multiple people. It also mentions the development of extensions like TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile to expand the ecosystem.\n",
      "\n",
      "The podcast discusses the challenges and developments in TensorFlow, focusing on the transition to version 2.0. The speaker emphasizes the importance of maintaining compatibility while making new changes, and the need for clearer interfaces as more devices come on board. They also discuss the competition between TensorFlow and PyTorch, and the benefits of exploring different spaces and building on previous work. The restructuring of TensorFlow into more modular pieces will be important for other organizations and the ecosystem, and the clean APIs in TensorFlow 2.0 enable performance improvements without manual tuning. The speaker is excited about the potential for a whole bunch of stuff behind the scenes once they are ready with 2.0, and the exploration of other spaces in future versions. Overall, the podcast highlights the challenges and advancements in TensorFlow, and the importance of considering both production and research in the development of the platform.\n",
      "\n",
      "The podcast also discusses the growth and evolution of TensorFlow, an open-source machine learning platform. Major corporations like Pepsi are already using TensorFlow, but may not contribute to its core development. Various organizations, including hardware vendors and companies like IBM, are involved in special interest groups for TensorFlow. The growth of TensorFlow is linked to the growth of deep learning itself, and the project has been downloaded 41 million times, with 1,800 contributors. The podcast also discusses the importance of community aspects in project growth and development, as well as the future advancements in TensorFlow 2.x. The field is rapidly evolving, allowing for more advancements and new developments, and there is uncertainty about the future of hardware accelerators and the ability to train with four bits instead of 32 bits. Efforts are being made to make TensorFlow accessible and easy to use, especially for beginners who want to train or do transfer learning on simple models, with different levels of support provided.\n",
      "\n",
      "The podcast also delves into Google's hiring process, which focuses on core technical skills and motivation. Motivation is crucial for long-term success, especially for senior positions. The company values individuals who are comfortable with fast-moving projects and are motivated to contribute to the team's success. Different projects and teams at Google have varying cultures and requirements, and balancing speed and quality is important in project development. The podcast also explores the challenges of managing individual superstars within a team and the importance of team culture and alignment of motivation with the team's goals. Google has a refined hiring process that emphasizes core technical skills and motivation, and the company values hiring people who care and have the same kind of culture. Overall, the podcast highlights the importance of motivation, culture fit, and team cohesion in the hiring process and in achieving long-term success within the company.\n",
      "\n",
      "The podcast discusses the importance of quick iteration and improvement in the development of TensorFlow 2.0, with a focus on making it a great product without rushing its release. Experimentation and feedback are valued, and there is pressure to make it stable but also a willingness to release updates quickly. The goal is to get it right rather than meet a specific deadline. The podcast also explores the role of advertisements in connecting users to what they want and need, and the balance between showing valuable ads and providing monetization. The future of TensorFlow in empowering students and the potential for a mixed model of free and paid services are also discussed. The podcast encourages beginners interested in machine learning and TensorFlow to explore tutorials and guides on TensorFlow.org, and mentions the availability of free and paid services for using TensorFlow.\n",
      "RRR rewritten summary\n",
      "[{'text': \"Rajat Manga, director of Google's TensorFlow team, discusses the evolution and impact of the open source library on deep learning. TensorFlow 2.0, currently in alpha, is being developed by a large team at Google Brain. The decision to open source TensorFlow was a pivotal moment in the tech industry, inspiring open innovation and leading to the growth of deep learning and machine learning. Google Cloud is providing H base APIs on top of Bigtable to help push a good standard forward. TensorFlow, open source since 2015, can be used anywhere and has many integrations on Google Cloud. The project, started in 2014, has seen different use cases at Google and needed to support running at large scale in the data center.\\n\\nThe evolution of TensorFlow is highlighted, focusing on the development of good documentation, tools, a community, and a YouTube channel that transformed deep learning from a research concept to something accessible to developers. The focus shifted to include stability and deployment, leading to the planning for version 1.0. Enterprise adoption took off after the initial release, and there was significant interest from researchers, hobbyists, and early adopters. The decision to use graphs in TensorFlow 2.0 was influenced by the team's prior experience and the desire for flexibility. The move towards eager execution was driven by the goal of making development more intuitive. The popularity of the project exceeded expectations, with 41 million downloads, and the open sourcing led to a significant increase in global developer attention. The project evolved to include good documentation, tools, and a community, and the team's experience in deploying production systems influenced their decisions in building the new system.\\n\\nThe increasing adoption of technology by enterprises is discussed, with a focus on the need for stability and simplicity. Deep learning is becoming more popular, especially with the use of transformers, RL, and GANs, and the developer summit is an important event for discussing advancements in the field. Many companies struggle with organizing their data, which is essential for benefiting from technologies like TensorFlow. Common challenges include not knowing where to start with machine learning, not having enough data, or not understanding how it can help. The key to automation and making predictions often lies in digitizing and organizing the data.\\n\\nThe TensorFlow ecosystem is highlighted, which provides data sets and pre-trained models, and the accessibility of Keras as a beginner-friendly interface for TensorFlow 2.0. The integration of Keras into TensorFlow, alongside the development of a parallel layers API, aims to simplify and integrate the APIs to address confusion in the community. The decision to use Keras was based on its popularity and positive feedback from the community, and it has become an empowering element of TensorFlow. The podcast also emphasizes the need for a final decision maker in successful open source projects like TensorFlow, and the growth of the ecosystem with the involvement of multiple people. It also mentions the development of extensions like TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile to expand the ecosystem.\\n\\nThe challenges and developments in TensorFlow are discussed, focusing on the transition to version 2.0. The speaker emphasizes the importance of maintaining compatibility while making new changes, and the need for clearer interfaces as more devices come on board. They also discuss the competition between TensorFlow and PyTorch, and the benefits of exploring different spaces and building on previous work. The restructuring of TensorFlow into more modular pieces will be important for other organizations and the ecosystem, and the clean APIs in TensorFlow 2.0 enable performance improvements without manual tuning. The speaker is excited about the potential for a whole bunch of stuff behind the scenes once they are ready with 2.0, and the exploration of other spaces in future versions. Overall, the podcast highlights the challenges and advancements in TensorFlow, and the importance of considering both production and research in the development of the platform.\\n\\nThe growth and evolution of TensorFlow, an open-source machine learning platform, is discussed. Major corporations like Pepsi are already using TensorFlow, but may not contribute to its core development. Various organizations, including hardware vendors and companies like IBM, are involved in special interest groups for TensorFlow. The growth of TensorFlow is linked to the growth of deep learning itself, and the project has been downloaded 41 million times, with 1,800 contributors. The podcast also discusses the importance of community aspects in project growth and development, as well as the future advancements in TensorFlow 2.x. The field is rapidly evolving, allowing for more advancements and new developments, and there is uncertainty about the future of hardware accelerators and the ability to train with four bits instead of 32 bits. Efforts are being made to make TensorFlow accessible and easy to use, especially for beginners who want to train or do transfer learning on simple models, with different levels of support provided.\\n\\nThe podcast also delves into Google's hiring process, which focuses on core technical skills and motivation. Motivation is crucial for long-term success, especially for senior positions. The company values individuals who are comfortable with fast-moving projects and are motivated to contribute to the team's success. Different projects and teams at Google have varying cultures and requirements, and balancing speed and quality is important in project development. The podcast also explores the challenges of managing individual superstars within a team and the importance of team culture and alignment of motivation with the team's goals. Google has a refined hiring process that emphasizes core technical skills and motivation, and the company values hiring people who care and have the same kind of culture. Overall, the podcast highlights the importance of motivation, culture fit, and team cohesion in the hiring process and in achieving long-term success within the company.\\n\\nThe importance of quick iteration and improvement in the development of TensorFlow 2.0 is discussed, with a focus on making it a great product without rushing its release. Experimentation and feedback are valued, and there is pressure to make it stable but also a willingness to release updates quickly. The goal is to get it right rather than meet a specific deadline. The podcast also explores the role of advertisements in connecting users to what they want and need, and the balance between showing valuable ads and providing monetization. The future of TensorFlow in empowering students and the potential for a mixed model of free and paid services are also discussed. The podcast encourages beginners interested in machine learning and TensorFlow to explore tutorials and guides on TensorFlow.org, and mentions the availability of free and paid services for using TensorFlow.\"}]\n",
      "Stage 2 done time 2024-03-23 21:49:08.734830\n",
      "stage_2_titles: len: 9\n",
      "['1. The Impact of Rajat Manga and TensorFlow on Deep Learning', '2. The Evolution of Deep Learning Framework', '3. Enterprise Adoption and Research Trends in Technology', '4. Machine Learning Beginners and the TensorFlow Ecosystem', '5. Advancements in Machine Learning with TensorFlow', '6. Challenges in Scaling TensorFlow for End Users', '7. Overview of TensorFlow and its Ecosystem', '8. Hiring Process Emphasizing Technical Skills, Motivation, and Culture Fit', '9. Importance of Iteration and Improvement in the Development of TensorFlow 2.0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
