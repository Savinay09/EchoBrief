{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b4\" # no rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "# REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f6c38812c50>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Deb Donig with technically Human, a podcast about ethics and technology, where I ask what it means to be human in the age of tech. Each week I interview industry leaders, thinkers, writers, and technologists, and I ask them about how they understand the relationship between humans and the technologies we create. We discuss how we can build a better vision for technology, one that represents the best of our human values. Today, I'm sitting down with Doctor Tamara Neese. Doctor Tamara Kneese is project director of Data and society's Algorithmic Impact Methods Lab, where she is also a senior researcher. For the 2023 and 2024 academic year, she's a visiting scholar at UC Berkeley's center for Science, Technology, Medicine and Society. Before joining data and society, she was lead researcher at Green Software foundation, director of developer engagement on the green software team at intel, and assistant professor of media studies and director of gender and sexuality studies at the University of San Francisco. Tamara holds a PhD in media, culture and communication from NYU and is the author of Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond. In her spare time, she's a volunteer with tech workers Coalition. Hi Tamara. Hi Deb. So Tamara, we're talking in a week where Congress just introduced a landmark bill that would move the government toward developing standards that would measure and report the full range of AI's environmental impact, as well as one that would create a voluntary framework for AI developers to report environmental impacts. This legislation, just to give a bird's eye view, also requires an interagency study that would aim to investigate and measure both the positive and the negative environmental impacts of AI. Can you talk a little bit about the legislation? Yeah, so, you know, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. So one of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So if you think about the fact that the kind of form of AI that we usually think about might be the thing that we interact with directly. So if you're thinking about Chad GBT, you're thinking about maybe the energy used to prop up that interaction that you're having with it. But behind that, there's a whole supply chain that has to go into the manufacturing of AI, which also requires the manufacturing of all the hardware it relies on, including chips and the raw earth minerals that are needed to make the chips possible. In the first place. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important. You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way? There are a few problems when it comes to measuring the environmental impact of something like AI, precisely because of all of the very complicated global supply chains that are involved in its production. And so you really need to have a lot of coordination across different parts of the supply chain, different companies, different parts of the world. And so the bill is actually sort of relying on not just the EPA, but also the National Institute of Standards and Technology to really convene a group of people who are going to be able to identify the methodologies and standards that are needed to do this kind of measurement work. What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI, with large language models in particular? So if we're talking about the kind of AI like, generative AI, that requires a large amount of compute, so you actually need specialized forms of chips in order to run AI efficiently. You also need just a tremendous amount of power. So you need a lot of energy, electricity. You also need a lot of water. And so researchers from different corners of tech have been trying to figure out exactly how much energy or how much water is needed to train these large language models. And then other researchers have looked at what happens when a kind of AI is actually deployed. And so what is the sort of energy or water consumption associated with the use of AI, not just the training part. And so this is something that has been a growing problem. So the entire ICT sector takes up a tremendous amount of energy. Many of us have probably read some of the articles that have come out over the years about the effects of data centers on local areas, maybe that are already experiencing drought or that already have issues with their grid. And so because data centers require just a tremendous amount of energy and can also pollute the areas in which they're located while also sucking up a lot of water, it really becomes an environmental justice issue. At the same time that it's sort of a larger question of worrying about the control of resources and the potential cost in terms of carbon emissions. So this sort of robust form of research that would actually kind of take all of these aspects into account about AI. It's actually just that AI is one small part of a much larger problem that's been going on for quite a long time. And actually, we can look to things that happened even it may sound like a really long time ago, but the 1980s and looking at things that happened in the Silicon Valley region when it comes to the manufacturing of chips and in some cases toxic chemicals got into the local water supply in San Jose, and it took community members really coming together and fighting back against this sort of contamination and asking for more regulation to prevent this sort of thing from happening in the future. And so I think if we look at what is happening right now, we have an AI boom. And so it's a good time to really focus on making some changes through regulation and to get a better grip on sort of what the real environmental costs are. But again, this is not a new problem. This is not unique to generative AI. I know that this question is, as we say sometimes in academia, problematic. I'm aware of the problematics of what I'm about to ask, but I want to ask it anyway. When we're talking about AI and generative AI in particular, there's a whole range of problems, concerns, issues and dangers cited from the automation of labor leading to job loss and questions around compensation for labor, copyright questions about creativity, misinformation in the circulation of false information using generative AI, to things like the changes to the structure of how we get and source information online. Environmental impact is one that we maybe sometimes hear about on the sidelines. It's not typically included in some of, at least to my knowledge, some of these more mainstream issues. And the problematics of this question, I am aware, is in trying to rank or propose a hierarchy of problems, but where would this rank in the hierarchy of problems caused by AI? So I think that the environmental impacts of AI are very much connected to a lot of the labor problems. So I think part of what focusing on the environmental impact of AI can do for researchers and for advocates for people who are trying to create policy is that it really forces you to have a much more holistic view of what the potential harms of AI are, because, you know, if we're talking about the entire AI supply chain, we really are talking about many different parts of the world. We're talking about many different kinds of work, many different kinds of labor exploitation that can be behind this. And often we're also looking at the trade off. Right? So between, you know, you could say, oh, a data center in this area is going to bring jobs. Well, and sometimes that isn't even true. Right? Like that might be the sort of line that communities are given about bringing a large scale data center to the town. But then what are the sort of downstream effects? And what does it mean to be living next to something that is taking a bunch of power and taking a bunch of water? Or what does it mean to live next to something that is actually quite noisy? So I think a lot of people maybe don't realize the extent to which data centers are super loud and can be really distracting for the people who are living around them, and then wondering also about the other sort of effects on health for people in the area, thinking about all of the sort of pollutants and other issues that are associated with it. And so I think when you're talking about sort of justice concerns, when you're talking about how particular marginalized communities are going to be impacted, the question of labor and ethics comes up in tandem with the discussion of the environmental impact. So I think if we really begin to think about entire communities, if we're thinking about effects on habitats and ecosystems and on other species as well as on humans directly, it becomes a way to really consider what the sort of full spectrum of impacts are going to be from AI. So what is that full spectrum of impact? The broader question here is how do you study AI's environmental impact? How do you measure it? What issues or areas comprehensively do you look at when you assess an environmental impact? Yeah, it's a great question. So, I mean, I think a lot of researchers have spent time focusing on measuring the carbon attached to training and maybe sometimes deployment. Some researchers have really pushed for more of a lifecycle analysis. So they're really looking at the carbon cost of AI, not just in the early training stages and deployment, but maybe even looking at the potential of e waste. So the fact that there's a tremendous amount of hardware that is also used in creating AI and deploying AI that also tends to be built not to last very long. And so there's a large amount of e-waste that also can be quite hazardous. And so one of the ways of measuring impact is at that very highly technical level when you're measuring the effects of, say, the training and deployment itself, but then you're also measuring social impacts. And to do that, you really do need to talk to communities who are being impacted. And so I think if we could find a way to kind of better or at least more effectively bridge the relationship between these sort of technical assessments of exactly how much carbon am I emitting when I'm training this model versus what are the downstream effects that this particular system might have on communities in x location five years down the road? I think it's incredibly difficult to measure, but you have to find a way of putting these ways of framing the problem together, because what I've seen from working in the tech industry, so I worked on a sustainability team at Intel. I was on what, what was called the green software team. And so a lot of that had to do with creating software that would help developers try to optimize the technology that they were building and using. And so if you're working in a lab, you're a machine learning specialist, you're training a large model, maybe you choose to do that at a time of day when there's more renewable energy available on the grid. And so a lot of the focus of our research was about kind of making these choices more visible to developers and having them feel kind of empowered to know, if I choose to do this at this time of day, it's a better choice for the environment. Right. This is just a completely different way of framing the problem than you're going to get from talking to grassroots community groups who are calling for environmental justice. They're going to have a very different way of setting the terms of how to measure and report the environmental impacts of the technology. That's where the kind of work that we're trying to do at Aim lab comes in, because we're very much trying to figure out how to really reconcile that more technical approach where the solution is sort of found through the technology itself and a tweak to the technology, versus a more socio technical perspective, which kind of understands the technical side, but also tries to look at all of the social factors that are around it. I want to take a little bit of advantage of having somebody on the show who has worked across so many different sectors. You're at data and society now, which I would broadly consider a kind of public interest technology or civilly minded research center. You've been a academic, you are academically trained and have worked in the ivory tower for a while. You've worked in industry as well. You've worked on the sustainability project that you just mentioned inside of a kind of industry located organization. As far as I see it, there are so many fractures in terms of the ways that these different kinds of communities are thinking about or approaching the question of the benefits of AI versus the harms or mitigating harms, while maximizing benefits or thinking critically about the problems that AI may introduce. What do these different sectors maybe not know? Or what are the gaps in knowledge between these different sectors that maybe cause some of these tensions or fissures,or maybe talking across purposes with one another or talking over each other? How do you see the kinds of silos in knowledge, and maybe some of the controversial or combative conversations that happen across these different industries, given your knowledge and experience in these very different areas? Yeah, no, it's a great question because I think, you know, the important thing, especially for doing this kind of research, is to kind of have a very broad understanding of the problem. You really want to be able to look at it from a lot of different perspectives. And what I have found sort of working across sectors, is that often people just sort of define the problem in a different way, and the questions that they're asking are going to be quite different. Another point of contention is that people often are working on very different timelines. So for academic researchers, we often do have the luxury of time. We may feel pressed for time, but we're working on a very different temporal scale than the quarterly financial system of tech. We're not trying to rush to ship a product. We're not worried about selling anything, at least in terms of a product. Maybe we're selling ourselves to a degree. But I think part of the problem is that the expectations of how long a research project should take and what a useful or effective research project is, it's just very different. So conducting socio technical research within a tech company, it's all about what insights you're giving people and demonstrating return on investment for the people who are funding the research, and also demonstrating the impact that you're having in a very real way. So are you directly influencing the design of a product? Are you helping the companies sell more stuff? And often, if you are not doing those things, then the research is not seen as being very valuable. And so trying to carve out space to actually do longer term research can be quite hard in tech itself. But what I think is really interesting is, and this is sort of the kind of work that we're trying to do at AIM Lab is we, you know, as sort of socio technical researchers who are at an independent nonprofit, but who are trained academics, we can kind of lend our expertise to people in the tech industry who are working on a problem and who maybe want to have other perspectives incorporated into the work that they're doing. And so I think some of the most interesting kinds of collaborations that you can have are when you really are sort of sharing perspectives with people who would ordinarily be addressing the problem of, say, of climate impacts, of AI through maybe decarbonization alone. And that's sort of the main way that they've thought about it. But if you can help them to think through what the user experience of actually using the thing that they're building is, and then also get them to kind of engage with the idea of downstream impacts to other communities after the fact, it's just a way of changing the perspective and reframing the problem. And I feel like that is something that can also really be helpful when you're trying to come up with policy, because, you know, obviously there's always going to be a gap between policy recommendations and how that is actually implemented. And so for me, I find it really helpful, having been in tech, in knowing exactly how power operates, how hierarchies work, and what the expectations are, and knowing how to talk the language, and so you can have a sense of how policy will actually be taken up within an organization. And so that's also something that we're really trying to focus on at AIM lab is understanding the methodologies that people will need to have to carry out the work of algorithmic impact assessment in a variety of different organizations. It's say a startup versus a large scale enterprise, or in a city government, or in other contexts that maybe don't get as much attention as the major tech companies like Google or something. Help me better understand the tensions around climate change, environmental harms and AI on the one hand, leaders in the tech industry often cite the ways that AI may be helping to solve or mitigating climate change and environmental harm by reducing inefficiencies in transportation, for example, finding ways to streamline the use of environmental resources and to reduce waste, or creating new technologies that either reduce dependencies on environmentally harmful products or counteract existing environmental and climate damage. On the other hand, we also hear news of the mass energy requirements of data centers that need to be powered and climate controlled, as you've just cited at vast environmental and energy expenses in order to run e-waste, which you've mentioned as well, and environmental damage caused by mining for the resources required to tech products. And I'll tack onto this, that when I talk to people in industry, particularly those who are developing and trying to sell products in AI, they talk about the fact that they're creating jobs, that they themselves have 300 jobs that they cannot fill, that ultimately AI will end up producing more jobs, and that the task is then on the government, typically, which as libertarians. They also don't want to fund. But we'll save that conversation for another time to provide retraining for these better and more thoughtful and larger number or quantity of jobs. And then we hear from people who are critical of this or those who are studying it or those who are losing their jobs, that this is a kind of labor crisis, that in fact, the kinds of trainings required in order to occupy these so called new jobs that AI industries are inventing or providing in surplus require particular set of skills that potentially many people may not have or that they may not want to have. And so I'm trying to think about this controversy both in terms of the environment, as you've already brought in the labor dimension of this. And then, you know, also think about, you know, I think the reasonable case that many of the jobs that are now gone are jobs that were not good for the environment, and also that we don't particularly miss. The job that comes up quite a bit in this is, for example, the driver of the horse and buggy. People say, well, it's a kind of outdated technology and the horses were causing a lot of pollution and a lot of city sanitary problems, and we're better off even if the horse and buggy drivers are no longer able to exist in that occupation without that particular form of occupation to begin with. So how do you think about assessing the benefits against the harms? And how do you go about trying to provide that as a impact assessment? Yeah, it's a great question. So I think if we take a step back and with a lot of the technology that's being developed in the name of sort of being climate friendly or also maybe actually furthering goals of climate justice in some way. And so, okay, so thinking about maybe AI that could be used to help farmers, let's say, particularly in drought stricken areas. And so the AI that kind of acts as a sensor, a way of detecting things, maybe AI that could be used for a kind of deforestation mitigation or say, helping keep the coral reefs healthy and using that to detect what's going on with the coral reefs, I think there are many kind of imagined uses of AI that would really be a boon to environmentalist efforts. But the question is, and this comes up a lot with the experiences that we're having with AIM lab, is does the tech actually do what you think it's going to do? You know, does the tech actually behave in a way that the kind of people developing imagine that that will work on the ground? And how do the people who are expected to use it? How are they, how are they interfacing with it? Are they having a good time when they, when they interact with the AI? Is it actually helping them with the work that they're already trying to do? Or is it creating new forms of work for, for them, new forms of maintenance and just new sort of things that they didn't have to do in their role before? And so one of the questions with especially technology that is developed to kind of serve a particular environmental purpose, we hear a lot about sort of climate tech being the wave of the future. Maybe this is what will save us from climate change. Are these technologies being built through conversations with the people that the technology is actually expected to be helping? And so this is something that was sort of an ongoing problem within the tech industry, where user experience is really kind of an afterthought. Or maybe it's sort of tacked on towards the end of product development, where you really just sort of do some usability testing and make sure it kind of works well enough most of the time. But to actually try to develop AI with the real input of the communities who, in theory, are going to be using and benefiting from the technology is something that really isn't done most of the time. And so I would say with a lot of the sort of climate related technologies that are being put out on the market, the question is, what does it look like in practice? And this is something that I think AI doesn't seem to attract the same kind of ire that crypto did, because crypto was very much viewed as a waste of energy. Right? Like, people really hated crypto pretty early on, and there were a lot of people talking about the fact that it was a scam and that it was, you know, just wasting all this energy for no reason. Everybody hated NFTs. Not everybody, but a lot of people, right? It had a lot of detractors. So the question is around kind of people making claims about what blockchain could do in terms of, oh, hey, if we tokenize trees in the Amazon, that will incentivize people through financialization, essentially not to cut down all the trees. And so this idea that if you assign monetary value to things, to natural resources in the world, like trees, like whales, that essentially it will help people treat the planet better and that it would, in theory, also be helping indigenous groups or something. And this was very often shown not to be the case. It was actually just a different form of exploitation. It was a different kind of scam. It was often sort of just another form of colonialism, because these things were not developed with the consent or the priorities of the people that it was actually trying to help. And so my fear would be that with a lot of the sort of climate related AI products that maybe have good intentions, it doesn't necessarily mean that they're going to be executed in a way that is actually helpful. I want to talk a little bit about algorithmic impact assessments. Specifically, you have a piece on algorithmic impact assessments, and you wrote in that piece, and I'll quote you here, that \"we know when people and ecologies meet technical systems, there will always be unanticipated consequences\". So how do we identify and document a push back against these sometimes ambient harms? Well, it's interesting because I think a lot of it does have to do with getting input from the people who are going to be impacted earlier in the process. So there are, you know, in a lot of the cases that we've been involved with, in a lot of the different sort of research projects and collaborations that we have going on right now. And I can't speak, you know, too specifically on them at the moment, but I can say that often when you engage a community, let's say a particular community, in a location that may be particularly invested in knowing about how a technology is going to be used and might have some questions about the technology, it really is helpful to get their perspective, because often they will raise issues or create scenarios for, say, a chatbot that the people creating the technology, not just the technologists, but also, you know, perhaps, like, advocacy groups and others who are really trying to do the right thing, they may not see the big picture. There might just be things that they miss. And sometimes bringing in community members earlier in the process can really help you kind of see the potential problems before they become a really big problem. And so that, I think, you know, of course, there could always be unintended consequences. Even after you've really put in that kind of due diligence and you've been as thorough and ethical as you possibly can be in engaging every potential community that could be impacted. And there may still be things that you'll miss, but I think it's less likely that you would actually have a huge number of things that are as glaring if you actually put the time into fully assessing the technology before you deploy it and maybe even before it's fully designed. Another issue is being able to document potential harms and raise red flags around things like privacy or bias that may not have been obvious to the developer and then actually have time to give that input to the technical team. So another question is, are you kind of just bringing community members in, and you're just planning on sort of having a rubber stamp, but not really changing or modifying the tech in any way? I'm still releasing it. And so you really need to kind of encourage developers to build in time to really fully assess the potential harms and actually work on fixing those issues before the thing is released. There's also a question of, should the technology be developed and released at all, and how do you sort of begin to weigh the cost and the benefits? And that can be quite tricky, but in a lot of cases, it may not be that the technology is actually doing the thing you want it to do at all, and there may be social problems that are happening that really require a different kind of solution. That AI just actually has nothing to do with it. Is there a better way to actually allocate resources? Is another question. Should this thing be built at all? And I think that has to be an open question when you're sort of engaging in this process, rather than people just sort of digging their heels in and putting a product out there that is probably going to end up wreaking havoc in some way. I want to push into this a little bit further because you talked a little bit about identifying and assessing the harms. Pushing back, I think, is a different kind of problem. When you discuss the possibility that AI product developed to do one thing or fix one kind of social problem actually isn't doing it, then, yes, you have demonstrated that the product maybe doesn't work as it is supposed to work, or maybe there's a non technical solution that might be better. But there's a kind of famous anecdote that I'd like to pull out to discuss the kind of problem with this. And both of us live in San Francisco, so both of us are aware of the problems around homelessness in San Francisco, one of which is the problem of the sanitariness of the city. And those of us who have traversed the streets of San Francisco recognize one particular sanitary problem, which is that homelessness has left a situation where oftentimes there are human feces on the streets. And an enterprising group of engineers in the Bay Area a couple of years ago came up with a solution to this problem, which was to develop AI poop, picking up robots that could detect human feces on the street and then deposit it into a central repository, thereby cleansing the streets of human feces. And when Dan Lyons, who was on the show many years ago, wrote for Silicon Valley, narrated the story. He said to me, you know, what was actually the better solution to the problem? Public restrooms. Public restrooms. Right. It's not a technical solution. But of course, I think what Dan's comment in many way misses is that the incentive is not just to solve the problem. The incentive for developers is oftentimes to recruit funding to create a product and then to sell the product and the company ultimately to get acquired or to get venture capital or to leverage that company into something larger. And so now we're talking about the kind of economics that undergird what gets developed and why, and the differences between creating a product that is marketable and sellable and solving the problem. Not always the same thing. Right? So I guess the question here is, what do does you pushing back actually look like? And how does one take the kind of understanding, documentation, identification and assessments you're talking about and mobilize it into pushing back? Yeah, it's a great question. And it's actually my favorite part of this kind of work, because I also do have a background as a labor organizer. And I think one of the most important aspects of this work is figuring out how to take any sort of empirical findings you have and then translate them into policy changes, into legal strategies and also bargaining strategies. And so how can labor groups, for example, figure out how to document the harms that are happening? How do you sort of prove that something like algorithmic wage discrimination is happening within a gig app? And then how do you then take that information and effectively lobby and advocate for changes to policy? How do you advocate for sort of some kind of legal recourse? And so knowing sort of what the sort of burden of proof would be right for any form of algorithmic discrimination. And so, you know, this is why, as my colleagues found when they were looking at sort of the, the New York City local law 144, which was requiring employers who were using automated employment decision tools to audit them in order to figure out if they were biased according to race or gender. And basically, a lot of these laws are not necessarily actually effective because it's actually quite hard to even demonstrate this sort of, this sort of bias. It actually is not always completely obvious, and it requires, in some cases, a real sort of technical knowledge or access that people don't have. And so one of the groups that we've been working with at AIM Lab is the Workers' Algorithm Observatory. And they're a group of researchers who are based at Princeton, but they're working with rideshare drivers who are trying to really conduct what they call a form of algorithmic inquiry. So from the marxist kind of workers inquiry, which is a way that workers kind of document data about themselves, and it's a way for workers to really use data collection to understand their working conditions and how to change them and to kind of compare notes about what is happening to them on the ground. And so for rideshare drivers to be able to sort of begin to share information with each other in a comprehensive way in order to really be able to get to the root of algorithmic wage discrimination so that it can be part of some kind of organizing strategy is incredibly useful. So another part of this is really trying to figure out what forms of research and what kinds of research questions are actually beneficial to the people that you're trying to advocate for. I guess I want to push a little bit more into this because I'm thinking about, you know, what you brought up, which is the kind of power differential between those who create AI and those who are subjected to it. What options do we have to push back on tech companies broadly, and AI companies specifically, companies that have oftentimes accrued both tremendous power and wealth, who have set norms and standards that maximize their ability to profit and also to limit their liability for the harms that they cause, and who are developing products with a mass reach in ways that have become so deeply institutionalized and part of our environment that we don't really have the ability to alter their effect or to extract ourselves from them or to say no, and who also have massive lobbying power and money to fight lawsuits, and whose wealth and profit often means that they can afford to pay fines that they may accrue from causing harm or violating a regulation, while continuing to pursue business practices that provide profit as normal, since the profit that they would get from creating the product and distributing it and deploying it oftentimes exceeds massively any penalty, financial or otherwise, that they might accrue from violations. Right. And I think this really gets to the root of the problem, and I can kind of tie that back a little bit to the environmental concern of AI as well. And so the fact that you really just have a handful of companies that completely control the production and use of large scale AI right now is quite, quite frightening. And so, you know, in order to have access to the technology, in order to do the kind of work you want to do, you really need to be tied to just a handful of companies and very elite research universities. And I think this sort of larger problem of how much control over people's lives should tech companies have. I mean, this is something that I obviously also write about in my book, where I'm really sort of grappling with the fact that platforms have an outsized amount of control over how people are memorialized and over how they're able to mourn as more and more people sort of use various social media platforms and other digital assets in order to maintain relationships with the dead or to try to create a kind of legacy for themselves. And I actually don't know if a lot of the sort of regulation that we have on the immediate horizon is really going to be enough to change that power dynamic. I think we would really need to think about the production of technology. We would need to really think about the system that we live in and change it in a much more radical way. And am I suggesting that we would need to basically stop a model of endless growth where profits and the well being of shareholders is held up above the welfare of everyone else on the planet? Yes, I think we would need to kind of radically rethink and reformulate the ways that we think about what a successful sort of product or a successful company looks like. But I'm not sure if I have a whole lot of hope of that happening. And so I think until we have some sort of really massive structural change, the best that we can do is sort of exert pressure from different areas. And so I think it is actually important to have people working from within tech companies who are pushing for some kind of change. I think you do need people inside who understand how power works internally and who are, you know, at least close to the machine in that way. I also think that you need policy changes and pressure from grassroots organizations, advocacy and civil rights organizations, legal advocacy groups, and other sort of labor organizers. And I think you do need also academic researchers who are able to sort of lobby critiques of tech and sort of these systems a bit from an outside perspective. But you really need kind of all of these things to work together in tandem. And you certainly need the input from the communities who are going to be most impacted, which is often even within the more participation based imaginings of impact assessment or thinking about the power of technology, it still is a problem because the company or the people who are attempting to bring communities in, for instance, us at AIM Lab, we're still kind of setting the terms right, like it isn't really, you know, we're sort of helping perhaps give marginalized people a platform, but the power dynamics are still a little bit off. Right. And so I think really thinking about how we can kind of collectively have more power over how technology is produced and used. That is what ultimately we need to have happen. I want to dig in a little bit into this idea of harm, because sometimes when we talk about harm, we talk about intentional harm. Sometimes we talk about harm as an unfortunate byproduct or an unintended byproduct of a certain technology's deployment. Can we, or should we make distinctions between different kinds of ways in which tech products cause harm? Between, for instance, a product that causes harm in unintended ways versus products that are released and cause harms in ways that are predictable but willfully ignored? Now, sometimes I have this conversation with somebody and their argument is that if we look more closely, almost always these so called unintended consequences are actually very predictable, but that either they are overlooked in favor of profit or progress, or that these, quote, unpredictable consequences were the result of negligence caused by overlooking or excluding already marginalized populations. Or alternatively, that the value system of tech production that already exists disregards the idea that the harm caused by the product is actually really harm to begin with. I'll give you an example of what I mean by the last part. The idea that something will take over jobs or something that will disconnect us from our immediate environment is not isn't actually a harm because it is providing efficiency. And in the context of this product's developers, efficiency is the value to maximize and maybe enjoying our environment or doing things in unpredictable ways because there isn't an algorithm guiding us about what exactly to do isn't actually a harm to begin with. Right? Some of us might think that the removal of the incidental or the coincidental from our environment, the capricious from our environment, might be a harm for those who are developing these technologies and algorithm that has a greater ability to predict and direct us in certain ways might be a benefit, and the loss of that would be be a harm at all. I do think that there are instances in which tech products cause harm as the result of bad actors using the product in unintended ways, or ways that a technology can go wrong despite the intentions of the creators. But I am also aware that the majority of harms that I think we're talking about are harms that come even as good users are using the product as it was intended to be used. How do you think about the idea of unintended consequences when it comes to what you see in tech culture? Yeah, that's a great question, and this is why I think we really need more historians to be in these spaces I'm always very happy to bring historians into the conversation whenever I can. And it's something that I actually did at Intel. So I brought in Mar Hicks to talk about the history of queer computing. I also brought in Cassidyre to talk about the history of trans computing and getting people in tech to think about how actually the thing that you're looking at and you think is brand new and shiny and full of innovation is actually connected to something that happened a long time ago that is similar. And we can kind of learn from the past to understand what these potential harms might be. And so something that came up a lot when I was at Intel, it was sort of the sort of middle of the metaverse fervor. And people were very excited about the metaverse and had a lot of ideas about what it might look like and what it might do for innovation and business and the future of work. And, you know, talking about some of the issues around sexual violence or women being harassed in sort of metaverse spaces, it was like, well, you know, if you kind of took a look at people who were users and people who are researchers of second life, or going back way further, you know, like, we could go back to, you know, Julian Dibble's a rape and cyber phase from 1993, talking about LambdaMOO. And so really beginning to frame sort of these problems about technology that look to be brand new and understand sort of the social dynamics and power structures that made them appear in different contexts. And so I think, for me, just being able to really use examples from the past and from other areas when you're talking about a new technology and trying to assess potential harms is actually quite useful because it really is, and sort of bigger than just a few bad actors or people who need to be removed from a platform. And it also is sort of a bigger problem than companies just creating something intentionally harmful to reap profits. But there's just obviously, we have a lot of bad things in our society. There's a lot of racism. There's still a lot of sexism, a lot of homophobia. There's just a lot of inequality. And to build things kind of without acknowledging that reality and without acknowledging these larger histories, I think, is when you really run into problems, because, you know, these things should be really obvious, it should not be hard to anticipate what some of the problems are going to be. Just a quick plug for folks who are listening to this episode to go check out Mar's episode of this show, we had a wonderful interview where she talks a lot about the importance of the present moment of technological production being informed by and thought through the narratives and the histories of the past. I want to ask you a question about a piece that you published recently in Wired. That piece is titled, using generative AI to resurrect the dead will create a burden for the living. The byline of that piece is AI technologies promise more chatbots and replicas of people who have passed of giving voice to the dead comes at a human cost. What are the specific concerns that you have about generative AI when it comes to environmental impact? And what are the human costs that you are talking about in that piece? Yeah, so, you know, in terms of preserving the data of the dead, it kind of gets at this larger kind of problem that's been very pervasive in the tech industry for quite some time, which is, you know, data is power. So therefore, for individual users, we're going to help you. We're going to collect as much data as we can about you, and we're also going to help you maintain your personal data forever. And so Google kind of had that line of reasoning for much of its existence, but recently it actually said that it was going to start deactivating the accounts of people who had been inactive for two years because there's some degree of recognition on the part of tech companies that, hey, wait a minute, if we actually promise to maintain everybody's data in perpetuity, that's a hell of a lot of data, given how much data people are creating, and that actually costs a lot of money. There is no cloud. I feel like people in STS are always railing about the materiality of computing. It's like, no, it's the undersea cables. No, these things are material. They're not ethereal. But I think that just becomes even more obvious when you're talking about something like generative AI, which, again, requires even more compute power. And so the expansion of data centers to kind of feed the hunger and the desire for generative AI is a problem because it will have a massive environmental impact. But then the idea that you would then sort of maintain something like what? You're going to maintain this sort of extremely powerful AI for all of eternity because it's actually like a simulation of your dead loved one. And so I think the ethics of sort of imagining that you're going to be able to perpetuate this kind of relationship with the dead through this kind of technology is also very misleading because these things will actually require system upgrades and software updates and maintenance, and eventually they may no longer run and they may become obsolete and the technology itself may die. So thinking about the fact that many people would then undergo a second form of grieving after having experienced grief already. So there are all these sort of ethical questions around who should actually have the authority to decide to create a simulation of a dead person through generative AI, and who should be able to kind of maintain that relationship and control that relationship. And should companies or employers or estates be able to profit from the kind of simulated version of that dead person? And so that gets into really thorny questions around not just kind of copyright, but also things like estate planning and kinship relations and also consent. So thinking about what it is that an individual really would have wanted and what their family members actually want, versus maybe somebody else who decides to revive the dead person. And so I think the sort of general problem of creating AI versions of the dead, which is not a new problem, but just something that is sort of in the news a lot right now, particularly because of deepfake technology and because of a lot of dead celebrities and such that have been revived for various specials. And so I think this sort of question of violation, and this is something that came out when Anthony Bourdain, when there was a documentary made of his life and the director made the decision to use a deepfake voice, to have Bourdain narrate this letter that he had written. It was his words, but the deepfake was just sort of seamlessly put into the documentary, which is a bit uncanny, you know, to sort of revive the dead in that way. But it was also done without the consent of bourdain's family. And so, you know, this is the kind of question that comes up a lot when we're talking about celebrities, but it's an issue for everyone. And, you know, the fact that, you know, for actors who are having their bodies scanned or models, anyone, academics who have all of their Zoom recordings available, the idea that your employer or that somebody could kind of try to monetize your likeness after you die in order to prevent them from having to hire living people is actually a lot of what the sort of fantasy is behind this. And so just thinking about how this actually creates a lot of problems in the realm of labor on top of all of the other terrible things that I just mentioned. Well, I want to dig into this a little bit and ask you to explicate a little bit more the links that you're making between labor and environmental damage and the broader context that you're talking about here of the attempts to preserve data, and in particular preserve the data of the dead at all costs, and in fact, at a very high cost. I know that you've written very extensively about the way that socio technical systems, particularly social media platforms, and increasingly AI and generative AI, may be changing the way that our culture thinks about and performs rituals around and navigates death. In your book, which you were just you were just talking about, Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond, takes up this inquiry. Can you help us better understand and maybe spend a little time digging into the link between the questions that you take up in the study and the current work that you're doing with the AI impact lab at data and society? Yeah, definitely. So I really feel like the problem of death is sort of, it's a very old human problem, and it's a fairly universal problem. So despite what the transhumanists who tell us that we can upload our brains, no matter what they say, death is definitely coming for everyone. And people have always had different ways of memorializing the dead. And the way that people treat the dead can also certainly reveal different sort of social relations hierarchies within a society. And so, of course, if we go back to archaeology or something, we can look at the mortuary rituals around kings and queens versus commoners. There's always a way to sort of understand the way that a society is structured through treatment of the dead. And so I find it really fascinating to look at the different ways that death kind of disrupts the original plan plan for a lot of technologies. So technologies like social media, platforms that were built for youthful users who were at elite colleges primarily in the beginning, and the idea that they would somehow become spaces for memorialization, for long term relationships with the dead, for mourning. This is not something that was an immediate sort of use case for them. And so I think death is actually a very useful way to kind of begin to think about that was something really obvious in a lot of ways. I mean, you know, death is definitely, again, you know, it's a universal thing and it's kind of everywhere and will come for us all, but it still is something that's very easy to overlook. And so it's just, how could it be that every single company always forgets about death, and then they forget about it repeatedly over time? And this is not an area that, you know, tech companies largely have focused on. This is not, people don't have a lot of big UX teams devoted to death. This is not something that is sort of the norm, even though it is something that happens all the time. So I think when we're talking about the larger problem of ethics and technology, when we're talking about how communities are going to be impacted, how individuals and their social networks are going to be impacted, that kind of relational aspect, that's something that I think is really an important lens for considering impact assessments for algorithmic systems overall, because we're really not just talking about the tech itself and auditing it to understand is the tech doing what we think it's doing, but we're also trying to examine all of the social relations that are clustered around the technology and what all of the potential downstream impacts are. And so I think death is one of those things that really helps you see a lot of those relationships and networks over time in a way that maybe other social things are not quite as rich. I think that death is an incredibly rich field site for really thinking about algorithmic impacts. I want to talk about a word in your title, that word techno solutionism. How do you think about techno solutionism as an ideology and as a practice and as a vision for how we move forward in our society? As a society, what does techno solutionism and its cousin, techno utopianism, get right? What do these terms and the visions that they represent miss or misunderstand? So I think there are aspects of techno solutionism that can be useful. So, for example, if you're kind of attempting to build the technology and you notice that there's a particular pain point or a point of failure, and you're able to make a small change in the technology itself, and then it runs much more efficiently. In that case, yes, there may very well be a solution to a problem that is really just a technical fix that will do a lot of good. But I think the larger problem is sort of this common issue of, as you mentioned before, where there's a problem that is very much a social problem that could be fixed by something that really has nothing to do with technology, and yet technology is kind of brought in anyway. But you also do have a problem where even the process that I described right, where, you know, with AIM lab, we're kind of helping different groups and different organizations understand the technology that they're building and help them kind of develop relationships with the potential communities that will be impacted by the technology. And so part of that process is really about maybe taking information back to the technical team and improving the technology and preventing bugs from occurring or also mitigating any potential impacts that would be harmful. But another part of the process that really has nothing to do with technology is the creation of relationships. The idea is that the partners that we're collaborating with, the hope is that they will continue to be in dialogue with the different communities that we're engaging with. We're not wanting it to be a kind of one off thing where, you know, okay, so we took information back to the technical team. We tweaked it a bit, and now we're done. The idea is that it should be something that continues, and it should be a long term relationship and process. And really, I would say that for a lot of the work that we're doing, that is the most important piece of it is really about the relationships with techno solutionism. I think the problem is that so often the focus on technology kind of outshines all of these other factors, and you can really miss the forest for the trees. And with techno utopianism, I mean, I think it is better to imagine other futures. So I think that we can learn from feminist and black Sci-Fi traditions. If we could think about afrofuturism, there are ways of imagining futures that are not in the vision of Elon Musk. There are alternatives to that kind of techno imaginary. And so it would be a mistake to completely throw away anything that's possible with the imagination. And I think that sort of reimagining technology, but then also building it differently, these things have to go together. So I think it's great to have the imaginary of how things could be different. And then I think we really need to kind of dismantle the way that tech is currently being built and deployed in order for that, some degree of that kind of imaginary to be. To be realized. And so for me, I am definitely interested in the magical and mystical and metaphysical qualities of technology. So I think that using technology to communicate with the dead is a thing that we've done as humans for a very long time. And you don't need to use a digital form of technology for that to happen. And so I would say that I am really interested in sort of the transcendent qualities of technology. And I believe that people derive a real sense of connection and something sacred out of technologies that are incredibly mundane. So I would not want to dismiss that. I think that there is a certain sort of magical quality to technology that can be a good thing, that can be not sort of the fetish of masking unequal labor relations, which it also is. But I'm interested in sort of digging into the other uses of technology that might kind of expand that. I think we have time for one last question. I teach a course on data and human values at UC Berkeley, and a lot of students there and elsewhere across college campuses and universities listen to this show. And I know that you come to data and society as we've talked about after many years as a professor working with students in the academy, many of them are going to end up in the tech industry. What would you want them to know or think about or understand or reconsider as they move into their careers? Yeah, it's a great question. And I have to say, when I talk to younger generations of people who want to go into tech, there is much more of an awareness of the potential problems. And people actually seem quite hungry to have some degree of training in things like STS and critical technology studies in general. So I think the important thing is to read widely and sort of talk to people outside of tech. I would say the most important things are to understand the history of technology, understand the history of the tech industry. So look to organizers from the past. And that was something that I did with a number of my collaborators for the tech workers coalition teach in, where we basically brought people from, IBM, black Workers alliance from the 1970s, and a number of other activist groups that had been really prominent decades before, along with current activists who were trying to change things from within in the tech industry now. And it was really interesting to have people from different generations talking to each other. Or again, the reason why I brought Mar and Cass and to talk to colleagues at Intel, just a way of helping people realize that there have been activists in tech. There are people been doing this kind of work in coalition building with people outside of tech for a long time, so you don't have to reinvent the wheel. You can kind of learn from people who came before you. And then the other important thing is to continue talking to people who are not in the tech industry. So not just on a kind of, like, interpersonal level, but also if you are interested in making tech more ethical, instead of just sort of, you know, taking one ethics class in data science or computer science, really try to, you know, expand your relationship with different communities that you are around in your day to day life. So one example that I can point to is knowing some developers that really, you know, sort of large tech companies who take on volunteer positions, working with, say, middle schoolers in Oakland who want to learn how to design video games. And so volunteering your time or getting to know different kinds of communities and getting to know their concerns and being more sort of involved with issues where you live can also be really, really helpful. So then I think that especially goes for those of us who live in the Bay Area. So how do you sort of make sure that you are firmly situated in a place and that you're not just sort of an interloper who spends all of your time only talking to other tech people? Thank you so much, Tamara. Thank you, Deb.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('taking_the_temp_of_ai.txt') as file:\n",
    "    line = file.readline()\n",
    "    print(line)\n",
    "\n",
    "    podcast = {    \n",
    "        \"episode_number\": \"taking_the_temp_of_ai\",    \n",
    "        \"transcript\": line,\n",
    "    }\n",
    "    podcast_data.append(podcast)\n",
    "\n",
    "print(len(podcast_data))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "#   final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-28 21:02:40.243982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 21:06:51.198839\n",
      "chunks_text len: 66\n",
      "extract_keypoints start time: 2024-03-28 21:06:51.198970\n",
      "extract_keypoints done time 2024-03-28 21:09:13.892703\n",
      "Start time: 2024-03-28 21:09:13.892908\n",
      "Stage 1 done time 2024-03-28 21:11:22.065981\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Exploring Ethics and Technology in the \"Technically Human\" Podcast ', 'text': 'Deb Donig hosts the podcast \"Technically Human\" about ethics and technology. The podcast explores the relationship between humans and the technologies we create. Deb Donig interviews industry leaders, thinkers, writers, and technologists. The goal is to build a better vision for technology that represents human values. Doctor Tamara Kneese is the project director of Data and Society\\'s Algorithmic Impact Methods Lab. She is also a senior researcher at the lab. She is a visiting scholar at UC Berkeley\\'s center for Science, Technology, Medicine and Society for the 2023 and 2024 academic year. Before her current roles, she was a lead researcher at Green Software Foundation, director of developer engagement on the green software team at Intel, and assistant professor of media studies and director of gender and sexuality studies.'}, {'title': 'The Impact of AI on the Environment ', 'text': \"Tamara is an assistant professor of media studies and director of gender and sexuality studies at the University of San Francisco. She holds a PhD in media, culture and communication from NYU and is the author of Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond. She is a volunteer with tech workers Coalition in her spare time. Congress introduced a landmark bill that would move the government toward developing standards to measure and report the full range of AI's environmental impact. The legislation also requires an interagency study to investigate and measure both the positive and negative environmental impacts of AI.\"}, {'title': 'The Need for Robust Research on Environmental Impacts of AI ', 'text': 'The bill calls for robust socio-technical research to measure the environmental impacts of AI. It is difficult to measure the impacts around every single part of the AI supply chain. The environmental impacts include the energy used in AI interactions, the manufacturing of AI hardware, including chips and raw earth minerals. The bill addresses the need to consider the entire supply chain of AI, not just the direct interactions with AI systems.'}, {'title': 'The Importance of Legislation and Coordination in Measuring the Environmental Impact of AI ', 'text': 'AI production and use can have various effects around the world. Legislation pushing for the creation of standards around measurement and reporting is important. Measuring the environmental impact of AI is challenging due to global supply chains. Coordination across different parts of the supply chain and companies is necessary. The bill is relying on not just the EPA, but other entities as well.'}, {'title': 'The Importance of Methodologies and Resources in AI Development ', 'text': 'The bill relies on the EPA and the National Institute of Standards and Technology to identify the methodologies and standards needed for measurement work. Generative AI requires specialized forms of chips and a large amount of power, energy, and water. Researchers are trying to determine the amount of energy and water needed to train large language models.'}, {'title': 'The Environmental Impact of Large Language Models and Data Centers ', 'text': 'Large language models require a significant amount of energy and water for training. The deployment of AI has become a growing problem due to the tremendous energy consumption of the entire ICT sector. Data centers have negative effects on local areas, especially those already experiencing drought or grid issues. Data centers require a massive amount of energy, can pollute the areas they are located in, and consume a lot of water, making it an environmental justice issue. There is a concern about the control of resources and the potential cost in terms of environmental impact.'}, {'title': 'Environmental Contamination in Silicon Valley ', 'text': \"AI is just one small part of a much larger problem that's been going on for quite a long time. The manufacturing of chips in the 1980s in Silicon Valley led to toxic chemicals getting into the local water supply in San Jose. Community members had to come together and fight back against contamination, asking for more regulation to prevent similar incidents in the future. The current AI boom is a good time to focus on making changes.\"}, {'title': 'The Environmental Impact of the AI Boom ', 'text': 'AI boom and the need for regulation to address environmental costs. Range of problems and concerns related to generative AI, including job loss, compensation, copyright, misinformation, and changes to information structure online. Environmental impact as a significant concern related to generative AI.'}, {'title': 'The Intersection of Environmental Impact and Labor Exploitation in the AI Supply Chain ', 'text': 'The environmental impacts of AI are connected to labor problems. Focusing on the environmental impact of AI forces a more holistic view of potential harms. The entire AI supply chain involves many different parts of the world and types of labor exploitation. There is a trade-off between different aspects of AI, such as environmental impact and labor exploitation.'}, {'title': 'Negative Effects of Data Centers on the Environment and Communities ', 'text': 'Data centers can have negative effects on the environment and surrounding communities. The trade-off of bringing a data center to an area, such as job creation, may not always be true. Data centers can be loud and disruptive to the people living around them. There are concerns about the effects of data centers on the health of people in the area. Data centers consume a significant amount of power and water, leading to potential environmental issues.'}, {'title': 'The Environmental Impact of AI ', 'text': 'The discussion of the environmental impact of AI is closely linked to labor and ethics concerns. It is important to consider the impact of AI on entire communities, habitats, ecosystems, and other species, not just on humans directly. Researchers have focused on measuring the carbon emissions associated with training and deployment of AI.'}, {'title': 'The Carbon Cost of AI: Lifecycle, E-Waste, and Social Impacts ', 'text': 'The importance of considering the carbon cost of AI throughout its lifecycle, including training, deployment, and potential e-waste. The significant amount of hardware used in creating and deploying AI, leading to a large amount of hazardous e-waste. The need to measure impact at both a technical level and in terms of social impacts, including speaking with impacted communities. The potential for bridging the relationship between technical assessments and social impacts to better understand the overall impact of AI on carbon emissions and communities.'}, {'title': 'The Importance of Integrating Technical Assessments and Community Impact in Addressing Carbon Emissions ', 'text': 'The need to bridge the relationship between technical assessments of carbon emissions and the downstream effects on communities. The difficulty in measuring the impact of technology on communities. The importance of framing the problem by considering both technical assessments and community impact. The experience of working on a sustainability team at Intel and the focus on creating software to optimize technology. The example of choosing to train a large model at a time of day when more renewable energy is available on the grid.'}, {'title': 'Empowering Developers to Make Environmentally Friendly Choices ', 'text': 'The focus of the research was to make choices more visible to developers and empower them to make environmentally friendly choices. The approach to measuring and reporting environmental impacts of technology differs between technical and grassroots community groups. Aim lab is working to reconcile the technical approach with a socio-technical perspective, considering both the technical and social factors.'}, {'title': 'Title ', 'text': 'Understanding the Intersection of Technical and Social Factors in AIText '}, {'title': 'The Importance of Understanding Problems in AI Research ', 'text': 'The importance of having a broad understanding of the problem in AI research. The need to look at the problem from different perspectives. Different sectors defining the problem in different ways. The variation in the questions asked by different people. The existence of points of contention among people.'}, {'title': 'Differences in Timelines and Expectations Between Academic Researchers and Tech Companies ', 'text': 'Academic researchers work on different timelines than tech companies, with a focus on long-term projects. Tech companies are focused on quarterly financial systems and shipping products, while academic researchers are not. The expectations for the duration and effectiveness of research projects are different between academic researchers and tech companies. Conducting socio technical research within a tech company requires demonstrating return on investment and real impact.'}, {'title': 'The Impact of Research in the Tech Industry ', 'text': 'The impact of research in the tech industry is measured by its direct influence on product design and sales. Carving out space for longer term research in tech can be challenging. AIM Lab aims to provide socio-technical research expertise to the tech industry. Collaboration between independent researchers and tech industry professionals can lead to valuable perspectives and solutions.'}, {'title': 'Addressing Climate Impacts and AI through Decarbonization ', 'text': 'Sharing perspectives with people addressing climate impacts and AI through decarbonization alone. Helping them think through the user experience and downstream impacts. Changing perspective and reframing the problem. Understanding power dynamics and hierarchies in tech. Knowing how to talk the language and have a sense of how power operates.'}, {'title': 'Algorithmic Impact Assessment in Various Organizational Contexts ', 'text': 'Understanding the methodologies for algorithmic impact assessment in different organizations. The focus on startups, large scale enterprises, city governments, and other contexts in addition to major tech companies. The tensions around climate change, environmental harms, and AI. The ways that AI may be helping to solve or mitigate climate change and environmental harm.'}, {'title': 'Challenges and Innovations in Environmental Technology ', 'text': 'Streamline use of environmental resources and reduce waste, create new technologies to reduce dependencies on harmful products or counteract environmental damage. Mass energy requirements of data centers and environmental and energy expenses to run e-waste. Environmental damage caused by mining for resources required for tech products. AI industry creating jobs and the challenge of filling those jobs. Government funding and the libertarian perspective on it.'}, {'title': 'Controversy and Criticism of Government Retraining for New AI Jobs ', 'text': \"1. The controversy surrounding the government's role in providing retraining for new jobs.2. The criticism and study of the labor crisis caused by the emergence of new AI industries.3. The particular set of skills required for the new jobs that many people may not have or want to have.4. The potential environmental impact of the new jobs and the absence of many of the old jobs.5. The debate over whether the old jobs were good for the environment and if they are missed.\"}, {'title': 'The Impact of Technology on the Environment ', 'text': 'Jobs that were not good for the environment are not particularly missed. Outdated technology like the horse and buggy caused pollution and city sanitary problems. Technology is being developed in the name of being climate friendly and furthering goals of climate justice. AI could be used to help farmers, particularly in drought-stricken areas.'}, {'title': 'The Potential of AI Technology in Addressing Environmental Challenges ', 'text': 'AI can be used to help farmers in drought-stricken areas. AI can be used for deforestation mitigation and keeping coral reefs healthy. The effectiveness of AI technology in real-world applications is a concern. The interaction and interface of people with AI technology is important. The impact of AI on the work that people are already doing needs to be considered.'}, {'title': 'The Importance of User-Centered Technology ', 'text': 'Technology should actually help with the work that users are already trying to do, rather than creating new forms of work or maintenance. User experience is often an afterthought in the tech industry, with usability testing being tacked on towards the end of product development. Development of AI should involve real input from users.'}, {'title': 'Challenges in Technology Development ', 'text': 'AI development often lacks real input from the communities who will use and benefit from the technology. Climate-related technologies need to be evaluated for their practical impact. AI does not attract the same level of criticism as crypto, which was viewed as a waste of energy and a scam. NFTs had a lot of detractors and were widely criticized. Claims about what blockchain could do, such as tokenizing trees in the Amazon, need to be scrutinized.'}, {'title': 'Tokenizing Natural Resources and Climate-Related AI Products ', 'text': 'Claims about using blockchain to tokenize natural resources like trees and whales to incentivize conservation. Concern that assigning monetary value to natural resources may lead to exploitation and colonialism. Doubt about the effectiveness of climate-related AI products in actually helping the planet and indigenous groups.'}, {'title': 'Importance of Algorithmic Impact Assessments ', 'text': 'Algorithmic impact assessments are important to consider when technical systems interact with people and ecologies. Unanticipated consequences can arise when people and ecologies meet technical systems. Input from the people who will be impacted should be gathered early in the process. Engaging with the community in a location can provide valuable insights and perspectives.'}, {'title': 'The Importance of Community Engagement in Technology Development ', 'text': 'Engaging a community in the technology development process can provide valuable perspectives and raise important issues. Community members may create scenarios or raise concerns that the technology creators may not have considered. Involving community members early in the process can help identify potential problems before they become significant issues. Even with thorough due diligence and ethical engagement, there may still be unintended consequences.'}, {'title': 'Importance of Ethical Engagement and Assessment in Technology Deployment ', 'text': 'Due diligence and thorough ethical engagement with potential impacted communities is important. Fully assessing technology before deployment and design can help identify and address potential issues. Documenting potential harms and raising red flags around privacy or bias is crucial. Encouraging developers to build in time to fully assess potential harms and modify the technology accordingly is necessary.'}, {'title': 'The Importance of Ethical Considerations in Technology Development ', 'text': 'Developers should build in time to assess potential harms and fix issues before releasing technology. There is a question of whether technology should be developed and released at all, and how to weigh the cost and benefits. In some cases, technology may not be achieving its intended purpose, and there may be social problems that require a different solution unrelated to AI.'}, {'title': 'Limitations of AI Products for Addressing Social Problems ', 'text': 'AI products may not always work as intended or may not be the best solution for a social problem. Non-technical solutions may be better for certain social problems. The example of homelessness in San Francisco is used to illustrate the limitations of technical solutions for social problems. San Francisco has a problem with the sanitariness of the city due to homelessness. Human feces on the streets is a specific sanitary problem caused by homelessness in San Francisco.'}, {'title': 'Addressing Homelessness and Public Health ', 'text': 'Homelessness has led to human feces on the streets. Engineers in the Bay Area developed AI poop picking up robots to address the issue. Dan Lyons suggested that public restrooms would be a better solution. The incentive for developers is often to recruit funding, create a product, and sell the company.'}, {'title': 'The Importance of Strategic Goals and Economic Understanding in Business Development ', 'text': 'The ultimate goal of the company is to either get acquired, get venture capital, or leverage the company into something larger. The importance of understanding the economics behind product development and the difference between creating a marketable product and solving a problem.The Role of Empirical Findings and Labor Organizing in Policy and Legal Strategies '}, {'title': 'Addressing Algorithmic Discrimination in Labor Advocacy ', 'text': 'Labor groups need to document the harms of algorithmic discrimination. They need to effectively lobby and advocate for changes to policy based on the documented harms. It is difficult to demonstrate algorithmic bias, making it hard for laws to be effective in addressing the issue. New York City local law 144 required employers using automated employment decision tools to audit them for bias based on race or gender.'}, {'title': 'Addressing Bias in Algorithms ', 'text': \"Bias in algorithms is difficult to demonstrate and may require technical knowledge or access. The Workers' Algorithm Observatory at AIM Lab is working with rideshare drivers to conduct algorithmic inquiry. This inquiry is based on the Marxist workers inquiry, where workers document data about themselves to understand and improve their working conditions. Rideshare drivers are sharing information with each other to better understand and address their working conditions.\"}, {'title': 'Understanding Algorithmic Wage Discrimination and Holding Companies Accountable ', 'text': 'Sharing comprehensive information is crucial for understanding algorithmic wage discrimination. Research should focus on questions that benefit the people being advocated for. Exploring options to push back on tech and AI companies with power and wealth. Companies need to be held accountable for the harms caused by their products with mass reach.'}, {'title': 'Title ', 'text': 'The Impact of Powerful Companies on Environmental and Social HarmText '}, {'title': 'Control and Influence of Tech Companies in AI and Society ', 'text': \"Large scale AI is controlled by a handful of companies, which is frightening. Access to technology and work is tied to a few companies and elite research universities. Tech companies have an outsized amount of control over people's lives. Platforms have control over how people are memorialized and able to mourn. Regulation may not be sufficient to address these issues.\"}, {'title': 'Rethinking Technology Production and System Reform ', 'text': 'The need to rethink the production of technology and the system we live in. The necessity to radically reformulate the definition of a successful product or company. The lack of hope for massive structural change. The importance of exerting pressure from different areas.'}, {'title': 'The Importance of Collaboration for Change in Tech Companies ', 'text': 'It is important to have people working from within tech companies who are pushing for change. Policy changes and pressure from grassroots organizations, advocacy and civil rights organizations, legal advocacy groups, and other labor organizers are needed. Academic researchers are needed to provide critiques of tech and these systems from an outside perspective. All of these elements need to work together in tandem. Input from the communities who are most impacted is essential.'}, {'title': 'Addressing Power Dynamics in Technology ', 'text': 'The power dynamics in technology need to be addressed, especially in terms of bringing marginalized communities into the conversation. Collective empowerment is necessary to have more control over how technology is produced and used. There should be distinctions made between intentional harm and unintended harm caused by technology products.'}, {'title': 'The Unintended Consequences of Tech Products ', 'text': 'Tech products can cause harm in unintended ways or in ways that are predictable but willfully ignored. Unintended consequences are often predictable but overlooked in favor of profit or progress, or result from negligence caused by overlooking marginalized populations. The value system of tech production may disregard the harm caused by the product. Examples of harm caused by tech products include taking over jobs and disconnecting us from our immediate environment.'}, {'title': 'The Impact of Efficiency on Our Environment ', 'text': \"Disconnecting from our immediate environment for efficiency is not necessarily harmful. Developers prioritize efficiency, which may not always align with enjoying our environment or unpredictable activities. Some may view the removal of incidental or coincidental elements from our environment as harmful, while others see it as a benefit. Technology can cause harm when used by bad actors or when it goes wrong despite the creators' intentions. Majority of tech products do not cause harm.\"}, {'title': 'The Importance of Historians in Tech Spaces ', 'text': 'The majority of harms come from good users using the product as intended. There is a need for more historians in tech spaces. Historians can provide valuable insights into the history of technology and its connections to the present. Bringing in historians can help tech professionals understand the historical context of their work.'}, {'title': 'Understanding Potential Harms in the Metaverse ', 'text': \"Learning from the past to understand potential harms in the metaverse. Excitement and ideas about the metaverse's impact on innovation, business, and the future of work. Concerns about sexual violence and harassment in metaverse spaces. Referencing past research and discussions on similar issues in virtual spaces. Framing technology problems as not entirely new, but rooted in social dynamics and power structures.\"}, {'title': 'The Importance of Understanding Social Dynamics and Power Structures in Assessing Potential Harms of New Technology ', 'text': 'Understanding social dynamics and power structures is important when assessing potential harms of new technology. It is important to use examples from the past and other areas when discussing new technology. The problem of harmful technology is bigger than just a few bad actors or intentionally harmful companies. Society has issues such as racism, sexism, homophobia, and inequality that need to be acknowledged when creating new technology. Building things without acknowledging these larger histories can lead to problems.'}, {'title': 'The Ethical Implications of Using AI to Resurrect the Dead ', 'text': 'The importance of the present moment of technological production being informed by and thought through the narratives and the histories of the past. Using generative AI to resurrect the dead will create a burden for the living. AI technologies promise more chatbots and replicas of people who have passed, giving voice to the dead comes at a human cost.'}, {'title': 'The Human Costs of Preserving Data in the Tech Industry ', 'text': \"The human costs of preserving the data of the dead is a larger problem in the tech industry. Data is power and tech companies collect as much data as possible about individual users. Google recently announced it would start deactivating accounts of inactive users after two years. Tech companies are recognizing the cost of maintaining everyone's data in perpetuity. The promise to maintain everyone's data in perpetuity is a significant financial burden.\"}, {'title': 'The Impact of Computing on Environment and Ethics ', 'text': 'The materiality of computing and the cost of creating it. The expansion of data centers to support generative AI and its environmental impact. The ethical implications of perpetuating relationships with the dead through technology.'}, {'title': 'Challenges and Ethical Considerations in Using Technology to Interact with the Dead ', 'text': 'The use of technology to interact with the dead requires system upgrades, software updates, and maintenance, which may eventually become obsolete. People may undergo a second form of grieving after experiencing grief already. Ethical questions arise around who should have the authority to create a simulation of a dead person through generative AI and who should control that relationship. Questions around companies, employers, or estates profiting from the simulated version of a dead person. Ethical considerations include copyright, estate planning, kinship relations, and consent.'}, {'title': 'The Ethical Considerations of Using AI to Recreate Deceased Individuals ', 'text': \"The ethical considerations of creating AI versions of the dead, especially with the use of deepfake technology. The potential violation of the deceased person's wishes and the impact on their family members. The example of Anthony Bourdain's documentary using deepfake technology to have him narrate a letter, raising questions about the uncanny nature of reviving the dead in this way. The prevalence of deepfake technology and its impact on the revival of dead celebrities for various specials. The ongoing debate and discussion surrounding the use of AI to recreate deceased individuals.\"}, {'title': \"The Ethical and Labor Concerns of Using Celebrities' Likeness After Death \", 'text': \"Reviving the dead without consent of family is uncanny. The issue of using celebrities' likeness after their death is a concern for everyone. The fantasy of monetizing someone's likeness after their death to avoid hiring living people creates labor problems. There are links between labor, environmental damage, and the broader context.\"}, {'title': 'The Impact of Technology on Death and Environmental Damage ', 'text': 'The links between labor and environmental damage are discussed in the broader context of preserving data, particularly the data of the dead. The impact of socio technical systems, social media platforms, AI, and generative AI on cultural rituals and navigation of death is explored. The book \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond\" delves into the inquiry of how technology affects our understanding and experience of death. The problem of death is described as a very old human issue.'}, {'title': 'The Impact of Death on Society ', 'text': 'Death is a universal and old human problem. Despite what transhumanists say, death is inevitable for everyone. Different ways of memorializing the dead exist. Treatment of the dead can reveal social hierarchies within a society. Mortuary rituals around kings and queens versus commoners can reveal societal structure. Death disrupts the original plan for many technologies, such as social media platforms.'}, {'title': 'The Overlooked Aspect of Death in Social Media Platforms ', 'text': 'Social media platforms were initially built for youthful users at elite colleges. The idea of using social media for memorialization and long-term relationships with the dead was not an immediate use case. Death is a universal and overlooked aspect of life. Tech companies have largely overlooked the concept of death in their products and services.'}, {'title': 'The Impact of Death on Social Networks in UX Teams ', 'text': \"Death is not the norm in big UX teams, but it happens all the time. The larger problem of ethics and technology impacts communities and individuals' social networks. It is important to consider the relational aspect when assessing the impact of algorithmic systems. Impact assessments should not only focus on the technology itself, but also on the social relations and potential downstream impacts. Death can help reveal relationships and networks over time in a way that other social things may not.\"}, {'title': 'The Impact of Algorithmic Thinking on Death and Technology ', 'text': 'Death is an incredibly rich field site for thinking about algorithmic impacts. Techno solutionism can be useful in addressing pain points or points of failure in technology. Making small changes in technology can lead to more efficient solutions to problems.'}, {'title': 'The Impact of Technology on Social Problem Solving ', 'text': 'Technology may be a solution to a technical problem. Technology is often brought in to solve social problems, even when it may not be the best solution. AIM lab helps different groups and organizations understand and develop relationships with the communities impacted by technology. The process involves taking information back to the technical team to improve technology and prevent harmful impacts.'}, {'title': 'The Importance of Relationships in Collaboration ', 'text': 'The importance of creating and maintaining relationships in the collaboration process. The emphasis on long-term relationships and processes rather than one-off solutions. The recognition of the most important aspect being the relationships with techno solutionism. The concern that the focus on technology can overshadow other important factors. The caution against techno utopianism and the suggestion to imagine better solutions.'}, {'title': 'Reimagining Technology and Building Alternative Futures ', 'text': 'It is better to imagine other futures and learn from feminist and black Sci-Fi traditions. There are alternatives to the techno imaginary of Elon Musk. Reimagining technology and building it differently need to go together. Dismantling the way that tech is currently being built and deployed is necessary for a different imaginary to be realized. Interested in the magical, mystical, and metaphysical qualities of technology.'}, {'title': 'The Transcendent Qualities of Technology ', 'text': 'Technology has been used by humans for a long time to communicate with the dead. The speaker is interested in the transcendent qualities of technology. People derive a real sense of connection and something sacred out of mundane technologies. The speaker is interested in exploring other uses of technology that expand its magical quality. The speaker teaches a course on data and human values at UC Berkeley.'}, {'title': 'The Importance of Critical Technology Studies for College Students ', 'text': 'Many students at UC Berkeley and other college campuses listen to the show. Younger generations going into tech are more aware of potential problems and are interested in training in STS and critical technology studies. It is important to read widely and talk to people outside of tech. Understanding the history of technology and the tech industry is crucial. Looking to organizers from the past is important for understanding the tech industry.'}, {'title': 'History and Lessons of Activism in the Tech Industry ', 'text': \"['Understand the history of the tech industry and learn from organizers from the past.', 'Bring together people from different generations and activist groups to learn from each other.', 'Recognize that there have been activists in tech and learn from their work in coalition building.', 'Continue talking to people who have been involved in activism in the tech industry.']\"}, {'title': 'Expanding Your Network and Learning from Diverse Communities ', 'text': 'Learn from people who came before you. Continue talking to people who are not in the tech industry. Expand your relationship with different communities in your day to day life. Volunteer your time or get to know different kinds of communities and their concerns. Get involved with issues where you live.'}, {'title': 'Helpful Feedback for Bay Area Residents ', 'text': 'Specifically relevant for those living in the Bay Area. Thanking Tamara and Deb.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 21:11:23.278687 ...\n",
      "Best SD: 2.331844763272204, Best iteration: 1\n",
      "done get topics 2024-03-28 21:11:23.972586.\n",
      "Stage 2 start time 2024-03-28 21:11:23.972610\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Exploring the Environmental Impact of AI in the \"Technically Human\" Podcast\n",
      "2. The Influence of Research and Legislation on AI's Environmental Impact\n",
      "3. Innovations in Environmental Technology and AI\n",
      "4. Ethical Considerations in Technology Development and Deployment\n",
      "5. Addressing Bias and Discrimination in Tech Companies and AI\n",
      "6. Understanding the Social and Environmental Impact of New Technology\n",
      "7. Ethical Implications of Using AI in Death and Resurrection\n",
      "8. Building Relationships and Collaboration in Technology and Social Problem Solving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-03-28 21:12:44.325127\n",
      "stage_2_titles: len: 8\n",
      "['1. Exploring the Environmental Impact of AI in the \"Technically Human\" Podcast', \"2. The Influence of Research and Legislation on AI's Environmental Impact\", '3. Innovations in Environmental Technology and AI', '4. Ethical Considerations in Technology Development and Deployment', '5. Addressing Bias and Discrimination in Tech Companies and AI', '6. Understanding the Social and Environmental Impact of New Technology', '7. Ethical Implications of Using AI in Death and Resurrection', '8. Building Relationships and Collaboration in Technology and Social Problem Solving']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
