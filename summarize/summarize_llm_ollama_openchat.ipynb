{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f189a949590>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_summary_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'summary': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'summary': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'summary': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'summary': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_1(chunks_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOllama(model=\"openchat\")\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# Use less word to try solve the warning/error:\n",
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). \n",
    "# Running this sequence through the model will result in indexing errors\n",
    "  map_prompt_template = \"\"\"Write a 75-85 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, \n",
    "  removing irrelevant information.\n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'summaries': [stage_1_outputs[chunk_id]['summary'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['summaries_concat'] = ' '.join(topic_data['summaries'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['summaries_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "\n",
    "  title_llm = ChatOllama(model=\"openchat\")\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  map_llm = ChatOllama(model=\"openchat\")\n",
    "  reduce_llm = ChatOllama(model=\"openchat\")\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 is not technical. skip\n",
      "episode 2 is not technical. skip\n",
      "Start time: 2024-02-25 21:34:51.135398\n",
      "Stage 1 done time 2024-02-25 21:35:48.172199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SD: 1.3635890143294642, Best iteration: 32\n",
      "Stage 2 start time 2024-02-25 21:35:49.705780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-02-25 21:36:11.490815\n",
      "Start time: 2024-02-25 21:36:11.505438\n",
      "Stage 1 done time 2024-02-25 21:37:06.722511\n",
      "Best SD: inf, Best iteration: 0\n",
      "Stage 2 start time 2024-02-25 21:37:08.861525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-02-25 21:37:33.153071\n",
      "Start time: 2024-02-25 21:37:33.166475\n",
      "Stage 1 done time 2024-02-25 21:38:34.940750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\u001b[39;00m\n\u001b[1;32m     40\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_1_chunks \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m topics_out \u001b[38;5;241m=\u001b[39m \u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_similarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbonus_constant\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m chunk_topics \u001b[38;5;241m=\u001b[39m topics_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_topics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m topics \u001b[38;5;241m=\u001b[39m topics_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mget_topics\u001b[0;34m(title_similarity, num_topics, bonus_constant, min_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m topics_title \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(topics_title) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [desired_num_topics, desired_num_topics \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desired_num_topics \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m---> 28\u001b[0m   topics_title \u001b[38;5;241m=\u001b[39m \u001b[43mcommunity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlouvain_communities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle_nx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m   resolution \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m resolution_step\n\u001b[1;32m     30\u001b[0m topic_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m topics_title]\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap_louvain_communities_5\u001b[0;34m(G, weight, resolution, threshold, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/networkx/algorithms/community/louvain.py:110\u001b[0m, in \u001b[0;36mlouvain_communities\u001b[0;34m(G, weight, resolution, threshold, seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Find the best partition of a graph using the Louvain Community Detection\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mAlgorithm.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mlouvain_partitions\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m d \u001b[38;5;241m=\u001b[39m louvain_partitions(G, weight, resolution, threshold, seed)\n\u001b[0;32m--> 110\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/networkx/algorithms/community/louvain.py:176\u001b[0m, in \u001b[0;36mlouvain_partitions\u001b[0;34m(G, weight, resolution, threshold, seed)\u001b[0m\n\u001b[1;32m    173\u001b[0m     graph\u001b[38;5;241m.\u001b[39madd_weighted_edges_from(G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39mweight, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    175\u001b[0m m \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39msize(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m partition, inner_partition, improvement \u001b[38;5;241m=\u001b[39m \u001b[43m_one_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_directed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m improvement:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# gh-5901 protect the sets in the yielded list from further manipulation here\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/networkx/algorithms/community/louvain.py:233\u001b[0m, in \u001b[0;36m_one_level\u001b[0;34m(G, m, partition, resolution, is_directed, seed)\u001b[0m\n\u001b[1;32m    231\u001b[0m     degrees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(G\u001b[38;5;241m.\u001b[39mdegree(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    232\u001b[0m     Stot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(degrees\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m--> 233\u001b[0m     nbrs \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mu\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    234\u001b[0m rand_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    235\u001b[0m seed\u001b[38;5;241m.\u001b[39mshuffle(rand_nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/networkx/algorithms/community/louvain.py:233\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m     degrees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(G\u001b[38;5;241m.\u001b[39mdegree(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    232\u001b[0m     Stot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(degrees\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m--> 233\u001b[0m     nbrs \u001b[38;5;241m=\u001b[39m {u: \u001b[43m{\u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m}\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m G}\n\u001b[1;32m    234\u001b[0m rand_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    235\u001b[0m seed\u001b[38;5;241m.\u001b[39mshuffle(rand_nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/networkx/algorithms/community/louvain.py:233\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m     degrees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(G\u001b[38;5;241m.\u001b[39mdegree(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    232\u001b[0m     Stot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(degrees\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m--> 233\u001b[0m     nbrs \u001b[38;5;241m=\u001b[39m {u: {v: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v, data \u001b[38;5;129;01min\u001b[39;00m G[u]\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m!=\u001b[39m u} \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m G}\n\u001b[1;32m    234\u001b[0m rand_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    235\u001b[0m seed\u001b[38;5;241m.\u001b[39mshuffle(rand_nodes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = summarize_stage_1(chunks_text)['stage_1_outputs']\n",
    "    # Split the titles and summaries\n",
    "    stage_1_summaries = [e['summary'] for e in stage_1_outputs]\n",
    "    stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_summaries)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    summary_embeds = generate_embeddings(stage_1_summaries)\n",
    "#     title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "        summary_similarity_matrix[row, col] = similarity\n",
    "        summary_similarity_matrix[col, row] = similarity\n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    topics_out = get_topics(summary_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "    # Plot a heatmap of this array\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "    # Draw vertical black lines for every 1 of the x-axis \n",
    "    for i in range(1, len(chunk_topics)):\n",
    "      plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250)\n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_ollama_openchat_{podcast['episode_number']}_v2.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
