{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb031bc6f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fb031aa2d90>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_summary_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'summary': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'summary': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'summary': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'summary': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_1(chunks_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following podcast an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# Use less word to try solve the warning/error:\n",
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). \n",
    "# Running this sequence through the model will result in indexing errors\n",
    "# 75-100\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'summaries': [stage_1_outputs[chunk_id]['summary'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['summaries_concat'] = ' '.join(topic_data['summaries'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['summaries_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      keypoints = keypoints + result['text'].split('- ')\n",
    "\n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return {\n",
    "    'keypoints': keypoints\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-17 20:45:55.808049\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-17 20:50:36.933666\n",
      "chunks_text len: 92\n",
      "extract_keypoints start time: 2024-03-17 20:50:36.933811\n",
      "chunks:\n",
      "{'text': \"The following is a conversation with Rajat Manga. He's an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work going on in the world in deep learning, both the cutting edge research and the large scale application of learning based approaches. But it's quickly becoming much more than a software library. It's now an ecosystem of tools for the deployment of machine learning in the cloud, on the phone, in the browser, on both generic and specialized hardware. TPU, GPU, and so on. Plus, there's a big emphasis on growing a passionate community of developers. Rajat, Jeff Dean, and a large team of engineers at Google Brain are working to define the future of machine learning with TensorFlow 2.0, which is now in alpha. I think the decision to open source TensorFlow is a definitive moment in the tech industry. It\"}\n",
      "keypoints:\n",
      "- Rajat Manga is an engineer and director of Google, leading the TensorFlow team.\n",
      "- TensorFlow is an open source library used in deep learning research and large scale applications.\n",
      "- It has evolved into an ecosystem of tools for machine learning deployment in various platforms.\n",
      "- There is a focus on growing a passionate community of developers.\n",
      "- TensorFlow 2.0 is being developed by a large team at Google Brain and is now in alpha.\n",
      "- The decision to open source TensorFlow is a definitive moment in the tech industry.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"Brain are working to define the future of machine learning with TensorFlow 2.0, which is now in alpha. I think the decision to open source TensorFlow is a definitive moment in the tech industry. It showed that open innovation can be successful and inspire many companies to open source their code, to publish, and in general engage in the open exchange of ideas. This conversation is part of the Artificial Intelligence podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter at Lex Friedman, spelled F R I D. And now, here's my conversation with Rajat Manga. You were involved with Google Brain since its start in 2011 with Jeff Dean. It started with this belief, the proprietary machine learning library, and turned into TensorFlow in 2014, the open source library.\"}\n",
      "keypoints:\n",
      "- TensorFlow 2.0 is in alpha and Brain is working on defining the future of machine learning with it.\n",
      "- The decision to open source TensorFlow is a definitive moment in the tech industry.\n",
      "- Open innovation can be successful and inspire many companies to open source their code and engage in the open exchange of ideas.\n",
      "- The conversation is part of the Artificial Intelligence podcast.\n",
      "- Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean.\n",
      "- Google Brain started with the belief in a proprietary machine learning library and turned into TensorFlow in 2014, the open source library.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"the proprietary machine learning library, and turned into TensorFlow in 2014, the open source library. It was interesting back then when I started, or when you were even just talking about it, the idea of deep learning was interesting and intriguing in some ways. It hadn't yet taken off, but it held some promise. It had shown some very promising and early results. I think the idea where Andrew and Jeff had started was, what if we can take this work people are doing in research and scale it to what Google has in terms of the compute power, and also put that kind of data together? And so far, the results had been, if you scale the compute, scale the data, it does better. And would that work? And so that was\"}\n",
      "keypoints:\n",
      "- The proprietary machine learning library was turned into TensorFlow in 2014, becoming an open source library.\n",
      "- Deep learning was an interesting and intriguing idea at the time, showing promising early results.\n",
      "- The idea was to scale research work to Google's compute power and data, with the belief that scaling the compute and data would lead to better results.\n",
      "- The results so far have shown that scaling the compute and data leads to better performance.\n",
      "-------\n",
      "chunks:\n",
      "{'text': 'power, and also put that kind of data together. And so far, the results had been, if you scale the compute, scale the data, it does better. And so that was the first year or two, can we prove that out? And with this belief, when we started the first year, we got some early wins, which is always great. I think there are two early wins where one was speech, that we collaborated very closely with the speech research team, who was also getting interested in this. And the other one was on images, where the cat paper, as we call it, that was covered by a lot of folks. And the birth of Google Brain was around neural networks. So it was deep learning from the very beginning. That was the whole mission. So what would, in terms of scale, what'}\n",
      "keypoints:\n",
      "- The results show that scaling the compute and data leads to better performance.\n",
      "- Early wins were achieved in speech and image recognition.\n",
      "- The birth of Google Brain was focused on neural networks and deep learning from the beginning.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"by a lot of folks. And the birth of Google Brain was around neural networks. So it was deep learning from the very beginning. That was the whole mission. Yeah, I think, so that was interesting. And if I think back to 2012 or 2011, and first was can we scale it in the year or so, we had started scaling it to hundreds and thousands of machines. In fact, we had some runs even going to 10,000 machines. And all of those shows great promise. In terms of machine learning at Google, the good thing was Google's been doing machine learning for a long time. Deep learning\"}\n",
      "keypoints:\n",
      "- Google Brain was focused on neural networks and deep learning from the beginning.\n",
      "- They were able to scale their deep learning system to hundreds and thousands of machines, with some runs even using 10,000 machines.\n",
      "- Google has been doing machine learning for a long time, which shows great promise for the future of machine learning at Google.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"even going to 10,000 machines. And all of those shows great promise. In terms of machine learning at Google, the good thing was Google's been doing machine learning for a long time. Deep learning was new, but as we scaled this up, we showed that, yes, that was possible. And it was going to impact lots of things. Like we started seeing real products wanting to use this. Again, speech was the first, there were image things that photos came out of and then many other products as well. So that was exciting. As we went into that a couple of years, externally also academia started to, there was lots of push on, okay, deep learning is interesting, we should be doing more and so on. And so by 2014, we were looking at, okay, this is a big thing, it's going to grow. And not just internally, externally as well. Yes, maybe Google's ahead of where everybody is, but there's a lot to do. So a lot of\"}\n",
      "keypoints:\n",
      "- Google has been doing machine learning for a long time.\n",
      "- Deep learning was new, but Google showed that it was possible to scale up.\n",
      "- The impact of deep learning was seen in real products, such as speech and image recognition.\n",
      "- Academia also started to show interest in deep learning by 2014.\n",
      "- Google recognized that deep learning was a big thing and it was going to grow both internally and externally.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"we were looking at, okay, this is a big thing, it's going to grow. And not just internally, externally as well. Yes, maybe Google's ahead of where everybody is, but there's a lot to do. So a lot of this started to make sense and come together. So the decision to open source, I was just chatting with Chris Glatner about this. The decision to go open source with TensorFlow, I would say sort of for me personally, seems to be one of the big seminal moments in all of software engineering ever. I think that's when a large company like Google decides to take a large project that many lawyers might argue has a lot of IP, just decide to go open source with it, and in so doing lead the entire world and saying, you know what, open innovation is a pretty powerful thing, and it's okay to do. That was, I mean, that's an incredible moment in time.\"}\n",
      "keypoints:\n",
      "- The decision to open source TensorFlow is considered a big seminal moment in software engineering.\n",
      "- Google's decision to open source a large project with a lot of IP was a significant move.\n",
      "- Open innovation is a powerful thing and it's okay to do.\n",
      "- This decision led the entire world in saying that open innovation is powerful.\n",
      "- The decision to open source TensorFlow was an incredible moment in time.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"and saying, you know what, open innovation is a pretty powerful thing, and it's okay to do. That was, I mean, that's an incredible moment in time. I would say, I think, so the initial idea came from Jeff, who was a big proponent of this. I think it came off of two big things. One was research wise, we were a research group. We were putting all our research out there. If you wanted to, we were building on others research and we wanted to push the state of the art forward. And part of that was to share the research. That's how I think deep learning and machine learning has really grown so fast. So the next step was, okay, now, would software help with that? And it seemed like they were existing a few libraries out there, Tiano being one, Torch being another, and a few others, but\"}\n",
      "keypoints:\n",
      "- Open innovation is a powerful thing and it's okay to do.\n",
      "- The initial idea came from Jeff, who was a big proponent of open innovation.\n",
      "- The research group was putting all their research out there and building on others' research to push the state of the art forward.\n",
      "- Sharing research has contributed to the rapid growth of deep learning and machine learning.\n",
      "- The next step was to consider how software could help with sharing research.\n",
      "- There were existing libraries such as Tiano and Torch that could potentially help with sharing research.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"so fast. So the next step was, okay, now, would software help with that. And it seemed like they were existing a few libraries out there, Tiano being one, Torch being another, and a few others, but they were all done by academia and so the level was significantly different. The other one was from a software perspective, Google had done lots of software or that we used internally, you know, and we published papers. Often there was an open source project that came out of that that somebody else picked up that paper and implemented and they were very successful. Back then it was like, okay, there's Hadoop, which has come off of tech that we've built. We know the tech we've built is way better for a number of different reasons. We've invested a lot of effort in that. And turns out we have Google Cloud and we are now not really providing our tech, but we are saying, okay, we have Bigtable,\"}\n",
      "keypoints:\n",
      "- The speaker considered whether software could help with a certain task.\n",
      "- Existing libraries such as Tiano and Torch were mentioned, but they were done by academia and the level was significantly different.\n",
      "- Google had developed lots of software for internal use and published papers, leading to successful open source projects.\n",
      "- The speaker believed that the tech built by Google was way better for a number of different reasons than existing tech like Hadoop.\n",
      "- Google Cloud was not providing their tech, but instead offering services like Bigtable.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"for a number of different reasons. We've invested a lot of effort in that. And turns out we have Google Cloud and we are now not really providing our tech, but we are saying, okay, we have Bigtable, which is the original thing. We are going to now provide H base APIs on top of that, which isn't as good, but that's what everybody's used to. So there's like, can we make something that is better and really just provide, helps the community in lots of ways, but also helps push a good standard forward. So how does Cloud fit into that? There's a TensorFlow open source library and how does the fact that you can use so many of the resources that Google provides and the Cloud fit into that strategy? So TensorFlow itself is open and you can use it anywhere, right? And we want to make sure that continues to be the case. On Google Cloud, we do make sure that there's lots of integrations with\"}\n",
      "keypoints:\n",
      "- Google has invested a lot of effort in providing tech through Google Cloud.\n",
      "- They are now providing H base APIs on top of Bigtable to help the community and push a good standard forward.\n",
      "- Google Cloud offers integrations with TensorFlow, an open source library, and provides access to many resources.\n",
      "- TensorFlow is open and can be used anywhere, and Google wants to ensure that continues to be the case.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"So TensorFlow itself is open and you can use it anywhere, right? And we want to make sure that continues to be the case. On Google Cloud, we do make sure that there's lots of integrations with everything else and we want to make sure that it works really, really well there. There's this incredible ecosystem that I'd like to talk about. Yes, yes, so looking back, we were building TensorFlow. I guess we open\"}\n",
      "keypoints:\n",
      "- TensorFlow is open and can be used anywhere.\n",
      "- Google Cloud ensures lots of integrations with everything else and works really well with TensorFlow.\n",
      "- There is an incredible ecosystem surrounding TensorFlow.\n",
      "- The speaker was involved in building TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"allowed to say history when it's just, but in deep learning, everything moves so fast and just a few years is already history. Yes, yes, so looking back, we were building TensorFlow. I guess we open sourced it in 2015, November 2015. We started on it in summer of 2014, I guess. And somewhere like three to six, late 2014, by then we had decided that, okay, there's a high likelihood we'll open source it. So we started thinking about that and making sure we're heading down that path. At that point, by that point, we had seen a few, lots of different use cases at Google. So there were things like, okay, yes, you wanna run it at large scale in the data center. Yes, we need to support different kind of hardware. We had GPUs at that point. We had our first GPU at that point or was about to come out roughly around that time. So the design sort of included those. We had started to push on\"}\n",
      "keypoints:\n",
      "- Deep learning moves fast, and just a few years is already considered history.\n",
      "- TensorFlow was open sourced in November 2015, after being started in the summer of 2014.\n",
      "- By late 2014, there was a high likelihood that TensorFlow would be open sourced.\n",
      "- The design of TensorFlow included support for running at large scale in the data center and different kinds of hardware, including GPUs.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"kind of hardware. We had GPUs at that point. We had our first GPU at that point or was about to come out roughly around that time. So the design sort of included those. We had started to push on mobile. So we were running models on mobile. At that point, people were customizing code. So we wanted to make sure TensorFlow could support that as well. So that sort of became part of that overall design. We already had a couple of products that were doing that by then. And in those cases, we had basically customized handcrafted code or some internal libraries that we're using. So I was actually at Google during\"}\n",
      "keypoints:\n",
      "- The design included GPUs and support for mobile models.\n",
      "- Customizing code was a priority for TensorFlow.\n",
      "- Existing products were already using customized code or internal libraries.\n",
      "- The speaker was at Google during this time.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"had a couple of products that were doing that by then. And in those cases, we had basically customized handcrafted code or some internal libraries that we're using. So I was actually at Google during this time in a parallel, I guess, universe, but we were using Theano and Caffe. Perhaps the Python part of thing, maybe did that influence any design decisions? Totally. So when we built this belief and some of that was in parallel with some of these libraries coming up, I mean, Theano itself is older, but we were building this belief focused on our internal thing because our systems were very different. By the time we got to this, we looked at a number of libraries that were out there.\"}\n",
      "keypoints:\n",
      "- Customized handcrafted code and internal libraries were used for products at Google.\n",
      "- Theano and Caffe were used at Google during the same time.\n",
      "- The Python part influenced design decisions.\n",
      "- The belief was built in parallel with the development of libraries like Theano.\n",
      "- The internal systems at Google were very different from the libraries available at the time.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"is older, but we were building this belief focused on our internal thing because our systems were very different. By the time we got to this, we looked at a number of libraries that were out there. Theano, there were folks in the group who had experience with Torch, with Lua. There were folks here who had seen Caffe. I mean, actually, Yang Jing was here as well. I think we looked at a number of things. Might even have looked at JNR back then. I'm trying to remember if it was there. In fact, yeah, we did discuss ideas around, okay, should we have a graph or not? So putting all these together was definitely, they were key decisions that we wanted. We had seen limitations in our prior disbelief things. A few of them were just in terms of research was moving so fast, we wanted the flexibility. The hardware was changing fast. We expected to change that so that\"}\n",
      "keypoints:\n",
      "- Building belief focused on internal systems due to different systems.\n",
      "- Looked at libraries such as Theano, Torch, Lua, Caffe, and possibly JNR.\n",
      "- Discussed ideas around having a graph or not.\n",
      "- Key decisions were made based on limitations in prior disbelief things.\n",
      "- Flexibility and fast-changing hardware were important factors in decision-making.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"in our prior disbelief things. A few of them were just in terms of research was moving so fast, we wanted the flexibility. The hardware was changing fast. We expected to change that so that those probably were two things. And yeah, I think the flexibility in terms of being able to express all kinds of crazy things was definitely a big one then. So what, the graph decisions though, with moving towards TensorFlow 2.0, there's more, by default, there'll be eager execution. So sort of hiding the graph a little bit because it's less intuitive in terms of the way people develop and so on. It seemed, it's kind of the Theano way. Did it seem the obvious choice? So I think where it came from was our disbelief had a graph like thing as well. A much more simple, it wasn't a general graph, it was more like a straight line thing. More like what\"}\n",
      "keypoints:\n",
      "- The flexibility in terms of being able to express all kinds of crazy things was a big factor in the decision.\n",
      "- The hardware was changing fast, so the decision was made to change accordingly.\n",
      "- Moving towards TensorFlow 2.0, there will be eager execution by default, which hides the graph a little bit and is less intuitive for developers.\n",
      "- The decision to move towards TensorFlow 2.0 was influenced by the disbelief in a graph-like thing in the prior system.\n",
      "- The prior system had a simpler graph-like structure, more like a straight line.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"obvious choice. So I think where it came from was our disbelief had a graph like thing as well. A much more simple, it wasn't a general graph, it was more like a straight line thing. More like what you might think of cafe, I guess in that sense. But the graph was, and we always cared about the production stuff. Like even with disbelief, we were deploying a whole bunch of stuff in production. So graph did come from that when we thought of, okay, should we do that in Python? And we experimented with some ideas where it looked a lot simpler to use, but not having a graph meant, okay, how do you deploy now? So that was probably what tilted the balance for us and eventually we ended up with a graph. And I guess the question there is, did you, I mean, so production seems to be the really good thing to focus on, but did you even anticipate the other side of it where there could be, what is it?\"}\n",
      "keypoints:\n",
      "- Disbelief had a graph like thing as well, but it was more like a straight line.\n",
      "- The team always cared about the production stuff and deployed a whole bunch of stuff in production.\n",
      "- The decision to use a graph in Python was influenced by the need for deployment.\n",
      "- The focus on production was important, but there was also consideration for the other side of it.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"And I guess the question there is, did you, I mean, so production seems to be the really good thing to focus on, but did you even anticipate the other side of it where there could be, what is it? It's been crazy, 41 million downloads. Yep. I mean, was that even like a possibility in your mind that it would be as popular as it became? So I think we did see a need for this a lot from the research perspective and like early days of deep learning in some ways. 41 million, no, I don't think I imagined this number. Then it seemed like there's a potential future where lots more people would be doing this and how do we enable that? I would say this kind of growth, I probably started seeing somewhat after the open sourcing where it was like, okay, deep learning is actually growing way faster for a lot of different reasons. And we are in just the right place to push on that\"}\n",
      "keypoints:\n",
      "- Production seems to be a really good thing to focus on.\n",
      "- Anticipation of the popularity of the product was not imagined.\n",
      "- There was a need for the product from a research perspective.\n",
      "- Open sourcing led to a significant growth in deep learning.\n",
      "- The potential future involves enabling more people to use deep learning.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"seeing somewhat after the open sourcing where it was like, okay, deep learning is actually growing way faster for a lot of different reasons. And we are in just the right place to push on that and leverage that and deliver on lots of things that people want. I don't even actually remember during those times. I know looking now, there's really good documentation, there's an ecosystem of tools, there's a community, there's a blog, there's a YouTube channel now, right? Yeah. It's very community driven. Back then, I guess 0.1 version, is that the version? I think we call it 0.6 or five, something like that, I forget. It's interesting. I think we've gone through a few things there. When we started out,\"}\n",
      "keypoints:\n",
      "- Deep learning is growing rapidly for various reasons.\n",
      "- The company is in a good position to leverage deep learning and deliver on people's needs.\n",
      "- There is now good documentation, a range of tools, a community, a blog, and a YouTube channel.\n",
      "- The company is community-driven.\n",
      "- The initial version of the product was 0.6 or 0.5.\n",
      "- The company has gone through a few changes since it started.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"that the version I think we call it 0.6 or five, something like that, I forget. I think we've gone through a few things there. When we started out, when we first came out, people loved the documentation we have because it was just a huge step up from everything else because all of those were academic projects, people doing, who don't think about documentation. And that I think really changed how things started to scale up in some ways and pushed on it. Over the next few months as we looked at how do we stabilize things, as we look at not just researchers, now we want stability, people want stability.\"}\n",
      "keypoints:\n",
      "- The version mentioned is either 0.6 or five, but it is not certain.\n",
      "- The documentation provided was well-received and considered a significant improvement from academic projects.\n",
      "- The improved documentation changed how things started to scale up and pushed for further progress.\n",
      "- There was a focus on stabilizing things and meeting the demand for stability from users, not just researchers.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"things started to scale up in some ways and pushed on it. Over the next few months as we looked at how do we stabilize things, as we look at not just researchers, now we want stability, people want to deploy things. That's how we started planning for 1.0 and there are certain needs for that perspective. And so again, documentation comes up, designs, more kinds of things to put that together. And so that was exciting to get that to a stage where more and more enterprises wanted to buy in and really get behind that. And I think post 1.0 and over the next few releases, that enterprise adoption also started to take off. I would say between the initial release and 1.0, it was, okay, researchers of course, then a lot of hobbies and early interest, people excited about this who started to get on board and then over the 1.x thing, lots of enterprises. I imagine anything that's below 1.0 gives\"}\n",
      "keypoints:\n",
      "- Scaling up and stabilizing the product was a priority over the next few months.\n",
      "- Planning for version 1.0 involved addressing certain needs such as documentation and designs.\n",
      "- Enterprise adoption started to increase after the initial release and over the next few releases.\n",
      "- Initial interest came from researchers and hobbyists, but later on, many enterprises showed interest in the product.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"of course, then a lot of hobbies and early interest, people excited about this who started to get on board and then over the 1.x thing, lots of enterprises. I imagine anything that's below 1.0 gives pressure to be, the enterprise probably wants something that's stable. Exactly. I mean, I know you're in the midst of it, but. Yeah, I think in the midst of it, it's often easy to forget what an enterprise wants and what some of the people on that side want. There are still people running models that are three years old, four years old. So Inception is still used by tons of people. Even ResNet 50 is what, couple of years old now or more, but there are\"}\n",
      "keypoints:\n",
      "- Early interest and excitement from people about certain hobbies and activities.\n",
      "- Enterprises prefer stability and may feel pressure to adopt technologies that are below version 1.0.\n",
      "- Enterprises have specific needs and desires that may be overlooked in the midst of technological advancements.\n",
      "- Some people still use older models such as Inception and ResNet 50, indicating the longevity and continued relevance of certain technologies.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"There are still people running models that are three years old, four years old. So Inception is still used by tons of people. Even ResNet 50 is what, couple of years old now or more, but there are tons of people who use that and they're fine. They don't need the last couple of bits of performance or quality, they want some stability in things that just work. And so there is value in providing that with that kind of stability and making it really simpler because that allows a lot more people to access it. And then there's the research crowd which wants, okay, they wanna do these crazy things exactly like you're saying, right? Not just deep learning in the straight up models that used to be there, they want RNNs and even RNNs are maybe old, they are transformers now. And now it needs to combine with RL and GANs and so on. So there's definitely that area that like the boundary that's\"}\n",
      "keypoints:\n",
      "- Many people still use older models like Inception and ResNet 50 for their stability and reliability.\n",
      "- Providing stability and simplicity in models allows more people to access and use them.\n",
      "- The research crowd is interested in pushing the boundaries with newer and more complex models like transformers, RL, and GANs.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"be there, they want RNNs and even RNNs are maybe old, they are transformers now. And now it needs to combine with RL and GANs and so on. So there's definitely that area that like the boundary that's shifting and pushing the state of the art. But I think there's more and more of the past that's much more stable and even stuff that was two, three years old is very, very usable by lots of people. So that part makes it a lot easier. So I imagine, maybe you can correct me if I'm wrong, one of the biggest use cases is essentially taking something like ResNet 50 and doing some kind of transfer learning on a very particular problem that you have. It's basically probably what majority of the world does. And you wanna make that as easy as possible. So I would say for the hobbyist perspective, that's the most common case, right? In fact, the apps and phones and stuff that you'll see, the early\"}\n",
      "keypoints:\n",
      "- RNNs are being replaced by transformers.\n",
      "- The boundary of technology is shifting and pushing the state of the art.\n",
      "- Older technology is still very usable by many people.\n",
      "- Transfer learning on specific problems is a common use case.\n",
      "- Making transfer learning as easy as possible is important for hobbyists.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"does. And you wanna make that as easy as possible. So I would say for the hobbyist perspective, that's the most common case, right? In fact, the apps and phones and stuff that you'll see, the early ones, that's the most common case. I would say there are a couple of reasons for that. It looks great on slides. That's a presentation, yeah, exactly. What enterprises want is that is part of it, but that's not the big thing. Enterprises really have data that they wanna make predictions on. This is often what they used to do with the people who were doing ML was just regression models, linear regression, logistic regression, linear models, or maybe gradient booster trees and so on. Some of them still benefit from deep learning, but they want that's the bread and butter, or like the structured data and so on. So depending on the audience you look at,\"}\n",
      "keypoints:\n",
      "- The most common case for hobbyists is to make data analysis as easy as possible.\n",
      "- Enterprises want to make predictions on their data, often using regression models or deep learning.\n",
      "- The focus for enterprises is on structured data and making predictions, rather than just creating presentations.\n",
      "- The early apps and phones focused on the hobbyist perspective.\n",
      "- Deep learning is still beneficial for some enterprises, but the focus is on structured data and making predictions.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"booster trees and so on. Some of them still benefit from deep learning, but they want that's the bread and butter, or like the structured data and so on. So depending on the audience you look at, they're a little bit different. And they just have, I mean, the best of enterprise probably just has a very large data set, or deep learning can probably shine. That's correct, that's right. And then I think the other pieces that they wanted, again, with 2.0, the developer summit we put together is the whole TensorFlow Extended piece, which is the entire pipeline. They care about stability across doing their entire thing. They want simplicity across the entire thing. I don't need to just train a model. I need to do that every day again, over and over again. I have people like lawyers come up to me\"}\n",
      "keypoints:\n",
      "- Different audiences have different preferences for deep learning and structured data.\n",
      "- Enterprise with large data sets can benefit from deep learning.\n",
      "- The audience at the developer summit is interested in TensorFlow Extended for stability and simplicity in their entire pipeline.\n",
      "- The need for repetitive model training and the involvement of non-technical staff like lawyers.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"I need to do that every day again, over and over again. I have people like lawyers come up to me and say, when is machine learning gonna enter legal, the legal realm? The same thing in all kinds of disciplines, immigration, insurance, often when I see what it boils down to is these companies are often a little bit old school in the way they organize the data. So the data is just not ready yet, it's not digitized. Do you also find yourself being in the role of an evangelist for like, let's get, organize your data, folks, and then you'll get the big benefit of TensorFlow. Yeah, yeah, you know, I get all kinds of questions there from, okay, what do I need to make this work, right? Do we really need deep learning? I mean, there are all these things, I\"}\n",
      "keypoints:\n",
      "- Machine learning is not yet widely used in legal and other disciplines due to old school data organization.\n",
      "- Companies need to organize their data in order to benefit from TensorFlow and machine learning.\n",
      "- There is a need for evangelists to promote the organization of data for the adoption of machine learning.\n",
      "- Questions arise about the requirements for making machine learning work, including the need for deep learning.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"conversations. Yeah, yeah, you know, I get all kinds of questions there from, okay, what do I need to make this work, right. I mean, there are all these things, I already use this linear model, why would this help. I don't have enough data, let's say, or I wanna use machine learning, but I have no clue where to start. So it varies, that to all the way to the experts to why support very specific things, it's interesting. It boils down to oftentimes digitizing data. So whatever you want automated, whatever data you want to make prediction based on, you have to make sure that it's in an organized form. Like within the TensorFlow ecosystem, there's now, you're providing more and more data sets and more and more pre trained models. Yes, I think the TensorFlow data sets that we\"}\n",
      "keypoints:\n",
      "- Questions range from basic needs for making machine learning work to specific support for experts.\n",
      "- The importance of organizing data for automation and prediction.\n",
      "- The availability of more data sets and pre-trained models within the TensorFlow ecosystem.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"there's now, you're providing more and more data sets and more and more pre trained models. Yes, I think the TensorFlow data sets that we just released, that's definitely come up where people want these data sets, can we organize them and can we make that easier? So that's definitely one important thing. The other related thing I would say is I often tell people, you know what, don't think of the most fanciest thing that the newest model that you see, make something very basic work and then you can improve it. There's just lots of things you can do with it. Yeah, start with the basics, true. One of the big things that makes TensorFlow even more accessible was the appearance whenever that happened of Keras, the Keras standard sort of outside of TensorFlow. I think it was Keras on top of Tiano at first only and then Keras became on\"}\n",
      "keypoints:\n",
      "- Providing more data sets and pre-trained models in TensorFlow.\n",
      "- The importance of organizing and making data sets easier to access.\n",
      "- Start with basic models and improve them.\n",
      "- The accessibility of TensorFlow was enhanced with the appearance of Keras.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"even more accessible was the appearance whenever that happened of Keras, the Keras standard sort of outside of TensorFlow. I think it was Keras on top of Tiano at first only and then Keras became on top of TensorFlow. Yeah, so Francois started the Keras project before he was at Google and the first thing was Tiano. I don't remember if that was after TensorFlow was created or way before. And then at some point, when TensorFlow started becoming popular, there were enough similarities that he decided to create this interface and put TensorFlow as a backend. I believe that might still have been before he joined Google. So we weren't really talking about that. He decided on his own and thought that was\"}\n",
      "keypoints:\n",
      "- Keras appeared as a standard outside of TensorFlow.\n",
      "- Keras started on top of Tiano and then became on top of TensorFlow.\n",
      "- Francois started the Keras project before he was at Google.\n",
      "- The first thing was Tiano, and then at some point, when TensorFlow started becoming popular, he decided to create an interface and put TensorFlow as a backend.\n",
      "- He decided on his own and thought that was before he joined Google.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"this interface and put TensorFlow as a backend. I believe that might still have been before he joined Google. So we weren't really talking about that. He decided on his own and thought that was interesting and relevant to the community. In fact, I didn't find out about him being at Google until a few months after he was here. He was working on some research ideas and doing Keras on his nights and weekends project. He wasn't like part of the TensorFlow. He didn't join initially. He joined research and he was doing some amazing research. He has some papers on that and research, so he's a great researcher as well. And at some point we realized, oh, he's doing this good stuff. People seem to like the API and he's right here. So we talked to him and he said, okay, why don't I come over to your team and work with you for a quarter and let's make that integration happen. And\"}\n",
      "keypoints:\n",
      "- The individual joined Google before the integration of TensorFlow as a backend.\n",
      "- He independently decided to work on integrating TensorFlow with Keras.\n",
      "- He initially joined Google for research and was working on research ideas and Keras as a side project.\n",
      "- He later joined the team working on the integration of TensorFlow and Keras.\n",
      "- He has contributed to research and has published papers in the field.\n",
      "- The decision to work on the integration happened after it was realized that he was doing good work and people liked the API.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"People seem to like the API and he's right here. So we talked to him and he said, okay, why don't I come over to your team and work with you for a quarter and let's make that integration happen. And we talked to his manager and he said, sure, quarter's fine. And that quarter's been something like two years now. And so he's fully on this. So Keras got integrated into TensorFlow in a deep way. And now with 2.0, TensorFlow 2.0, sort of Keras is kind of the recommended way for a beginner to interact with TensorFlow. That's correct, that's right. We did spend a lot of time thinking about that one. We had a bunch of APIs, some built by us. There was a parallel layers API that we were\"}\n",
      "keypoints:\n",
      "- Keras got integrated into TensorFlow in a deep way.\n",
      "- TensorFlow 2.0 recommends Keras as the way for beginners to interact with TensorFlow.\n",
      "- The initial plan for integration was for a quarter, but it has been two years now.\n",
      "- The person who worked on the integration is fully committed to it.\n",
      "- There was a lot of thought put into the integration and the choice of APIs.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"like. That seems like it's kind of a bold decision as well. We did spend a lot of time thinking about that one. We had a bunch of APIs, some built by us. There was a parallel layers API that we were building. And when we decided to do Keras in parallel, so there were like, okay, two things that we are looking at. And the first thing we was trying to do is just have them look similar, like be as integrated as possible, share all of that stuff. There were also like three other APIs that others had built over time because we didn't have a standard one. But one of the messages that we kept hearing from the community, okay, which one do we use? And they kept seeing like, okay, here's a model in this one and here's a model in this one, which should I pick? So that's sort of like, okay, we had to address that straight on with 2.0. The whole idea was we need to simplify. We had to pick one.\"}\n",
      "keypoints:\n",
      "- The decision to do Keras in parallel with other APIs was a bold one.\n",
      "- The goal was to make the APIs look similar and be as integrated as possible.\n",
      "- The community was confused about which API to use and kept asking for a standard one.\n",
      "- The main goal of version 2.0 was to simplify and pick one API.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"in this one and here's a model in this one, which should I pick. So that's sort of like, okay, we had to address that straight on with 2.0. The whole idea was we need to simplify. We had to pick one. Based on where we were, we were like, okay, let's see what are the people like? And Keras was clearly one that lots of people loved. There were lots of great things about it. So we settled on that. Organically, that's kind of the best way to do it. It was great. It was surprising, nevertheless, to sort of bring in an outside. I mean, there was a feeling like Keras might be almost like a competitor in a certain kind of, to TensorFlow. And in a sense, it became an empowering element of TensorFlow. That's right. Yeah, it's interesting how you can put two things together, which can align. In this case, I think Francois, the team, and a bunch of us have chatted, and I think we all want to see\"}\n",
      "keypoints:\n",
      "- The decision to simplify and pick one model for version 2.0 was made based on what people liked.\n",
      "- Keras was chosen as the model because it was popular and had many great features.\n",
      "- Initially, there was a feeling that Keras might be a competitor to TensorFlow, but it ended up becoming an empowering element of TensorFlow.\n",
      "- Bringing in an outside model like Keras was surprising but ultimately beneficial.\n",
      "- There was collaboration and discussion among the team members about the decision to choose Keras.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"That's right. Yeah, it's interesting how you can put two things together, which can align. In this case, I think Francois, the team, and a bunch of us have chatted, and I think we all want to see the same kind of things. We all care about making it easier for the huge set of developers out there, and that makes a difference. So Python has Guido van Rossum, who until recently held the position of benevolent dictator for life. All right, so there's a huge successful open source project like TensorFlow need one person who makes a final decision. So you've did a pretty successful TensorFlow Dev Summit just now, last couple days. There's clearly a lot of different new features being incorporated, an amazing ecosystem, so on. I think it's somewhat different, I would say. I've\"}\n",
      "keypoints:\n",
      "- The team and Francois are aligned in their goals for making it easier for developers.\n",
      "- Python has Guido van Rossum as the benevolent dictator for life.\n",
      "- Successful open source projects like TensorFlow need one person to make final decisions.\n",
      "- The TensorFlow Dev Summit was successful with new features being incorporated.\n",
      "- There is an amazing ecosystem surrounding TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"an amazing ecosystem, so on. I think it's somewhat different, I would say. I've always been involved in the key design directions, but there are lots of things that are distributed where there are a number of people, Martin Wick being one, who has really driven a lot of our open source stuff, a lot of the APIs, and there are a number of other people who've been, you know, pushed and been responsible for different parts of it. We do have regular design reviews. Over the last year, we've had a lot of we've really spent a lot of time opening up to the community and adding transparency. We're setting more processes in place, so RFCs, special interest groups, to really grow that community and scale that. I think the kind of scale that ecosystem is in, I don't think we could scale\"}\n",
      "keypoints:\n",
      "- Involvement in key design directions and open source development.\n",
      "- Contribution of Martin Wick and other individuals in driving open source projects and APIs.\n",
      "- Regular design reviews and efforts to increase transparency.\n",
      "- Implementation of processes such as RFCs and special interest groups to grow the community and scale the ecosystem.\n",
      "- Recognition of the scale of the ecosystem and the challenges in scaling it.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"We're setting more processes in place, so RFCs, special interest groups, to really grow that community and scale that. I think the kind of scale that ecosystem is in, I don't think we could scale with having me as the lone point of decision maker. I got it. So, yeah, the growth of that ecosystem, maybe you can talk about it a little bit. First of all, it started with Andrej Karpathy when he first did ComNetJS. The fact that you can train and you'll network in the browser was, in JavaScript, was incredible. So now TensorFlow.js is really making that a serious, like a legit thing, a way to operate, whether it's in the backend or the front end. Then there's the TensorFlow Extended, like you mentioned. There's TensorFlow Lite for mobile. And all of it, as far as I can tell, it's really converging towards being able to save models in the same kind of way. You can move around, you can train\"}\n",
      "keypoints:\n",
      "- Setting more processes in place, such as RFCs and special interest groups, to grow the community and scale it.\n",
      "- Acknowledgment that the ecosystem cannot scale with a lone decision maker.\n",
      "- The growth of the ecosystem, starting with Andrej Karpathy's ComNetJS and the development of TensorFlow.js.\n",
      "- The emergence of TensorFlow Extended, TensorFlow Lite for mobile, and the convergence towards being able to save models in the same way.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"There's TensorFlow Lite for mobile. And all of it, as far as I can tell, it's really converging towards being able to save models in the same kind of way. You can move around, you can train on the desktop and then move it to mobile and so on. That's right. So there's that cohesiveness. So in short, the way I like to think of this is our goals to enable machine learning. And in a couple of ways, you know, one is we have lots of exciting things going on in ML today. We started with deep learning, but we now support a bunch of other algorithms too. So one is to, on the research side, keep pushing on the state of the art. Can we, you know, how do we enable researchers to build the next amazing thing? So BERT came out recently, you\"}\n",
      "keypoints:\n",
      "- TensorFlow Lite is for mobile.\n",
      "- The goal is to enable machine learning in multiple ways.\n",
      "- Support for a variety of algorithms, not just deep learning.\n",
      "- Research focus on pushing the state of the art in machine learning.\n",
      "- Emphasis on enabling researchers to build the next amazing thing.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"too. So one is to, on the research side, keep pushing on the state of the art. So BERT came out recently, you know, it's great that people are able to do new kinds of research. And there are lots of amazing research that happens across the world. So that's one direction. The other is how do you take that across all the people outside who want to take that research and do some great things with it and integrate it to build real products, to have a real impact on people. And so if that's the other axes in some ways, you know, at a high level, one way I think about it is there are a crazy number of compute devices across the world. And we often used to think of ML and training and all of this as, okay, something you do either in the workstation or the data center or cloud. But we see things running on the\"}\n",
      "keypoints:\n",
      "- Pushing on the state of the art in research, such as with the recent development of BERT.\n",
      "- Integrating research into real products to have a real impact on people.\n",
      "- Considering the wide range of compute devices across the world for ML and training.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"across the world. And we often used to think of ML and training and all of this as, okay, something you do either in the workstation or the data center or cloud. But we see things running on the phones. We see things running on really tiny chips. I mean, we had some demos at the developer summit. And so the way I think about this ecosystem is how do we help get machine learning on every device that has a compute capability. And that continues to grow and so in some ways this ecosystem is looked at, you know, various aspects of that and grown over time to cover more of those. And we continue to push the boundaries. In some areas we've built more tooling and things around that to help you. I mean, the first tool we started was TensorBoard. You wanted to learn just the training piece, the effects or TensorFlow extended to really do your entire ML pipelines. If you're, you know, care about\"}\n",
      "keypoints:\n",
      "- Machine learning is no longer limited to workstations, data centers, or the cloud.\n",
      "- ML is now running on phones and tiny chips.\n",
      "- The goal is to get machine learning on every device with compute capability.\n",
      "- The ecosystem for machine learning is growing and evolving over time.\n",
      "- Tooling like TensorBoard and TensorFlow extended are being developed to support ML pipelines.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"mean, the first tool we started was TensorBoard. You wanted to learn just the training piece, the effects or TensorFlow extended to really do your entire ML pipelines. If you're, you know, care about all that production stuff, but then going to the edge, going to different kinds of things. And it's not just us now. We are a place where there are lots of libraries being built on top. So there are some for research, maybe things like TensorFlow agents or TensorFlow probability that started as research things or for researchers for focusing on certain kinds of algorithms, but they're also being deployed or used by, you know, production folks. And some have come from within Google, just teams across Google who wanted to build these things. Others have come from just the community because there are different pieces that different parts of the community care about. And I see our goal as\"}\n",
      "keypoints:\n",
      "- TensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended to ML pipelines.\n",
      "- There are many libraries being built on top of TensorFlow for research and production purposes.\n",
      "- Some libraries, like TensorFlow agents and TensorFlow probability, started as research tools but are now being used in production.\n",
      "- Libraries have come from both within Google and from the community, showing a wide range of interests and needs within the community.\n",
      "- The goal is to cater to the diverse needs of the community.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"across Google who wanted to build these things. Others have come from just the community because there are different pieces that different parts of the community care about. And I see our goal as enabling even that, right. It's not, we cannot and won't build every single thing. That just doesn't make sense. But if we can enable others to build the things that they care about, and there's a broader community that cares about that, and we can help encourage that, and that's great. That really helps the entire ecosystem, not just those. One of the big things about 2.0 that we're pushing on is, okay, we have these so many different pieces, right. How do we help make all of them work well together? So there are a few key pieces there that we're pushing on, one being the core format in there and how we share the models themselves through save model and TensorFlow hub and so on. And a few of\"}\n",
      "keypoints:\n",
      "- The goal is to enable others to build things that they care about, rather than building every single thing.\n",
      "- The focus is on making different pieces work well together in TensorFlow 2.0.\n",
      "- The core format and sharing of models through save model and TensorFlow hub are key areas of focus.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"So there are a few key pieces there that we're pushing on, one being the core format in there and how we share the models themselves through save model and TensorFlow hub and so on. And a few of the pieces that we really put this together. I was very skeptical that that's, you know, when TensorFlow.js came out, it didn't seem, or deep learning JS as it was earlier. Yeah, that was the first. It seems like technically very difficult project. As a standalone, it's not as difficult, but as a thing that integrates into the ecosystem, it seems very difficult. And still have to be overcome. That's the question here too. And we've iterated over the last few years, so there's a lot we've learned. I, yeah, and\"}\n",
      "keypoints:\n",
      "- Pushing on the core format and sharing models through save model and TensorFlow hub.\n",
      "- Skepticism about the difficulty of integrating TensorFlow.js into the ecosystem.\n",
      "- Learning and iteration over the last few years.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"A lot. And still have to be overcome. There are lots of steps to it, right. And we've iterated over the last few years, so there's a lot we've learned. I, yeah, and often when things come together well, things look easy and that's exactly the point. It should be easy for the end user, but there are lots of things that go behind that. If I think about still challenges ahead, there are, you know, we have a lot more devices coming on board, for example, from the hardware perspective. So there's a lot of compiler stuff that others are working on. There are things we can do in terms of our APIs and so on that we can do. As we, you know, TensorFlow started as a very monolithic system and to some extent it still is. There are less, lots of tools around it, but the\"}\n",
      "keypoints:\n",
      "- There are lots of steps to it, and the process has been iterated over the last few years, resulting in a lot of learning.\n",
      "- The goal is to make things look easy for the end user, but there are many complexities behind the scenes.\n",
      "- Challenges ahead include more devices coming on board, hardware perspective, compiler stuff, and improvements in APIs.\n",
      "- TensorFlow started as a very monolithic system and is still to some extent, with lots of tools around it.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"can do in terms of our APIs and so on that we can do. As we, you know, TensorFlow started as a very monolithic system and to some extent it still is. There are less, lots of tools around it, but the core is still pretty large and monolithic. It's, you know, in some ways it's software engineering 101, but for a system that's now four years old, I guess, or more, and that's still rapidly evolving and that we're not slowing down with, it's hard to change and modify and really break apart. It's sort of like, as people say, right, it's like changing the engine with a car running or trying to fix that. That's exactly what we're trying to do. So there's a challenge here because the downside of so many people being excited about TensorFlow and coming to rely on it in many of their\"}\n",
      "keypoints:\n",
      "- TensorFlow started as a monolithic system and is still largely monolithic.\n",
      "- There are many tools around TensorFlow, but the core is still large and monolithic.\n",
      "- Despite being rapidly evolving, it is hard to change and modify the system.\n",
      "- The challenge lies in the downside of many people relying on TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"trying to fix that. That's exactly what we're trying to do. So there's a challenge here because the downside of so many people being excited about TensorFlow and coming to rely on it in many of their applications is that you're kind of responsible, like it's the technical debt. You're responsible for previous versions to some degree still working. Absolutely. 2.0 does break some back compatibility, but not too much. It seems like the conversion is pretty straightforward. It's a tricky balance. So if it was just a researcher writing a paper who a year later will not look\"}\n",
      "keypoints:\n",
      "- The challenge of so many people relying on TensorFlow in their applications.\n",
      "- The responsibility for previous versions to still work.\n",
      "- The balance between breaking back compatibility and making the conversion straightforward.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"It's a tricky balance. So if it was just a researcher writing a paper who a year later will not look at that code again, sure, it doesn't matter. There are a lot of production systems that rely on TensorFlow, both at Google and across the world. And people worry about this. I mean, these systems run for a long time. So it is important to keep that compatibility and so on. And yes, it does come with a huge cost. There's, we have to think about a lot of things as we do new things and make new changes. You can, you might slow certain kinds of things down, but the overall value you're bringing because of that is much bigger because it's not just about breaking the person yesterday. It's also about telling the person tomorrow that, you know what, this is how we\"}\n",
      "keypoints:\n",
      "- Balancing the need for compatibility with the cost of making new changes.\n",
      "- The importance of keeping compatibility for production systems that rely on TensorFlow.\n",
      "- The concern about the impact of changes on systems that run for a long time.\n",
      "- The overall value of making changes, despite potential slowdowns.\n",
      "- The need to consider a lot of things when making new changes.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"overall value you're bringing because of that is much bigger because it's not just about breaking the person yesterday. It's also about telling the person tomorrow that, you know what, this is how we do things. We're not gonna break you when you come on board because there are lots of new people who are also gonna come on board. And, you know, one way I like to think about this, and I always push the team to think about it as well, when you wanna do new things, you wanna start with a clean slate. Design with a clean slate in mind, and then we'll figure out how to make sure all the other things work. And yes, we do make compromises occasionally, but unless you design with the clean slate and not worry about that, you'll never get to a good place. Oh, that's brilliant, so even if you are responsible when you're in the idea stage, when you're thinking of new, just put all that behind you.\"}\n",
      "keypoints:\n",
      "- The overall value is much bigger because it's not just about breaking the person yesterday.\n",
      "- It's also about telling the person tomorrow that this is how we do things.\n",
      "- When you want to do new things, start with a clean slate in mind.\n",
      "- Design with a clean slate and then figure out how to make sure all the other things work.\n",
      "- Making compromises occasionally is necessary, but designing with a clean slate is important.\n",
      "- Put all that behind you when thinking of new ideas.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"not worry about that, you'll never get to a good place. Oh, that's brilliant, so even if you are responsible when you're in the idea stage, when you're thinking of new, just put all that behind you. Okay, that's really, really well put. So I have to ask this because a lot of students, developers ask me how I feel about PyTorch versus TensorFlow. So I've recently completely switched my research group to TensorFlow. I wish everybody would just use the same thing, and TensorFlow is as close to that, I believe, as we have. But do you enjoy competition? So TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, in terms of number of users, momentum, power, production levels, so on, but a lot of researchers are now also using PyTorch. Do you enjoy that kind of competition or do you just ignore it and focus on making TensorFlow the best that it can be? So just like\"}\n",
      "keypoints:\n",
      "- The speaker has recently switched their research group to TensorFlow.\n",
      "- They believe that TensorFlow is leading in many ways, including ecosystem, number of users, momentum, power, and production levels.\n",
      "- The speaker wishes that everyone would use the same thing, and sees TensorFlow as the closest to that ideal.\n",
      "- They are asked about their feelings on the competition between PyTorch and TensorFlow.\n",
      "- The speaker acknowledges that many researchers are also using PyTorch.\n",
      "- The question is posed whether the speaker enjoys the competition or focuses on making TensorFlow the best it can be.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"levels, so on, but a lot of researchers are now also using PyTorch. So just like research or anything people are doing, it's great to get different kinds of ideas. And when we started with TensorFlow, like I was saying earlier, one, it was very important for us to also have production in mind. We didn't want just research, right? And that's why we chose certain things. Now PyTorch came along and said, you know what, I only care about research. This is what I'm trying to do. And it started iterating and said, okay, I don't need to worry about graphs. Let me just run things. And I don't care if it's not as fast as it can be, but let me just make this part easy. And there are things you can learn from that, right? They, again, had\"}\n",
      "keypoints:\n",
      "- Researchers are now using PyTorch in addition to TensorFlow for their work.\n",
      "- PyTorch is focused on research and making things easy, without worrying about production or speed.\n",
      "- TensorFlow was chosen initially with production in mind, not just for research.\n",
      "- PyTorch's approach of prioritizing research and ease of use can provide valuable lessons.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"about graphs. Let me just run things. And I don't care if it's not as fast as it can be, but let me just make this part easy. And there are things you can learn from that, right? They, again, had the benefit of seeing what had come before, but also exploring certain different kinds of spaces. And they had some good things there, building on say things like JNR and so on before that. So competition is definitely interesting. It made us, you know, this is an area that we had thought about, like I said, way early on. Over time we had revisited this a couple of times, should we add this again? At some point we said, you know what, it seems like this can be done well, so let's try it again. And that's how we started pushing on eager execution. How do we combine those two together? Which has finally come very well together in 2.0, but it took us a while to get all the things together and so\"}\n",
      "keypoints:\n",
      "- The speaker is discussing the importance of making things easy, even if it's not the fastest option.\n",
      "- They mention the benefit of learning from previous experiences and exploring different spaces.\n",
      "- Competition is mentioned as an interesting factor.\n",
      "- The speaker revisited the topic multiple times before deciding to try it again.\n",
      "- They discuss the process of pushing on eager execution and combining different elements together.\n",
      "- It took a while to get everything together for version 2.0.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"that's how we started pushing on eager execution. Which has finally come very well together in 2.0, but it took us a while to get all the things together and so on. So let me ask, put another way, I think eager execution is a really powerful thing that was added. It might have taken longer. No longer? Yeah, it was, I mean, we had tried some variants of that before, so I'm sure it would have happened, but it might have taken longer. I'm grateful that TensorFlow is finally in the way they did. It's doing some incredible work last couple years. What other things that we didn't talk about are you looking forward in 2.0? That comes to mind. So we talked about some of the ecosystem stuff, making it\"}\n",
      "keypoints:\n",
      "- Eager execution has finally come together in 2.0 after some time.\n",
      "- Eager execution is a powerful addition to TensorFlow.\n",
      "- It might have taken longer for eager execution to be implemented.\n",
      "- Grateful for the progress and work done by TensorFlow in the last couple of years.\n",
      "- Looking forward to other things in 2.0, such as ecosystem improvements.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"doing some incredible work last couple years. That comes to mind. So we talked about some of the ecosystem stuff, making it easily accessible to Keras, eager execution. Yeah, so I would say one is just where 2.0 is, and you know, with all the things that we've talked about, I think as we think beyond that, there are lots of other things that it enables us to do and that we're excited about. What it's setting us up for, okay, here are these really clean APIs. We've cleaned up the surface for what the users want. What it also allows us to do a whole bunch of stuff behind the scenes once we are ready with 2.0. So for example, in TensorFlow with graphs and all the things you could do, you could always get a lot of good performance if you spent the time to tune it, right? And\"}\n",
      "keypoints:\n",
      "- Incredible work has been done in the last couple of years.\n",
      "- Focus on ecosystem stuff, making it easily accessible to Keras, eager execution.\n",
      "- Excitement about the possibilities enabled by TensorFlow 2.0.\n",
      "- Clean APIs and improved user experience.\n",
      "- Potential for improved performance with TensorFlow 2.0.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"once we are ready with 2.0. So for example, in TensorFlow with graphs and all the things you could do, you could always get a lot of good performance if you spent the time to tune it, right? And we've clearly shown that, lots of people do that. With 2.0, with these APIs, where we are, we can give you a lot of performance just with whatever you do. You know, because we see these, it's much cleaner. We know most people are gonna do things this way. We can really optimize for that and get a lot of those things out of the box. And it really allows us, you know, both for single machine and distributed and so on, to really explore other spaces behind the scenes after 2.0 in the future versions as well. So right now the team's really excited about that, that over time I think we'll see that. The other piece that I was talking about in terms of just restructuring the monolithic thing into more\"}\n",
      "keypoints:\n",
      "- TensorFlow 2.0 offers improved performance without the need for extensive tuning.\n",
      "- The new APIs in TensorFlow 2.0 allow for cleaner and more optimized code.\n",
      "- The team is excited about the potential for exploring new spaces and features in future versions after 2.0.\n",
      "- There is a focus on restructuring the monolithic structure into a more efficient system.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"well. So right now the team's really excited about that, that over time I think we'll see that. The other piece that I was talking about in terms of just restructuring the monolithic thing into more pieces and making it more modular, I think that's gonna be really important for a lot of the other people in the ecosystem, other organizations and so on that wanted to build things. The way it's organized today is there's one, there are lots of repositories in the TensorFlow organization at GitHub. The core one where we have TensorFlow, it has the execution engine, it has the key backends for CPUs and GPUs, it has the work to do distributed stuff. And all of these just work together in a single library or binary. There's no way to split them apart easily. I mean, there are some interfaces, but\"}\n",
      "keypoints:\n",
      "- The team is excited about the restructuring of the monolithic thing into more modular pieces.\n",
      "- This restructuring will be important for other organizations and individuals in the ecosystem who want to build things.\n",
      "- The current organization of TensorFlow on GitHub consists of lots of repositories, with the core one containing TensorFlow, the execution engine, key backends for CPUs and GPUs, and work for distributed tasks.\n",
      "- All these components work together in a single library or binary, with no easy way to split them apart.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"and GPUs, it has the work to do distributed stuff. And all of these just work together in a single library or binary. There's no way to split them apart easily. I mean, there are some interfaces, but they're not very clean. In a perfect world, you would have clean interfaces where, okay, I wanna run it on my fancy cluster with some custom networking, just implement this and do that. I mean, we kind of support that, but it's hard for people today. I think as we are starting to see more interesting things in some of these spaces, having that clean separation will really start to help. And again, going to the large size of the ecosystem and the different groups involved there, enabling people to evolve and push on things more independently just allows it to scale better. And by people, you mean individual developers and? And organizations. And organizations. That's right. So the hope is\"}\n",
      "keypoints:\n",
      "- GPUs are used for distributed work.\n",
      "- All components work together in a single library or binary.\n",
      "- It is difficult to split the components apart easily.\n",
      "- Clean interfaces are needed for running on custom clusters with custom networking.\n",
      "- Clean separation will help as more interesting things are seen in some spaces.\n",
      "- Enabling people to evolve and push on things independently allows for better scalability.\n",
      "- This includes individual developers and organizations.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"to evolve and push on things more independently just allows it to scale better. And by people, you mean individual developers and. And organizations. And organizations. That's right. So the hope is that everybody sort of major, I don't know, Pepsi or something uses, like major corporations go to TensorFlow to this kind of. Yeah, if you look at enterprises like Pepsi or these, I mean, a lot of them are already using TensorFlow. They are not the ones that do the development or changes in the core. Some of them do, but a lot of them don't. I mean, they touch small pieces. There are lots of these, some of them being, let's say, hardware vendors who are building their custom hardware and they want their own pieces. Or some of them being bigger companies, say, IBM. I mean, they're involved in some of our special interest groups, and they see a lot of users who want certain things and they\"}\n",
      "keypoints:\n",
      "- Independence allows for better scalability.\n",
      "- Individual developers and organizations are the target users of TensorFlow.\n",
      "- Major corporations like Pepsi are already using TensorFlow.\n",
      "- Some enterprises use TensorFlow but do not make changes to the core.\n",
      "- Hardware vendors and bigger companies like IBM are also involved in using TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"want their own pieces. Or some of them being bigger companies, say, IBM. I mean, they're involved in some of our special interest groups, and they see a lot of users who want certain things and they want to optimize for that. So folks like that often. Autonomous vehicle companies, perhaps. Exactly, yes. So, yeah, like I mentioned, TensorFlow has been downloaded 41 million times, 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. So I'm not sure if you can explain it, but what does it take to build a community like that? In retrospect, what do you think, what is the critical thing that allowed for this growth to happen, and how does that growth continue? Yeah, yeah, that's an interesting question. I wish I had all the answers there, I guess, so you could replicate it. I think there are a number of things that need to come together, right? One, just like any new thing,\"}\n",
      "keypoints:\n",
      "- Companies like IBM are involved in special interest groups and want to optimize for certain user needs.\n",
      "- Autonomous vehicle companies are interested in TensorFlow.\n",
      "- TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors.\n",
      "- Building a community like TensorFlow's requires several factors to come together.\n",
      "- The critical thing that allowed for TensorFlow's growth is not clearly defined.\n",
      "- The growth of TensorFlow's community is an ongoing process.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"an interesting question. I wish I had all the answers there, I guess, so you could replicate it. I think there are a number of things that need to come together, right? One, just like any new thing, it is about, there's a sweet spot of timing, what's needed, does it grow with, what's needed, so in this case, for example, TensorFlow's not just grown because it was a good tool, it's also grown with the growth of deep learning itself. So those factors come into play. Other than that, though, I think just hearing, listening to the community, what they do, what they need, being open to, like in terms of external contributions, we've spent a lot of time in making sure we can accept those contributions well, we can help the contributors in adding those, putting the right process in place, getting the right kind of community, welcoming them and so on. Like over the last year, we've really\"}\n",
      "keypoints:\n",
      "- Timing and growth are important factors in the success of a new tool or technology.\n",
      "- TensorFlow's growth is linked to the growth of deep learning.\n",
      "- Listening to the community and understanding their needs is crucial.\n",
      "- Being open to external contributions and having processes in place to accept and integrate them is important for development and growth.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"well, we can help the contributors in adding those, putting the right process in place, getting the right kind of community, welcoming them and so on. Like over the last year, we've really pushed on transparency, that's important for an open source project. People wanna know where things are going, and we're like, okay, here's a process where you can do that, here are our RFCs and so on. So thinking through, there are lots of community aspects that come into that you can really work on. As a small project, it's maybe easy to do because there's like two developers and you can do those. As you grow, putting more of these processes in place, thinking about the documentation, thinking about what two developers care about, what kind of tools would they want to use, all of these come into play, I think. So one of the big things I think that feeds the TensorFlow fire is people building\"}\n",
      "keypoints:\n",
      "- Transparency is important for an open source project.\n",
      "- Providing a process for contributors to know where things are going.\n",
      "- Implementing RFCs (Request for Comments) to facilitate transparency.\n",
      "- Community aspects play a significant role in the project's success.\n",
      "- As the project grows, more processes need to be put in place.\n",
      "- Considering documentation and tools that developers care about.\n",
      "- The success of TensorFlow is attributed to people building on it.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"what two developers care about, what kind of tools would they want to use, all of these come into play, I think. So one of the big things I think that feeds the TensorFlow fire is people building something on TensorFlow, and implement a particular architecture that does something cool and useful, and they put that on GitHub. And so it just feeds this growth. There's lots of tooling that we talked about at the developer summit this week, and we'll continue to invest in that tooling. It's, you know, when\"}\n",
      "keypoints:\n",
      "- Developers care about the tools they use.\n",
      "- Building something on TensorFlow and implementing a particular architecture drives the growth of TensorFlow.\n",
      "- People put their cool and useful projects on GitHub, which further fuels the growth of TensorFlow.\n",
      "- There is a lot of tooling discussed at the developer summit and continued investment in that tooling.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"working hard to make that very easy to do. There's lots of tooling that we talked about at the developer summit this week, and we'll continue to invest in that tooling. It's, you know, when you think of these significant version changes, that's always a risk, and we are really pushing hard to make that transition very, very smooth. So I think, so at some level, people wanna move and they see the value in the new thing. They don't wanna move just because it's a new thing, and some people do, but most people want a really good thing. And I think over the next few months, as people start to see the value, we'll definitely see that shift happening. So I'm pretty excited and confident that we will see people moving. As you said earlier, this field is also moving rapidly, so that'll help because we can do more things and all the new things will clearly happen in 2.x, so people will have lots\"}\n",
      "keypoints:\n",
      "- Tooling investment and focus on making transition smooth.\n",
      "- People want to move for the value, not just because it's new.\n",
      "- Anticipated shift towards the new version in the next few months.\n",
      "- Confidence in seeing people moving to the new version.\n",
      "- Rapidly evolving field will facilitate the transition and enable new features in 2.x.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"see people moving. As you said earlier, this field is also moving rapidly, so that'll help because we can do more things and all the new things will clearly happen in 2.x, so people will have lots of good reasons to move. There are some things that we can expect in terms of, okay, change, yes, change is gonna happen. I would say the basics of deep learning, the, you know, say convolution models or the basic kind of things, they'll probably be around in some form still in five years. Very likely, based on where they are. Will we have new things? Probably, but those\"}\n",
      "keypoints:\n",
      "- The field is moving rapidly, which will allow for more things to be done and new developments to happen in 2.x.\n",
      "- People will have lots of good reasons to move due to the rapid advancements in the field.\n",
      "- Change is expected to happen, especially in terms of the basics of deep learning such as convolution models.\n",
      "- The basics of deep learning, such as convolution models, will likely still be around in some form in five years.\n",
      "- There will probably be new developments in deep learning in the future.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"or the basic kind of things, they'll probably be around in some form still in five years. Will RL and GAN stay? Very likely, based on where they are. Will we have new things? Probably, but those are hard to predict. And some directionally, some things that we can see is, you know, in things that we're starting to do, right, with some of our projects right now is just 2.0 combining eager execution and graphs where we're starting to make it more like just your natural programming language. You're not trying to program something else. Similarly, with Swift for TensorFlow, we're taking that approach. Can you do something ground up, right? So some of those ideas seem like, okay, that's the right direction. In five years, we expect to see more in that area. Other things we don't know is, will hardware accelerators be the same? Will we be able to train with four bits instead of 32 bits? And I\"}\n",
      "keypoints:\n",
      "- Basic things will probably still be around in some form in five years.\n",
      "- RL and GAN are very likely to stay based on their current status.\n",
      "- New things are hard to predict, but there may be some new developments.\n",
      "- Some directionally, some things that can be seen in the direction of combining eager execution and graphs to make it more like natural programming language.\n",
      "- Swift for TensorFlow is taking a ground-up approach, which seems to be the right direction for the future.\n",
      "- In five years, there is an expectation to see more developments in the area of combining eager execution and graphs.\n",
      "- Uncertainty about whether hardware accelerators will remain the same and if training with four bits instead of 32 bits will be possible.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"direction. In five years, we expect to see more in that area. Other things we don't know is, will hardware accelerators be the same? Will we be able to train with four bits instead of 32 bits? And I think the TPU side of things is exploring that. I mean, TPU is already on version three. It seems that the evolution of TPU and TensorFlow are sort of, they're coevolving almost in terms of both are learning from each other and from the community and from the applications where the biggest benefit is achieved. That's right. You've been trying to sort of, with Eager, with Keras, to make TensorFlow as accessible and easy to use as possible. Yeah, for some of them, like you said, right, the beginners want to just be able\"}\n",
      "keypoints:\n",
      "- In five years, there is an expectation to see more development in the area of hardware accelerators and training with four bits instead of 32 bits.\n",
      "- The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications.\n",
      "- Efforts have been made to make TensorFlow as accessible and easy to use as possible, especially for beginners.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"struggle with. Or is basically what Keras is solving is that Eager, like we talked about. Yeah, for some of them, like you said, right, the beginners want to just be able to take some image model, they don't care if it's Inception or ResNet or something else, and do some training or transfer learning on their kind of model. Being able to make that easy is important. So in some ways, if you do that by providing them simple models with say, in hub or so on, they don't care about what's inside that box, but they want to be able to use it. So we're pushing on, I think, different levels. If you look at just a component that you get, which has the layers already smooshed in, the beginners probably just want that. Then the next step is, okay, look at building layers with Keras. If you go out to research, then they are probably writing custom layers themselves or\"}\n",
      "keypoints:\n",
      "- Keras is solving the struggle with Eager.\n",
      "- Beginners want to easily use image models for training or transfer learning.\n",
      "- Providing simple models in hub or similar platforms is important for beginners.\n",
      "- Different levels of users have different needs, from simple pre-built models to custom layers for research.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"in, the beginners probably just want that. Then the next step is, okay, look at building layers with Keras. If you go out to research, then they are probably writing custom layers themselves or doing their own loops. So there's a whole spectrum there. And then providing the pre trained models seems to really decrease the time from you trying to start. You could basically in a Colab notebook achieve what you need. So I'm basically answering my own question because I think what TensorFlow delivered on recently is trivial for beginners. No, those are probably the big ones. I see high schoolers doing a whole bunch of things now, which is pretty amazing. It's both amazing and terrifying. In a sense that when they grow up, it's some incredible ideas will be coming from them. So\"}\n",
      "keypoints:\n",
      "- Beginners may start with using pre-trained models and then move on to building layers with Keras.\n",
      "- Research may involve writing custom layers or doing their own loops.\n",
      "- Providing pre-trained models can decrease the time needed to start a project.\n",
      "- TensorFlow has made it easier for beginners to get started.\n",
      "- High schoolers are now capable of doing advanced things in the field of machine learning, which is both amazing and terrifying.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"schoolers doing a whole bunch of things now, which is pretty amazing. It's both amazing and terrifying. Yes. In a sense that when they grow up, it's some incredible ideas will be coming from them. So there's certainly a technical aspect to your work, but you also have a management aspect to your role with TensorFlow leading the project, a large number of developers and people. Google has been at the forefront of exploring what it takes to build a good team and TensorFlow is one of the most cutting edge technologies in the world. It's definitely something I think a favorite about. I think in terms of the team being able to deliver something well, one of the things that's important is a cohesion across the team. So being able to execute together in doing things that's\"}\n",
      "keypoints:\n",
      "- Schoolers are doing a whole bunch of things, which is amazing and terrifying.\n",
      "- Incredible ideas will come from them when they grow up.\n",
      "- There is a technical aspect to the work, as well as a management aspect with TensorFlow leading the project.\n",
      "- Google has been at the forefront of exploring what it takes to build a good team.\n",
      "- TensorFlow is one of the most cutting-edge technologies in the world.\n",
      "- Cohesion across the team is important for delivering something well.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"about. I think in terms of the team being able to deliver something well, one of the things that's important is a cohesion across the team. So being able to execute together in doing things that's not an end, like at this scale, an individual engineer can only do so much. There's a lot more that they can do together, even though we have some amazing superstars across Google and in the team, but there's, you know, often the way I see it as the product of what the team generates is way larger than the whole or the individual put together. And so how do we have all of them work together, the culture of the team itself, hiring good people is important. But part of that is it's not just that, okay, we hire a bunch of smart people and throw them together and let them do things. It's also people have to care about what they're building, people have to be motivated for the right kind of things.\"}\n",
      "keypoints:\n",
      "- Cohesion across the team is important for delivering something well.\n",
      "- The product of what the team generates is larger than the individual contributions.\n",
      "- Hiring good people is important, but they also need to care about what they're building and be motivated for the right things.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"we hire a bunch of smart people and throw them together and let them do things. It's also people have to care about what they're building, people have to be motivated for the right kind of things. That's often an important factor. Google's a very bottom up organization in some sense, also research even more so, and that's how we started. But as we've become this larger product and ecosystem, I think it's also important to combine that well with a mix of, okay, here's the direction we wanna go in. There is exploration we'll do around that, but let's keep staying in that direction, not just all over the place. And is there a way you monitor the health of the team? Sort of like, is there\"}\n",
      "keypoints:\n",
      "- Hiring smart people and allowing them to do things.\n",
      "- The importance of caring and motivation in building things.\n",
      "- Google's bottom-up organization and the need for direction in a larger product and ecosystem.\n",
      "- Monitoring the health of the team.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"go in. There is exploration we'll do around that, but let's keep staying in that direction, not just all over the place. And is there a way you monitor the health of the team? Sort of like, is there a way you know you did a good job? The team is good? Like, I mean, you're sort of, you're saying nice things, but it's sometimes difficult to determine how aligned. Yes. Because it's not binary. It's not like there's tensions and complexities and so on. And the other element of the mission of superstars, there's so much, even at Google, such a large percentage of work is done by individual superstars too. So there's a, and sometimes those superstars can be against the dynamic of a team and those tensions. I mean, I'm sure in TensorFlow it might be a little bit easier because the mission of the project is so sort of beautiful. You're at the cutting edge, so it's exciting. But have you had\"}\n",
      "keypoints:\n",
      "- Exploration around staying in a specific direction.\n",
      "- Monitoring the health of the team and determining if a good job has been done.\n",
      "- The complexities and tensions within a team.\n",
      "- The impact of individual superstars on team dynamics.\n",
      "- The mission of the project and its impact on team dynamics.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"tensions. I mean, I'm sure in TensorFlow it might be a little bit easier because the mission of the project is so sort of beautiful. You're at the cutting edge, so it's exciting. There are always people challenges in different kinds of ways. That said, I think we've been what's good about getting people who care and are, you know, have the same kind of culture, and that's Google in general to a large extent. But also, like you said, given that the project has had so many exciting things to do, there's been room for lots of people to do different kinds of things and grow, which does make the problem a bit easier, I guess. And it allows people, depending on what they're doing, if there's room around them, then that's fine. But yes, we do care about whether a superstar or not, that they need to work well with the team across\"}\n",
      "keypoints:\n",
      "- The mission of the TensorFlow project is beautiful and exciting.\n",
      "- There are always people challenges in different ways.\n",
      "- It is important to have people who care and have the same culture.\n",
      "- The project allows for lots of people to do different things and grow.\n",
      "- It is important for people to work well with the team.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"allows people, depending on what they're doing, if there's room around them, then that's fine. But yes, we do care about whether a superstar or not, that they need to work well with the team across Google. That's interesting to hear. So it's like superstar or not, the productivity broadly is about the team. I mean, they might add a lot of value, but if they're hurting the team, then that's a problem. So in hiring engineers, it's so interesting, right, the hiring process. Again, no magic answers, I'm sure. Google has a hiring process that we've refined over the last 20 years, I guess, and that you've probably heard and seen a lot about. So we do work with the same hiring process and that's really helped. For me in particular, I would\"}\n",
      "keypoints:\n",
      "- Productivity at Google is about the team, not just individual superstars.\n",
      "- Hiring engineers at Google involves a refined process that has been developed over the last 20 years.\n",
      "- The hiring process at Google has been successful and has helped the company.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"we've refined over the last 20 years, I guess, and that you've probably heard and seen a lot about. So we do work with the same hiring process and that's really helped. For me in particular, I would say, in addition to the core technical skills, what does matter is their motivation in what they wanna do. Because if that doesn't align well with where we wanna go, that's not gonna lead to long term success for either them or the team. And I think that becomes more important the more senior the person is, but it's important at every level. Like even the junior most engineer, if they're not motivated to do well at what they're trying to do, however smart they are, it's gonna be hard for them to succeed.\"}\n",
      "keypoints:\n",
      "- The hiring process has been refined over the last 20 years.\n",
      "- Core technical skills are important, but motivation is also crucial.\n",
      "- Alignment of motivation with the company's goals is important for long term success.\n",
      "- Motivation is important at every level, from junior to senior positions.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"succeed. So like trying to determine, because I think as far as I understand, maybe you can speak to it, that the Google hiring process sort of helps in the initial like determines the skill set there, is your puzzle solving ability, problem solving ability good? But like, I'm not sure, but it seems that the determining whether the person is like fire inside them, that burns to do anything really, it doesn't really matter. It's just some cool stuff, I'm gonna do it. Is that something that ultimately ends up when they have a conversation with you or once it gets closer to the team? So one of the things we do have as part of the process is just a culture fit, like part of the interview process itself, in addition to just the technical skills and each engineer or whoever the interviewer is, is supposed to rate the person on the culture\"}\n",
      "keypoints:\n",
      "- Google hiring process focuses on determining puzzle solving and problem solving abilities.\n",
      "- The process also aims to determine if the person has a strong internal drive and passion for doing cool stuff.\n",
      "- Culture fit is an important part of the interview process at Google.\n",
      "- Interviewers are supposed to rate the person on both technical skills and cultural fit.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"just a culture fit, like part of the interview process itself, in addition to just the technical skills and each engineer or whoever the interviewer is, is supposed to rate the person on the culture and the culture fit with Google and so on. So that is definitely part of the process. Now, there are various kinds of projects and different kinds of things. So there might be variants and of the kind of culture you want there and so on. And yes, that does vary. So for example, TensorFlow has always been a fast moving project and we want people who are comfortable with that. But at the same time now, for example, we are at a place where we are also very full fledged product and we wanna make sure things that work really, really work, right? You can't cut corners all the time. So balancing that out and finding the people who are the right fit for those is important. And I think those kinds of\"}\n",
      "keypoints:\n",
      "- Culture fit is an important part of the interview process at Google.\n",
      "- Engineers are supposed to rate the person on the culture fit with Google.\n",
      "- Different projects and teams may have different cultures and requirements.\n",
      "- TensorFlow is a fast-moving project and requires people who are comfortable with that.\n",
      "- There is also a focus on making sure things work really well, without cutting corners.\n",
      "- Finding the right fit for different project requirements is important.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"things that work really, really work, right. You can't cut corners all the time. So balancing that out and finding the people who are the right fit for those is important. And I think those kinds of things do vary a bit across projects and teams and product areas across Google. And so you'll see some differences there in the final checklist. But a lot of the core culture, it comes along with just the engineering excellence and so on. I'll take your pick, I guess. It's fun, I would say, right. Hard, yes. I mean, lots of things at different times. I think that does vary. So let me clarify that difficult things are fun when you solve them, right. So it's fun in that sense. I think the key to a successful thing across the board and in this case, it's a large ecosystem now, but even a small product, is striking that fine balance across different aspects.\"}\n",
      "keypoints:\n",
      "- Balancing work and finding the right fit for people is important.\n",
      "- Core culture at Google includes engineering excellence.\n",
      "- Difficult things are fun when solved.\n",
      "- Striking a fine balance across different aspects is key to success.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"in that sense. I think the key to a successful thing across the board and in this case, it's a large ecosystem now, but even a small product, is striking that fine balance across different aspects of it. Sometimes it's how fast do you go versus how perfect it is. Sometimes it's how do you involve this huge community. Sometimes it's saying no to certain kinds of things. Those are often the hard decisions. Some of them you make quickly because you don't have the time. Some of them you get time to think about them, but they're always hard. So both choices are pretty good, those decisions. You had the Dev\"}\n",
      "keypoints:\n",
      "- The key to success is finding a balance across different aspects of a product or ecosystem.\n",
      "- Balancing speed and perfection, community involvement, and saying no to certain things are hard decisions.\n",
      "- Some decisions are made quickly due to time constraints, while others are given more time for consideration.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"decisions. You had the Dev Summit today that came together incredibly. Looked like there's a lot of moving pieces and so on. I'm sure that was done last minute as well. I mean, up to the last point. Again, it's one of those things that you need to strike the good balance. There's some value that deadlines bring that does bring a sense of urgency to get the right things together. Instead of getting the perfect thing out, you need something that's good and works well. And the team definitely did a great job in putting that together. So I was very amazed and excited by everything how that came together. That said, across the year, we\"}\n",
      "keypoints:\n",
      "- The Dev Summit came together incredibly.\n",
      "- There were a lot of moving pieces and it seemed like it was done last minute.\n",
      "- It's important to strike a good balance and value deadlines bring a sense of urgency.\n",
      "- Instead of perfection, it's important to have something that's good and works well.\n",
      "- The team did a great job in putting everything together.\n",
      "- The speaker was amazed and excited by how everything came together.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"that's good and works well. And the team definitely did a great job in putting that together. So I was very amazed and excited by everything how that came together. We focus on key things that are important, figure out how much of it's important. And we are developing in the open, both internally and externally, everything's available to everybody. So you can pick and look at where things are. We do releases at a regular cadence. So fine, if something doesn't necessarily end up this month, it'll end up in the next release in a month or two. And that's okay, but we want to keep moving as fast as we can in these different areas. Because we can iterate and improve on things, sometimes it's okay to put things out that aren't fully ready. We'll make sure it's clear that okay, this is experimental, but it's out there if you\"}\n",
      "keypoints:\n",
      "- The team did a great job in putting things together.\n",
      "- Focus on key things that are important and figure out their importance.\n",
      "- Developing in the open, both internally and externally, with everything available to everybody.\n",
      "- Regular releases at a regular cadence, with the flexibility to include unfinished items in the next release.\n",
      "- Moving as fast as possible in different areas to iterate and improve on things.\n",
      "- Sometimes it's okay to put out experimental things that aren't fully ready, with clear communication about their status.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"Because we can iterate and improve on things, sometimes it's okay to put things out that aren't fully ready. We'll make sure it's clear that okay, this is experimental, but it's out there if you want to try and give feedback. That's very, very useful. I think that quick cycle and quick iteration is important. That's what we often focus on rather than here's a deadline where you get everything else. So it's\"}\n",
      "keypoints:\n",
      "- Iterating and improving on things allows for putting out experimental work that may not be fully ready.\n",
      "- It's important to make it clear that the work is experimental and open to feedback.\n",
      "- Quick cycle and quick iteration are important in the development process.\n",
      "- Focus is on quick iteration rather than meeting a deadline for everything.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"TensorFlow 2.0 in that same kind of way or is there this pressure to once it hits 2.0, once you get to the release candidate and then you get to the final, that's gonna be the stable thing. So it's gonna be stable in, just like when NodeX was where every API that's there is gonna remain in work. It doesn't mean we can't change things under the covers. It doesn't mean we can't add things. So there's still a lot more for us to do and we'll continue to have more releases. So in that sense, there's still, I don't think we'll be done in like two months when we release this. I don't know if you can say, but is there, there's not external deadlines for TensorFlow 2.0, but is there internal deadlines, the artificial or otherwise, that you're trying to set for yourself or is it whenever it's ready? So we want it to be a great product, right? And that's a big important piece for us. TensorFlow's\"}\n",
      "keypoints:\n",
      "- TensorFlow 2.0 is aiming to be stable and every API that's there is going to remain in work.\n",
      "- There is still a lot more for the team to do and they will continue to have more releases.\n",
      "- There are no external deadlines for TensorFlow 2.0, but there may be internal deadlines or goals for the team.\n",
      "- The team wants TensorFlow 2.0 to be a great product and that is a big important piece for them.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"the artificial or otherwise, that you're trying to set for yourself or is it whenever it's ready. So we want it to be a great product, right. And that's a big important piece for us. TensorFlow's already out there. We have 41 million downloads for 1.0 X. So it's not like we have to have this. Yeah, exactly. So it's not like, a lot of the features that we've really polishing and putting them together are there. We don't have to rush that just because. So in that sense, we wanna get it right and really focus on that. That said, we have said that we are looking to get this out in the next few months, in the next quarter. And as far as possible, we'll definitely try to make that happen. Yeah, my favorite line was, spring is a relative concept. I love it. Yes. Spoken like a true developer. So something I'm really interested in and your previous line of work is, before TensorFlow, you led a\"}\n",
      "keypoints:\n",
      "- The focus is on creating a great product with TensorFlow.\n",
      "- There have been 41 million downloads for TensorFlow 1.0 X.\n",
      "- The team is not rushing the release of the product and is focused on getting it right.\n",
      "- The goal is to release the product in the next few months or the next quarter.\n",
      "- The concept of \"spring is a relative concept\" is appreciated.\n",
      "- The speaker is interested in the previous line of work before TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"my favorite line was, spring is a relative concept. I love it. Yes. Spoken like a true developer. So something I'm really interested in and your previous line of work is, before TensorFlow, you led a team at Google on search ads. I think this is a very interesting topic on every level, on a technical level, because at their best, ads connect people to the things they want and need. So, and at their worst, they're just these things that annoy the heck out of you to the point of ruining the entire user experience of whatever you're actually doing. So they have a bad rep, I guess. And on the other end, so that this connecting users to the thing they need and want is a beautiful opportunity for machine learning to shine. Like huge amounts of data that's personalized and you kind of map to the thing they actually want won't get annoyed. So what have you learned from this, Google that's\"}\n",
      "keypoints:\n",
      "- Spring is a relative concept, spoken like a true developer.\n",
      "- Previous work at Google on search ads, which is an interesting topic on a technical level.\n",
      "- Ads at their best connect people to the things they want and need, but at their worst, they can ruin the user experience.\n",
      "- Machine learning has the opportunity to shine in connecting users to what they want and need, using personalized data.\n",
      "- Learning from Google's experience with ads.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"for machine learning to shine. Like huge amounts of data that's personalized and you kind of map to the thing they actually want won't get annoyed. So what have you learned from this, Google that's leading the world in this aspect, what have you learned from that experience and what do you think is the future of ads? Take you back to that. Yeah, yes, it's been a while, but I totally agree with what you said. I think the search ads, the way it was always looked at and I believe it still is, is it's an extension of what search is trying to do. And the goal is to make the information and make the world's information accessible. That's it's not just information, but maybe products or other things that people care about. And so it's really important for them to align with what the users need. And in search ads, there's a minimum quality level before that ad would be shown. If you don't have\"}\n",
      "keypoints:\n",
      "- Machine learning shines with huge amounts of personalized data.\n",
      "- Google is leading in this aspect.\n",
      "- The future of ads is important to consider.\n",
      "- Search ads are an extension of what search is trying to do.\n",
      "- The goal is to make information and products accessible to users.\n",
      "- There is a minimum quality level for search ads to be shown.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"that people care about. And so it's really important for them to align with what the users need. And in search ads, there's a minimum quality level before that ad would be shown. If you don't have an ad that hits that quality, but it will not be shown even if we have it and okay, maybe we lose some money there, that's fine. That is really, really important. And I think that that is something I really liked about being there. Advertising is a key part. I mean, as a model, it's been around for ages, right? It's not a new model, it's been adapted to the web and became a core part of search and many other search engines across the world. And I do hope, like you said, there are aspects of ads that are annoying and I go to a website and if it just keeps popping an ad in my face not to let me read, that's gonna be annoying clearly. So I hope we can strike that balance between showing a good ad\"}\n",
      "keypoints:\n",
      "- It's important for ads to align with what users need.\n",
      "- In search ads, there's a minimum quality level before an ad is shown.\n",
      "- Advertising has been around for ages and has been adapted to the web.\n",
      "- There are aspects of ads that can be annoying, such as ads that keep popping up and interrupting the user.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"are annoying and I go to a website and if it just keeps popping an ad in my face not to let me read, that's gonna be annoying clearly. So I hope we can strike that balance between showing a good ad where it's valuable to the user and provides the monetization to the service. And this might be search, this might be a website, all of these, they do need the monetization for them to provide that service. But if it's done in a good balance between showing just some random stuff that's distracting versus showing something that's actually valuable. And advertisements, again, coupled at their best, are actually really useful and not\"}\n",
      "keypoints:\n",
      "- Balance between showing valuable ads and annoying ads is important.\n",
      "- Monetization is necessary for services like search and websites.\n",
      "- Good balance is needed to avoid showing distracting ads.\n",
      "- Advertisements, when done well, can be really useful.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"one of the most exciting things but also limiting things in the internet is nobody wants to pay for anything. And advertisements, again, coupled at their best, are actually really useful and not annoying. I think it's a mix. I think it's gonna take a long while for everything to be paid on the internet, if at all, probably not. I mean, I think there's always gonna be things that are sort of monetized with things like ads. But over the last few years, I would say we've definitely seen that transition towards more paid services across the web and people are willing to pay for them because they do see the value. I mean, Netflix is a great example. I mean, we have YouTube doing things. People pay for the apps they buy. More people I find\"}\n",
      "keypoints:\n",
      "- The internet is exciting but also limiting because nobody wants to pay for anything.\n",
      "- Advertisements, when done well, are useful and not annoying.\n",
      "- It will take a long time for everything to be paid for on the internet, if at all.\n",
      "- There has been a transition towards more paid services across the web in recent years.\n",
      "- People are willing to pay for services they see value in, such as Netflix and paid apps.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"and people are willing to pay for them because they do see the value. I mean, Netflix is a great example. I mean, we have YouTube doing things. People pay for the apps they buy. More people I find are willing to pay for newspaper content for the good news websites across the web. That wasn't the case a few years, even a few years ago, I would say. And I just see that change in myself as well and just lots of people around me. So definitely hopeful that we'll transition to that mix model where maybe you get to try something out for free, maybe with ads, but then there's a more clear revenue model that sort of helps go beyond that. And I'm asked by MIT, what is going to\"}\n",
      "keypoints:\n",
      "- People are willing to pay for content they see value in, such as Netflix and newspaper websites.\n",
      "- There is a shift towards more people being willing to pay for online content compared to a few years ago.\n",
      "- The speaker and people around them are also seeing a change in their willingness to pay for content.\n",
      "- There is hope for a transition to a mixed model where content can be tried for free with ads, but also has a clear revenue model.\n",
      "- The speaker was asked by MIT about the future of content consumption.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"TPU in a Google call app for free. So what's the, I guess the question is, what's the future of TensorFlow in terms of empowering, say, a class of 300 students. And I'm asked by MIT, what is going to be the future of them being able to do their homework in TensorFlow. Like, where are they going to train these networks, right. What's that future look like with TPUs, with cloud services, and so on. I think a number of things there. I mean, any TensorFlow open source, you can run it wherever, you can run it on your desktop and your desktops always keep getting more powerful, so maybe you can do more. My phone is like, I don't know how many times more powerful than my first desktop. You'll probably train it on your phone though, yeah, that's true. Right, so in that sense, the power you have in your hands is a lot more. Clouds are actually very interesting from, say, students or courses.\"}\n",
      "keypoints:\n",
      "- The future of TensorFlow in terms of empowering a class of 300 students.\n",
      "- The ability for students to do their homework in TensorFlow.\n",
      "- The potential for training networks with TPUs and cloud services.\n",
      "- The flexibility of running TensorFlow open source on various devices.\n",
      "- The increasing power of desktops and mobile phones for running TensorFlow.\n",
      "- The potential for training models on mobile phones.\n",
      "- The interest in cloud services for students and courses.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"probably train it on your phone though, yeah, that's true. Right, so in that sense, the power you have in your hands is a lot more. Clouds are actually very interesting from, say, students or courses perspective, because they make it very easy to get started. I mean, Colab, the great thing about it is, go to a website and it just works. No installation needed, nothing to, you're just there and things are working. That's really the power of cloud as well. And so I do expect that to grow. Again, Colab is a free service. It's great to get started, to play with things, to explore things. That said, with free, you can only get so much. You'd be, yeah. So just like we were talking about, free versus paid, yeah, there are services you can pay for and get a lot more. Great, so if I'm a complete beginner interested in machine learning and TensorFlow, what should I do? Probably start with going\"}\n",
      "keypoints:\n",
      "- Cloud computing makes it easy to get started with machine learning and TensorFlow.\n",
      "- Colab is a free service that allows beginners to play and explore machine learning.\n",
      "- Paid services offer more features and capabilities for machine learning.\n",
      "- Starting with going through a beginner's guide is recommended for complete beginners interested in machine learning and TensorFlow.\n",
      "-------\n",
      "chunks:\n",
      "{'text': \"paid, yeah, there are services you can pay for and get a lot more. Great, so if I'm a complete beginner interested in machine learning and TensorFlow, what should I do? Probably start with going to our website and playing there. So just go to TensorFlow.org and start clicking on things. Yep, check out tutorials and guides. There's stuff you can just click there and go to a Colab and do things. No installation needed, you can get started right there. Okay, awesome, Rajit, thank you so much for talking today. Thank you, Lex, it was great.\"}\n",
      "keypoints:\n",
      "- Services available for payment to get more in machine learning and TensorFlow.\n",
      "- Complete beginners should start by visiting TensorFlow.org and exploring tutorials and guides.\n",
      "- No installation needed to get started, can use Colab for practice.\n",
      "- Rajit and Lex had a great conversation.\n",
      "-------\n",
      "extract_keypoints done time 2024-03-17 20:52:30.809513\n",
      "RRR keypoints\n",
      "keypoints\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text, True)\n",
    "    \n",
    "    print(\"RRR keypoints\")\n",
    "    for keypoint in keypoints:\n",
    "        print(keypoint)\n",
    "        \n",
    "    continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = summarize_stage_1(chunks_text)['stage_1_outputs']\n",
    "    # Split the titles and summaries\n",
    "    stage_1_summaries = [e['summary'] for e in stage_1_outputs]\n",
    "    stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_summaries)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    summary_embeds = generate_embeddings(stage_1_summaries)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "        summary_similarity_matrix[row, col] = similarity\n",
    "        summary_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "    time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(summary_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "    chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "    print(f\"topics out: {len(topics)}\")\n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_v3_stage3_extractkeypoints.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "    time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
