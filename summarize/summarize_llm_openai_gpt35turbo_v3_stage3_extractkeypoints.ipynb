{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb031bc6f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fb031aa2d90>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_summary_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'summary': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'summary': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'summary': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'summary': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_1(chunks_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following podcast an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# Use less word to try solve the warning/error:\n",
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). \n",
    "# Running this sequence through the model will result in indexing errors\n",
    "# 75-100\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'summaries': [stage_1_outputs[chunk_id]['summary'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['summaries_concat'] = ' '.join(topic_data['summaries'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['summaries_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints = keypoints + result_keypoints\n",
    "\n",
    "  # deduplication\n",
    "  keypoints = list(set(keypoints)) \n",
    "\n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-17 21:23:09.608663\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-17 21:27:51.814655\n",
      "chunks_text len: 92\n",
      "extract_keypoints start time: 2024-03-17 21:27:51.814801\n",
      "extract_keypoints done time 2024-03-17 21:29:43.430702\n",
      "RRR keypoints\n",
      "Rajat Manga is an engineer and director of Google, leading the TensorFlow team.\n",
      "TensorFlow is an open source library at the center of much of the work in deep learning.\n",
      "It is now an ecosystem of tools for the deployment of machine learning in various platforms.\n",
      "There is a big emphasis on growing a passionate community of developers.\n",
      "TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain.\n",
      "The decision to open source TensorFlow is a definitive moment in the tech industry.\n",
      "TensorFlow 2.0 is now in alpha.\n",
      "The decision to open source TensorFlow is a definitive moment in the tech industry.\n",
      "Open innovation can be successful and inspire many companies to open source their code.\n",
      "Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean.\n",
      "Google Brain started with the belief in a proprietary machine learning library and turned into TensorFlow in 2014, the open source library.\n",
      "The proprietary machine learning library was turned into TensorFlow in 2014, the open source library.\n",
      "Deep learning was interesting and intriguing in some ways, showing promising and early results.\n",
      "The idea was to scale research work to Google's compute power and data, with promising results so far.\n",
      "The results show that scaling the compute and data leads to better performance.\n",
      "Early wins were achieved in speech and image recognition.\n",
      "The birth of Google Brain was focused on neural networks and deep learning from the beginning.\n",
      "Google Brain was focused on neural networks and deep learning from the beginning.\n",
      "The mission was to scale deep learning to hundreds and thousands of machines.\n",
      "Some runs even scaled to 10,000 machines, showing great promise.\n",
      "Google has been doing machine learning for a long time, including deep learning.\n",
      "Google has been doing machine learning for a long time.\n",
      "Deep learning was new, but as they scaled it up, they showed that it was possible and would impact many things.\n",
      "Real products started to use machine learning, such as speech and image recognition.\n",
      "Academia also started to show interest in deep learning around 2014.\n",
      "Google recognized that deep learning was a big thing and would continue to grow both internally and externally.\n",
      "The decision to open source TensorFlow is considered a big seminal moment in software engineering.\n",
      "Google's decision to go open source with a large project like TensorFlow showed the power of open innovation.\n",
      "This decision led the entire world in embracing open innovation.\n",
      "It was a significant moment in time when a large company like Google decided to go open source with a project that had a lot of intellectual property.\n",
      "Open innovation is a powerful thing and it's okay to do.\n",
      "The initial idea came from Jeff, who was a big proponent of open innovation.\n",
      "The research group was putting all their research out there and building on others' research to push the state of the art forward.\n",
      "Sharing research has contributed to the rapid growth of deep learning and machine learning.\n",
      "The next step was to consider how software could help with sharing research.\n",
      "There were existing libraries such as Tiano and Torch that could potentially help with sharing research.\n",
      "The speaker considered using software to improve the speed of a process.\n",
      "Existing libraries such as Tiano and Torch were mentioned, but they were developed by academia and had a different level of quality.\n",
      "Google had developed and used a lot of software internally, and often published papers that led to successful open source projects.\n",
      "The speaker believed that the technology developed by Google was superior to existing options, such as Hadoop.\n",
      "Google Cloud was not providing their technology, but instead offering services like Bigtable.\n",
      "Google Cloud is providing Bigtable and H base APIs.\n",
      "The goal is to provide something better and push a good standard forward.\n",
      "Cloud fits into the strategy by providing resources and integrations for TensorFlow.\n",
      "TensorFlow is open and can be used anywhere.\n",
      "Google Cloud ensures lots of integrations with TensorFlow.\n",
      "TensorFlow is open and can be used anywhere.\n",
      "Google Cloud ensures integrations with other tools and works well with TensorFlow.\n",
      "TensorFlow has an incredible ecosystem.\n",
      "The history of TensorFlow is defined by sample moments in deep learning.\n",
      "Building TensorFlow was a fast-moving process due to the rapid pace of deep learning advancements.\n",
      "Deep learning moves fast, making just a few years already history.\n",
      "TensorFlow was open sourced in November 2015, after starting development in the summer of 2014.\n",
      "Consideration for open sourcing TensorFlow began in late 2014, with a focus on supporting different hardware and use cases at Google.\n",
      "The design of TensorFlow included support for running at large scale in the data center and for different kinds of hardware, including GPUs.\n",
      "The design included GPUs and support for mobile models.\n",
      "Customizing code was important, so TensorFlow was designed to support that.\n",
      "Google already had products using customized code and internal libraries.\n",
      "The speaker was at Google during this time.\n",
      "The speaker had experience with customized handcrafted code and internal libraries at Google.\n",
      "The use of Theano and Caffe at Google influenced design decisions for the speaker's project.\n",
      "The speaker's project focused on building belief and was tailored to their internal systems.\n",
      "The speaker looked at a number of libraries that were available at the time.\n",
      "Our systems were very different, so we focused on building our internal belief.\n",
      "We looked at a number of libraries such as Theano, Torch, Lua, Caffe, and possibly JNR.\n",
      "We discussed ideas around whether to have a graph or not.\n",
      "We wanted flexibility due to the fast-moving research and changing hardware.\n",
      "The research was moving fast and required flexibility in terms of hardware and expression of ideas.\n",
      "The decision to move towards TensorFlow 2.0 and eager execution was influenced by the need for a more intuitive development process.\n",
      "The disbelief had a graph-like structure, similar to Theano, which influenced the decision-making process.\n",
      "The decision to use a graph came from the need to deploy a lot of stuff in production.\n",
      "The graph was chosen because it made deployment simpler.\n",
      "The focus on production was a key factor in the decision-making process.\n",
      "Anticipation of the other side of production was also considered.\n",
      "Production seems to be a really good thing to focus on.\n",
      "The popularity of the product was not anticipated, with 41 million downloads.\n",
      "There was a need for this from a research perspective and early days of deep learning.\n",
      "Open sourcing led to a potential future where more people would be doing this.\n",
      "Deep learning is growing way faster for a lot of different reasons.\n",
      "The company is in the right place to push on the growth of deep learning.\n",
      "Deep learning is growing rapidly for various reasons.\n",
      "The company is in a good position to leverage deep learning and deliver on people's needs.\n",
      "There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel.\n",
      "The company's approach is very community-driven.\n",
      "The initial version of the product was 0.6 or 0.5.\n",
      "The company has gone through a few changes since starting out.\n",
      "The documentation for the version 0.6 or five was well-received initially.\n",
      "It was a significant improvement from academic projects in terms of documentation.\n",
      "The improved documentation changed how things started to scale up.\n",
      "There was a focus on stabilizing the project and providing stability for users.\n",
      "The shift from just researchers to a wider user base led to a need for stability.\n",
      "Things started to scale up and stabilize over the next few months.\n",
      "Planning for version 1.0 involved addressing certain needs such as documentation and designs.\n",
      "More and more enterprises wanted to buy in and support the product after version 1.0.\n",
      "Between the initial release and 1.0, there was initial interest from researchers and hobbyists, followed by enterprise adoption after 1.0.\n",
      "Anything below 1.0 was mainly for researchers and early interest.\n",
      "People are excited about hobbies and early interests.\n",
      "Enterprises prefer stability, especially for versions below 1.0.\n",
      "Enterprises have specific wants and needs that can be easy to forget.\n",
      "Some people still use older models like Inception and ResNet 50.\n",
      "People are still using older models like Inception and ResNet 50.\n",
      "Some users prioritize stability and simplicity over the latest performance or quality improvements.\n",
      "Providing stability and simplicity allows more people to access the models.\n",
      "The research crowd is interested in pushing the boundaries with new and complex models like RNNs, transformers, RL, and GANs.\n",
      "RNNs are being replaced by transformers.\n",
      "The boundary of technology is shifting and pushing the state of the art.\n",
      "Older technology is still very usable by many people.\n",
      "Transfer learning on specific problems is a common use case.\n",
      "Making transfer learning as easy as possible is important for hobbyists.\n",
      "Majority of the world uses transfer learning on existing models like ResNet 50.\n",
      "The use of RL and GANs is becoming more prevalent in the field.\n",
      "The most common case for hobbyists is using apps and phones for data analysis and predictions.\n",
      "Enterprises use data for making predictions, often using regression models or deep learning for structured data.\n",
      "The audience determines the type of data analysis and prediction methods used.\n",
      "Some audiences still benefit from deep learning, while others prefer structured data.\n",
      "Enterprise with large datasets can benefit from deep learning.\n",
      "The audience at the developer summit is interested in TensorFlow Extended for stability and simplicity in their entire pipeline.\n",
      "Users want to be able to train models daily and have simplicity across the entire process.\n",
      "Machine learning is not yet widely adopted in legal and other disciplines due to old school data organization.\n",
      "Companies need to digitize and organize their data in order to benefit from TensorFlow and other technologies.\n",
      "There is a need for evangelists to promote the importance of organizing data for reaping the benefits of machine learning.\n",
      "Questions arise about the necessity of deep learning and the requirements for making machine learning work effectively.\n",
      "Conversations range from basic questions about getting started with machine learning to more specific expert support.\n",
      "The key to automation and making predictions is to ensure that data is organized and digitized.\n",
      "The TensorFlow ecosystem provides a growing number of data sets and pre-trained models for use in machine learning.\n",
      "More data sets and pre-trained models are being provided in TensorFlow.\n",
      "The release of TensorFlow data sets has made it easier for people to access and organize data sets.\n",
      "It is important to start with basic models and then improve them.\n",
      "The appearance of Keras has made TensorFlow more accessible.\n",
      "Keras was initially outside of TensorFlow, on top of Tiano.\n",
      "Francois started the Keras project before he was at Google.\n",
      "Keras became on top of TensorFlow at some point.\n",
      "There were enough similarities between Keras and TensorFlow that Francois decided to create an interface and put TensorFlow as a backend.\n",
      "This decision may have been made before Francois joined Google.\n",
      "The individual joined Google before the integration of TensorFlow as a backend.\n",
      "He independently decided to work on integrating TensorFlow with the interface.\n",
      "He initially worked on research ideas and Keras as a side project.\n",
      "He was not initially part of the TensorFlow team but joined the research team.\n",
      "He has contributed to significant research and has published papers.\n",
      "The decision to integrate with the interface was made after realizing his valuable contributions.\n",
      "He eventually joined the team to work on the integration.\n",
      "Keras got integrated into TensorFlow in a deep way.\n",
      "TensorFlow 2.0 recommends Keras as the recommended way for a beginner to interact with TensorFlow.\n",
      "The integration was initially planned for a quarter but has been ongoing for two years.\n",
      "The person responsible for the API integration is fully committed to the project.\n",
      "The decision to integrate Keras into TensorFlow was the result of careful consideration and evaluation of various APIs.\n",
      "The decision to do Keras in parallel with other APIs was a bold one.\n",
      "The goal was to make the APIs look similar and be as integrated as possible.\n",
      "There were multiple APIs built by different parties, causing confusion in the community.\n",
      "The community was unsure which API to use and kept seeing models in different APIs.\n",
      "The main goal of version 2.0 was to simplify and pick one API.\n",
      "The decision to simplify and pick one model for 2.0 was necessary.\n",
      "Keras was chosen as the model due to its popularity and positive feedback from users.\n",
      "Initially, there was concern that Keras might be a competitor to TensorFlow, but it ended up being an empowering element.\n",
      "The team was surprised to bring in an outside model like Keras.\n",
      "There was collaboration and alignment between Francois, the team, and others in the decision to choose Keras.\n",
      "The team and Francois are aligned in their goals for making it easier for developers.\n",
      "Python has Guido van Rossum as the benevolent dictator for life.\n",
      "Successful open source projects like TensorFlow need one person to make final decisions.\n",
      "The TensorFlow Dev Summit was successful with new features being incorporated.\n",
      "There is an amazing ecosystem surrounding TensorFlow.\n",
      "The ecosystem has been driven by a number of people, including Martin Wick.\n",
      "Regular design reviews are conducted.\n",
      "Efforts have been made to open up to the community and add transparency.\n",
      "Processes such as RFCs and special interest groups are being set in place to grow the community and scale it.\n",
      "The ecosystem is at a scale that is difficult to scale further.\n",
      "Setting more processes in place, such as RFCs and special interest groups, to grow the community and scale it.\n",
      "Acknowledgment that the ecosystem cannot scale with a lone decision maker.\n",
      "Growth of the ecosystem, starting with Andrej Karpathy's ComNetJS and the development of TensorFlow.js.\n",
      "Mention of TensorFlow Extended, TensorFlow Lite for mobile, and the convergence towards being able to save models in the same way.\n",
      "TensorFlow Lite for mobile is converging towards being able to save models in the same kind of way.\n",
      "The goal is to enable machine learning in multiple ways, including supporting a variety of algorithms and pushing the state of the art in research.\n",
      "The cohesiveness of being able to train on the desktop and then move models to mobile is important.\n",
      "There are lots of exciting things going on in machine learning, including support for deep learning and other algorithms.\n",
      "The focus is on enabling researchers to build the next amazing thing, such as BERT.\n",
      "Research side needs to keep pushing on the state of the art, such as with the recent development of BERT.\n",
      "The challenge is to integrate research into real products that have a meaningful impact on people.\n",
      "ML and training are no longer limited to workstations, data centers, or the cloud, but can run on a wide range of compute devices across the world.\n",
      "Machine learning is now running on phones and tiny chips.\n",
      "The goal is to get machine learning on every device with compute capability.\n",
      "The ecosystem continues to grow and cover more aspects of machine learning.\n",
      "Tooling like TensorBoard and TensorFlow extended have been developed to help with machine learning pipelines.\n",
      "TensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended to ML pipelines.\n",
      "There are lots of libraries being built on top of TensorFlow for research and production purposes.\n",
      "Some libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production.\n",
      "Libraries have come from both within Google and from the community, showing a wide range of interests and needs within the ML community.\n",
      "Google and the community are working together to build different pieces.\n",
      "The goal is to enable others to build the things they care about.\n",
      "The focus is on making different pieces work well together in version 2.0.\n",
      "Core format and sharing models through save model and TensorFlow hub are key pieces being pushed on.\n",
      "The core format and sharing of models through save model and TensorFlow hub are key pieces being pushed on.\n",
      "TensorFlow.js (formerly deep learning JS) was initially seen as a technically difficult project, especially in terms of integration into the ecosystem.\n",
      "There have been many technical challenges to overcome in the development of TensorFlow.js.\n",
      "The project has required a lot of iteration and learning over the last few years.\n",
      "There are lots of steps and challenges to be overcome in the development process.\n",
      "The team has iterated over the last few years and learned a lot.\n",
      "The goal is to make things easy for the end user, but there are many complexities behind the scenes.\n",
      "There are still challenges ahead, such as more devices coming on board and the need for improvements in APIs.\n",
      "TensorFlow started as a monolithic system and has evolved with many tools around it.\n",
      "TensorFlow started as a monolithic system and is still largely monolithic.\n",
      "There are many tools around TensorFlow, but the core is still large and monolithic.\n",
      "It is difficult to change and modify the system because it is rapidly evolving and not slowing down.\n",
      "The challenge is to break apart the system without disrupting its functionality.\n",
      "Many people rely on TensorFlow and are excited about it.\n",
      "The challenge of maintaining compatibility with previous versions of TensorFlow.\n",
      "The balance between making updates and maintaining compatibility.\n",
      "The impact of TensorFlow 2.0 on back compatibility.\n",
      "The responsibility of ensuring previous versions still work to some degree.\n",
      "It's important to maintain compatibility for production systems that rely on TensorFlow.\n",
      "There is a huge cost associated with making new changes and updates.\n",
      "The overall value of making changes is much bigger, despite potential slowdowns.\n",
      "It's not just about breaking the system for the current user, but also about informing future users about the changes.\n",
      "The overall value of bringing new people on board is important, not just for the present but also for the future.\n",
      "It's important to start with a clean slate when doing new things and not worry about compromises initially.\n",
      "Designing with a clean slate in mind is crucial for reaching a good place in the end.\n",
      "It's important to put past responsibilities behind when thinking of new ideas.\n",
      "The speaker switched their research group to TensorFlow.\n",
      "They believe that TensorFlow is leading in many ways, such as ecosystem, number of users, momentum, power, and production levels.\n",
      "They wish everyone would use the same thing, and see TensorFlow as the closest to that ideal.\n",
      "They acknowledge that some researchers are also using PyTorch, but they focus on making TensorFlow the best it can be.\n",
      "Researchers are now using PyTorch in addition to TensorFlow.\n",
      "PyTorch focuses on research and making things easy, without worrying about production or speed.\n",
      "TensorFlow was chosen with production in mind, not just for research.\n",
      "PyTorch does not worry about graphs and focuses on making things easy to run.\n",
      "There are things to learn from both PyTorch and TensorFlow's approaches.\n",
      "The text discusses the importance of making things easy and learning from previous experiences.\n",
      "It mentions the benefit of competition and exploring different spaces.\n",
      "The text talks about revisiting the idea of eager execution multiple times before finally pushing for it in version 2.0.\n",
      "It highlights the challenge of combining different elements and the time it took to get everything together.\n",
      "Eager execution in TensorFlow 2.0 has come together well after some time and effort.\n",
      "Eager execution is a powerful addition to TensorFlow.\n",
      "It may have taken longer to develop eager execution, but it was worth the wait.\n",
      "The speaker is grateful for the progress TensorFlow has made in the last couple of years.\n",
      "The speaker is interested in other aspects of TensorFlow 2.0 that were not discussed.\n",
      "TensorFlow 2.0 is setting up clean APIs for users.\n",
      "It enables us to do a lot of other things beyond what we've talked about.\n",
      "Eager execution and making it easily accessible to Keras are important ecosystem improvements.\n",
      "There are opportunities for improved performance in TensorFlow with graphs and tuning.\n",
      "With TensorFlow 2.0, the APIs allow for improved performance without the need for extensive tuning.\n",
      "The new APIs are cleaner and optimized for common use cases, providing better performance out of the box.\n",
      "The team is excited about the potential for further exploration and optimization in future versions after 2.0.\n",
      "There is a focus on restructuring the monolithic system into more efficient components.\n",
      "The team is excited about the restructuring of the monolithic thing into more modular pieces.\n",
      "This restructuring will be important for other organizations and individuals in the ecosystem who want to build things.\n",
      "The current organization of TensorFlow on GitHub is in one core repository with no easy way to split the components apart.\n",
      "The core repository includes the execution engine, key backends for CPUs and GPUs, and work for distributed computing.\n",
      "GPUs and distributed computing work together in a single library or binary.\n",
      "There are some interfaces for splitting them apart, but they are not very clean.\n",
      "Clean interfaces for running on custom clusters with custom networking would be ideal.\n",
      "Clean separation in the ecosystem will help with scalability.\n",
      "Enabling individual developers and organizations to evolve and push on things independently allows for better scaling.\n",
      "TensorFlow allows for independent scaling and evolution.\n",
      "Individual developers and organizations are the target users of TensorFlow.\n",
      "Major corporations like Pepsi are already using TensorFlow.\n",
      "Some enterprises use TensorFlow for specific purposes, such as custom hardware development.\n",
      "IBM is involved in special interest groups for TensorFlow and addresses user needs.\n",
      "Companies like IBM and autonomous vehicle companies are involved in special interest groups and want to optimize for user needs.\n",
      "TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors.\n",
      "Building a community like TensorFlow's requires multiple factors coming together.\n",
      "The critical thing that allowed for TensorFlow's growth is not clearly defined.\n",
      "The growth of TensorFlow's community continues, but the exact reasons for this are not known.\n",
      "Timing and growth are important factors in the success of new tools like TensorFlow.\n",
      "Listening to the community and understanding their needs is crucial.\n",
      "Being open to external contributions and having a process in place to accept and integrate them.\n",
      "Building a welcoming and supportive community for contributors.\n",
      "Transparency is important for an open source project.\n",
      "Processes need to be put in place to support contributors.\n",
      "Building the right kind of community and welcoming contributors is important.\n",
      "As a project grows, more processes need to be put in place, such as documentation and tools.\n",
      "People building is one of the big things that feeds the TensorFlow fire.\n",
      "Developers care about the tools they use.\n",
      "TensorFlow growth is fueled by people building and implementing cool and useful architectures on GitHub.\n",
      "There is ongoing investment in tooling for TensorFlow.\n",
      "Tooling was a topic of discussion at the developer summit.\n",
      "The company is working hard to make the transition to significant version changes very smooth.\n",
      "People want to move to the new thing because they see value in it, not just because it's new.\n",
      "The company is confident that people will start to see the value and make the shift to the new thing in the next few months.\n",
      "The field is moving rapidly, which will help the company do more things and implement new features in 2.x.\n",
      "The field is moving rapidly, leading to new developments in 2.x.\n",
      "Change is expected in terms of deep learning, but the basics will likely still be around in some form in five years.\n",
      "New developments are probable in the future, but the basics of deep learning will likely remain.\n",
      "RL and GAN are likely to stay in the next five years.\n",
      "New developments are hard to predict.\n",
      "There is a focus on combining eager execution and graphs to make programming more natural.\n",
      "Swift for TensorFlow is taking a ground-up approach.\n",
      "Expect to see more developments in these areas in the next five years.\n",
      "Uncertainty about the future of hardware accelerators and the possibility of training with four bits instead of 32 bits.\n",
      "In five years, we expect to see more advancements in the area of hardware accelerators and training with four bits instead of 32 bits.\n",
      "The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications.\n",
      "Efforts have been made to make TensorFlow as accessible and easy to use as possible, especially for beginners.\n",
      "Keras is solving the struggle of beginners in using image models for training or transfer learning.\n",
      "Beginners want simple models and easy access to use them, without needing to understand the details inside the box.\n",
      "Providing simple models with pre-built layers is important for beginners.\n",
      "The next step for beginners is to learn how to build layers with Keras.\n",
      "Advanced users may write custom layers for research purposes.\n",
      "Beginners are interested in starting with pre-trained models and building layers with Keras.\n",
      "Research shows that some beginners are writing custom layers themselves or doing their own loops.\n",
      "Providing pre-trained models decreases the time needed to start and achieve goals.\n",
      "TensorFlow has recently delivered on something trivial for beginners.\n",
      "High schoolers are doing amazing things with TensorFlow, which is both amazing and terrifying.\n",
      "Schoolers are doing a whole bunch of things now, which is amazing and terrifying.\n",
      "Incredible ideas will be coming from them when they grow up.\n",
      "There is a technical aspect to the work, as well as a management aspect with TensorFlow leading the project.\n",
      "Google has been at the forefront of exploring what it takes to build a good team.\n",
      "TensorFlow is one of the most cutting edge technologies in the world.\n",
      "Cohesion across the team is important for delivering something well.\n",
      "Cohesion across the team is important for delivering something well.\n",
      "The product of what the team generates is way larger than the whole or the individual put together.\n",
      "Hiring good people is important, but they also have to care about what they're building and be motivated for the right kind of things.\n",
      "Google values hiring smart and motivated people.\n",
      "The organization is bottom-up and research-focused.\n",
      "There is a balance between exploration and staying focused on a direction.\n",
      "Monitoring the health of the team is important.\n",
      "There is exploration to be done in a specific direction, not all over the place.\n",
      "Monitoring the health of the team and knowing if a good job has been done is important.\n",
      "It can be difficult to determine alignment within the team.\n",
      "Work at Google is often done by individual superstars, which can create tensions within a team.\n",
      "The mission of the project at TensorFlow is exciting and at the cutting edge.\n",
      "The mission of the TensorFlow project is beautiful and exciting.\n",
      "Google has a culture of caring and collaboration.\n",
      "The project allows for growth and different kinds of work.\n",
      "It is important for team members to work well together.\n",
      "Productivity at Google is about the team, not just individual superstars.\n",
      "Hiring process at Google has been refined over the last 20 years.\n",
      "Google cares about whether a superstar can work well with the team.\n",
      "The hiring process has been refined over the last 20 years.\n",
      "Core technical skills are important, but motivation is also crucial for success.\n",
      "Alignment of motivation with the company's goals is important for long term success.\n",
      "Motivation is important at every level, from junior to senior positions.\n",
      "The Google hiring process focuses on determining the skill set and problem-solving ability of the candidate.\n",
      "There is also an emphasis on determining if the candidate has a strong internal drive and passion for their work.\n",
      "Culture fit is an important part of the interview process at Google, in addition to technical skills.\n",
      "Culture fit is an important part of the interview process at Google.\n",
      "Engineers are supposed to rate the person on the culture fit with Google.\n",
      "Different projects may have different cultures and requirements.\n",
      "TensorFlow is a fast moving project and requires people who are comfortable with that.\n",
      "Google also values full fledged products and wants to ensure things work really well.\n",
      "Balancing the need for speed with the need for quality is important in hiring.\n",
      "Balancing out and finding the right fit for different projects and teams is important.\n",
      "Core culture at Google includes engineering excellence.\n",
      "Difficult things are fun when you solve them.\n",
      "Striking a fine balance across different aspects is key to success in a large ecosystem or even a small product.\n",
      "The key to a successful thing is striking a fine balance across different aspects.\n",
      "Balancing speed and perfection, community involvement, and saying no to certain things are hard decisions.\n",
      "Some decisions are made quickly due to time constraints, while others are given time for consideration.\n",
      "The Dev Summit came together incredibly, with a lot of moving pieces and last-minute decisions.\n",
      "Deadlines bring a sense of urgency and value, leading to the need for a good balance between perfection and functionality.\n",
      "The team did a great job in putting everything together, and the speaker was amazed and excited by the outcome.\n",
      "The team did a great job in putting things together.\n",
      "Focus on key things that are important and figure out their importance.\n",
      "Developing in the open, both internally and externally.\n",
      "Releases are done at a regular cadence.\n",
      "It's okay to put things out that aren't fully ready as long as it's clear that it's experimental.\n",
      "Iterating and improving on things allows for releasing experimental versions.\n",
      "Quick cycle and quick iteration are important in the development process.\n",
      "There may be pressure to make TensorFlow 2.0 stable once it reaches the release candidate and final version.\n",
      "TensorFlow 2.0 is still a work in progress and there will be more releases in the future.\n",
      "There are no external deadlines for TensorFlow 2.0, but the goal is to make it a great product.\n",
      "The stability of TensorFlow 2.0 will be similar to NodeX, where every API that's there is going to remain in work.\n",
      "There may be internal deadlines, artificial or otherwise, but the focus is on making the product great rather than rushing to release it.\n",
      "The focus is on creating a great product with TensorFlow.\n",
      "There have been 41 million downloads for TensorFlow 1.0 X.\n",
      "The release of the new version is not rushed, and the focus is on getting it right.\n",
      "The goal is to release the new version in the next few months or the next quarter.\n",
      "The speaker appreciates the concept of \"spring is a relative concept\" and relates it to the development process.\n",
      "The speaker is interested in the previous work experience of the person they are speaking to, specifically before TensorFlow.\n",
      "Spring is a relative concept.\n",
      "Ads connect people to the things they want and need.\n",
      "Ads can also annoy users and ruin the user experience.\n",
      "Machine learning has the opportunity to shine in connecting users to what they want.\n",
      "Google's experience with search ads has provided valuable insights.\n",
      "Machine learning shines with personalized data that aligns with user needs.\n",
      "Google is leading in personalized ads and the future of ads.\n",
      "Search ads are an extension of making information and products accessible to users.\n",
      "Minimum quality level is required for search ads to be shown.\n",
      "Advertising is a key part of search and has been adapted to the web.\n",
      "There is a minimum quality level for search ads to be shown.\n",
      "It is important for ads to align with what users need.\n",
      "The balance between showing a good ad and not being annoying is crucial.\n",
      "Advertisements can be annoying if they interrupt the user's experience on a website.\n",
      "There needs to be a balance between showing valuable ads and providing monetization for the service.\n",
      "Monetization is necessary for services like search engines and websites to continue providing their service.\n",
      "Good advertisements can be useful and valuable to the user.\n",
      "The internet is exciting but also limiting because nobody wants to pay for anything.\n",
      "Advertisements, when done well, can be really useful and not annoying.\n",
      "It will take a long time for everything to be paid for on the internet, if at all.\n",
      "There is a transition towards more paid services across the web.\n",
      "People are willing to pay for services they see value in, such as Netflix and paid apps.\n",
      "YouTube is also moving towards paid services.\n",
      "People are willing to pay for content they see value in, such as Netflix and newspaper websites.\n",
      "There is a shift towards more people being willing to pay for apps and news content.\n",
      "The speaker sees a change in themselves and those around them in being more willing to pay for content.\n",
      "Hopeful for a transition to a mixed model where there is a clear revenue model for content.\n",
      "The future of TensorFlow in terms of empowering a class of 300 students is a question raised by MIT.\n",
      "The ability for students to do their homework in TensorFlow and where they will train their networks is a concern.\n",
      "TensorFlow is open source and can be run on desktops, which are continually becoming more powerful.\n",
      "The power of smartphones has increased significantly, making it possible to train models on a phone.\n",
      "Cloud services are an interesting option for students and courses.\n",
      "Cloud computing makes it easy to get started with machine learning and TensorFlow.\n",
      "Colab is a free service that allows beginners to play and explore machine learning.\n",
      "Paid services offer more features and capabilities for machine learning.\n",
      "Training machine learning models on a phone is possible.\n",
      "The power of cloud computing is accessible through mobile devices.\n",
      "There are paid services available for machine learning and TensorFlow.\n",
      "Beginners should start by visiting TensorFlow.org and exploring tutorials and guides.\n",
      "No installation is needed to get started with TensorFlow, as you can use Colab to do things right away.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "    print(\"RRR keypoints\")\n",
    "    for keypoint in keypoints:\n",
    "        print(keypoint)\n",
    "        \n",
    "    continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = summarize_stage_1(chunks_text)['stage_1_outputs']\n",
    "    # Split the titles and summaries\n",
    "    stage_1_summaries = [e['summary'] for e in stage_1_outputs]\n",
    "    stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_summaries)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    summary_embeds = generate_embeddings(stage_1_summaries)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "        summary_similarity_matrix[row, col] = similarity\n",
    "        summary_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "    time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(summary_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "    chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "    print(f\"topics out: {len(topics)}\")\n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_v3_stage3_extractkeypoints.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "    time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
