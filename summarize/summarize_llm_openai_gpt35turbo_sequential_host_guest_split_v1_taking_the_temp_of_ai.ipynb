{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v1\" # no rewritten\n",
    "METHOD_NAME=\"sequential_host_guest_split\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fa0cae59850>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRR # lines: 69\n",
      "RRR # lines after cleaned: 35\n"
     ]
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open(f'taking_the_temp_of_ai_{METHOD_NAME}.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    print(f\"RRR # lines: {len(lines)}\")\n",
    "    \n",
    "    lines = [ line for line in lines if line.strip()]\n",
    "    \n",
    "    print(f\"RRR # lines after cleaned: {len(lines)}\")\n",
    "\n",
    "    podcast = {    \n",
    "        \"episode_number\": \"taking_the_temp_of_ai\",    \n",
    "        \"transcript\": lines,\n",
    "    }\n",
    "    podcast_data.append(podcast)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyPointsFromSpeaker(speaker, speech):\n",
    "    \n",
    "    if len(speech) <= 200:\n",
    "        return speech\n",
    "    \n",
    "    eval_prompt_template = \"\"\"\n",
    "    You will extract the key points in the given speech in the first person.\n",
    "    \n",
    "    Then, combine them to form a complete speech as if it is by the speaker him/herself.\n",
    "\n",
    "    Here is the speech:\n",
    "    SPEECH: {speech}\n",
    "    \n",
    "    Here is the output format:\n",
    "    KEY_POINTS\n",
    "    COMBIMED_SPEECH\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"speaker\", \"speech\"])\n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"speech\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "#             'speaker': speaker,\n",
    "            'speech': speech,    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    return map_llm_chain_results[0]['text']    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker: Deb\n",
      "speech: This is Deb Donig with technically Human, a podcast about ethics and technology, where I ask what it means to be human in the age of tech. Each week I interview industry leaders, thinkers, writers, and technologists, and I ask them about how they understand the relationship between humans and the technologies we create. We discuss how we can build a better vision for technology, one that represents the best of our human values. Today, I'm sitting down with Doctor Tamara Neese. Doctor Tamara Kneese is project director of Data and society's Algorithmic Impact Methods Lab, where she is also a senior researcher. For the 2023 and 2024 academic year, she's a visiting scholar at UC Berkeley's center for Science, Technology, Medicine and Society. Before joining data and society, she was lead researcher at Green Software foundation, director of developer engagement on the green software team at intel, and assistant professor of media studies and director of gender and sexuality studies at the University of San Francisco. Tamara holds a PhD in media, culture and communication from NYU and is the author of Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond. In her spare time, she's a volunteer with tech workers Coalition.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Deb Donig hosts the podcast \"Technically Human\" about ethics and technology\n",
      "- She interviews industry leaders, thinkers, writers, and technologists about the relationship between humans and technology\n",
      "- Dr. Tamara Kneese is the project director of Data and Society's Algorithmic Impact Methods Lab\n",
      "- She is a visiting scholar at UC Berkeley's center for Science, Technology, Medicine and Society\n",
      "- Dr. Kneese has a background in media studies, gender and sexuality studies, and technology\n",
      "- She holds a PhD in media, culture and communication from NYU and is the author of \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond\"\n",
      "- Dr. Kneese is also a volunteer with the Tech Workers Coalition\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "Hello, I'm Deb Donig, the host of the podcast \"Technically Human,\" where I explore the intersection of ethics and technology. Each week, I have the privilege of speaking with industry leaders, thinkers, writers, and technologists to understand the impact of technology on humanity. Today, I am honored to have Dr. Tamara Kneese as my guest. Dr. Kneese is the project director of Data and Society's Algorithmic Impact Methods Lab and a visiting scholar at UC Berkeley's center for Science, Technology, Medicine and Society. With her background in media studies, gender and sexuality studies, and technology, she brings a unique perspective to our discussion. She holds a PhD from NYU and is the author of \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond.\" In addition to her impressive professional achievements, Dr. Kneese also volunteers with the Tech Workers Coalition. I am excited to delve into our conversation about the relationship between humans and the technologies we create.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: Hi Tamara.\n",
      "summarized speech:\n",
      "Hi Tamara.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Hi Deb.\n",
      "summarized speech:\n",
      "Hi Deb.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: So Tamara, we're talking in a week where Congress just introduced a landmark bill that would move the government toward developing standards that would measure and report the full range of AI's environmental impact, as well as one that would create a voluntary framework for AI developers to report environmental impacts. This legislation, just to give a bird's eye view, also requires an interagency study that would aim to investigate and measure both the positive and the negative environmental impacts of AI. Can you talk a little bit about the legislation?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Congress introduced a landmark bill for developing standards to measure and report AI's environmental impact\n",
      "- The bill also aims to create a voluntary framework for AI developers to report environmental impacts\n",
      "- The legislation requires an interagency study to investigate and measure both positive and negative environmental impacts of AI\n",
      "\n",
      "COMBINED_SPEECH\n",
      "So, I want to talk about the recent landmark bill introduced by Congress. This bill is a significant step towards developing standards to measure and report the environmental impact of AI. It also aims to create a voluntary framework for AI developers to report their environmental impacts. Additionally, the legislation requires an interagency study to investigate and measure both the positive and negative environmental impacts of AI. This is a crucial development in our efforts to understand and address the environmental impact of AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, so, you know, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. So one of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So if you think about the fact that the kind of form of AI that we usually think about might be the thing that we interact with directly. So if you're thinking about Chad GBT, you're thinking about maybe the energy used to prop up that interaction that you're having with it. But behind that, there's a whole supply chain that has to go into the manufacturing of AI, which also requires the manufacturing of all the hardware it relies on, including chips and the raw earth minerals that are needed to make the chips possible. In the first place. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Excitement at data and society about the bill calling for robust socio technical research\n",
      "- Difficulty in measuring environmental impacts of AI, especially across the entire supply chain\n",
      "- Importance of considering the effects of AI production and use globally\n",
      "- Advocacy for legislation pushing for standards around measurement and reporting\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am really excited at data and society to hear about this bill because it calls for a very robust form of socio technical research. Measuring the environmental impacts of AI is quite hard, especially across the entire supply chain. It's important to consider the effects of AI production and use globally, and having legislation that pushes for standards around measurement and reporting is incredibly important.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?\n",
      "summarized speech:\n",
      "You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: There are a few problems when it comes to measuring the environmental impact of something like AI, precisely because of all of the very complicated global supply chains that are involved in its production. And so you really need to have a lot of coordination across different parts of the supply chain, different companies, different parts of the world. And so the bill is actually sort of relying on not just the EPA, but also the National Institute of Standards and Technology to really convene a group of people who are going to be able to identify the methodologies and standards that are needed to do this kind of measurement work.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Measuring the environmental impact of AI is challenging due to complex global supply chains.\n",
      "- Coordination across different parts of the supply chain and different companies is necessary.\n",
      "- The bill relies on the EPA and the National Institute of Standards and Technology to convene a group to identify measurement methodologies and standards.\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "Measuring the environmental impact of something like AI is quite challenging because of the complex global supply chains involved in its production. It requires coordination across different parts of the supply chain and different companies from around the world. That's why the bill is relying on the EPA and the National Institute of Standards and Technology to convene a group of experts who can identify the necessary methodologies and standards for this kind of measurement work.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.\n",
      "summarized speech:\n",
      "What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: With large language models in particular? So if we're talking about the kind of AI like, generative AI, that requires a large amount of compute, so you actually need specialized forms of chips in order to run AI efficiently. You also need just a tremendous amount of power. So you need a lot of energy, electricity. You also need a lot of water. And so researchers from different corners of tech have been trying to figure out exactly how much energy or how much water is needed to train these large language models. And then other researchers have looked at what happens when a kind of AI is actually deployed. And so what is the sort of energy or water consumption associated with the use of AI, not just the training part. And so this is something that has been a growing problem. So the entire ICT sector takes up a tremendous amount of energy. Many of us have probably read some of the articles that have come out over the years about the effects of data centers on local areas, maybe that are already experiencing drought or that already have issues with their grid. And so because data centers require just a tremendous amount of energy and can also pollute the areas in which they're located while also sucking up a lot of water, it really becomes an environmental justice issue. At the same time that it's sort of a larger question of worrying about the control of resources and the potential cost in terms of carbon emissions. So this sort of robust form of research that would actually kind of take all of these aspects into account about AI. It's actually just that AI is one small part of a much larger problem that's been going on for quite a long time. And actually, we can look to things that happened even it may sound like a really long time ago, but the 1980s and looking at things that happened in the Silicon Valley region when it comes to the manufacturing of chips and in some cases toxic chemicals got into the local water supply in San Jose, and it took community members really coming together and fighting back against this sort of contamination and asking for more regulation to prevent this sort of thing from happening in the future. And so I think if we look at what is happening right now, we have an AI boom. And so it's a good time to really focus on making some changes through regulation and to get a better grip on sort of what the real environmental costs are. But again, this is not a new problem. This is not unique to generative AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Large language models and generative AI require specialized chips and a lot of power, energy, and water for efficient operation.\n",
      "- The energy and water consumption associated with training and deploying AI is a growing problem, especially in the ICT sector.\n",
      "- Data centers that support AI operations consume a tremendous amount of energy, pollute local areas, and deplete water resources, leading to environmental justice issues and carbon emissions.\n",
      "- AI is just one part of a larger problem of resource control and environmental impact, which has historical precedents such as toxic chemical contamination in the 1980s.\n",
      "- The current AI boom presents an opportunity to focus on regulation and understanding the real environmental costs.\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "As we delve into the world of large language models and generative AI, it becomes evident that these technologies require specialized chips and a significant amount of power, energy, and water for efficient operation. The growing concern lies in the energy and water consumption associated with training and deploying AI, particularly within the ICT sector. Data centers supporting AI operations not only consume a massive amount of energy but also pollute local areas and deplete water resources, leading to environmental justice issues and carbon emissions. It's important to recognize that AI is just one part of a larger problem of resource control and environmental impact, with historical precedents such as toxic chemical contamination in the 1980s. The current AI boom presents an opportunity to focus on regulation and understanding the real environmental costs, which is crucial for addressing this longstanding issue.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I know that this question is, as we say sometimes in academia, problematic. I'm aware of the problematics of what I'm about to ask, but I want to ask it anyway. When we're talking about AI and generative AI in particular, there's a whole range of problems, concerns, issues and dangers cited from the automation of labor leading to job loss and questions around compensation for labor, copyright questions about creativity, misinformation in the circulation of false information using generative AI, to things like the changes to the structure of how we get and source information online. Environmental impact is one that we maybe sometimes hear about on the sidelines. It's not typically included in some of, at least to my knowledge, some of these more mainstream issues. And the problematics of this question, I am aware, is in trying to rank or propose a hierarchy of problems, but where would this rank in the hierarchy of problems caused by AI?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Acknowledgment of the problematic nature of the question\n",
      "- Mention of various problems and concerns related to generative AI, such as job loss, compensation for labor, copyright issues, misinformation, changes to information sourcing, and environmental impact\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I understand that the question I am about to ask is problematic, but I feel it is important to address it. When discussing AI, especially generative AI, there are numerous issues and concerns that arise, including job loss, compensation for labor, copyright questions, misinformation, changes to information sourcing, and even environmental impact. While I am aware of the difficulty in ranking these problems, it is important to consider where environmental impact would fit in the hierarchy of issues caused by AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: So I think that the environmental impacts of AI are very much connected to a lot of the labor problems. So I think part of what focusing on the environmental impact of AI can do for researchers and for advocates for people who are trying to create policy is that it really forces you to have a much more holistic view of what the potential harms of AI are, because, you know, if we're talking about the entire AI supply chain, we really are talking about many different parts of the world. We're talking about many different kinds of work, many different kinds of labor exploitation that can be behind this. And often we're also looking at the trade off. Right? So between, you know, you could say, oh, a data center in this area is going to bring jobs. Well, and sometimes that isn't even true. Right? Like that might be the sort of line that communities are given about bringing a large scale data center to the town. But then what are the sort of downstream effects? And what does it mean to be living next to something that is taking a bunch of power and taking a bunch of water? Or what does it mean to live next to something that is actually quite noisy? So I think a lot of people maybe don't realize the extent to which data centers are super loud and can be really distracting for the people who are living around them, and then wondering also about the other sort of effects on health for people in the area, thinking about all of the sort of pollutants and other issues that are associated with it. And so I think when you're talking about sort of justice concerns, when you're talking about how particular marginalized communities are going to be impacted, the question of labor and ethics comes up in tandem with the discussion of the environmental impact. So I think if we really begin to think about entire communities, if we're thinking about effects on habitats and ecosystems and on other species as well as on humans directly, it becomes a way to really consider what the sort of full spectrum of impacts are going to be from AI.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Environmental impacts of AI are connected to labor problems\n",
      "- Focusing on the environmental impact of AI forces a more holistic view of potential harms\n",
      "- The entire AI supply chain involves different parts of the world and different types of labor exploitation\n",
      "- Trade-offs exist, such as the potential job creation from data centers versus their negative effects on communities\n",
      "- Data centers have environmental and health impacts on surrounding communities\n",
      "- Justice concerns and impacts on marginalized communities are tied to labor and ethics in the discussion of environmental impact\n",
      "- Considering effects on habitats, ecosystems, and other species is important in understanding the full spectrum of AI impacts\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I believe that the environmental impacts of AI are closely linked to labor problems. Focusing on the environmental impact of AI forces us to take a more holistic view of the potential harms it can cause. The entire AI supply chain involves different parts of the world and various types of labor exploitation. We need to consider trade-offs, such as the potential job creation from data centers versus their negative effects on communities. Data centers have significant environmental and health impacts on the surrounding communities, and we must also consider justice concerns and the impacts on marginalized communities in the discussion of environmental impact. It's important to consider the effects on habitats, ecosystems, and other species to understand the full spectrum of AI impacts.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: So what is that full spectrum of impact? The broader question here is how do you study AI's environmental impact? How do you measure it? What issues or areas comprehensively do you look at when you assess an environmental impact?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Understanding the full spectrum of impact of AI\n",
      "- Studying AI's environmental impact\n",
      "- Measuring AI's environmental impact\n",
      "- Comprehensive assessment of environmental impact\n",
      "\n",
      "COMBINED_SPEECH\n",
      "So, when we talk about the impact of AI on the environment, we need to understand the full spectrum of impact. This involves studying and measuring AI's environmental impact comprehensively. It's important to consider all the issues and areas that contribute to the environmental impact of AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. So, I mean, I think a lot of researchers have spent time focusing on measuring the carbon attached to training and maybe sometimes deployment. Some researchers have really pushed for more of a lifecycle analysis. So they're really looking at the carbon cost of AI, not just in the early training stages and deployment, but maybe even looking at the potential of e waste. So the fact that there's a tremendous amount of hardware that is also used in creating AI and deploying AI that also tends to be built not to last very long. And so there's a large amount of e-waste that also can be quite hazardous. And so one of the ways of measuring impact is at that very highly technical level when you're measuring the effects of, say, the training and deployment itself, but then you're also measuring social impacts. And to do that, you really do need to talk to communities who are being impacted. And so I think if we could find a way to kind of better or at least more effectively bridge the relationship between these sort of technical assessments of exactly how much carbon am I emitting when I'm training this model versus what are the downstream effects that this particular system might have on communities in x location five years down the road? I think it's incredibly difficult to measure, but you have to find a way of putting these ways of framing the problem together, because what I've seen from working in the tech industry, so I worked on a sustainability team at Intel. I was on what, what was called the green software team. And so a lot of that had to do with creating software that would help developers try to optimize the technology that they were building and using. And so if you're working in a lab, you're a machine learning specialist, you're training a large model, maybe you choose to do that at a time of day when there's more renewable energy available on the grid. And so a lot of the focus of our research was about kind of making these choices more visible to developers and having them feel kind of empowered to know, if I choose to do this at this time of day, it's a better choice for the environment. Right. This is just a completely different way of framing the problem than you're going to get from talking to grassroots community groups who are calling for environmental justice. They're going to have a very different way of setting the terms of how to measure and report the environmental impacts of the technology. That's where the kind of work that we're trying to do at Aim lab comes in, because we're very much trying to figure out how to really reconcile that more technical approach where the solution is sort of found through the technology itself and a tweak to the technology, versus a more socio technical perspective, which kind of understands the technical side, but also tries to look at all of the social factors that are around it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Researchers have focused on measuring the carbon attached to AI training and deployment\n",
      "- Some have pushed for a lifecycle analysis, including the potential e-waste impact\n",
      "- Measuring impact requires considering both technical and social impacts\n",
      "- Bridging the relationship between technical assessments and downstream effects is challenging\n",
      "- Working in the tech industry, there is a need to make choices more visible to developers for environmental impact\n",
      "- Different perspectives on measuring and reporting environmental impacts exist, from grassroots community groups to technical experts\n",
      "- Aim lab is working to reconcile the technical and socio-technical perspectives\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, it's a great question. I believe that researchers have spent a lot of time focusing on measuring the carbon attached to AI training and deployment. Some have even pushed for a lifecycle analysis, including the potential e-waste impact. It's important to consider both the technical and social impacts when measuring the impact of AI. Bridging the relationship between technical assessments and downstream effects is challenging, but necessary. In my experience working in the tech industry, it's important to make choices more visible to developers for environmental impact. There are different perspectives on measuring and reporting environmental impacts, from grassroots community groups to technical experts. This is where the work of Aim lab comes in, as we are trying to reconcile the technical and socio-technical perspectives.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I want to take a little bit of advantage of having somebody on the show who has worked across so many different sectors. You're at data and society now, which I would broadly consider a kind of public interest technology or civilly minded research center. You've been a academic, you are academically trained and have worked in the ivory tower for a while. You've worked in industry as well. You've worked on the sustainability project that you just mentioned inside of a kind of industry located organization. As far as I see it, there are so many fractures in terms of the ways that these different kinds of communities are thinking about or approaching the question of the benefits of AI versus the harms or mitigating harms, while maximizing benefits or thinking critically about the problems that AI may introduce. What do these different sectors maybe not know? Or what are the gaps in knowledge between these different sectors that maybe cause some of these tensions or fissures,or maybe talking across purposes with one another or talking over each other? How do you see the kinds of silos in knowledge, and maybe some of the controversial or combative conversations that happen across these different industries, given your knowledge and experience in these very different areas?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Worked across different sectors including academia, industry, and public interest technology\n",
      "- Observes fractures in the ways different communities approach the benefits and harms of AI\n",
      "- Identifies gaps in knowledge between sectors causing tensions and combative conversations\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I have had the opportunity to work in various sectors, from academia to industry to public interest technology. Through my experiences, I have noticed significant differences in how these communities approach the impact of AI. There are gaps in knowledge between these sectors, leading to tensions and combative conversations. It is important for us to bridge these silos in knowledge and have more productive discussions about the benefits and potential harms of AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, no, it's a great question because I think, you know, the important thing, especially for doing this kind of research, is to kind of have a very broad understanding of the problem. You really want to be able to look at it from a lot of different perspectives. And what I have found sort of working across sectors, is that often people just sort of define the problem in a different way, and the questions that they're asking are going to be quite different. Another point of contention is that people often are working on very different timelines. So for academic researchers, we often do have the luxury of time. We may feel pressed for time, but we're working on a very different temporal scale than the quarterly financial system of tech. We're not trying to rush to ship a product. We're not worried about selling anything, at least in terms of a product. Maybe we're selling ourselves to a degree. But I think part of the problem is that the expectations of how long a research project should take and what a useful or effective research project is, it's just very different. So conducting socio technical research within a tech company, it's all about what insights you're giving people and demonstrating return on investment for the people who are funding the research, and also demonstrating the impact that you're having in a very real way. So are you directly influencing the design of a product? Are you helping the companies sell more stuff? And often, if you are not doing those things, then the research is not seen as being very valuable. And so trying to carve out space to actually do longer term research can be quite hard in tech itself. But what I think is really interesting is, and this is sort of the kind of work that we're trying to do at AIM Lab is we, you know, as sort of socio technical researchers who are at an independent nonprofit, but who are trained academics, we can kind of lend our expertise to people in the tech industry who are working on a problem and who maybe want to have other perspectives incorporated into the work that they're doing. And so I think some of the most interesting kinds of collaborations that you can have are when you really are sort of sharing perspectives with people who would ordinarily be addressing the problem of, say, of climate impacts, of AI through maybe decarbonization alone. And that's sort of the main way that they've thought about it. But if you can help them to think through what the user experience of actually using the thing that they're building is, and then also get them to kind of engage with the idea of downstream impacts to other communities after the fact, it's just a way of changing the perspective and reframing the problem. And I feel like that is something that can also really be helpful when you're trying to come up with policy, because, you know, obviously there's always going to be a gap between policy recommendations and how that is actually implemented. And so for me, I find it really helpful, having been in tech, in knowing exactly how power operates, how hierarchies work, and what the expectations are, and knowing how to talk the language, and so you can have a sense of how policy will actually be taken up within an organization. And so that's also something that we're really trying to focus on at AIM lab is understanding the methodologies that people will need to have to carry out the work of algorithmic impact assessment in a variety of different organizations. It's say a startup versus a large scale enterprise, or in a city government, or in other contexts that maybe don't get as much attention as the major tech companies like Google or something.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Importance of having a broad understanding of the problem\n",
      "- Different perspectives and definitions of the problem\n",
      "- Different timelines for research in academia and tech\n",
      "- Expectations of research project timelines and effectiveness\n",
      "- Conducting socio technical research within a tech company\n",
      "- Collaborations between socio technical researchers and tech industry\n",
      "- Changing perspectives and reframing problems\n",
      "- Understanding power, hierarchies, and policy implementation\n",
      "- Methodologies for algorithmic impact assessment in different organizations\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I believe that it's crucial to have a broad understanding of the problem when conducting research, especially in the tech industry. Different perspectives and definitions of the problem can lead to varied questions and timelines for research. In academia, we have the luxury of time, but in tech, the focus is on demonstrating return on investment and real impact. This can make it challenging to carve out space for longer-term research in tech. However, at AIM Lab, we aim to collaborate with the tech industry to lend our expertise and provide different perspectives on problems such as climate impacts and AI. By changing perspectives and reframing problems, we can have a real impact, not only in tech but also in policy implementation. Understanding power, hierarchies, and the language of tech allows us to focus on methodologies for algorithmic impact assessment in various organizations, from startups to city governments.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: Help me better understand the tensions around climate change, environmental harms and AI on the one hand, leaders in the tech industry often cite the ways that AI may be helping to solve or mitigating climate change and environmental harm by reducing inefficiencies in transportation, for example, finding ways to streamline the use of environmental resources and to reduce waste, or creating new technologies that either reduce dependencies on environmentally harmful products or counteract existing environmental and climate damage. On the other hand, we also hear news of the mass energy requirements of data centers that need to be powered and climate controlled, as you've just cited at vast environmental and energy expenses in order to run e-waste, which you've mentioned as well, and environmental damage caused by mining for the resources required to tech products. And I'll tack onto this, that when I talk to people in industry, particularly those who are developing and trying to sell products in AI, they talk about the fact that they're creating jobs, that they themselves have 300 jobs that they cannot fill, that ultimately AI will end up producing more jobs, and that the task is then on the government, typically, which as libertarians. They also don't want to fund. But we'll save that conversation for another time to provide retraining for these better and more thoughtful and larger number or quantity of jobs. And then we hear from people who are critical of this or those who are studying it or those who are losing their jobs, that this is a kind of labor crisis, that in fact, the kinds of trainings required in order to occupy these so called new jobs that AI industries are inventing or providing in surplus require particular set of skills that potentially many people may not have or that they may not want to have. And so I'm trying to think about this controversy both in terms of the environment, as you've already brought in the labor dimension of this. And then, you know, also think about, you know, I think the reasonable case that many of the jobs that are now gone are jobs that were not good for the environment, and also that we don't particularly miss. The job that comes up quite a bit in this is, for example, the driver of the horse and buggy. People say, well, it's a kind of outdated technology and the horses were causing a lot of pollution and a lot of city sanitary problems, and we're better off even if the horse and buggy drivers are no longer able to exist in that occupation without that particular form of occupation to begin with. So how do you think about assessing the benefits against the harms? And how do you go about trying to provide that as a impact assessment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. AI may help solve or mitigate climate change and environmental harm by reducing inefficiencies in transportation, streamlining the use of environmental resources, and creating new technologies.\n",
      "2. However, there are concerns about the mass energy requirements of data centers, environmental damage caused by mining for tech resources, and e-waste.\n",
      "3. The tech industry claims that AI will create more jobs, but there are concerns about the specific skills required for these new jobs and the potential labor crisis.\n",
      "4. Some argue that many of the jobs lost to AI were not environmentally friendly and are not missed.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am grappling with the tensions surrounding climate change, environmental harms, and AI. On one hand, leaders in the tech industry emphasize the ways AI can help solve climate change and environmental harm by reducing inefficiencies and creating new technologies. However, there are valid concerns about the environmental impact of data centers, mining for tech resources, and e-waste. Additionally, while the tech industry claims that AI will create more jobs, there are concerns about the specific skills required for these new jobs and the potential labor crisis. Some argue that many of the jobs lost to AI were not environmentally friendly and are not missed. How do we assess the benefits against the harms and provide an impact assessment for these complex issues?\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. So I think if we take a step back and with a lot of the technology that's being developed in the name of sort of being climate friendly or also maybe actually furthering goals of climate justice in some way. And so, okay, so thinking about maybe AI that could be used to help farmers, let's say, particularly in drought stricken areas. And so the AI that kind of acts as a sensor, a way of detecting things, maybe AI that could be used for a kind of deforestation mitigation or say, helping keep the coral reefs healthy and using that to detect what's going on with the coral reefs, I think there are many kind of imagined uses of AI that would really be a boon to environmentalist efforts. But the question is, and this comes up a lot with the experiences that we're having with AIM lab, is does the tech actually do what you think it's going to do? You know, does the tech actually behave in a way that the kind of people developing imagine that that will work on the ground? And how do the people who are expected to use it? How are they, how are they interfacing with it? Are they having a good time when they, when they interact with the AI? Is it actually helping them with the work that they're already trying to do? Or is it creating new forms of work for, for them, new forms of maintenance and just new sort of things that they didn't have to do in their role before? And so one of the questions with especially technology that is developed to kind of serve a particular environmental purpose, we hear a lot about sort of climate tech being the wave of the future. Maybe this is what will save us from climate change. Are these technologies being built through conversations with the people that the technology is actually expected to be helping? And so this is something that was sort of an ongoing problem within the tech industry, where user experience is really kind of an afterthought. Or maybe it's sort of tacked on towards the end of product development, where you really just sort of do some usability testing and make sure it kind of works well enough most of the time. But to actually try to develop AI with the real input of the communities who, in theory, are going to be using and benefiting from the technology is something that really isn't done most of the time. And so I would say with a lot of the sort of climate related technologies that are being put out on the market, the question is, what does it look like in practice? And this is something that I think AI doesn't seem to attract the same kind of ire that crypto did, because crypto was very much viewed as a waste of energy. Right? Like, people really hated crypto pretty early on, and there were a lot of people talking about the fact that it was a scam and that it was, you know, just wasting all this energy for no reason. Everybody hated NFTs. Not everybody, but a lot of people, right? It had a lot of detractors. So the question is around kind of people making claims about what blockchain could do in terms of, oh, hey, if we tokenize trees in the Amazon, that will incentivize people through financialization, essentially not to cut down all the trees. And so this idea that if you assign monetary value to things, to natural resources in the world, like trees, like whales, that essentially it will help people treat the planet better and that it would, in theory, also be helping indigenous groups or something. And this was very often shown not to be the case. It was actually just a different form of exploitation. It was a different kind of scam. It was often sort of just another form of colonialism, because these things were not developed with the consent or the priorities of the people that it was actually trying to help. And so my fear would be that with a lot of the sort of climate related AI products that maybe have good intentions, it doesn't necessarily mean that they're going to be executed in a way that is actually helpful.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Technology being developed for climate-friendly purposes\n",
      "- AI can be used to help farmers in drought-stricken areas\n",
      "- AI can be used for deforestation mitigation and coral reef health\n",
      "- Concern about whether the technology actually works as intended\n",
      "- Questioning if the technology is developed in consultation with the people it is meant to help\n",
      "- Comparison with the negative perception of crypto and NFTs\n",
      "- Criticism of the idea of tokenizing natural resources as a form of exploitation\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that the development of technology for climate-friendly purposes is crucial. AI, for example, can be used to assist farmers in drought-stricken areas and for mitigating deforestation and maintaining the health of coral reefs. However, there is a concern about whether the technology actually works as intended and if it is developed in consultation with the people it is meant to help. This is similar to the negative perception of crypto and NFTs, where there was criticism of tokenizing natural resources as a form of exploitation. It is important to ensure that technology developed for climate-related purposes is executed in a way that is actually helpful.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I want to talk a little bit about algorithmic impact assessments. Specifically, you have a piece on algorithmic impact assessments, and you wrote in that piece, and I'll quote you here, that \"we know when people and ecologies meet technical systems, there will always be unanticipated consequences\". So how do we identify and document a push back against these sometimes ambient harms?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. Algorithmic impact assessments are important.\n",
      "2. Unanticipated consequences occur when people and ecologies meet technical systems.\n",
      "3. It is necessary to identify and document a push back against ambient harms.\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I believe that algorithmic impact assessments are crucial in today's technological landscape. As I have mentioned before, when people and ecologies intersect with technical systems, there are always unanticipated consequences. It is imperative for us to not only identify these consequences but also document and push back against the ambient harms they may cause.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Well, it's interesting because I think a lot of it does have to do with getting input from the people who are going to be impacted earlier in the process. So there are, you know, in a lot of the cases that we've been involved with, in a lot of the different sort of research projects and collaborations that we have going on right now. And I can't speak, you know, too specifically on them at the moment, but I can say that often when you engage a community, let's say a particular community, in a location that may be particularly invested in knowing about how a technology is going to be used and might have some questions about the technology, it really is helpful to get their perspective, because often they will raise issues or create scenarios for, say, a chatbot that the people creating the technology, not just the technologists, but also, you know, perhaps, like, advocacy groups and others who are really trying to do the right thing, they may not see the big picture. There might just be things that they miss. And sometimes bringing in community members earlier in the process can really help you kind of see the potential problems before they become a really big problem. And so that, I think, you know, of course, there could always be unintended consequences. Even after you've really put in that kind of due diligence and you've been as thorough and ethical as you possibly can be in engaging every potential community that could be impacted. And there may still be things that you'll miss, but I think it's less likely that you would actually have a huge number of things that are as glaring if you actually put the time into fully assessing the technology before you deploy it and maybe even before it's fully designed. Another issue is being able to document potential harms and raise red flags around things like privacy or bias that may not have been obvious to the developer and then actually have time to give that input to the technical team. So another question is, are you kind of just bringing community members in, and you're just planning on sort of having a rubber stamp, but not really changing or modifying the tech in any way? I'm still releasing it. And so you really need to kind of encourage developers to build in time to really fully assess the potential harms and actually work on fixing those issues before the thing is released. There's also a question of, should the technology be developed and released at all, and how do you sort of begin to weigh the cost and the benefits? And that can be quite tricky, but in a lot of cases, it may not be that the technology is actually doing the thing you want it to do at all, and there may be social problems that are happening that really require a different kind of solution. That AI just actually has nothing to do with it. Is there a better way to actually allocate resources? Is another question. Should this thing be built at all? And I think that has to be an open question when you're sort of engaging in this process, rather than people just sort of digging their heels in and putting a product out there that is probably going to end up wreaking havoc in some way.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Getting input from impacted people early in the process is important\n",
      "- Engaging the community can help identify potential problems\n",
      "- Documenting potential harms and raising red flags is crucial\n",
      "- Developers should build in time to assess potential harms and fix issues before release\n",
      "- Questioning whether the technology should be developed and released at all\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that it is crucial to involve the community and gather input from those who will be impacted by the technology early in the process. Engaging with the community can help us identify potential problems and scenarios that we may have overlooked. It is important to document potential harms, raise red flags, and give input to the technical team to address issues before the technology is released. We should also question whether the technology should be developed and released at all, and consider if there are better ways to allocate resources. It is essential to fully assess the potential harms and weigh the cost and benefits before releasing a product that could potentially cause havoc.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I want to push into this a little bit further because you talked a little bit about identifying and assessing the harms. Pushing back, I think, is a different kind of problem. When you discuss the possibility that AI product developed to do one thing or fix one kind of social problem actually isn't doing it, then, yes, you have demonstrated that the product maybe doesn't work as it is supposed to work, or maybe there's a non technical solution that might be better. But there's a kind of famous anecdote that I'd like to pull out to discuss the kind of problem with this. And both of us live in San Francisco, so both of us are aware of the problems around homelessness in San Francisco, one of which is the problem of the sanitariness of the city. And those of us who have traversed the streets of San Francisco recognize one particular sanitary problem, which is that homelessness has left a situation where oftentimes there are human feces on the streets. And an enterprising group of engineers in the Bay Area a couple of years ago came up with a solution to this problem, which was to develop AI poop, picking up robots that could detect human feces on the street and then deposit it into a central repository, thereby cleansing the streets of human feces. And when Dan Lyons, who was on the show many years ago, wrote for Silicon Valley, narrated the story. He said to me, you know, what was actually the better solution to the problem? Public restrooms. Public restrooms. Right. It's not a technical solution. But of course, I think what Dan's comment in many way misses is that the incentive is not just to solve the problem. The incentive for developers is oftentimes to recruit funding to create a product and then to sell the product and the company ultimately to get acquired or to get venture capital or to leverage that company into something larger. And so now we're talking about the kind of economics that undergird what gets developed and why, and the differences between creating a product that is marketable and sellable and solving the problem. Not always the same thing. Right? So I guess the question here is, what do does you pushing back actually look like? And how does one take the kind of understanding, documentation, identification and assessments you're talking about and mobilize it into pushing back?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Identifying and assessing the harms of AI products\n",
      "- The problem of AI products not working as intended\n",
      "- Example of AI poop picking up robots as a solution to homelessness in San Francisco\n",
      "- The incentive for developers to create marketable products rather than solving the problem\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I want to delve deeper into the issue of identifying and assessing the harms of AI products. It's not just about recognizing the problems, but also about pushing back when AI products don't work as intended. Take the example of the AI poop picking up robots developed to address the issue of homelessness in San Francisco. While it may seem like a technical solution, the better alternative would have been public restrooms. This brings us to the underlying economics of AI development - the incentive for developers is often to create marketable products rather than truly solving the problem. So, the question is, how do we push back and mobilize our understanding and assessments to make a real difference?\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. And it's actually my favorite part of this kind of work, because I also do have a background as a labor organizer. And I think one of the most important aspects of this work is figuring out how to take any sort of empirical findings you have and then translate them into policy changes, into legal strategies and also bargaining strategies. And so how can labor groups, for example, figure out how to document the harms that are happening? How do you sort of prove that something like algorithmic wage discrimination is happening within a gig app? And then how do you then take that information and effectively lobby and advocate for changes to policy? How do you advocate for sort of some kind of legal recourse? And so knowing sort of what the sort of burden of proof would be right for any form of algorithmic discrimination. And so, you know, this is why, as my colleagues found when they were looking at sort of the, the New York City local law 144, which was requiring employers who were using automated employment decision tools to audit them in order to figure out if they were biased according to race or gender. And basically, a lot of these laws are not necessarily actually effective because it's actually quite hard to even demonstrate this sort of, this sort of bias. It actually is not always completely obvious, and it requires, in some cases, a real sort of technical knowledge or access that people don't have. And so one of the groups that we've been working with at AIM Lab is the Workers' Algorithm Observatory. And they're a group of researchers who are based at Princeton, but they're working with rideshare drivers who are trying to really conduct what they call a form of algorithmic inquiry. So from the marxist kind of workers inquiry, which is a way that workers kind of document data about themselves, and it's a way for workers to really use data collection to understand their working conditions and how to change them and to kind of compare notes about what is happening to them on the ground. And so for rideshare drivers to be able to sort of begin to share information with each other in a comprehensive way in order to really be able to get to the root of algorithmic wage discrimination so that it can be part of some kind of organizing strategy is incredibly useful. So another part of this is really trying to figure out what forms of research and what kinds of research questions are actually beneficial to the people that you're trying to advocate for.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The speaker's favorite part of their work is translating empirical findings into policy changes, legal strategies, and bargaining strategies.\n",
      "- They discuss the challenges of documenting and proving algorithmic wage discrimination and advocating for policy and legal changes.\n",
      "- They mention the difficulty in demonstrating bias in automated employment decision tools and the limitations of existing laws.\n",
      "- The Workers' Algorithm Observatory at AIM Lab is conducting algorithmic inquiry with rideshare drivers to understand and combat algorithmic wage discrimination.\n",
      "- The speaker emphasizes the importance of conducting research that is beneficial to the people being advocated for.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am often asked about the most rewarding aspect of my work, and I must say that I find great satisfaction in the process of translating empirical findings into tangible policy changes, legal strategies, and bargaining tactics. One of the key challenges we face is documenting and proving algorithmic wage discrimination, and advocating for the necessary policy and legal changes. It's not always easy to demonstrate bias in automated employment decision tools, and existing laws have limitations in addressing this issue. That's why our collaboration with the Workers' Algorithm Observatory at AIM Lab is crucial, as they are conducting algorithmic inquiry with rideshare drivers to combat algorithmic wage discrimination. It's essential to conduct research that directly benefits the people we are advocating for.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I guess I want to push a little bit more into this because I'm thinking about, you know, what you brought up, which is the kind of power differential between those who create AI and those who are subjected to it. What options do we have to push back on tech companies broadly, and AI companies specifically, companies that have oftentimes accrued both tremendous power and wealth, who have set norms and standards that maximize their ability to profit and also to limit their liability for the harms that they cause, and who are developing products with a mass reach in ways that have become so deeply institutionalized and part of our environment that we don't really have the ability to alter their effect or to extract ourselves from them or to say no, and who also have massive lobbying power and money to fight lawsuits, and whose wealth and profit often means that they can afford to pay fines that they may accrue from causing harm or violating a regulation, while continuing to pursue business practices that provide profit as normal, since the profit that they would get from creating the product and distributing it and deploying it oftentimes exceeds massively any penalty, financial or otherwise, that they might accrue from violations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Power differential between creators of AI and those subjected to it\n",
      "- Limited options to push back on tech and AI companies\n",
      "- Companies have accrued tremendous power and wealth\n",
      "- Set norms and standards to maximize profit and limit liability for harms\n",
      "- Developing products with mass reach\n",
      "- Deeply institutionalized in our environment\n",
      "- Massive lobbying power and money to fight lawsuits\n",
      "- Ability to afford fines and continue profit-driven business practices\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I want to address the power differential between those who create AI and those who are subjected to it. Tech and AI companies have accrued tremendous power and wealth, setting norms and standards to maximize their profit and limit their liability for the harms they cause. Their products have become deeply institutionalized in our environment, making it difficult for us to alter their effects or extract ourselves from them. They also have massive lobbying power and money to fight lawsuits, and their wealth and profit often mean that they can afford to pay fines for any harm caused. It's clear that we have limited options to push back on these companies, and it's a concerning issue that needs to be addressed.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Right. And I think this really gets to the root of the problem, and I can kind of tie that back a little bit to the environmental concern of AI as well. And so the fact that you really just have a handful of companies that completely control the production and use of large scale AI right now is quite, quite frightening. And so, you know, in order to have access to the technology, in order to do the kind of work you want to do, you really need to be tied to just a handful of companies and very elite research universities. And I think this sort of larger problem of how much control over people's lives should tech companies have. I mean, this is something that I obviously also write about in my book, where I'm really sort of grappling with the fact that platforms have an outsized amount of control over how people are memorialized and over how they're able to mourn as more and more people sort of use various social media platforms and other digital assets in order to maintain relationships with the dead or to try to create a kind of legacy for themselves. And I actually don't know if a lot of the sort of regulation that we have on the immediate horizon is really going to be enough to change that power dynamic. I think we would really need to think about the production of technology. We would need to really think about the system that we live in and change it in a much more radical way. And am I suggesting that we would need to basically stop a model of endless growth where profits and the well being of shareholders is held up above the welfare of everyone else on the planet? Yes, I think we would need to kind of radically rethink and reformulate the ways that we think about what a successful sort of product or a successful company looks like. But I'm not sure if I have a whole lot of hope of that happening. And so I think until we have some sort of really massive structural change, the best that we can do is sort of exert pressure from different areas. And so I think it is actually important to have people working from within tech companies who are pushing for some kind of change. I think you do need people inside who understand how power works internally and who are, you know, at least close to the machine in that way. I also think that you need policy changes and pressure from grassroots organizations, advocacy and civil rights organizations, legal advocacy groups, and other sort of labor organizers. And I think you do need also academic researchers who are able to sort of lobby critiques of tech and sort of these systems a bit from an outside perspective. But you really need kind of all of these things to work together in tandem. And you certainly need the input from the communities who are going to be most impacted, which is often even within the more participation based imaginings of impact assessment or thinking about the power of technology, it still is a problem because the company or the people who are attempting to bring communities in, for instance, us at AIM Lab, we're still kind of setting the terms right, like it isn't really, you know, we're sort of helping perhaps give marginalized people a platform, but the power dynamics are still a little bit off. Right. And so I think really thinking about how we can kind of collectively have more power over how technology is produced and used. That is what ultimately we need to have happen.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The control of large scale AI by a handful of companies is frightening\n",
      "- Tech companies have an outsized amount of control over people's lives\n",
      "- Radical reformulation of the ways we think about successful products and companies is needed\n",
      "- Pressure for change needs to come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities\n",
      "- Collective power over how technology is produced and used is necessary\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that the control of large scale AI by just a handful of companies is quite frightening. It's not just about the environmental concern, but also about how much control tech companies have over people's lives. We really need to radically reformulate the ways we think about what a successful product or company looks like. Until we have some sort of massive structural change, the best we can do is exert pressure from different areas. This includes having people working from within tech companies pushing for change, policy changes, grassroots organizations, academic researchers, and impacted communities. Ultimately, we need to collectively have more power over how technology is produced and used.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I want to dig in a little bit into this idea of harm, because sometimes when we talk about harm, we talk about intentional harm. Sometimes we talk about harm as an unfortunate byproduct or an unintended byproduct of a certain technology's deployment. Can we, or should we make distinctions between different kinds of ways in which tech products cause harm? Between, for instance, a product that causes harm in unintended ways versus products that are released and cause harms in ways that are predictable but willfully ignored? Now, sometimes I have this conversation with somebody and their argument is that if we look more closely, almost always these so called unintended consequences are actually very predictable, but that either they are overlooked in favor of profit or progress, or that these, quote, unpredictable consequences were the result of negligence caused by overlooking or excluding already marginalized populations. Or alternatively, that the value system of tech production that already exists disregards the idea that the harm caused by the product is actually really harm to begin with. I'll give you an example of what I mean by the last part. The idea that something will take over jobs or something that will disconnect us from our immediate environment is not isn't actually a harm because it is providing efficiency. And in the context of this product's developers, efficiency is the value to maximize and maybe enjoying our environment or doing things in unpredictable ways because there isn't an algorithm guiding us about what exactly to do isn't actually a harm to begin with. Right? Some of us might think that the removal of the incidental or the coincidental from our environment, the capricious from our environment, might be a harm for those who are developing these technologies and algorithm that has a greater ability to predict and direct us in certain ways might be a benefit, and the loss of that would be be a harm at all. I do think that there are instances in which tech products cause harm as the result of bad actors using the product in unintended ways, or ways that a technology can go wrong despite the intentions of the creators. But I am also aware that the majority of harms that I think we're talking about are harms that come even as good users are using the product as it was intended to be used. How do you think about the idea of unintended consequences when it comes to what you see in tech culture?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Harm caused by tech products can be intentional or unintended.\n",
      "- There may be distinctions between different kinds of harm caused by tech products.\n",
      "- Unintended consequences of tech products may actually be predictable but overlooked in favor of profit or progress.\n",
      "- The value system of tech production may disregard the harm caused by the product.\n",
      "- Some may argue that certain consequences, such as job displacement or disconnection from the environment, are not actually harms but rather provide efficiency.\n",
      "- Tech products can cause harm due to bad actors or unintended consequences despite the intentions of the creators.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe it's important to delve into the concept of harm caused by tech products. We need to consider the distinctions between intentional and unintended harm. It's possible that some unintended consequences are actually predictable but are overlooked in favor of profit or progress. The value system of tech production may disregard the harm caused by the product. Some argue that certain consequences, such as job displacement or disconnection from the environment, are not actually harms but rather provide efficiency. However, we must also acknowledge that tech products can cause harm due to bad actors or unintended consequences despite the intentions of the creators. How do you think about the idea of unintended consequences when it comes to what you see in tech culture?\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, that's a great question, and this is why I think we really need more historians to be in these spaces I'm always very happy to bring historians into the conversation whenever I can. And it's something that I actually did at Intel. So I brought in Mar Hicks to talk about the history of queer computing. I also brought in Cassidyre to talk about the history of trans computing and getting people in tech to think about how actually the thing that you're looking at and you think is brand new and shiny and full of innovation is actually connected to something that happened a long time ago that is similar. And we can kind of learn from the past to understand what these potential harms might be. And so something that came up a lot when I was at Intel, it was sort of the sort of middle of the metaverse fervor. And people were very excited about the metaverse and had a lot of ideas about what it might look like and what it might do for innovation and business and the future of work. And, you know, talking about some of the issues around sexual violence or women being harassed in sort of metaverse spaces, it was like, well, you know, if you kind of took a look at people who were users and people who are researchers of second life, or going back way further, you know, like, we could go back to, you know, Julian Dibble's a rape and cyber phase from 1993, talking about LambdaMOO. And so really beginning to frame sort of these problems about technology that look to be brand new and understand sort of the social dynamics and power structures that made them appear in different contexts. And so I think, for me, just being able to really use examples from the past and from other areas when you're talking about a new technology and trying to assess potential harms is actually quite useful because it really is, and sort of bigger than just a few bad actors or people who need to be removed from a platform. And it also is sort of a bigger problem than companies just creating something intentionally harmful to reap profits. But there's just obviously, we have a lot of bad things in our society. There's a lot of racism. There's still a lot of sexism, a lot of homophobia. There's just a lot of inequality. And to build things kind of without acknowledging that reality and without acknowledging these larger histories, I think, is when you really run into problems, because, you know, these things should be really obvious, it should not be hard to anticipate what some of the problems are going to be.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Historians are needed in tech spaces to understand the historical context of new innovations\n",
      "- Bringing in historians can help understand potential harms of new technology\n",
      "- Examples from the past can be useful in assessing potential harms of new technology\n",
      "- There are social dynamics and power structures that contribute to problems with new technology\n",
      "- Building things without acknowledging historical realities can lead to problems\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that we really need more historians to be in tech spaces to understand the historical context of new innovations. Bringing in historians can help us understand the potential harms of new technology and learn from the past. Examples from the past can be useful in assessing potential harms of new technology, as there are social dynamics and power structures that contribute to these problems. Building things without acknowledging historical realities can lead to problems, and it's important to acknowledge the larger histories and social inequalities in our society.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: Just a quick plug for folks who are listening to this episode to go check out Mar's episode of this show, we had a wonderful interview where she talks a lot about the importance of the present moment of technological production being informed by and thought through the narratives and the histories of the past. I want to ask you a question about a piece that you published recently in Wired. That piece is titled, using generative AI to resurrect the dead will create a burden for the living. The byline of that piece is AI technologies promise more chatbots and replicas of people who have passed of giving voice to the dead comes at a human cost. What are the specific concerns that you have about generative AI when it comes to environmental impact? And what are the human costs that you are talking about in that piece?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Importance of present moment of technological production being informed by narratives and histories of the past\n",
      "- Concerns about generative AI and its environmental impact\n",
      "- Human costs of using generative AI to resurrect the dead\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I just want to take a moment to encourage everyone listening to check out Mar's episode of this show. In our interview, she emphasized the significance of the present moment in technological production being influenced by the narratives and histories of the past. This is something we should all consider as we move forward with new technologies.\n",
      "\n",
      "Speaking of new technologies, I recently read a piece in Wired about using generative AI to bring back the voices of the deceased. It raised some important concerns about the environmental impact of this technology and the potential human costs. It's a topic that we need to carefully consider as we continue to advance in the field of AI.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, so, you know, in terms of preserving the data of the dead, it kind of gets at this larger kind of problem that's been very pervasive in the tech industry for quite some time, which is, you know, data is power. So therefore, for individual users, we're going to help you. We're going to collect as much data as we can about you, and we're also going to help you maintain your personal data forever. And so Google kind of had that line of reasoning for much of its existence, but recently it actually said that it was going to start deactivating the accounts of people who had been inactive for two years because there's some degree of recognition on the part of tech companies that, hey, wait a minute, if we actually promise to maintain everybody's data in perpetuity, that's a hell of a lot of data, given how much data people are creating, and that actually costs a lot of money. There is no cloud. I feel like people in STS are always railing about the materiality of computing. It's like, no, it's the undersea cables. No, these things are material. They're not ethereal. But I think that just becomes even more obvious when you're talking about something like generative AI, which, again, requires even more compute power. And so the expansion of data centers to kind of feed the hunger and the desire for generative AI is a problem because it will have a massive environmental impact. But then the idea that you would then sort of maintain something like what? You're going to maintain this sort of extremely powerful AI for all of eternity because it's actually like a simulation of your dead loved one. And so I think the ethics of sort of imagining that you're going to be able to perpetuate this kind of relationship with the dead through this kind of technology is also very misleading because these things will actually require system upgrades and software updates and maintenance, and eventually they may no longer run and they may become obsolete and the technology itself may die. So thinking about the fact that many people would then undergo a second form of grieving after having experienced grief already. So there are all these sort of ethical questions around who should actually have the authority to decide to create a simulation of a dead person through generative AI, and who should be able to kind of maintain that relationship and control that relationship. And should companies or employers or estates be able to profit from the kind of simulated version of that dead person? And so that gets into really thorny questions around not just kind of copyright, but also things like estate planning and kinship relations and also consent. So thinking about what it is that an individual really would have wanted and what their family members actually want, versus maybe somebody else who decides to revive the dead person. And so I think the sort of general problem of creating AI versions of the dead, which is not a new problem, but just something that is sort of in the news a lot right now, particularly because of deepfake technology and because of a lot of dead celebrities and such that have been revived for various specials. And so I think this sort of question of violation, and this is something that came out when Anthony Bourdain, when there was a documentary made of his life and the director made the decision to use a deepfake voice, to have Bourdain narrate this letter that he had written. It was his words, but the deepfake was just sort of seamlessly put into the documentary, which is a bit uncanny, you know, to sort of revive the dead in that way. But it was also done without the consent of bourdain's family. And so, you know, this is the kind of question that comes up a lot when we're talking about celebrities, but it's an issue for everyone. And, you know, the fact that, you know, for actors who are having their bodies scanned or models, anyone, academics who have all of their Zoom recordings available, the idea that your employer or that somebody could kind of try to monetize your likeness after you die in order to prevent them from having to hire living people is actually a lot of what the sort of fantasy is behind this. And so just thinking about how this actually creates a lot of problems in the realm of labor on top of all of the other terrible things that I just mentioned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The tech industry has been collecting and maintaining personal data of users for a long time\n",
      "- Google recently announced deactivating accounts of inactive users after two years\n",
      "- The expansion of data centers for generative AI will have a massive environmental impact\n",
      "- Creating AI versions of the dead raises ethical questions around authority, control, and profit\n",
      "- Deepfake technology and the use of deceased celebrities without consent is a violation\n",
      "- The issue of monetizing likeness after death creates labor problems\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "Yeah, so, you know, in terms of preserving the data of the dead, it kind of gets at this larger kind of problem that's been very pervasive in the tech industry for quite some time. Data is power, and tech companies have been collecting and maintaining personal data of users. Google recently announced deactivating accounts of inactive users after two years, recognizing the cost and volume of data. The expansion of data centers for generative AI will have a massive environmental impact. Creating AI versions of the dead raises ethical questions around authority, control, and profit. Deepfake technology and the use of deceased celebrities without consent is a violation. The issue of monetizing likeness after death creates labor problems. These are all thorny questions that need to be addressed.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: Well, I want to dig into this a little bit and ask you to explicate a little bit more the links that you're making between labor and environmental damage and the broader context that you're talking about here of the attempts to preserve data, and in particular preserve the data of the dead at all costs, and in fact, at a very high cost. I know that you've written very extensively about the way that socio technical systems, particularly social media platforms, and increasingly AI and generative AI, may be changing the way that our culture thinks about and performs rituals around and navigates death. In your book, which you were just you were just talking about, Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond, takes up this inquiry. Can you help us better understand and maybe spend a little time digging into the link between the questions that you take up in the study and the current work that you're doing with the AI impact lab at data and society?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The speaker wants to delve deeper into the links between labor and environmental damage in the context of preserving data, especially the data of the deceased.\n",
      "- The speaker has extensively written about how socio-technical systems, such as social media platforms and AI, are changing cultural rituals and perceptions of death.\n",
      "- The speaker's book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond,\" explores these themes.\n",
      "- The speaker is also involved in the AI impact lab at data and society.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am deeply interested in exploring the connections between labor, environmental damage, and the preservation of data, particularly the data of the deceased. My extensive writing has focused on how socio-technical systems, like social media platforms and AI, are reshaping cultural rituals and attitudes towards death. This is a central theme in my book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond.\" Additionally, my current work with the AI impact lab at data and society allows me to further investigate these important issues.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, definitely. So I really feel like the problem of death is sort of, it's a very old human problem, and it's a fairly universal problem. So despite what the transhumanists who tell us that we can upload our brains, no matter what they say, death is definitely coming for everyone. And people have always had different ways of memorializing the dead. And the way that people treat the dead can also certainly reveal different sort of social relations hierarchies within a society. And so, of course, if we go back to archaeology or something, we can look at the mortuary rituals around kings and queens versus commoners. There's always a way to sort of understand the way that a society is structured through treatment of the dead. And so I find it really fascinating to look at the different ways that death kind of disrupts the original plan plan for a lot of technologies. So technologies like social media, platforms that were built for youthful users who were at elite colleges primarily in the beginning, and the idea that they would somehow become spaces for memorialization, for long term relationships with the dead, for mourning. This is not something that was an immediate sort of use case for them. And so I think death is actually a very useful way to kind of begin to think about that was something really obvious in a lot of ways. I mean, you know, death is definitely, again, you know, it's a universal thing and it's kind of everywhere and will come for us all, but it still is something that's very easy to overlook. And so it's just, how could it be that every single company always forgets about death, and then they forget about it repeatedly over time? And this is not an area that, you know, tech companies largely have focused on. This is not, people don't have a lot of big UX teams devoted to death. This is not something that is sort of the norm, even though it is something that happens all the time. So I think when we're talking about the larger problem of ethics and technology, when we're talking about how communities are going to be impacted, how individuals and their social networks are going to be impacted, that kind of relational aspect, that's something that I think is really an important lens for considering impact assessments for algorithmic systems overall, because we're really not just talking about the tech itself and auditing it to understand is the tech doing what we think it's doing, but we're also trying to examine all of the social relations that are clustered around the technology and what all of the potential downstream impacts are. And so I think death is one of those things that really helps you see a lot of those relationships and networks over time in a way that maybe other social things are not quite as rich. I think that death is an incredibly rich field site for really thinking about algorithmic impacts.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Death is a universal and old human problem\n",
      "- Different ways of memorializing the dead reveal social hierarchies\n",
      "- Mortuary rituals around kings and queens vs commoners reflect societal structure\n",
      "- Death disrupts the original plan for technologies like social media\n",
      "- Tech companies overlook the impact of death and do not focus on it\n",
      "- Examining the social relations around technology is important for impact assessments\n",
      "- Death is a rich field site for thinking about algorithmic impacts\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I strongly believe that death is a universal and old human problem that cannot be avoided. The way people memorialize the dead reveals social hierarchies and societal structures. It's fascinating to see how death disrupts the original plan for technologies like social media, which were not initially built for memorialization. It's surprising how tech companies repeatedly overlook the impact of death and do not focus on it. Examining the social relations around technology is crucial for impact assessments, and death provides a rich field site for thinking about algorithmic impacts.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I want to talk about a word in your title, that word techno solutionism. How do you think about techno solutionism as an ideology and as a practice and as a vision for how we move forward in our society? As a society, what does techno solutionism and its cousin, techno utopianism, get right? What do these terms and the visions that they represent miss or misunderstand?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Techno solutionism is a word in the title that I want to talk about.\n",
      "- I want to discuss techno solutionism as an ideology, practice, and vision for moving forward in society.\n",
      "- I want to explore what techno solutionism and its cousin, techno utopianism, get right and what they miss or misunderstand.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I want to address the concept of techno solutionism, which is a word in the title that I find important. I believe it is crucial to consider techno solutionism as an ideology, a practice, and a vision for the future of our society. I am interested in examining what techno solutionism and its related concept, techno utopianism, get right, and also what they may miss or misunderstand.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: So I think there are aspects of techno solutionism that can be useful. So, for example, if you're kind of attempting to build the technology and you notice that there's a particular pain point or a point of failure, and you're able to make a small change in the technology itself, and then it runs much more efficiently. In that case, yes, there may very well be a solution to a problem that is really just a technical fix that will do a lot of good. But I think the larger problem is sort of this common issue of, as you mentioned before, where there's a problem that is very much a social problem that could be fixed by something that really has nothing to do with technology, and yet technology is kind of brought in anyway. But you also do have a problem where even the process that I described right, where, you know, with AIM lab, we're kind of helping different groups and different organizations understand the technology that they're building and help them kind of develop relationships with the potential communities that will be impacted by the technology. And so part of that process is really about maybe taking information back to the technical team and improving the technology and preventing bugs from occurring or also mitigating any potential impacts that would be harmful. But another part of the process that really has nothing to do with technology is the creation of relationships. The idea is that the partners that we're collaborating with, the hope is that they will continue to be in dialogue with the different communities that we're engaging with. We're not wanting it to be a kind of one off thing where, you know, okay, so we took information back to the technical team. We tweaked it a bit, and now we're done. The idea is that it should be something that continues, and it should be a long term relationship and process. And really, I would say that for a lot of the work that we're doing, that is the most important piece of it is really about the relationships with techno solutionism. I think the problem is that so often the focus on technology kind of outshines all of these other factors, and you can really miss the forest for the trees. And with techno utopianism, I mean, I think it is better to imagine other futures. So I think that we can learn from feminist and black Sci-Fi traditions. If we could think about afrofuturism, there are ways of imagining futures that are not in the vision of Elon Musk. There are alternatives to that kind of techno imaginary. And so it would be a mistake to completely throw away anything that's possible with the imagination. And I think that sort of reimagining technology, but then also building it differently, these things have to go together. So I think it's great to have the imaginary of how things could be different. And then I think we really need to kind of dismantle the way that tech is currently being built and deployed in order for that, some degree of that kind of imaginary to be. To be realized. And so for me, I am definitely interested in the magical and mystical and metaphysical qualities of technology. So I think that using technology to communicate with the dead is a thing that we've done as humans for a very long time. And you don't need to use a digital form of technology for that to happen. And so I would say that I am really interested in sort of the transcendent qualities of technology. And I believe that people derive a real sense of connection and something sacred out of technologies that are incredibly mundane. So I would not want to dismiss that. I think that there is a certain sort of magical quality to technology that can be a good thing, that can be not sort of the fetish of masking unequal labor relations, which it also is. But I'm interested in sort of digging into the other uses of technology that might kind of expand that.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Techno solutionism can be useful for fixing specific pain points or failures in technology.\n",
      "- However, technology is often brought in to solve social problems that have nothing to do with technology.\n",
      "- The process of developing technology should also focus on building relationships with the communities impacted by the technology.\n",
      "- The focus on technology often overshadows the importance of building and maintaining relationships.\n",
      "- It's important to imagine alternative futures to the current techno imaginary, such as afrofuturism.\n",
      "- Reimagining technology and building it differently should go hand in hand.\n",
      "- There are magical and mystical qualities of technology that can be explored and utilized beyond just mundane uses.\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "I believe that techno solutionism can be useful for addressing specific pain points or failures in technology. However, it's important to recognize that technology is often brought in to solve social problems that have nothing to do with technology. In the process of developing technology, we should also prioritize building relationships with the communities impacted by the technology, rather than solely focusing on the technical aspects. It's crucial to imagine alternative futures beyond the current techno imaginary, such as afrofuturism, and to reimagine technology while also building it differently. Additionally, I am interested in exploring the magical and mystical qualities of technology, as I believe they can be utilized beyond just mundane uses.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: I think we have time for one last question. I teach a course on data and human values at UC Berkeley, and a lot of students there and elsewhere across college campuses and universities listen to this show. And I know that you come to data and society as we've talked about after many years as a professor working with students in the academy, many of them are going to end up in the tech industry. What would you want them to know or think about or understand or reconsider as they move into their careers?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Teaching a course on data and human values at UC Berkeley\n",
      "- Many students across college campuses and universities listen to the show\n",
      "- Coming to data and society after many years as a professor\n",
      "- Students going to end up in the tech industry\n",
      "- What would I want them to know or think about as they move into their careers?\n",
      "\n",
      "COMBIMED_SPEECH\n",
      "As a professor teaching a course on data and human values at UC Berkeley, I am aware that many students across college campuses and universities listen to this show. After many years in the academy, I have come to data and society, and I understand that many of my students are going to end up in the tech industry. So, what I want them to know or think about as they move into their careers is crucial.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. And I have to say, when I talk to younger generations of people who want to go into tech, there is much more of an awareness of the potential problems. And people actually seem quite hungry to have some degree of training in things like STS and critical technology studies in general. So I think the important thing is to read widely and sort of talk to people outside of tech. I would say the most important things are to understand the history of technology, understand the history of the tech industry. So look to organizers from the past. And that was something that I did with a number of my collaborators for the tech workers coalition teach in, where we basically brought people from, IBM, black Workers alliance from the 1970s, and a number of other activist groups that had been really prominent decades before, along with current activists who were trying to change things from within in the tech industry now. And it was really interesting to have people from different generations talking to each other. Or again, the reason why I brought Mar and Cass and to talk to colleagues at Intel, just a way of helping people realize that there have been activists in tech. There are people been doing this kind of work in coalition building with people outside of tech for a long time, so you don't have to reinvent the wheel. You can kind of learn from people who came before you. And then the other important thing is to continue talking to people who are not in the tech industry. So not just on a kind of, like, interpersonal level, but also if you are interested in making tech more ethical, instead of just sort of, you know, taking one ethics class in data science or computer science, really try to, you know, expand your relationship with different communities that you are around in your day to day life. So one example that I can point to is knowing some developers that really, you know, sort of large tech companies who take on volunteer positions, working with, say, middle schoolers in Oakland who want to learn how to design video games. And so volunteering your time or getting to know different kinds of communities and getting to know their concerns and being more sort of involved with issues where you live can also be really, really helpful. So then I think that especially goes for those of us who live in the Bay Area. So how do you sort of make sure that you are firmly situated in a place and that you're not just sort of an interloper who spends all of your time only talking to other tech people?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Younger generations are more aware of potential problems in tech and are interested in training in STS and critical technology studies.\n",
      "- It is important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past.\n",
      "- Building coalitions with people outside of tech and learning from their experiences is valuable.\n",
      "- Continuing to engage with communities outside of the tech industry and getting involved in their concerns is important for making tech more ethical.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, it's a great question. When I talk to younger generations interested in tech, I see a strong awareness of potential problems and a hunger for training in STS and critical technology studies. It's important to understand the history of technology and the tech industry, and to learn from past activists and organizers. Building coalitions with people outside of tech and learning from their experiences is valuable. Continuing to engage with communities outside of the tech industry and getting involved in their concerns is important for making tech more ethical. So, it's about reading widely, talking to people outside of tech, and learning from those who came before us. And it's also about being actively involved in communities and their concerns, not just on an interpersonal level, but also in a way that contributes to making tech more ethical. Especially for those of us in the Bay Area, it's important to be firmly situated in a place and not just spend all our time talking to other tech people.\n",
      "===============================================================\n",
      "speaker: Deb\n",
      "speech: Thank you so much, Tamara.\n",
      "summarized speech:\n",
      "Thank you so much, Tamara.\n",
      "===============================================================\n",
      "speaker: Tamara\n",
      "speech: Thank you, Deb.\n",
      "summarized speech:\n",
      "Thank you, Deb.\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for podcast in podcast_data:\n",
    "    transcript = podcast['transcript']\n",
    "    \n",
    "    for line in transcript:\n",
    "        speaker = line[0:line.find(\":\")].strip()\n",
    "        speech = line[line.find(\":\")+1:].strip()\n",
    "        print(f\"speaker: {speaker}\")\n",
    "        print(f\"speech: {speech}\")\n",
    "        \n",
    "        \n",
    "        summarized_speech = getKeyPointsFromSpeaker(speaker, speech)\n",
    "        \n",
    "        print(\"summarized speech:\")\n",
    "        print(summarized_speech)\n",
    "        print(\"===============================================================\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
