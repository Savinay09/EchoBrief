{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v1\" # no rewritten\n",
    "METHOD_NAME=\"sequential_host_guest_split\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fa0cae59850>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRR # lines: 69\n",
      "RRR # lines after cleaned: 35\n"
     ]
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open(f'taking_the_temp_of_ai_{METHOD_NAME}.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    print(f\"RRR # lines: {len(lines)}\")\n",
    "    \n",
    "    lines = [ line for line in lines if line.strip()]\n",
    "    \n",
    "    print(f\"RRR # lines after cleaned: {len(lines)}\")\n",
    "\n",
    "    podcast = {    \n",
    "        \"episode_number\": \"taking_the_temp_of_ai\",    \n",
    "        \"transcript\": lines,\n",
    "    }\n",
    "    podcast_data.append(podcast)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyPointsFromSpeaker(speaker, speech):\n",
    "    \n",
    "    if len(speech) <= 200:\n",
    "        return speech\n",
    "    \n",
    "    eval_prompt_template = \"\"\"\n",
    "    You will extract the key points in the given speech in the first person.\n",
    "    \n",
    "    Then, combine them to form a complete speech as if it is by the speaker him/herself.\n",
    "\n",
    "    Here is the speech:\n",
    "    SPEECH: {speech}\n",
    "    \n",
    "    Here is the output format:\n",
    "    KEY_POINTS\n",
    "    COMBINED_SPEECH\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"speaker\", \"speech\"])\n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"speech\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "#             'speaker': speaker,\n",
    "            'speech': speech,    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    return map_llm_chain_results[0]['text']    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker: Deb\n",
      "speech: This is Deb Donig with technically Human, a podcast about ethics and technology, where I ask what it means to be human in the age of tech. Each week I interview industry leaders, thinkers, writers, and technologists, and I ask them about how they understand the relationship between humans and the technologies we create. We discuss how we can build a better vision for technology, one that represents the best of our human values. Today, I'm sitting down with Doctor Tamara Neese. Doctor Tamara Kneese is project director of Data and society's Algorithmic Impact Methods Lab, where she is also a senior researcher. For the 2023 and 2024 academic year, she's a visiting scholar at UC Berkeley's center for Science, Technology, Medicine and Society. Before joining data and society, she was lead researcher at Green Software foundation, director of developer engagement on the green software team at intel, and assistant professor of media studies and director of gender and sexuality studies at the University of San Francisco. Tamara holds a PhD in media, culture and communication from NYU and is the author of Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond. In her spare time, she's a volunteer with tech workers Coalition.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Technically Human podcast focuses on ethics and technology\n",
      "- Interviews industry leaders, thinkers, writers, and technologists\n",
      "- Discusses the relationship between humans and technology\n",
      "- Aims to build a better vision for technology based on human values\n",
      "- Interviewing Doctor Tamara Neese, project director of Data and Society's Algorithmic Impact Methods Lab\n",
      "- Doctor Tamara Neese's background and qualifications\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Hello, I'm Deb Donig, the host of Technically Human, a podcast that delves into the intersection of ethics and technology. Each week, I have the privilege of speaking with industry leaders, thinkers, writers, and technologists to explore what it truly means to be human in the age of tech. Together, we aim to understand the relationship between humans and the technologies we create and work towards building a better vision for technology that aligns with our human values. Today, I am thrilled to have Doctor Tamara Neese, the project director of Data and Society's Algorithmic Impact Methods Lab, as my guest. Doctor Neese brings a wealth of experience and expertise, having worked in various prestigious roles and holding a PhD in media, culture, and communication. I am excited to delve into a thought-provoking discussion with her.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- Technically Human podcast focuses on ethics and technology\\n- Interviews industry leaders, thinkers, writers, and technologists\\n- Discusses the relationship between humans and technology\\n- Aims to build a better vision for technology based on human values\\n- Interviewing Doctor Tamara Neese, project director of Data and Society's Algorithmic Impact Methods Lab\\n- Doctor Tamara Neese's background and qualifications\", 'combined_speech': \"Hello, I'm Deb Donig, the host of Technically Human, a podcast that delves into the intersection of ethics and technology. Each week, I have the privilege of speaking with industry leaders, thinkers, writers, and technologists to explore what it truly means to be human in the age of tech. Together, we aim to understand the relationship between humans and the technologies we create and work towards building a better vision for technology that aligns with our human values. Today, I am thrilled to have Doctor Tamara Neese, the project director of Data and Society's Algorithmic Impact Methods Lab, as my guest. Doctor Neese brings a wealth of experience and expertise, having worked in various prestigious roles and holding a PhD in media, culture, and communication. I am excited to delve into a thought-provoking discussion with her.\"}\n",
      "speaker: Deb\n",
      "speech: Hi Tamara.\n",
      "summarized speech:\n",
      "Hi Tamara.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': 'Hi Tamara.'}\n",
      "speaker: Tamara\n",
      "speech: Hi Deb.\n",
      "summarized speech:\n",
      "Hi Deb.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': 'Hi Deb.'}\n",
      "speaker: Deb\n",
      "speech: So Tamara, we're talking in a week where Congress just introduced a landmark bill that would move the government toward developing standards that would measure and report the full range of AI's environmental impact, as well as one that would create a voluntary framework for AI developers to report environmental impacts. This legislation, just to give a bird's eye view, also requires an interagency study that would aim to investigate and measure both the positive and the negative environmental impacts of AI. Can you talk a little bit about the legislation?\n",
      "summarized speech:\n",
      "KEY_POINTS:\n",
      "- Congress introduced a landmark bill for developing standards to measure and report AI's environmental impact\n",
      "- The bill also aims to create a voluntary framework for AI developers to report environmental impacts\n",
      "- The legislation requires an interagency study to investigate and measure both positive and negative environmental impacts of AI\n",
      "\n",
      "COMBINED_SPEECH:\n",
      "So, I want to talk about the recent landmark bill introduced by Congress. This bill is a significant step towards developing standards to measure and report the environmental impact of AI. It also aims to create a voluntary framework for AI developers to report their environmental impacts. Additionally, the legislation requires an interagency study to investigate and measure both the positive and negative environmental impacts of AI. This is a crucial development in the regulation of AI and its environmental effects.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- Congress introduced a landmark bill for developing standards to measure and report AI's environmental impact\\n- The bill also aims to create a voluntary framework for AI developers to report environmental impacts\\n- The legislation requires an interagency study to investigate and measure both positive and negative environmental impacts of AI\", 'combined_speech': 'So, I want to talk about the recent landmark bill introduced by Congress. This bill is a significant step towards developing standards to measure and report the environmental impact of AI. It also aims to create a voluntary framework for AI developers to report their environmental impacts. Additionally, the legislation requires an interagency study to investigate and measure both the positive and negative environmental impacts of AI. This is a crucial development in the regulation of AI and its environmental effects.'}\n",
      "speaker: Tamara\n",
      "speech: Yeah, so, you know, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. So one of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So if you think about the fact that the kind of form of AI that we usually think about might be the thing that we interact with directly. So if you're thinking about Chad GBT, you're thinking about maybe the energy used to prop up that interaction that you're having with it. But behind that, there's a whole supply chain that has to go into the manufacturing of AI, which also requires the manufacturing of all the hardware it relies on, including chips and the raw earth minerals that are needed to make the chips possible. In the first place. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Excitement at data and society about the bill calling for robust socio technical research\n",
      "- Difficulty in measuring environmental impacts of AI, especially across the entire supply chain\n",
      "- Importance of considering the effects of AI production and use globally\n",
      "- Advocacy for legislation pushing for standards around measurement and reporting\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, so, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. One of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Excitement at data and society about the bill calling for robust socio technical research\\n- Difficulty in measuring environmental impacts of AI, especially across the entire supply chain\\n- Importance of considering the effects of AI production and use globally\\n- Advocacy for legislation pushing for standards around measurement and reporting', 'combined_speech': \"Yeah, so, first of all, we were really excited at data and society to hear about this bill because it is a bill that really calls for a very robust form of socio technical research. One of the problems with measuring the environmental impacts of AI is that it's actually quite hard to do, particularly if you're trying to measure impacts around every single part of the AI supply chain. So really thinking about all of the different effects that AI production and use can have around the world is something that a lot of technologists and advocates have been wanting to measure for quite some time. And so having legislation that is really pushing for it and pushing for the creation of standards around measurement and reporting is incredibly important.\"}\n",
      "speaker: Deb\n",
      "speech: You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?\n",
      "summarized speech:\n",
      "You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': \"You said that researchers have wanted this legislation for a long time. Why hasn't it happened already? Has gotten in the way?\"}\n",
      "speaker: Tamara\n",
      "speech: There are a few problems when it comes to measuring the environmental impact of something like AI, precisely because of all of the very complicated global supply chains that are involved in its production. And so you really need to have a lot of coordination across different parts of the supply chain, different companies, different parts of the world. And so the bill is actually sort of relying on not just the EPA, but also the National Institute of Standards and Technology to really convene a group of people who are going to be able to identify the methodologies and standards that are needed to do this kind of measurement work.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Measuring the environmental impact of AI is challenging due to complex global supply chains.\n",
      "- Coordination across different parts of the supply chain and different companies is necessary.\n",
      "- The bill relies on the EPA and the National Institute of Standards and Technology to convene a group to identify measurement methodologies and standards.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Measuring the environmental impact of something like AI is quite challenging because of the complex global supply chains involved in its production. It requires a lot of coordination across different parts of the supply chain, different companies, and different parts of the world. The bill is relying on not just the EPA, but also the National Institute of Standards and Technology to convene a group of people who can identify the methodologies and standards needed for this kind of measurement work.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Measuring the environmental impact of AI is challenging due to complex global supply chains.\\n- Coordination across different parts of the supply chain and different companies is necessary.\\n- The bill relies on the EPA and the National Institute of Standards and Technology to convene a group to identify measurement methodologies and standards.', 'combined_speech': 'Measuring the environmental impact of something like AI is quite challenging because of the complex global supply chains involved in its production. It requires a lot of coordination across different parts of the supply chain, different companies, and different parts of the world. The bill is relying on not just the EPA, but also the National Institute of Standards and Technology to convene a group of people who can identify the methodologies and standards needed for this kind of measurement work.'}\n",
      "speaker: Deb\n",
      "speech: What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.\n",
      "summarized speech:\n",
      "What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': \"What's the history behind this legislation? What are the concerns or incidents or discoveries that led to this proposed need to assess the environmental impact of AI.\"}\n",
      "speaker: Tamara\n",
      "speech: With large language models in particular? So if we're talking about the kind of AI like, generative AI, that requires a large amount of compute, so you actually need specialized forms of chips in order to run AI efficiently. You also need just a tremendous amount of power. So you need a lot of energy, electricity. You also need a lot of water. And so researchers from different corners of tech have been trying to figure out exactly how much energy or how much water is needed to train these large language models. And then other researchers have looked at what happens when a kind of AI is actually deployed. And so what is the sort of energy or water consumption associated with the use of AI, not just the training part. And so this is something that has been a growing problem. So the entire ICT sector takes up a tremendous amount of energy. Many of us have probably read some of the articles that have come out over the years about the effects of data centers on local areas, maybe that are already experiencing drought or that already have issues with their grid. And so because data centers require just a tremendous amount of energy and can also pollute the areas in which they're located while also sucking up a lot of water, it really becomes an environmental justice issue. At the same time that it's sort of a larger question of worrying about the control of resources and the potential cost in terms of carbon emissions. So this sort of robust form of research that would actually kind of take all of these aspects into account about AI. It's actually just that AI is one small part of a much larger problem that's been going on for quite a long time. And actually, we can look to things that happened even it may sound like a really long time ago, but the 1980s and looking at things that happened in the Silicon Valley region when it comes to the manufacturing of chips and in some cases toxic chemicals got into the local water supply in San Jose, and it took community members really coming together and fighting back against this sort of contamination and asking for more regulation to prevent this sort of thing from happening in the future. And so I think if we look at what is happening right now, we have an AI boom. And so it's a good time to really focus on making some changes through regulation and to get a better grip on sort of what the real environmental costs are. But again, this is not a new problem. This is not unique to generative AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Large language models and generative AI require specialized chips and a lot of power, energy, and water for efficient operation.\n",
      "- The energy and water consumption associated with training and deploying AI is a growing problem, especially in the ICT sector.\n",
      "- Data centers require a tremendous amount of energy, can pollute local areas, and consume a lot of water, leading to environmental justice issues and potential carbon emissions.\n",
      "- AI is just one part of a larger problem that has been ongoing for a long time, and there is a need for robust research to address all aspects of AI's environmental impact.\n",
      "- The history of environmental issues related to technology, such as toxic chemicals in the water supply in the Silicon Valley region in the 1980s, highlights the importance of community action and regulation to prevent future contamination.\n",
      "- The current AI boom presents an opportunity to focus on making regulatory changes and understanding the real environmental costs, but this is not a new problem and is not unique to generative AI.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "As we delve into the world of large language models and generative AI, it becomes clear that these technologies require specialized chips and a significant amount of power, energy, and water for efficient operation. The growing problem of energy and water consumption associated with training and deploying AI is particularly evident in the ICT sector, where data centers play a significant role. These data centers not only consume a tremendous amount of energy but also have the potential to pollute local areas and deplete water resources, leading to environmental justice issues and potential carbon emissions. It's important to recognize that AI is just one part of a larger problem that has been ongoing for a long time, and there is a need for robust research to address all aspects of AI's environmental impact. Looking back at the history of environmental issues related to technology, such as the contamination of the water supply in the Silicon Valley region in the 1980s, we see the importance of community action and regulation to prevent future environmental damage. The current AI boom presents an opportunity to focus on making regulatory changes and understanding the real environmental costs, but it's crucial to remember that this is not a new problem and is not unique to generative AI.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- Large language models and generative AI require specialized chips and a lot of power, energy, and water for efficient operation.\\n- The energy and water consumption associated with training and deploying AI is a growing problem, especially in the ICT sector.\\n- Data centers require a tremendous amount of energy, can pollute local areas, and consume a lot of water, leading to environmental justice issues and potential carbon emissions.\\n- AI is just one part of a larger problem that has been ongoing for a long time, and there is a need for robust research to address all aspects of AI's environmental impact.\\n- The history of environmental issues related to technology, such as toxic chemicals in the water supply in the Silicon Valley region in the 1980s, highlights the importance of community action and regulation to prevent future contamination.\\n- The current AI boom presents an opportunity to focus on making regulatory changes and understanding the real environmental costs, but this is not a new problem and is not unique to generative AI.\", 'combined_speech': \"As we delve into the world of large language models and generative AI, it becomes clear that these technologies require specialized chips and a significant amount of power, energy, and water for efficient operation. The growing problem of energy and water consumption associated with training and deploying AI is particularly evident in the ICT sector, where data centers play a significant role. These data centers not only consume a tremendous amount of energy but also have the potential to pollute local areas and deplete water resources, leading to environmental justice issues and potential carbon emissions. It's important to recognize that AI is just one part of a larger problem that has been ongoing for a long time, and there is a need for robust research to address all aspects of AI's environmental impact. Looking back at the history of environmental issues related to technology, such as the contamination of the water supply in the Silicon Valley region in the 1980s, we see the importance of community action and regulation to prevent future environmental damage. The current AI boom presents an opportunity to focus on making regulatory changes and understanding the real environmental costs, but it's crucial to remember that this is not a new problem and is not unique to generative AI.\"}\n",
      "speaker: Deb\n",
      "speech: I know that this question is, as we say sometimes in academia, problematic. I'm aware of the problematics of what I'm about to ask, but I want to ask it anyway. When we're talking about AI and generative AI in particular, there's a whole range of problems, concerns, issues and dangers cited from the automation of labor leading to job loss and questions around compensation for labor, copyright questions about creativity, misinformation in the circulation of false information using generative AI, to things like the changes to the structure of how we get and source information online. Environmental impact is one that we maybe sometimes hear about on the sidelines. It's not typically included in some of, at least to my knowledge, some of these more mainstream issues. And the problematics of this question, I am aware, is in trying to rank or propose a hierarchy of problems, but where would this rank in the hierarchy of problems caused by AI?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. The speaker acknowledges the problematic nature of the question about AI and generative AI.\n",
      "2. The speaker lists a range of problems, concerns, issues, and dangers related to AI, including job loss, compensation for labor, copyright questions, misinformation, changes to the structure of information sourcing, and environmental impact.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I know that this question is problematic, but I want to ask it anyway. When we're talking about AI and generative AI, there's a whole range of problems, concerns, issues, and dangers cited. These include the automation of labor leading to job loss, questions around compensation for labor, copyright questions about creativity, misinformation using generative AI, changes to the structure of how we get and source information online, and even environmental impact. I am aware of the problematics of trying to rank or propose a hierarchy of problems, but where would environmental impact rank in the hierarchy of problems caused by AI?\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. The speaker acknowledges the problematic nature of the question about AI and generative AI.\\n2. The speaker lists a range of problems, concerns, issues, and dangers related to AI, including job loss, compensation for labor, copyright questions, misinformation, changes to the structure of information sourcing, and environmental impact.', 'combined_speech': \"I know that this question is problematic, but I want to ask it anyway. When we're talking about AI and generative AI, there's a whole range of problems, concerns, issues, and dangers cited. These include the automation of labor leading to job loss, questions around compensation for labor, copyright questions about creativity, misinformation using generative AI, changes to the structure of how we get and source information online, and even environmental impact. I am aware of the problematics of trying to rank or propose a hierarchy of problems, but where would environmental impact rank in the hierarchy of problems caused by AI?\"}\n",
      "speaker: Tamara\n",
      "speech: So I think that the environmental impacts of AI are very much connected to a lot of the labor problems. So I think part of what focusing on the environmental impact of AI can do for researchers and for advocates for people who are trying to create policy is that it really forces you to have a much more holistic view of what the potential harms of AI are, because, you know, if we're talking about the entire AI supply chain, we really are talking about many different parts of the world. We're talking about many different kinds of work, many different kinds of labor exploitation that can be behind this. And often we're also looking at the trade off. Right? So between, you know, you could say, oh, a data center in this area is going to bring jobs. Well, and sometimes that isn't even true. Right? Like that might be the sort of line that communities are given about bringing a large scale data center to the town. But then what are the sort of downstream effects? And what does it mean to be living next to something that is taking a bunch of power and taking a bunch of water? Or what does it mean to live next to something that is actually quite noisy? So I think a lot of people maybe don't realize the extent to which data centers are super loud and can be really distracting for the people who are living around them, and then wondering also about the other sort of effects on health for people in the area, thinking about all of the sort of pollutants and other issues that are associated with it. And so I think when you're talking about sort of justice concerns, when you're talking about how particular marginalized communities are going to be impacted, the question of labor and ethics comes up in tandem with the discussion of the environmental impact. So I think if we really begin to think about entire communities, if we're thinking about effects on habitats and ecosystems and on other species as well as on humans directly, it becomes a way to really consider what the sort of full spectrum of impacts are going to be from AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. Environmental impacts of AI are connected to labor problems.\n",
      "2. Focusing on the environmental impact of AI forces a more holistic view of potential harms.\n",
      "3. The entire AI supply chain involves different parts of the world and various forms of labor exploitation.\n",
      "4. Trade-offs need to be considered, such as the impact of data centers on communities.\n",
      "5. Data centers can have negative effects on health and the environment.\n",
      "6. Justice concerns and impacts on marginalized communities are linked to labor and ethics in AI.\n",
      "7. Considering the effects on habitats, ecosystems, and other species is important in understanding the full spectrum of AI impacts.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that the environmental impacts of AI are closely tied to labor problems. Focusing on the environmental impact of AI forces us to take a more holistic view of the potential harms it can cause. The entire AI supply chain involves different parts of the world and various forms of labor exploitation, which means we need to consider the trade-offs, such as the impact of data centers on communities. Data centers can have negative effects on health and the environment, and we must also consider the justice concerns and impacts on marginalized communities in relation to labor and ethics in AI. It's important to consider the effects on habitats, ecosystems, and other species to truly understand the full spectrum of AI impacts.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. Environmental impacts of AI are connected to labor problems.\\n2. Focusing on the environmental impact of AI forces a more holistic view of potential harms.\\n3. The entire AI supply chain involves different parts of the world and various forms of labor exploitation.\\n4. Trade-offs need to be considered, such as the impact of data centers on communities.\\n5. Data centers can have negative effects on health and the environment.\\n6. Justice concerns and impacts on marginalized communities are linked to labor and ethics in AI.\\n7. Considering the effects on habitats, ecosystems, and other species is important in understanding the full spectrum of AI impacts.', 'combined_speech': \"I believe that the environmental impacts of AI are closely tied to labor problems. Focusing on the environmental impact of AI forces us to take a more holistic view of the potential harms it can cause. The entire AI supply chain involves different parts of the world and various forms of labor exploitation, which means we need to consider the trade-offs, such as the impact of data centers on communities. Data centers can have negative effects on health and the environment, and we must also consider the justice concerns and impacts on marginalized communities in relation to labor and ethics in AI. It's important to consider the effects on habitats, ecosystems, and other species to truly understand the full spectrum of AI impacts.\"}\n",
      "speaker: Deb\n",
      "speech: So what is that full spectrum of impact? The broader question here is how do you study AI's environmental impact? How do you measure it? What issues or areas comprehensively do you look at when you assess an environmental impact?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Understanding the full spectrum of impact of AI on the environment\n",
      "- Studying and measuring AI's environmental impact\n",
      "- Comprehensive assessment of environmental impact\n",
      "\n",
      "COMBINED_SPEECH\n",
      "So, when we talk about the impact of AI on the environment, we need to understand the full spectrum of its impact. This raises the broader question of how we study and measure AI's environmental impact. It's important to comprehensively look at all the issues and areas when assessing the environmental impact of AI.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- Understanding the full spectrum of impact of AI on the environment\\n- Studying and measuring AI's environmental impact\\n- Comprehensive assessment of environmental impact\", 'combined_speech': \"So what is that full spectrum of impact? The broader question here is how do you study AI's environmental impact? How do you measure it? What issues or areas comprehensively do you look at when you assess an environmental impact?\"}\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. So, I mean, I think a lot of researchers have spent time focusing on measuring the carbon attached to training and maybe sometimes deployment. Some researchers have really pushed for more of a lifecycle analysis. So they're really looking at the carbon cost of AI, not just in the early training stages and deployment, but maybe even looking at the potential of e waste. So the fact that there's a tremendous amount of hardware that is also used in creating AI and deploying AI that also tends to be built not to last very long. And so there's a large amount of e-waste that also can be quite hazardous. And so one of the ways of measuring impact is at that very highly technical level when you're measuring the effects of, say, the training and deployment itself, but then you're also measuring social impacts. And to do that, you really do need to talk to communities who are being impacted. And so I think if we could find a way to kind of better or at least more effectively bridge the relationship between these sort of technical assessments of exactly how much carbon am I emitting when I'm training this model versus what are the downstream effects that this particular system might have on communities in x location five years down the road? I think it's incredibly difficult to measure, but you have to find a way of putting these ways of framing the problem together, because what I've seen from working in the tech industry, so I worked on a sustainability team at Intel. I was on what, what was called the green software team. And so a lot of that had to do with creating software that would help developers try to optimize the technology that they were building and using. And so if you're working in a lab, you're a machine learning specialist, you're training a large model, maybe you choose to do that at a time of day when there's more renewable energy available on the grid. And so a lot of the focus of our research was about kind of making these choices more visible to developers and having them feel kind of empowered to know, if I choose to do this at this time of day, it's a better choice for the environment. Right. This is just a completely different way of framing the problem than you're going to get from talking to grassroots community groups who are calling for environmental justice. They're going to have a very different way of setting the terms of how to measure and report the environmental impacts of the technology. That's where the kind of work that we're trying to do at Aim lab comes in, because we're very much trying to figure out how to really reconcile that more technical approach where the solution is sort of found through the technology itself and a tweak to the technology, versus a more socio technical perspective, which kind of understands the technical side, but also tries to look at all of the social factors that are around it.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. Researchers have focused on measuring the carbon cost of AI, including e-waste from hardware used in creating and deploying AI.\n",
      "2. Measuring impact requires considering both technical assessments and social impacts, including talking to impacted communities.\n",
      "3. The challenge is to bridge the gap between technical assessments and the downstream effects on communities.\n",
      "4. The focus of research at Intel was on creating software to help developers optimize technology for environmental impact.\n",
      "5. Aim lab is working to reconcile the technical and socio-technical perspectives on measuring and reporting the environmental impacts of technology.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, it's a great question. I think a lot of researchers have spent time focusing on measuring the carbon attached to training and deployment of AI. Some have pushed for a lifecycle analysis, looking at the potential e-waste from the hardware used in creating AI. Measuring impact requires considering both technical assessments and social impacts, including talking to impacted communities. It's incredibly difficult to measure, but we need to bridge the gap between technical assessments and the downstream effects on communities. At Intel, the focus was on creating software to help developers optimize technology for environmental impact. Aim lab is working to reconcile the technical and socio-technical perspectives on measuring and reporting the environmental impacts of technology.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. Researchers have focused on measuring the carbon cost of AI, including e-waste from hardware used in creating and deploying AI.\\n2. Measuring impact requires considering both technical assessments and social impacts, including talking to impacted communities.\\n3. The challenge is to bridge the gap between technical assessments and the downstream effects on communities.\\n4. The focus of research at Intel was on creating software to help developers optimize technology for environmental impact.\\n5. Aim lab is working to reconcile the technical and socio-technical perspectives on measuring and reporting the environmental impacts of technology.', 'combined_speech': \"Yeah, it's a great question. I think a lot of researchers have spent time focusing on measuring the carbon attached to training and deployment of AI. Some have pushed for a lifecycle analysis, looking at the potential e-waste from the hardware used in creating AI. Measuring impact requires considering both technical assessments and social impacts, including talking to impacted communities. It's incredibly difficult to measure, but we need to bridge the gap between technical assessments and the downstream effects on communities. At Intel, the focus was on creating software to help developers optimize technology for environmental impact. Aim lab is working to reconcile the technical and socio-technical perspectives on measuring and reporting the environmental impacts of technology.\"}\n",
      "speaker: Deb\n",
      "speech: I want to take a little bit of advantage of having somebody on the show who has worked across so many different sectors. You're at data and society now, which I would broadly consider a kind of public interest technology or civilly minded research center. You've been a academic, you are academically trained and have worked in the ivory tower for a while. You've worked in industry as well. You've worked on the sustainability project that you just mentioned inside of a kind of industry located organization. As far as I see it, there are so many fractures in terms of the ways that these different kinds of communities are thinking about or approaching the question of the benefits of AI versus the harms or mitigating harms, while maximizing benefits or thinking critically about the problems that AI may introduce. What do these different sectors maybe not know? Or what are the gaps in knowledge between these different sectors that maybe cause some of these tensions or fissures,or maybe talking across purposes with one another or talking over each other? How do you see the kinds of silos in knowledge, and maybe some of the controversial or combative conversations that happen across these different industries, given your knowledge and experience in these very different areas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Working across different sectors including academia, industry, and public interest technology\n",
      "- Identifying fractures in the approach to AI benefits and harms\n",
      "- Recognizing gaps in knowledge between different sectors\n",
      "- Addressing silos in knowledge and controversial conversations across industries\n",
      "\n",
      "COMBINED_SPEECH\n",
      "As someone who has worked across academia, industry, and public interest technology, I have noticed significant fractures in the way different sectors approach the benefits and harms of AI. There are gaps in knowledge between these sectors, leading to silos in knowledge and controversial conversations. It is important to address these tensions and work towards a more cohesive approach to AI across industries.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Working across different sectors including academia, industry, and public interest technology\\n- Identifying fractures in the approach to AI benefits and harms\\n- Recognizing gaps in knowledge between different sectors\\n- Addressing silos in knowledge and controversial conversations across industries', 'combined_speech': 'As someone who has worked across academia, industry, and public interest technology, I have noticed significant fractures in the way different sectors approach the benefits and harms of AI. There are gaps in knowledge between these sectors, leading to silos in knowledge and controversial conversations. It is important to address these tensions and work towards a more cohesive approach to AI across industries.'}\n",
      "speaker: Tamara\n",
      "speech: Yeah, no, it's a great question because I think, you know, the important thing, especially for doing this kind of research, is to kind of have a very broad understanding of the problem. You really want to be able to look at it from a lot of different perspectives. And what I have found sort of working across sectors, is that often people just sort of define the problem in a different way, and the questions that they're asking are going to be quite different. Another point of contention is that people often are working on very different timelines. So for academic researchers, we often do have the luxury of time. We may feel pressed for time, but we're working on a very different temporal scale than the quarterly financial system of tech. We're not trying to rush to ship a product. We're not worried about selling anything, at least in terms of a product. Maybe we're selling ourselves to a degree. But I think part of the problem is that the expectations of how long a research project should take and what a useful or effective research project is, it's just very different. So conducting socio technical research within a tech company, it's all about what insights you're giving people and demonstrating return on investment for the people who are funding the research, and also demonstrating the impact that you're having in a very real way. So are you directly influencing the design of a product? Are you helping the companies sell more stuff? And often, if you are not doing those things, then the research is not seen as being very valuable. And so trying to carve out space to actually do longer term research can be quite hard in tech itself. But what I think is really interesting is, and this is sort of the kind of work that we're trying to do at AIM Lab is we, you know, as sort of socio technical researchers who are at an independent nonprofit, but who are trained academics, we can kind of lend our expertise to people in the tech industry who are working on a problem and who maybe want to have other perspectives incorporated into the work that they're doing. And so I think some of the most interesting kinds of collaborations that you can have are when you really are sort of sharing perspectives with people who would ordinarily be addressing the problem of, say, of climate impacts, of AI through maybe decarbonization alone. And that's sort of the main way that they've thought about it. But if you can help them to think through what the user experience of actually using the thing that they're building is, and then also get them to kind of engage with the idea of downstream impacts to other communities after the fact, it's just a way of changing the perspective and reframing the problem. And I feel like that is something that can also really be helpful when you're trying to come up with policy, because, you know, obviously there's always going to be a gap between policy recommendations and how that is actually implemented. And so for me, I find it really helpful, having been in tech, in knowing exactly how power operates, how hierarchies work, and what the expectations are, and knowing how to talk the language, and so you can have a sense of how policy will actually be taken up within an organization. And so that's also something that we're really trying to focus on at AIM lab is understanding the methodologies that people will need to have to carry out the work of algorithmic impact assessment in a variety of different organizations. It's say a startup versus a large scale enterprise, or in a city government, or in other contexts that maybe don't get as much attention as the major tech companies like Google or something.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. It is important to have a broad understanding of the problem and look at it from different perspectives.\n",
      "2. People define the problem differently and work on different timelines.\n",
      "3. Conducting socio technical research in a tech company requires demonstrating return on investment and real impact.\n",
      "4. Collaboration between socio technical researchers and tech industry can provide valuable perspectives and reframing of problems.\n",
      "5. Understanding power dynamics and hierarchies in tech is helpful in policy implementation.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that having a broad understanding of a problem and looking at it from different perspectives is crucial, especially in research. People define problems differently and work on different timelines, which can create challenges. In tech companies, demonstrating return on investment and real impact is essential for the value of research. Collaboration between socio technical researchers and the tech industry can provide valuable perspectives and reframing of problems. Understanding power dynamics and hierarchies in tech is also helpful in policy implementation. This is the kind of work we are trying to do at AIM Lab, where we aim to lend our expertise to the tech industry and provide different perspectives on problems.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. It is important to have a broad understanding of the problem and look at it from different perspectives.\\n2. People define the problem differently and work on different timelines.\\n3. Conducting socio technical research in a tech company requires demonstrating return on investment and real impact.\\n4. Collaboration between socio technical researchers and tech industry can provide valuable perspectives and reframing of problems.\\n5. Understanding power dynamics and hierarchies in tech is helpful in policy implementation.', 'combined_speech': 'I believe that having a broad understanding of a problem and looking at it from different perspectives is crucial, especially in research. People define problems differently and work on different timelines, which can create challenges. In tech companies, demonstrating return on investment and real impact is essential for the value of research. Collaboration between socio technical researchers and the tech industry can provide valuable perspectives and reframing of problems. Understanding power dynamics and hierarchies in tech is also helpful in policy implementation. This is the kind of work we are trying to do at AIM Lab, where we aim to lend our expertise to the tech industry and provide different perspectives on problems.'}\n",
      "speaker: Deb\n",
      "speech: Help me better understand the tensions around climate change, environmental harms and AI on the one hand, leaders in the tech industry often cite the ways that AI may be helping to solve or mitigating climate change and environmental harm by reducing inefficiencies in transportation, for example, finding ways to streamline the use of environmental resources and to reduce waste, or creating new technologies that either reduce dependencies on environmentally harmful products or counteract existing environmental and climate damage. On the other hand, we also hear news of the mass energy requirements of data centers that need to be powered and climate controlled, as you've just cited at vast environmental and energy expenses in order to run e-waste, which you've mentioned as well, and environmental damage caused by mining for the resources required to tech products. And I'll tack onto this, that when I talk to people in industry, particularly those who are developing and trying to sell products in AI, they talk about the fact that they're creating jobs, that they themselves have 300 jobs that they cannot fill, that ultimately AI will end up producing more jobs, and that the task is then on the government, typically, which as libertarians. They also don't want to fund. But we'll save that conversation for another time to provide retraining for these better and more thoughtful and larger number or quantity of jobs. And then we hear from people who are critical of this or those who are studying it or those who are losing their jobs, that this is a kind of labor crisis, that in fact, the kinds of trainings required in order to occupy these so called new jobs that AI industries are inventing or providing in surplus require particular set of skills that potentially many people may not have or that they may not want to have. And so I'm trying to think about this controversy both in terms of the environment, as you've already brought in the labor dimension of this. And then, you know, also think about, you know, I think the reasonable case that many of the jobs that are now gone are jobs that were not good for the environment, and also that we don't particularly miss. The job that comes up quite a bit in this is, for example, the driver of the horse and buggy. People say, well, it's a kind of outdated technology and the horses were causing a lot of pollution and a lot of city sanitary problems, and we're better off even if the horse and buggy drivers are no longer able to exist in that occupation without that particular form of occupation to begin with. So how do you think about assessing the benefits against the harms? And how do you go about trying to provide that as a impact assessment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. AI is seen as a solution to climate change and environmental harm by reducing inefficiencies and creating new technologies.\n",
      "2. However, there are concerns about the energy requirements of data centers, e-waste, and environmental damage caused by mining for tech resources.\n",
      "3. There is a debate about the job creation and retraining efforts in the AI industry.\n",
      "4. The controversy is viewed from both environmental and labor perspectives.\n",
      "5. The assessment of benefits against harms and impact assessment is questioned.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am grappling with the tensions surrounding climate change, environmental harms, and AI. On one hand, leaders in the tech industry tout AI as a solution to mitigate climate change and environmental harm by reducing inefficiencies and creating new technologies. However, there are valid concerns about the energy requirements of data centers, e-waste, and environmental damage caused by mining for tech resources. Additionally, there is a debate about job creation and retraining efforts in the AI industry, viewed from both environmental and labor perspectives. The controversy raises questions about assessing the benefits against the harms and the need for impact assessment.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. AI is seen as a solution to climate change and environmental harm by reducing inefficiencies and creating new technologies.\\n2. However, there are concerns about the energy requirements of data centers, e-waste, and environmental damage caused by mining for tech resources.\\n3. There is a debate about the job creation and retraining efforts in the AI industry.\\n4. The controversy is viewed from both environmental and labor perspectives.\\n5. The assessment of benefits against harms and impact assessment is questioned.', 'combined_speech': 'I am grappling with the tensions surrounding climate change, environmental harms, and AI. On one hand, leaders in the tech industry tout AI as a solution to mitigate climate change and environmental harm by reducing inefficiencies and creating new technologies. However, there are valid concerns about the energy requirements of data centers, e-waste, and environmental damage caused by mining for tech resources. Additionally, there is a debate about job creation and retraining efforts in the AI industry, viewed from both environmental and labor perspectives. The controversy raises questions about assessing the benefits against the harms and the need for impact assessment.'}\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. So I think if we take a step back and with a lot of the technology that's being developed in the name of sort of being climate friendly or also maybe actually furthering goals of climate justice in some way. And so, okay, so thinking about maybe AI that could be used to help farmers, let's say, particularly in drought stricken areas. And so the AI that kind of acts as a sensor, a way of detecting things, maybe AI that could be used for a kind of deforestation mitigation or say, helping keep the coral reefs healthy and using that to detect what's going on with the coral reefs, I think there are many kind of imagined uses of AI that would really be a boon to environmentalist efforts. But the question is, and this comes up a lot with the experiences that we're having with AIM lab, is does the tech actually do what you think it's going to do? You know, does the tech actually behave in a way that the kind of people developing imagine that that will work on the ground? And how do the people who are expected to use it? How are they, how are they interfacing with it? Are they having a good time when they, when they interact with the AI? Is it actually helping them with the work that they're already trying to do? Or is it creating new forms of work for, for them, new forms of maintenance and just new sort of things that they didn't have to do in their role before? And so one of the questions with especially technology that is developed to kind of serve a particular environmental purpose, we hear a lot about sort of climate tech being the wave of the future. Maybe this is what will save us from climate change. Are these technologies being built through conversations with the people that the technology is actually expected to be helping? And so this is something that was sort of an ongoing problem within the tech industry, where user experience is really kind of an afterthought. Or maybe it's sort of tacked on towards the end of product development, where you really just sort of do some usability testing and make sure it kind of works well enough most of the time. But to actually try to develop AI with the real input of the communities who, in theory, are going to be using and benefiting from the technology is something that really isn't done most of the time. And so I would say with a lot of the sort of climate related technologies that are being put out on the market, the question is, what does it look like in practice? And this is something that I think AI doesn't seem to attract the same kind of ire that crypto did, because crypto was very much viewed as a waste of energy. Right? Like, people really hated crypto pretty early on, and there were a lot of people talking about the fact that it was a scam and that it was, you know, just wasting all this energy for no reason. Everybody hated NFTs. Not everybody, but a lot of people, right? It had a lot of detractors. So the question is around kind of people making claims about what blockchain could do in terms of, oh, hey, if we tokenize trees in the Amazon, that will incentivize people through financialization, essentially not to cut down all the trees. And so this idea that if you assign monetary value to things, to natural resources in the world, like trees, like whales, that essentially it will help people treat the planet better and that it would, in theory, also be helping indigenous groups or something. And this was very often shown not to be the case. It was actually just a different form of exploitation. It was a different kind of scam. It was often sort of just another form of colonialism, because these things were not developed with the consent or the priorities of the people that it was actually trying to help. And so my fear would be that with a lot of the sort of climate related AI products that maybe have good intentions, it doesn't necessarily mean that they're going to be executed in a way that is actually helpful.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Technology being developed for climate-friendly purposes\n",
      "- AI can be used to help farmers in drought-stricken areas\n",
      "- AI can be used for deforestation mitigation and coral reef health\n",
      "- Concerns about whether the technology actually works as intended\n",
      "- Lack of input from the communities who will benefit from the technology\n",
      "- Comparison to the negative reception of crypto and NFTs\n",
      "- Criticism of tokenizing natural resources as a form of exploitation\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, it's a great question. I think if we take a step back, a lot of technology is being developed in the name of being climate friendly or furthering goals of climate justice. AI could be used to help farmers in drought-stricken areas and for deforestation mitigation and coral reef health. But the question is, does the tech actually do what you think it's going to do? Are the people who are expected to use it having a good experience? Are these technologies being built through conversations with the people they are expected to help? User experience is often an afterthought in tech development. There are concerns about whether climate-related AI products, despite good intentions, will actually be executed in a helpful way. This is similar to the negative reception of crypto and NFTs, and the criticism of tokenizing natural resources as a form of exploitation.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Technology being developed for climate-friendly purposes\\n- AI can be used to help farmers in drought-stricken areas\\n- AI can be used for deforestation mitigation and coral reef health\\n- Concerns about whether the technology actually works as intended\\n- Lack of input from the communities who will benefit from the technology\\n- Comparison to the negative reception of crypto and NFTs\\n- Criticism of tokenizing natural resources as a form of exploitation', 'combined_speech': \"Yeah, it's a great question. I think if we take a step back, a lot of technology is being developed in the name of being climate friendly or furthering goals of climate justice. AI could be used to help farmers in drought-stricken areas and for deforestation mitigation and coral reef health. But the question is, does the tech actually do what you think it's going to do? Are the people who are expected to use it having a good experience? Are these technologies being built through conversations with the people they are expected to help? User experience is often an afterthought in tech development. There are concerns about whether climate-related AI products, despite good intentions, will actually be executed in a helpful way. This is similar to the negative reception of crypto and NFTs, and the criticism of tokenizing natural resources as a form of exploitation.\"}\n",
      "speaker: Deb\n",
      "speech: I want to talk a little bit about algorithmic impact assessments. Specifically, you have a piece on algorithmic impact assessments, and you wrote in that piece, and I'll quote you here, that \"we know when people and ecologies meet technical systems, there will always be unanticipated consequences\". So how do we identify and document a push back against these sometimes ambient harms?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS:\n",
      "1. Algorithmic impact assessments are important.\n",
      "2. Unanticipated consequences occur when people and ecologies meet technical systems.\n",
      "3. The need to identify and document push back against ambient harms.\n",
      "\n",
      "COMBINED_SPEECH:\n",
      "I believe that algorithmic impact assessments are crucial in today's technological world. As I have mentioned before, when people and ecologies intersect with technical systems, there are always unanticipated consequences. It is essential for us to be able to identify and document a push back against these sometimes ambient harms.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. Algorithmic impact assessments are important.\\n2. Unanticipated consequences occur when people and ecologies meet technical systems.\\n3. The need to identify and document push back against ambient harms.', 'combined_speech': \"I believe that algorithmic impact assessments are crucial in today's technological world. As I have mentioned before, when people and ecologies intersect with technical systems, there are always unanticipated consequences. It is essential for us to be able to identify and document a push back against these sometimes ambient harms.\"}\n",
      "speaker: Tamara\n",
      "speech: Well, it's interesting because I think a lot of it does have to do with getting input from the people who are going to be impacted earlier in the process. So there are, you know, in a lot of the cases that we've been involved with, in a lot of the different sort of research projects and collaborations that we have going on right now. And I can't speak, you know, too specifically on them at the moment, but I can say that often when you engage a community, let's say a particular community, in a location that may be particularly invested in knowing about how a technology is going to be used and might have some questions about the technology, it really is helpful to get their perspective, because often they will raise issues or create scenarios for, say, a chatbot that the people creating the technology, not just the technologists, but also, you know, perhaps, like, advocacy groups and others who are really trying to do the right thing, they may not see the big picture. There might just be things that they miss. And sometimes bringing in community members earlier in the process can really help you kind of see the potential problems before they become a really big problem. And so that, I think, you know, of course, there could always be unintended consequences. Even after you've really put in that kind of due diligence and you've been as thorough and ethical as you possibly can be in engaging every potential community that could be impacted. And there may still be things that you'll miss, but I think it's less likely that you would actually have a huge number of things that are as glaring if you actually put the time into fully assessing the technology before you deploy it and maybe even before it's fully designed. Another issue is being able to document potential harms and raise red flags around things like privacy or bias that may not have been obvious to the developer and then actually have time to give that input to the technical team. So another question is, are you kind of just bringing community members in, and you're just planning on sort of having a rubber stamp, but not really changing or modifying the tech in any way? I'm still releasing it. And so you really need to kind of encourage developers to build in time to really fully assess the potential harms and actually work on fixing those issues before the thing is released. There's also a question of, should the technology be developed and released at all, and how do you sort of begin to weigh the cost and the benefits? And that can be quite tricky, but in a lot of cases, it may not be that the technology is actually doing the thing you want it to do at all, and there may be social problems that are happening that really require a different kind of solution. That AI just actually has nothing to do with it. Is there a better way to actually allocate resources? Is another question. Should this thing be built at all? And I think that has to be an open question when you're sort of engaging in this process, rather than people just sort of digging their heels in and putting a product out there that is probably going to end up wreaking havoc in some way.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Getting input from impacted people earlier in the process is important\n",
      "- Engaging the community can help identify potential problems before deployment\n",
      "- Documenting potential harms and raising red flags around privacy and bias is crucial\n",
      "- Encouraging developers to fully assess potential harms and work on fixing issues before release\n",
      "- Questioning whether the technology should be developed and released at all\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that it is crucial to get input from the people who will be impacted by a technology early in the process. Engaging the community can help us identify potential problems before deployment. It is important to document potential harms and raise red flags around privacy and bias, and encourage developers to fully assess potential harms and work on fixing issues before release. We should also question whether the technology should be developed and released at all, and consider if there are better ways to allocate resources. It's important to keep an open mind and not just push a product out there that could end up causing havoc.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Getting input from impacted people earlier in the process is important\\n- Engaging the community can help identify potential problems before deployment\\n- Documenting potential harms and raising red flags around privacy and bias is crucial\\n- Encouraging developers to fully assess potential harms and work on fixing issues before release\\n- Questioning whether the technology should be developed and released at all', 'combined_speech': \"I believe that it is crucial to get input from the people who will be impacted by a technology early in the process. Engaging the community can help us identify potential problems before deployment. It is important to document potential harms and raise red flags around privacy and bias, and encourage developers to fully assess potential harms and work on fixing issues before release. We should also question whether the technology should be developed and released at all, and consider if there are better ways to allocate resources. It's important to keep an open mind and not just push a product out there that could end up causing havoc.\"}\n",
      "speaker: Deb\n",
      "speech: I want to push into this a little bit further because you talked a little bit about identifying and assessing the harms. Pushing back, I think, is a different kind of problem. When you discuss the possibility that AI product developed to do one thing or fix one kind of social problem actually isn't doing it, then, yes, you have demonstrated that the product maybe doesn't work as it is supposed to work, or maybe there's a non technical solution that might be better. But there's a kind of famous anecdote that I'd like to pull out to discuss the kind of problem with this. And both of us live in San Francisco, so both of us are aware of the problems around homelessness in San Francisco, one of which is the problem of the sanitariness of the city. And those of us who have traversed the streets of San Francisco recognize one particular sanitary problem, which is that homelessness has left a situation where oftentimes there are human feces on the streets. And an enterprising group of engineers in the Bay Area a couple of years ago came up with a solution to this problem, which was to develop AI poop, picking up robots that could detect human feces on the street and then deposit it into a central repository, thereby cleansing the streets of human feces. And when Dan Lyons, who was on the show many years ago, wrote for Silicon Valley, narrated the story. He said to me, you know, what was actually the better solution to the problem? Public restrooms. Public restrooms. Right. It's not a technical solution. But of course, I think what Dan's comment in many way misses is that the incentive is not just to solve the problem. The incentive for developers is oftentimes to recruit funding to create a product and then to sell the product and the company ultimately to get acquired or to get venture capital or to leverage that company into something larger. And so now we're talking about the kind of economics that undergird what gets developed and why, and the differences between creating a product that is marketable and sellable and solving the problem. Not always the same thing. Right? So I guess the question here is, what do does you pushing back actually look like? And how does one take the kind of understanding, documentation, identification and assessments you're talking about and mobilize it into pushing back?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Identifying and assessing the harms of AI products\n",
      "- The problem of AI products not working as intended\n",
      "- Example of AI poop picking up robots as a solution to homelessness in San Francisco\n",
      "- The incentive for developers to create marketable products rather than solving the problem\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I want to delve deeper into the issue of identifying and assessing the harms of AI products. It's not just about recognizing the problems, but also about pushing back when the products don't work as intended. Take the example of AI poop picking up robots developed to address the issue of homelessness in San Francisco. While it may seem like a technical solution, the better alternative, as pointed out by Dan Lyons, is public restrooms. This brings to light the underlying economics of product development - the incentive for developers is often to create marketable products rather than truly solving the problem at hand. So, the question is, how do we push back and mobilize our understanding and assessments to make a real difference?\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Identifying and assessing the harms of AI products\\n- The problem of AI products not working as intended\\n- Example of AI poop picking up robots as a solution to homelessness in San Francisco\\n- The incentive for developers to create marketable products rather than solving the problem', 'combined_speech': \"I want to delve deeper into the issue of identifying and assessing the harms of AI products. It's not just about recognizing the problems, but also about pushing back when the products don't work as intended. Take the example of AI poop picking up robots developed to address the issue of homelessness in San Francisco. While it may seem like a technical solution, the better alternative, as pointed out by Dan Lyons, is public restrooms. This brings to light the underlying economics of product development - the incentive for developers is often to create marketable products rather than truly solving the problem at hand. So, the question is, how do we push back and mobilize our understanding and assessments to make a real difference?\"}\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. And it's actually my favorite part of this kind of work, because I also do have a background as a labor organizer. And I think one of the most important aspects of this work is figuring out how to take any sort of empirical findings you have and then translate them into policy changes, into legal strategies and also bargaining strategies. And so how can labor groups, for example, figure out how to document the harms that are happening? How do you sort of prove that something like algorithmic wage discrimination is happening within a gig app? And then how do you then take that information and effectively lobby and advocate for changes to policy? How do you advocate for sort of some kind of legal recourse? And so knowing sort of what the sort of burden of proof would be right for any form of algorithmic discrimination. And so, you know, this is why, as my colleagues found when they were looking at sort of the, the New York City local law 144, which was requiring employers who were using automated employment decision tools to audit them in order to figure out if they were biased according to race or gender. And basically, a lot of these laws are not necessarily actually effective because it's actually quite hard to even demonstrate this sort of, this sort of bias. It actually is not always completely obvious, and it requires, in some cases, a real sort of technical knowledge or access that people don't have. And so one of the groups that we've been working with at AIM Lab is the Workers' Algorithm Observatory. And they're a group of researchers who are based at Princeton, but they're working with rideshare drivers who are trying to really conduct what they call a form of algorithmic inquiry. So from the marxist kind of workers inquiry, which is a way that workers kind of document data about themselves, and it's a way for workers to really use data collection to understand their working conditions and how to change them and to kind of compare notes about what is happening to them on the ground. And so for rideshare drivers to be able to sort of begin to share information with each other in a comprehensive way in order to really be able to get to the root of algorithmic wage discrimination so that it can be part of some kind of organizing strategy is incredibly useful. So another part of this is really trying to figure out what forms of research and what kinds of research questions are actually beneficial to the people that you're trying to advocate for.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. The speaker's favorite part of their work is translating empirical findings into policy changes, legal strategies, and bargaining strategies.\n",
      "2. The importance of documenting harms and proving algorithmic wage discrimination within gig apps.\n",
      "3. The challenges in demonstrating bias in automated employment decision tools and the limitations of existing laws.\n",
      "4. The work of the Workers' Algorithm Observatory in conducting algorithmic inquiry with rideshare drivers.\n",
      "5. The need to conduct research that is beneficial to the people being advocated for.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I'm often asked about the most important aspect of my work, and I have to say it's my favorite part - translating empirical findings into real changes in policy, legal strategies, and bargaining strategies. It's crucial to document harms and prove algorithmic wage discrimination within gig apps. However, demonstrating bias in automated employment decision tools is challenging, and existing laws have limitations in addressing this issue. That's why the Workers' Algorithm Observatory is conducting algorithmic inquiry with rideshare drivers to understand their working conditions and combat algorithmic wage discrimination. Ultimately, the research we do must be beneficial to the people we're advocating for.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"1. The speaker's favorite part of their work is translating empirical findings into policy changes, legal strategies, and bargaining strategies.\\n2. The importance of documenting harms and proving algorithmic wage discrimination within gig apps.\\n3. The challenges in demonstrating bias in automated employment decision tools and the limitations of existing laws.\\n4. The work of the Workers' Algorithm Observatory in conducting algorithmic inquiry with rideshare drivers.\\n5. The need to conduct research that is beneficial to the people being advocated for.\", 'combined_speech': \"I'm often asked about the most important aspect of my work, and I have to say it's my favorite part - translating empirical findings into real changes in policy, legal strategies, and bargaining strategies. It's crucial to document harms and prove algorithmic wage discrimination within gig apps. However, demonstrating bias in automated employment decision tools is challenging, and existing laws have limitations in addressing this issue. That's why the Workers' Algorithm Observatory is conducting algorithmic inquiry with rideshare drivers to understand their working conditions and combat algorithmic wage discrimination. Ultimately, the research we do must be beneficial to the people we're advocating for.\"}\n",
      "speaker: Deb\n",
      "speech: I guess I want to push a little bit more into this because I'm thinking about, you know, what you brought up, which is the kind of power differential between those who create AI and those who are subjected to it. What options do we have to push back on tech companies broadly, and AI companies specifically, companies that have oftentimes accrued both tremendous power and wealth, who have set norms and standards that maximize their ability to profit and also to limit their liability for the harms that they cause, and who are developing products with a mass reach in ways that have become so deeply institutionalized and part of our environment that we don't really have the ability to alter their effect or to extract ourselves from them or to say no, and who also have massive lobbying power and money to fight lawsuits, and whose wealth and profit often means that they can afford to pay fines that they may accrue from causing harm or violating a regulation, while continuing to pursue business practices that provide profit as normal, since the profit that they would get from creating the product and distributing it and deploying it oftentimes exceeds massively any penalty, financial or otherwise, that they might accrue from violations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. The power differential between those who create AI and those who are subjected to it.\n",
      "2. The options to push back on tech companies and AI companies.\n",
      "3. The norms and standards set by these companies to maximize profit and limit liability for harms caused.\n",
      "4. The mass reach and institutionalization of their products in our environment.\n",
      "5. The ability of these companies to fight lawsuits and pay fines without affecting their profit.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I want to delve deeper into the power differential between those who create AI and those who are subjected to it. We need to consider our options to push back on tech companies and AI companies, especially those who have accrued tremendous power and wealth. These companies have set norms and standards that maximize their ability to profit and limit their liability for the harms they cause. Their products have become deeply institutionalized in our environment, making it difficult for us to alter their effect or extract ourselves from them. Additionally, these companies have massive lobbying power and money to fight lawsuits, and their wealth and profit often allow them to pay fines without affecting their business practices. It's clear that the profit they gain from creating and distributing their products far exceeds any penalty they may accrue from violations.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '1. The power differential between those who create AI and those who are subjected to it.\\n2. The options to push back on tech companies and AI companies.\\n3. The norms and standards set by these companies to maximize profit and limit liability for harms caused.\\n4. The mass reach and institutionalization of their products in our environment.\\n5. The ability of these companies to fight lawsuits and pay fines without affecting their profit.', 'combined_speech': \"I want to delve deeper into the power differential between those who create AI and those who are subjected to it. We need to consider our options to push back on tech companies and AI companies, especially those who have accrued tremendous power and wealth. These companies have set norms and standards that maximize their ability to profit and limit their liability for the harms they cause. Their products have become deeply institutionalized in our environment, making it difficult for us to alter their effect or extract ourselves from them. Additionally, these companies have massive lobbying power and money to fight lawsuits, and their wealth and profit often allow them to pay fines without affecting their business practices. It's clear that the profit they gain from creating and distributing their products far exceeds any penalty they may accrue from violations.\"}\n",
      "speaker: Tamara\n",
      "speech: Right. And I think this really gets to the root of the problem, and I can kind of tie that back a little bit to the environmental concern of AI as well. And so the fact that you really just have a handful of companies that completely control the production and use of large scale AI right now is quite, quite frightening. And so, you know, in order to have access to the technology, in order to do the kind of work you want to do, you really need to be tied to just a handful of companies and very elite research universities. And I think this sort of larger problem of how much control over people's lives should tech companies have. I mean, this is something that I obviously also write about in my book, where I'm really sort of grappling with the fact that platforms have an outsized amount of control over how people are memorialized and over how they're able to mourn as more and more people sort of use various social media platforms and other digital assets in order to maintain relationships with the dead or to try to create a kind of legacy for themselves. And I actually don't know if a lot of the sort of regulation that we have on the immediate horizon is really going to be enough to change that power dynamic. I think we would really need to think about the production of technology. We would need to really think about the system that we live in and change it in a much more radical way. And am I suggesting that we would need to basically stop a model of endless growth where profits and the well being of shareholders is held up above the welfare of everyone else on the planet? Yes, I think we would need to kind of radically rethink and reformulate the ways that we think about what a successful sort of product or a successful company looks like. But I'm not sure if I have a whole lot of hope of that happening. And so I think until we have some sort of really massive structural change, the best that we can do is sort of exert pressure from different areas. And so I think it is actually important to have people working from within tech companies who are pushing for some kind of change. I think you do need people inside who understand how power works internally and who are, you know, at least close to the machine in that way. I also think that you need policy changes and pressure from grassroots organizations, advocacy and civil rights organizations, legal advocacy groups, and other sort of labor organizers. And I think you do need also academic researchers who are able to sort of lobby critiques of tech and sort of these systems a bit from an outside perspective. But you really need kind of all of these things to work together in tandem. And you certainly need the input from the communities who are going to be most impacted, which is often even within the more participation based imaginings of impact assessment or thinking about the power of technology, it still is a problem because the company or the people who are attempting to bring communities in, for instance, us at AIM Lab, we're still kind of setting the terms right, like it isn't really, you know, we're sort of helping perhaps give marginalized people a platform, but the power dynamics are still a little bit off. Right. And so I think really thinking about how we can kind of collectively have more power over how technology is produced and used. That is what ultimately we need to have happen.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The control of large scale AI by a handful of companies is frightening\n",
      "- Tech companies have an outsized amount of control over people's lives\n",
      "- Radical reformulation of the ways we think about successful products and companies is needed\n",
      "- Pressure for change should come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities\n",
      "- Collective power over how technology is produced and used is necessary\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that the control of large scale AI by just a few companies is quite frightening. These companies have an outsized amount of control over people's lives, and we need radical reformulation of what we consider successful products and companies. The pressure for change should come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities. Ultimately, we need to collectively have more power over how technology is produced and used.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- The control of large scale AI by a handful of companies is frightening\\n- Tech companies have an outsized amount of control over people's lives\\n- Radical reformulation of the ways we think about successful products and companies is needed\\n- Pressure for change should come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities\\n- Collective power over how technology is produced and used is necessary\", 'combined_speech': \"I believe that the control of large scale AI by just a few companies is quite frightening. These companies have an outsized amount of control over people's lives, and we need radical reformulation of what we consider successful products and companies. The pressure for change should come from within tech companies, policy changes, grassroots organizations, academic researchers, and impacted communities. Ultimately, we need to collectively have more power over how technology is produced and used.\"}\n",
      "speaker: Deb\n",
      "speech: I want to dig in a little bit into this idea of harm, because sometimes when we talk about harm, we talk about intentional harm. Sometimes we talk about harm as an unfortunate byproduct or an unintended byproduct of a certain technology's deployment. Can we, or should we make distinctions between different kinds of ways in which tech products cause harm? Between, for instance, a product that causes harm in unintended ways versus products that are released and cause harms in ways that are predictable but willfully ignored? Now, sometimes I have this conversation with somebody and their argument is that if we look more closely, almost always these so called unintended consequences are actually very predictable, but that either they are overlooked in favor of profit or progress, or that these, quote, unpredictable consequences were the result of negligence caused by overlooking or excluding already marginalized populations. Or alternatively, that the value system of tech production that already exists disregards the idea that the harm caused by the product is actually really harm to begin with. I'll give you an example of what I mean by the last part. The idea that something will take over jobs or something that will disconnect us from our immediate environment is not isn't actually a harm because it is providing efficiency. And in the context of this product's developers, efficiency is the value to maximize and maybe enjoying our environment or doing things in unpredictable ways because there isn't an algorithm guiding us about what exactly to do isn't actually a harm to begin with. Right? Some of us might think that the removal of the incidental or the coincidental from our environment, the capricious from our environment, might be a harm for those who are developing these technologies and algorithm that has a greater ability to predict and direct us in certain ways might be a benefit, and the loss of that would be be a harm at all. I do think that there are instances in which tech products cause harm as the result of bad actors using the product in unintended ways, or ways that a technology can go wrong despite the intentions of the creators. But I am also aware that the majority of harms that I think we're talking about are harms that come even as good users are using the product as it was intended to be used. How do you think about the idea of unintended consequences when it comes to what you see in tech culture?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Harm caused by tech products can be intentional or unintended.\n",
      "- There may be distinctions between different ways in which tech products cause harm.\n",
      "- Some argue that so-called unintended consequences are actually predictable but overlooked in favor of profit or progress.\n",
      "- The value system of tech production may disregard the harm caused by the product.\n",
      "- The idea of harm caused by tech products can vary depending on the perspective of the developers and users.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I want to delve into the concept of harm caused by tech products. It's important to consider that harm can be intentional or unintended. There may be distinctions between different ways in which tech products cause harm. Some argue that so-called unintended consequences are actually predictable but overlooked in favor of profit or progress. The value system of tech production may disregard the harm caused by the product. The idea of harm caused by tech products can vary depending on the perspective of the developers and users. It's crucial to think about the unintended consequences when it comes to what we see in tech culture.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Harm caused by tech products can be intentional or unintended.\\n- There may be distinctions between different ways in which tech products cause harm.\\n- Some argue that so-called unintended consequences are actually predictable but overlooked in favor of profit or progress.\\n- The value system of tech production may disregard the harm caused by the product.\\n- The idea of harm caused by tech products can vary depending on the perspective of the developers and users.', 'combined_speech': \"I want to delve into the concept of harm caused by tech products. It's important to consider that harm can be intentional or unintended. There may be distinctions between different ways in which tech products cause harm. Some argue that so-called unintended consequences are actually predictable but overlooked in favor of profit or progress. The value system of tech production may disregard the harm caused by the product. The idea of harm caused by tech products can vary depending on the perspective of the developers and users. It's crucial to think about the unintended consequences when it comes to what we see in tech culture.\"}\n",
      "speaker: Tamara\n",
      "speech: Yeah, that's a great question, and this is why I think we really need more historians to be in these spaces I'm always very happy to bring historians into the conversation whenever I can. And it's something that I actually did at Intel. So I brought in Mar Hicks to talk about the history of queer computing. I also brought in Cassidyre to talk about the history of trans computing and getting people in tech to think about how actually the thing that you're looking at and you think is brand new and shiny and full of innovation is actually connected to something that happened a long time ago that is similar. And we can kind of learn from the past to understand what these potential harms might be. And so something that came up a lot when I was at Intel, it was sort of the sort of middle of the metaverse fervor. And people were very excited about the metaverse and had a lot of ideas about what it might look like and what it might do for innovation and business and the future of work. And, you know, talking about some of the issues around sexual violence or women being harassed in sort of metaverse spaces, it was like, well, you know, if you kind of took a look at people who were users and people who are researchers of second life, or going back way further, you know, like, we could go back to, you know, Julian Dibble's a rape and cyber phase from 1993, talking about LambdaMOO. And so really beginning to frame sort of these problems about technology that look to be brand new and understand sort of the social dynamics and power structures that made them appear in different contexts. And so I think, for me, just being able to really use examples from the past and from other areas when you're talking about a new technology and trying to assess potential harms is actually quite useful because it really is, and sort of bigger than just a few bad actors or people who need to be removed from a platform. And it also is sort of a bigger problem than companies just creating something intentionally harmful to reap profits. But there's just obviously, we have a lot of bad things in our society. There's a lot of racism. There's still a lot of sexism, a lot of homophobia. There's just a lot of inequality. And to build things kind of without acknowledging that reality and without acknowledging these larger histories, I think, is when you really run into problems, because, you know, these things should be really obvious, it should not be hard to anticipate what some of the problems are going to be.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "1. Historians are needed in tech spaces to understand the historical context of new innovations.\n",
      "2. Bringing in historians can help in understanding potential harms of new technologies.\n",
      "3. The excitement around the metaverse should be tempered with an understanding of past issues like sexual violence and harassment in virtual spaces.\n",
      "4. It's important to acknowledge and address societal issues like racism, sexism, and homophobia when creating new technologies.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that we really need more historians to be in tech spaces. Bringing in historians can help us understand the historical context of new innovations and the potential harms they may bring. For example, when I was at Intel, I brought in historians to talk about the history of queer and trans computing, which shed light on the connections between past events and current technologies. The excitement around the metaverse should be tempered with an understanding of past issues like sexual violence and harassment in virtual spaces. It's important to acknowledge and address societal issues like racism, sexism, and homophobia when creating new technologies. We should not ignore these larger histories, as they can help us anticipate and address potential problems.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"1. Historians are needed in tech spaces to understand the historical context of new innovations.\\n2. Bringing in historians can help in understanding potential harms of new technologies.\\n3. The excitement around the metaverse should be tempered with an understanding of past issues like sexual violence and harassment in virtual spaces.\\n4. It's important to acknowledge and address societal issues like racism, sexism, and homophobia when creating new technologies.\", 'combined_speech': \"I believe that we really need more historians to be in tech spaces. Bringing in historians can help us understand the historical context of new innovations and the potential harms they may bring. For example, when I was at Intel, I brought in historians to talk about the history of queer and trans computing, which shed light on the connections between past events and current technologies. The excitement around the metaverse should be tempered with an understanding of past issues like sexual violence and harassment in virtual spaces. It's important to acknowledge and address societal issues like racism, sexism, and homophobia when creating new technologies. We should not ignore these larger histories, as they can help us anticipate and address potential problems.\"}\n",
      "speaker: Deb\n",
      "speech: Just a quick plug for folks who are listening to this episode to go check out Mar's episode of this show, we had a wonderful interview where she talks a lot about the importance of the present moment of technological production being informed by and thought through the narratives and the histories of the past. I want to ask you a question about a piece that you published recently in Wired. That piece is titled, using generative AI to resurrect the dead will create a burden for the living. The byline of that piece is AI technologies promise more chatbots and replicas of people who have passed of giving voice to the dead comes at a human cost. What are the specific concerns that you have about generative AI when it comes to environmental impact? And what are the human costs that you are talking about in that piece?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Importance of present moment of technological production being informed by narratives and histories of the past\n",
      "- Concerns about generative AI and its environmental impact\n",
      "- Human costs of using generative AI to resurrect the dead\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I just want to take a moment to encourage everyone listening to check out Mar's episode of this show. In our interview, she emphasized the significance of the present moment of technological production being shaped by the narratives and histories of the past. This is something that we should all consider as we move forward with AI technologies.\n",
      "\n",
      "Speaking of AI, I recently read a piece in Wired about using generative AI to bring back the voices of the deceased. It raised some important concerns about the environmental impact of this technology and the human costs associated with it. It's a topic that we should all be thinking about as we continue to develop and utilize AI in our society.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Importance of present moment of technological production being informed by narratives and histories of the past\\n- Concerns about generative AI and its environmental impact\\n- Human costs of using generative AI to resurrect the dead', 'combined_speech': \"I just want to take a moment to encourage everyone listening to check out Mar's episode of this show. In our interview, she emphasized the significance of the present moment of technological production being shaped by the narratives and histories of the past. This is something that we should all consider as we move forward with AI technologies.\\nSpeaking of AI, I recently read a piece in Wired about using generative AI to bring back the voices of the deceased. It raised some important concerns about the environmental impact of this technology and the human costs associated with it. It's a topic that we should all be thinking about as we continue to develop and utilize AI in our society.\"}\n",
      "speaker: Tamara\n",
      "speech: Yeah, so, you know, in terms of preserving the data of the dead, it kind of gets at this larger kind of problem that's been very pervasive in the tech industry for quite some time, which is, you know, data is power. So therefore, for individual users, we're going to help you. We're going to collect as much data as we can about you, and we're also going to help you maintain your personal data forever. And so Google kind of had that line of reasoning for much of its existence, but recently it actually said that it was going to start deactivating the accounts of people who had been inactive for two years because there's some degree of recognition on the part of tech companies that, hey, wait a minute, if we actually promise to maintain everybody's data in perpetuity, that's a hell of a lot of data, given how much data people are creating, and that actually costs a lot of money. There is no cloud. I feel like people in STS are always railing about the materiality of computing. It's like, no, it's the undersea cables. No, these things are material. They're not ethereal. But I think that just becomes even more obvious when you're talking about something like generative AI, which, again, requires even more compute power. And so the expansion of data centers to kind of feed the hunger and the desire for generative AI is a problem because it will have a massive environmental impact. But then the idea that you would then sort of maintain something like what? You're going to maintain this sort of extremely powerful AI for all of eternity because it's actually like a simulation of your dead loved one. And so I think the ethics of sort of imagining that you're going to be able to perpetuate this kind of relationship with the dead through this kind of technology is also very misleading because these things will actually require system upgrades and software updates and maintenance, and eventually they may no longer run and they may become obsolete and the technology itself may die. So thinking about the fact that many people would then undergo a second form of grieving after having experienced grief already. So there are all these sort of ethical questions around who should actually have the authority to decide to create a simulation of a dead person through generative AI, and who should be able to kind of maintain that relationship and control that relationship. And should companies or employers or estates be able to profit from the kind of simulated version of that dead person? And so that gets into really thorny questions around not just kind of copyright, but also things like estate planning and kinship relations and also consent. So thinking about what it is that an individual really would have wanted and what their family members actually want, versus maybe somebody else who decides to revive the dead person. And so I think the sort of general problem of creating AI versions of the dead, which is not a new problem, but just something that is sort of in the news a lot right now, particularly because of deepfake technology and because of a lot of dead celebrities and such that have been revived for various specials. And so I think this sort of question of violation, and this is something that came out when Anthony Bourdain, when there was a documentary made of his life and the director made the decision to use a deepfake voice, to have Bourdain narrate this letter that he had written. It was his words, but the deepfake was just sort of seamlessly put into the documentary, which is a bit uncanny, you know, to sort of revive the dead in that way. But it was also done without the consent of bourdain's family. And so, you know, this is the kind of question that comes up a lot when we're talking about celebrities, but it's an issue for everyone. And, you know, the fact that, you know, for actors who are having their bodies scanned or models, anyone, academics who have all of their Zoom recordings available, the idea that your employer or that somebody could kind of try to monetize your likeness after you die in order to prevent them from having to hire living people is actually a lot of what the sort of fantasy is behind this. And so just thinking about how this actually creates a lot of problems in the realm of labor on top of all of the other terrible things that I just mentioned.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The tech industry has been collecting and maintaining personal data of individual users.\n",
      "- Google recently announced deactivating accounts of inactive users due to the cost of maintaining data.\n",
      "- The expansion of data centers for generative AI will have a massive environmental impact.\n",
      "- Creating AI versions of the dead raises ethical questions about authority, control, and profit.\n",
      "- Deepfake technology and the use of deceased celebrities without consent are concerning.\n",
      "- The idea of monetizing a person's likeness after death creates labor-related problems.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, so, in terms of preserving the data of the dead, it kind of gets at this larger problem in the tech industry, where data is power. Individual users' data is collected and maintained, but Google recently announced deactivating accounts of inactive users due to the cost of maintaining data. The expansion of data centers for generative AI will have a massive environmental impact. Creating AI versions of the dead raises ethical questions about authority, control, and profit. Deepfake technology and the use of deceased celebrities without consent are concerning. The idea of monetizing a person's likeness after death creates labor-related problems. These are all thorny issues that need to be addressed.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': \"- The tech industry has been collecting and maintaining personal data of individual users.\\n- Google recently announced deactivating accounts of inactive users due to the cost of maintaining data.\\n- The expansion of data centers for generative AI will have a massive environmental impact.\\n- Creating AI versions of the dead raises ethical questions about authority, control, and profit.\\n- Deepfake technology and the use of deceased celebrities without consent are concerning.\\n- The idea of monetizing a person's likeness after death creates labor-related problems.\", 'combined_speech': \"Yeah, so, in terms of preserving the data of the dead, it kind of gets at this larger problem in the tech industry, where data is power. Individual users' data is collected and maintained, but Google recently announced deactivating accounts of inactive users due to the cost of maintaining data. The expansion of data centers for generative AI will have a massive environmental impact. Creating AI versions of the dead raises ethical questions about authority, control, and profit. Deepfake technology and the use of deceased celebrities without consent are concerning. The idea of monetizing a person's likeness after death creates labor-related problems. These are all thorny issues that need to be addressed.\"}\n",
      "speaker: Deb\n",
      "speech: Well, I want to dig into this a little bit and ask you to explicate a little bit more the links that you're making between labor and environmental damage and the broader context that you're talking about here of the attempts to preserve data, and in particular preserve the data of the dead at all costs, and in fact, at a very high cost. I know that you've written very extensively about the way that socio technical systems, particularly social media platforms, and increasingly AI and generative AI, may be changing the way that our culture thinks about and performs rituals around and navigates death. In your book, which you were just you were just talking about, Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond, takes up this inquiry. Can you help us better understand and maybe spend a little time digging into the link between the questions that you take up in the study and the current work that you're doing with the AI impact lab at data and society?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- The speaker wants to delve deeper into the links between labor, environmental damage, and the preservation of data, especially data related to the deceased.\n",
      "- The speaker has extensively written about how socio-technical systems, including social media platforms and AI, are changing cultural rituals and perceptions of death.\n",
      "- The speaker's book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond,\" explores these themes.\n",
      "- The speaker is also involved in work with the AI impact lab at data and society.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I am deeply interested in exploring the connections between labor, environmental harm, and the preservation of data, particularly data related to the deceased. My extensive writing has focused on how socio-technical systems, such as social media platforms and AI, are reshaping cultural rituals and attitudes towards death. My book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond,\" delves into these topics. Additionally, I am currently involved in work with the AI impact lab at data and society, furthering my exploration of these themes.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- The speaker wants to delve deeper into the links between labor, environmental damage, and the preservation of data, especially data related to the deceased.\\n- The speaker has extensively written about how socio-technical systems, including social media platforms and AI, are changing cultural rituals and perceptions of death.\\n- The speaker\\'s book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond,\" explores these themes.\\n- The speaker is also involved in work with the AI impact lab at data and society.', 'combined_speech': 'I am deeply interested in exploring the connections between labor, environmental harm, and the preservation of data, particularly data related to the deceased. My extensive writing has focused on how socio-technical systems, such as social media platforms and AI, are reshaping cultural rituals and attitudes towards death. My book, \"Death Glitch: How Techno-Solutionism Fails Us in This Life and Beyond,\" delves into these topics. Additionally, I am currently involved in work with the AI impact lab at data and society, furthering my exploration of these themes.'}\n",
      "speaker: Tamara\n",
      "speech: Yeah, definitely. So I really feel like the problem of death is sort of, it's a very old human problem, and it's a fairly universal problem. So despite what the transhumanists who tell us that we can upload our brains, no matter what they say, death is definitely coming for everyone. And people have always had different ways of memorializing the dead. And the way that people treat the dead can also certainly reveal different sort of social relations hierarchies within a society. And so, of course, if we go back to archaeology or something, we can look at the mortuary rituals around kings and queens versus commoners. There's always a way to sort of understand the way that a society is structured through treatment of the dead. And so I find it really fascinating to look at the different ways that death kind of disrupts the original plan plan for a lot of technologies. So technologies like social media, platforms that were built for youthful users who were at elite colleges primarily in the beginning, and the idea that they would somehow become spaces for memorialization, for long term relationships with the dead, for mourning. This is not something that was an immediate sort of use case for them. And so I think death is actually a very useful way to kind of begin to think about that was something really obvious in a lot of ways. I mean, you know, death is definitely, again, you know, it's a universal thing and it's kind of everywhere and will come for us all, but it still is something that's very easy to overlook. And so it's just, how could it be that every single company always forgets about death, and then they forget about it repeatedly over time? And this is not an area that, you know, tech companies largely have focused on. This is not, people don't have a lot of big UX teams devoted to death. This is not something that is sort of the norm, even though it is something that happens all the time. So I think when we're talking about the larger problem of ethics and technology, when we're talking about how communities are going to be impacted, how individuals and their social networks are going to be impacted, that kind of relational aspect, that's something that I think is really an important lens for considering impact assessments for algorithmic systems overall, because we're really not just talking about the tech itself and auditing it to understand is the tech doing what we think it's doing, but we're also trying to examine all of the social relations that are clustered around the technology and what all of the potential downstream impacts are. And so I think death is one of those things that really helps you see a lot of those relationships and networks over time in a way that maybe other social things are not quite as rich. I think that death is an incredibly rich field site for really thinking about algorithmic impacts.\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Death is a universal and old human problem\n",
      "- The treatment of the dead can reveal social hierarchies within a society\n",
      "- Technologies like social media were not originally built for memorialization or mourning\n",
      "- Tech companies often overlook the impact of death in their products\n",
      "- Considering the impact of technology on social relations and networks is important\n",
      "- Death is a rich field for thinking about algorithmic impacts\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, definitely. I really feel like the problem of death is a very old human problem and a fairly universal one. Despite what transhumanists say about uploading our brains, death is definitely coming for everyone. The treatment of the dead can reveal social hierarchies within a society, and technologies like social media were not originally built for memorialization or mourning. It's interesting how tech companies often overlook the impact of death in their products. Considering the impact of technology on social relations and networks is important, and death is a rich field for thinking about algorithmic impacts.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Death is a universal and old human problem\\n- The treatment of the dead can reveal social hierarchies within a society\\n- Technologies like social media were not originally built for memorialization or mourning\\n- Tech companies often overlook the impact of death in their products\\n- Considering the impact of technology on social relations and networks is important\\n- Death is a rich field for thinking about algorithmic impacts', 'combined_speech': \"Yeah, definitely. I really feel like the problem of death is a very old human problem and a fairly universal one. Despite what transhumanists say about uploading our brains, death is definitely coming for everyone. The treatment of the dead can reveal social hierarchies within a society, and technologies like social media were not originally built for memorialization or mourning. It's interesting how tech companies often overlook the impact of death in their products. Considering the impact of technology on social relations and networks is important, and death is a rich field for thinking about algorithmic impacts.\"}\n",
      "speaker: Deb\n",
      "speech: I want to talk about a word in your title, that word techno solutionism. How do you think about techno solutionism as an ideology and as a practice and as a vision for how we move forward in our society? As a society, what does techno solutionism and its cousin, techno utopianism, get right? What do these terms and the visions that they represent miss or misunderstand?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Techno solutionism and techno utopianism as ideologies and practices\n",
      "- Considering their impact on society\n",
      "- What they get right and what they miss or misunderstand\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I want to discuss the concept of techno solutionism and its cousin, techno utopianism. How do we view these ideologies and practices as we move forward in our society? What do they get right and where do they miss the mark? It's important to consider their impact on our society and what they truly represent.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Techno solutionism and techno utopianism as ideologies and practices\\n- Considering their impact on society\\n- What they get right and what they miss or misunderstand', 'combined_speech': \"I want to discuss the concept of techno solutionism and its cousin, techno utopianism. How do we view these ideologies and practices as we move forward in our society? What do they get right and where do they miss the mark? It's important to consider their impact on our society and what they truly represent.\"}\n",
      "speaker: Tamara\n",
      "speech: So I think there are aspects of techno solutionism that can be useful. So, for example, if you're kind of attempting to build the technology and you notice that there's a particular pain point or a point of failure, and you're able to make a small change in the technology itself, and then it runs much more efficiently. In that case, yes, there may very well be a solution to a problem that is really just a technical fix that will do a lot of good. But I think the larger problem is sort of this common issue of, as you mentioned before, where there's a problem that is very much a social problem that could be fixed by something that really has nothing to do with technology, and yet technology is kind of brought in anyway. But you also do have a problem where even the process that I described right, where, you know, with AIM lab, we're kind of helping different groups and different organizations understand the technology that they're building and help them kind of develop relationships with the potential communities that will be impacted by the technology. And so part of that process is really about maybe taking information back to the technical team and improving the technology and preventing bugs from occurring or also mitigating any potential impacts that would be harmful. But another part of the process that really has nothing to do with technology is the creation of relationships. The idea is that the partners that we're collaborating with, the hope is that they will continue to be in dialogue with the different communities that we're engaging with. We're not wanting it to be a kind of one off thing where, you know, okay, so we took information back to the technical team. We tweaked it a bit, and now we're done. The idea is that it should be something that continues, and it should be a long term relationship and process. And really, I would say that for a lot of the work that we're doing, that is the most important piece of it is really about the relationships with techno solutionism. I think the problem is that so often the focus on technology kind of outshines all of these other factors, and you can really miss the forest for the trees. And with techno utopianism, I mean, I think it is better to imagine other futures. So I think that we can learn from feminist and black Sci-Fi traditions. If we could think about afrofuturism, there are ways of imagining futures that are not in the vision of Elon Musk. There are alternatives to that kind of techno imaginary. And so it would be a mistake to completely throw away anything that's possible with the imagination. And I think that sort of reimagining technology, but then also building it differently, these things have to go together. So I think it's great to have the imaginary of how things could be different. And then I think we really need to kind of dismantle the way that tech is currently being built and deployed in order for that, some degree of that kind of imaginary to be. To be realized. And so for me, I am definitely interested in the magical and mystical and metaphysical qualities of technology. So I think that using technology to communicate with the dead is a thing that we've done as humans for a very long time. And you don't need to use a digital form of technology for that to happen. And so I would say that I am really interested in sort of the transcendent qualities of technology. And I believe that people derive a real sense of connection and something sacred out of technologies that are incredibly mundane. So I would not want to dismiss that. I think that there is a certain sort of magical quality to technology that can be a good thing, that can be not sort of the fetish of masking unequal labor relations, which it also is. But I'm interested in sort of digging into the other uses of technology that might kind of expand that.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Techno solutionism can be useful for making small changes in technology to solve specific problems efficiently.\n",
      "- The larger problem is using technology to fix social issues that may not require technological solutions.\n",
      "- Building relationships and maintaining dialogue with communities impacted by technology is crucial.\n",
      "- It is important to focus on relationships with techno solutionism rather than just the technology itself.\n",
      "- Imagining alternative futures and reimagining technology is essential.\n",
      "- Dismantling the current way technology is built and deployed is necessary for realizing alternative futures.\n",
      "- Interested in the magical and mystical qualities of technology and its transcendent effects on people.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "I believe that techno solutionism can be useful for making small changes in technology to solve specific problems efficiently. However, the larger problem lies in using technology to fix social issues that may not require technological solutions. Building relationships and maintaining dialogue with communities impacted by technology is crucial, and it is important to focus on these relationships with techno solutionism rather than just the technology itself. Imagining alternative futures and reimagining technology is essential, and dismantling the current way technology is built and deployed is necessary for realizing these alternative futures. Personally, I am interested in the magical and mystical qualities of technology and its transcendent effects on people. I believe that using technology to communicate with the dead is a practice that has been done for a very long time, and it doesn't necessarily require a digital form of technology. Overall, I think it's important to dig into the other uses of technology that might expand its magical qualities.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Techno solutionism can be useful for making small changes in technology to solve specific problems efficiently.\\n- The larger problem is using technology to fix social issues that may not require technological solutions.\\n- Building relationships and maintaining dialogue with communities impacted by technology is crucial.\\n- It is important to focus on relationships with techno solutionism rather than just the technology itself.\\n- Imagining alternative futures and reimagining technology is essential.\\n- Dismantling the current way technology is built and deployed is necessary for realizing alternative futures.\\n- Interested in the magical and mystical qualities of technology and its transcendent effects on people.', 'combined_speech': \"I believe that techno solutionism can be useful for making small changes in technology to solve specific problems efficiently. However, the larger problem lies in using technology to fix social issues that may not require technological solutions. Building relationships and maintaining dialogue with communities impacted by technology is crucial, and it is important to focus on these relationships with techno solutionism rather than just the technology itself. Imagining alternative futures and reimagining technology is essential, and dismantling the current way technology is built and deployed is necessary for realizing these alternative futures. Personally, I am interested in the magical and mystical qualities of technology and its transcendent effects on people. I believe that using technology to communicate with the dead is a practice that has been done for a very long time, and it doesn't necessarily require a digital form of technology. Overall, I think it's important to dig into the other uses of technology that might expand its magical qualities.\"}\n",
      "speaker: Deb\n",
      "speech: I think we have time for one last question. I teach a course on data and human values at UC Berkeley, and a lot of students there and elsewhere across college campuses and universities listen to this show. And I know that you come to data and society as we've talked about after many years as a professor working with students in the academy, many of them are going to end up in the tech industry. What would you want them to know or think about or understand or reconsider as they move into their careers?\n",
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Teaching a course on data and human values at UC Berkeley\n",
      "- Many students across college campuses and universities listen to the show\n",
      "- Transitioning from academia to data and society\n",
      "- Students moving into the tech industry\n",
      "- What they should know, think about, understand, or reconsider in their careers\n",
      "\n",
      "COMBINED_SPEECH\n",
      "As a professor teaching a course on data and human values at UC Berkeley, I am aware that many students across college campuses and universities listen to this show. Transitioning from academia to data and society, I understand the importance of preparing students who are going to end up in the tech industry. I want them to know, think about, understand, and reconsider the impact of their work on human values and society as they move into their careers.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Teaching a course on data and human values at UC Berkeley\\n- Many students across college campuses and universities listen to the show\\n- Transitioning from academia to data and society\\n- Students moving into the tech industry\\n- What they should know, think about, understand, or reconsider in their careers', 'combined_speech': 'As a professor teaching a course on data and human values at UC Berkeley, I am aware that many students across college campuses and universities listen to this show. Transitioning from academia to data and society, I understand the importance of preparing students who are going to end up in the tech industry. I want them to know, think about, understand, and reconsider the impact of their work on human values and society as they move into their careers.'}\n",
      "speaker: Tamara\n",
      "speech: Yeah, it's a great question. And I have to say, when I talk to younger generations of people who want to go into tech, there is much more of an awareness of the potential problems. And people actually seem quite hungry to have some degree of training in things like STS and critical technology studies in general. So I think the important thing is to read widely and sort of talk to people outside of tech. I would say the most important things are to understand the history of technology, understand the history of the tech industry. So look to organizers from the past. And that was something that I did with a number of my collaborators for the tech workers coalition teach in, where we basically brought people from, IBM, black Workers alliance from the 1970s, and a number of other activist groups that had been really prominent decades before, along with current activists who were trying to change things from within in the tech industry now. And it was really interesting to have people from different generations talking to each other. Or again, the reason why I brought Mar and Cass and to talk to colleagues at Intel, just a way of helping people realize that there have been activists in tech. There are people been doing this kind of work in coalition building with people outside of tech for a long time, so you don't have to reinvent the wheel. You can kind of learn from people who came before you. And then the other important thing is to continue talking to people who are not in the tech industry. So not just on a kind of, like, interpersonal level, but also if you are interested in making tech more ethical, instead of just sort of, you know, taking one ethics class in data science or computer science, really try to, you know, expand your relationship with different communities that you are around in your day to day life. So one example that I can point to is knowing some developers that really, you know, sort of large tech companies who take on volunteer positions, working with, say, middle schoolers in Oakland who want to learn how to design video games. And so volunteering your time or getting to know different kinds of communities and getting to know their concerns and being more sort of involved with issues where you live can also be really, really helpful. So then I think that especially goes for those of us who live in the Bay Area. So how do you sort of make sure that you are firmly situated in a place and that you're not just sort of an interloper who spends all of your time only talking to other tech people?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized speech:\n",
      "KEY_POINTS\n",
      "- Younger generations are more aware of potential problems in the tech industry and are eager for training in critical technology studies.\n",
      "- It is important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past.\n",
      "- Building coalitions with people outside of tech and learning from their experiences is valuable.\n",
      "- It is important to engage with different communities and their concerns, and to volunteer or get involved with issues where you live.\n",
      "\n",
      "COMBINED_SPEECH\n",
      "Yeah, it's a great question. When I talk to younger generations interested in tech, I see a strong awareness of potential problems and a hunger for training in critical technology studies. It's important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past. Building coalitions with people outside of tech and learning from their experiences is valuable. Engaging with different communities and their concerns, and volunteering or getting involved with local issues is also important. We can all learn from those who came before us and work together to make the tech industry more ethical and inclusive.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '- Younger generations are more aware of potential problems in the tech industry and are eager for training in critical technology studies.\\n- It is important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past.\\n- Building coalitions with people outside of tech and learning from their experiences is valuable.\\n- It is important to engage with different communities and their concerns, and to volunteer or get involved with issues where you live.', 'combined_speech': \"Yeah, it's a great question. When I talk to younger generations interested in tech, I see a strong awareness of potential problems and a hunger for training in critical technology studies. It's important to understand the history of technology and the tech industry, and to learn from activists and organizers from the past. Building coalitions with people outside of tech and learning from their experiences is valuable. Engaging with different communities and their concerns, and volunteering or getting involved with local issues is also important. We can all learn from those who came before us and work together to make the tech industry more ethical and inclusive.\"}\n",
      "speaker: Deb\n",
      "speech: Thank you so much, Tamara.\n",
      "summarized speech:\n",
      "Thank you so much, Tamara.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': 'Thank you so much, Tamara.'}\n",
      "speaker: Tamara\n",
      "speech: Thank you, Deb.\n",
      "summarized speech:\n",
      "Thank you, Deb.\n",
      "===============================================================\n",
      "--- keypoints_summaries:\n",
      "{'keypoints': '', 'combined_speech': 'Thank you, Deb.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summarized_podcast_conversations = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    transcript = podcast['transcript']\n",
    "    \n",
    "    for line in transcript:\n",
    "        speaker = line[0:line.find(\":\")].strip()\n",
    "        speech = line[line.find(\":\")+1:].strip()\n",
    "        print(f\"speaker: {speaker}\")\n",
    "        print(f\"speech: {speech}\")\n",
    "        \n",
    "        \n",
    "        summarized_speech = getKeyPointsFromSpeaker(speaker, speech)\n",
    "        \n",
    "        print(\"summarized speech:\")\n",
    "        print(summarized_speech)\n",
    "        print(\"===============================================================\")\n",
    "        \n",
    "        keypoints_summaries = {\n",
    "            'keypoints': '',\n",
    "            'combined_speech':''\n",
    "        }\n",
    "        \n",
    "        is_keypoints = False\n",
    "        is_combined = False\n",
    "        for line in summarized_speech.splitlines():\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "                \n",
    "            if 'KEY_POINTS' in line.strip():\n",
    "                is_keypoints = True\n",
    "                is_combined = False\n",
    "                continue\n",
    "            elif 'COMBINED_SPEECH' in line.strip():\n",
    "                is_combined = True\n",
    "                is_keypoints = False\n",
    "                continue\n",
    "                \n",
    "            if is_keypoints:\n",
    "                keypoints_summaries['keypoints'] = keypoints_summaries['keypoints'] +'\\n'+ line\n",
    "                keypoints_summaries['keypoints'] = keypoints_summaries['keypoints'].strip()\n",
    "            else:    \n",
    "                keypoints_summaries['combined_speech'] = keypoints_summaries['combined_speech'] +'\\n'+ line\n",
    "                keypoints_summaries['combined_speech'] = keypoints_summaries['combined_speech'].strip()\n",
    "             \n",
    "        # if the combined summary is longer than the original speech, not need to summarize.    \n",
    "        if len(keypoints_summaries['combined_speech']) > len(speech):\n",
    "            keypoints_summaries['combined_speech'] = speech     \n",
    "                \n",
    "        print(\"--- keypoints_summaries:\")\n",
    "        print(keypoints_summaries)\n",
    "        \n",
    "        summarized_podcast_conversations.append({\n",
    "            'speaker': speaker,\n",
    "            'keypoints_summaries': keypoints_summaries\n",
    "        })\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "with open(f'./summarized_dataset/podcast_summaries_openai_gpt35turbo_taking_the_temp_of_ai_{METHOD_NAME}_{VERSION}.txt', 'w') as f:\n",
    "        \n",
    "    for speaker_combined_summaries in summarized_podcast_conversations:\n",
    "        speaker = speaker_combined_summaries['speaker']\n",
    "        keypoints_summaries = speaker_combined_summaries['keypoints_summaries']\n",
    "        combined_speech = keypoints_summaries['combined_speech']\n",
    "        \n",
    "        f.write(f'{speaker}:\\n')\n",
    "        f.write(f'{combined_speech}')\n",
    "        f.write('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
