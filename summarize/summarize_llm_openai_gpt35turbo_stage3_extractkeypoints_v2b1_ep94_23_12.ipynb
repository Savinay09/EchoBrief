{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b1\" # no rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fea17f6d0d0>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "#   final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-25 22:53:01.129385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 22:57:47.624547\n",
      "chunks_text len: 66\n",
      "extract_keypoints start time: 2024-03-25 22:57:47.624688\n",
      "extract_keypoints done time 2024-03-25 23:00:15.339173\n",
      "Start time: 2024-03-25 23:00:15.339365\n",
      "Stage 1 done time 2024-03-25 23:02:51.205751\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Thomas Sanholm: Professor, AI Pioneer, and Game Theory Expert ', 'text': 'Thomas Sanholm is a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. He has published over 450 papers on game theory and machine learning, including a best paper in 2017 at NIPS, now renamed to Newrips. His research and companies have had a wide-reaching impact in the real world, proposing new ideas and building systems to prove that these ideas work. The conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast.'}, {'title': 'Heads Up No Limit Texas Holdem: A Benchmark for AI Algorithms ', 'text': 'Heads Up No Limit Texas Holdem is a main benchmark for testing AI algorithms for imperfect information game solving. It is a game played by humans, but not often seen on TV or in casinos. It is played in expert level casinos and in the World Series of Poker. It is mostly played online for big sums of money. It is a game usually only played by experts. It is different from regular No Limit Texas Holdem and is more competitive.'}, {'title': 'Introduction to Texas Holdem Poker ', 'text': \"Texas Holdem is typically played with a big group and is not as competitive. Heads Up means it's a game between two players, much like chess or go. Texas Holdem is an imperfect information game, making it harder to play. Each player has two private cards and there are gradually laid out public cards. The imperfect nature of the game comes from the private cards and the public cards. The game involves betting rounds after receiving private and public cards.\"}, {'title': 'Popular Imperfect Information Game with Four Betting Rounds ', 'text': 'The game being described involves four betting rounds and four tranches of information revelation. The first tranche is private, and the rest are public. This game is popular in AI and among the general public due to its imperfect information nature. It is considered one of the most popular spectator games to watch. The game is compared to chess in terms of popularity and as a benchmark for intelligence in AI. In 2017, a program called Labratus beat four expert human players in this game. The process and learnings from this event are of interest.'}, {'title': 'High Stakes Poker Tournament at Rivers Casino ', 'text': 'The event involved inviting four of the top 10 players in Heads Up No Limit, Texas Holdem. The game is different from the multiplayer version, so statistical significance was important. The players were brought to Pittsburgh to play at the Rivers Casino for 20 days. The goal was to get 120,000 hands in to achieve statistical significance. The players played from morning to evening for 20 days. A 200,000 incentive was raised for the players to participate. The players were paid based on how they did against the AI. This setup provided an incentive for the players to play as hard as possible.'}, {'title': 'Title ', 'text': \"Players' Incentive to Play Against AIText \"}, {'title': 'Title ', 'text': 'Playing Online Poker with AI through a User InterfaceText '}, {'title': 'AI Libratus Beats Humans in Competition ', 'text': 'The AI, Libratus, was able to achieve an incredible accomplishment by beating humans in a competition. The previous AI, Cloudyco, was unable to beat humans in a similar competition organized 18 months earlier. People tend to have more confidence in other people compared to the performance of AI, as evidenced by the betting odds favoring humans over AI in the competition. The performance of AI, Libratus, was underestimated by the international betting sites, as they put humans as a four to one or five to one underdog.'}, {'title': 'Underestimating AI in Poker ', 'text': 'People underestimated the performance of AI in poker. Despite winning against humans, AI was still considered an underdog in betting sites. There is a belief that human facial expressions and body language are critical to poker. People have confidence that humans will outperform AI because AI cannot perceive human tells. AI systems only look at betting patterns and statistics, not human tells. The importance of human perception in poker is questioned.'}, {'title': 'The Importance of Betting Patterns and Statistics in Human vs. Human Games ', 'text': 'Betting patterns and statistics are important in human versus human games. It is hard to believe that an AI could beat top human players until it is seen overperforming them. Top players are very good at hiding tells, making it not worth the effort to find tells in each other.'}, {'title': 'The Importance of Tells in Poker ', 'text': 'Tells are important in poker, especially at lower levels of play. At higher levels of poker, tells become less important as the game involves a larger number of strategies and possible actions. The amount of possible actions in poker is very large, making it difficult to solve directly. Abstraction is necessary in order to simplify the game and make it solvable. Abstraction in games is more challenging than in other types of games.'}, {'title': 'The Challenges of Abstraction in Games ', 'text': 'Abstraction in games is trickier than in MDPs or other single agent settings. Abstraction pathologies can lead to worse strategies with finer grained abstraction. There are different kinds of abstractions in games, such as hands abstractions and betting strategies. Information abstraction involves abstracting what chance does, such as the cards in the case of poker. Action abstraction involves abstracting the actions of the actual players, such as bets in the case of poker.'}, {'title': 'Automated Action Abstraction Technology in Gaming ', 'text': 'The algorithms used for information abstraction are potential aware and consider how the hand might materialize over time. The action abstraction is largely based on how humans and other AIs have played the game in the past. Automated action abstraction technology was used in the beginning, but it is not very scalable. The strength of the hand and how it is played are both important factors in the game.'}, {'title': 'Importance of Hand Strength and Information Abstraction in Poker ', 'text': 'The strength of the hand and the information abstraction are important in playing poker. The betting actions may be the key to winning regardless of the hands you have. Playing a lot of hands reduces the role of luck in the game. No Limit Texas Holdem has a high level of variance and massive swings. Statistical significance in the game requires playing over 100,000 hands.'}, {'title': 'The Use of Learning Methods in Poker Playing ', 'text': 'Annette Oberstad, a Norwegian female poker player, won a tournament by using a unique playing style. Labradus does not use deep learning methods, unlike DeepStack. The effectiveness of deep learning in poker playing is unclear. The discussion is about the use of learning methods to aid in the way Labradus plays poker.'}, {'title': 'The Importance of Learning Techniques in Games ', 'text': \"Labradus did not use learning methods and played very well without them. There are papers on things that do use learning techniques, including deep learning. In imperfect information games like poker, the value of an information set depends on both players' beliefs. The value of a state in poker is not just a function of the cards, but also depends on the path of play and both players' beliefs.\"}, {'title': 'Understanding State and Strategy in Games ', 'text': 'A state in games is not just a function of the cards, but also depends on the path of play and belief distributions. Perfect information games have a straightforward concept of state and evaluation function, but in other games, such as the one discussed, it is more complicated. In the discussed paper, the opponent is allowed to take different strategies at the leaf of the search tree, which is a different approach. Allowing the opponent to choose from a set of different continuation strategies forces a more realistic look ahead search. This approach does not assume a particular way that the opponent plays, making the search more sound.'}, {'title': 'Challenges in Look Ahead Search for Imperfect Information Games ', 'text': 'Look ahead search in imperfect information games is very difficult. Randomly generating various situations in the game. Using deep learning to learn values of states, including belief distributions. Similar techniques to alpha beta search or Monte Carlo tree search, but with different techniques.'}, {'title': 'Title ', 'text': 'Different Search Algorithms and Depth Limited Lookaheads in Game PlayingText '}, {'title': 'Understanding the Role of Beliefs in Game Theory ', 'text': 'Beliefs are actually output, not input. Starting beliefs are input, but they just fall from the rules of the game. The dealer deals uniformly from the deck, so every pair of cards is equally likely. Card removal is very important in the game. Dealing always comes from a single deck in Heads Up. Adjusting beliefs is where the beauty of game theory comes in.'}, {'title': 'Introduction to Nash Equilibrium in Game Theory ', 'text': \"Nash equilibrium is introduced in game theory by John Nash in 1950. It defines rational play when there is more than one player. Nash equilibrium involves pairs of strategies for each player, where neither player wants to deviate given the other doesn't deviate. It also defines beliefs for both players and for each state in the game. Nash equilibrium provides a probability distribution over real world states in the mind of a player at each information set in the game.\"}, {'title': \"Understanding Probability Distributions and Bayes' Theorem in Game Theory \", 'text': \"Probability distribution over real world states in the mind. Example of the game Rock, Paper, Scissors and the concept of information sets. Nash equilibrium strategy for player one in the game. Deriving beliefs on the information set using Bayes' theorem. The role of Bayes' theorem in game theory.\"}, {'title': 'Understanding Game Theory and Opponent Modeling ', 'text': 'Game theory is not player specific and does not require any data or history of how players have played in the past. It is based on rationality and thinking about what a rational opponent would do. Game theory is a data-free and opponent-free approach, coming from the design of the game rather than the design of the player. There is no opponent modeling in game theory, but there is potential for combining opponent modeling with game theory to exploit weak players. Exploiting an opponent can open oneself up to exploitation, and in the case of Librarus, the decision was made not to exploit the players because they were too good and had few holes to exploit.'}, {'title': 'Strategies and Exploitation in Two-Player Games ', 'text': \"Opening oneself up to exploitation by turning on certain strategies. The opponents are experts in counter exploitation. The decision not to turn on certain strategies. Interest in exploring papers exploiting opponents. Work done on exploiting opponents. Appreciation for the work at hybrid digested. Safety in two player zero sum games. The impact of opponent's irrational behavior on beliefs and gains.\"}, {'title': 'Game Theory Strategies and Examples ', 'text': \"The player can gain by throwing off the opponent's belief is always less than they lose by playing poorly. A game theoretic strategy is unbeatable, but it doesn't maximally beat the other opponent. The hybrid strategy involves starting from a game theoretic approach and then tweaking the strategy based on opponent data. Repeated games and the Prisoner's Dilemma are mentioned as examples.\"}, {'title': 'Understanding Repeated Games and Game Theory ', 'text': \"Prisoner's Dilemma is a repeated game. There is no proof that repeated games are the best strategy, but experimentally they do well. There are perfect information games and imperfect information games. Repeated games are games that are played over and over. There are zero sum games and non zero sum games. There is a distinction between two player games and games with more players.\"}, {'title': 'Types of Games in Game Theory ', 'text': '[\\'Extensive form games involve repetitive interactions and incomplete information sets.\\', \\'Repeated games are a special case of extensive form games.\\', \\'Sourcing auctions involve repetitive interactions with variations in the supply base and the items being bought.\\', \"Purely repeated games are rare in the world and are a coarse model of what\\'s going on.\", \\'Stochastic games are in between simple repeated matrix games and extensive form games, involving little matrix games and actions taken by opponents.\\']'}, {'title': 'Understanding Extensive Form Games ', 'text': 'Stochastic games involve actions that determine the distribution over next games. Extensive form games are more general than matrix games. Poker is an example of an extensive form game. The AI community has been working on extensive form games, benchmarked with Heads Up No Limit Texas Holdem. Extensive form games can be represented as a tree form, unlike matrix games which are represented as a matrix. There are different types of reasoning that can be applied in tree form games.'}, {'title': 'Importance of Tree Form and Two Player Games in Game Theory ', 'text': 'Tree form allows for certain types of reasoning that are lost in normal form. Equivalence exists between tree form and normal form, but sequentiality is lost in the transition. Multiplayer versus two player distinction is important in game theory. Two player games in zero sum are conceptually and computationally easier. In two player games, any equilibrium strategy is a best response to any other equilibrium strategy.'}, {'title': 'The Presence of Nash Equilibrium in Finite Games ', 'text': 'Nash equilibrium is present in all finite games, as proven by John Nash. The problem lies in the existence of multiple Nash equilibriums and the selection of which one to use. In non zero sum games, there may be a loss of joint benefit when different strategies are selected from different equilibriums.'}, {'title': 'Challenges and Strategies in Multiplayer Games ', 'text': 'In non zero sum games, joint benefit may be lost by being simply stupid, and both parties could be better off by doing something else. In three player games, collusion can occur, where two players gang up on a third player to achieve better outcomes. Collaboration or cooperation between poker players can make the game extremely difficult for current AI methods to solve. The ability of poker players to collaborate introduces new challenges and complexities to the game.'}, {'title': 'Advancements in Coalitional Game Theory ', 'text': 'The speaker has done a lot of work on coalitional games and has a paper on the topic. They recently presented their work at a poster session at NIPS. Collusion in games presents a different and typically harder problem. Some game representations do not allow for good computation. The speaker introduced a new game representation for dealing with collusion. There are still some unknowns in this area.'}, {'title': 'The Importance of Coordination in Strategic Games ', 'text': 'Coordination in games like bridge requires strategies to be coordinated ahead of time and signals to be understood by both teams. In many other situations like auctions, negotiations, and diplomatic relationships, coordination is not built into the rules but can still be helpful for colluders. Prior strategies and willingness to negotiate are important in negotiations and other applications beyond poker.'}, {'title': 'Transitioning from Poker to Business and Security Applications ', 'text': 'Moving away from poker and into other applications like negotiations. Has two startup companies - Strategic Machine and Strategy Robot. Strategic Machine is for business applications, gaming, sports, etc. Strategy Robot is for military security, cyber security, and intelligence applications. Also involved in another company called Optimized Markets, focused on combinatorial market and optimization based technology.'}, {'title': 'Challenges in Utilizing Game Theoretic Reasoning Technologies ', 'text': 'Game theoretic reasoning technologies are not being used. The ability to use game theoretic concepts to model human behavior is questioned. The goal may not be to model humans, especially in zero sum games. Formalizing the interaction in games is a prerequisite for analysis. Mechanism design has been used to design games with certain outcomes. An example from the world of autonomous vehicles is given, where pedestrians and cars negotiate in a nonverbal way.'}, {'title': 'Modeling Human Behavior in Pedestrian-Vehicle Interactions ', 'text': 'Pedestrians and cars engage in nonverbal communication, creating a tension and trust dynamic. Modeling human behavior in pedestrian-vehicle interactions is challenging, especially in terms of intent. Game theory and imperfect information approaches may be useful in modeling human behavior in pedestrian-vehicle interactions. Autonomous vehicles could potentially benefit from understanding and modeling pedestrian-vehicle interactions. Fleets of autonomous cars operated by different companies may present new challenges and opportunities in pedestrian-vehicle interactions.'}, {'title': 'Fleets of Autonomous Cars and Prenegotiated Merging Situations ', 'text': 'Fleets of autonomous cars operated by different companies. The need for prenegotiated situations for merging to increase speed and efficiency. The idea of using automated negotiation to handle many different situations in advance.'}, {'title': 'Artificial Intelligence and Game Theory ', 'text': 'The negotiation involves a combinatorial exchange of letting each other go first in different situations. The example of merging can be modeled as an imperfect information game. Games with perfect information are easier to deal with. Dealing with imperfect information requires adjusting to the real situation. The Annual Computer Poker Competition is an incredible accomplishment of AI. AI has had significant moments in history, such as Deep Blue and AlphaGo, where it stepped up in engineering efforts.'}, {'title': \"AI's Role in Performance-Oriented Research \", 'text': \"AI has stepped up in an engineering and scientific effort to beat human players. The speaker's group is involved in performance-oriented research spanning from idea to commercialization. Building big systems and evaluating them at scale is important in AI. Techniques that look good in small scale may not work in large scale.\"}, {'title': 'Importance of Testing Algorithm Performance in Real-world Applications ', 'text': 'Theory and practice can have different outcomes in terms of algorithm performance. Testing algorithms in reality is necessary to determine their effectiveness. Small-scale results may not accurately reflect performance in larger-scale applications. Personal experience with organizing a poker competition involving AI and human players.'}, {'title': 'The Impact of AI on Heads Up No Limit Poker ', 'text': 'This was the first organized competition for Heads Up No Limit poker. The speaker became the most hated person in the world of poker due to their involvement in AI cracking the game. Many people felt that AI becoming better than humans posed a real threat to the existence of the game. The speaker received aggressive comments and even death threats for their involvement in AI in poker. Despite AI outperforming humans in chess, humans still enjoy the game and find it beautiful.'}, {'title': 'The Impact of AIs on the Game of Poker ', 'text': 'The AIs have changed how the game of poker is played. The top humans are now incorporating the AIs\\' strategies into their own play. The AIs have made poker a richer and more interesting game for humans to play. The speaker has learned to love the game of poker through working with AIs. The speaker did not think of themselves as someone who would \"kill the game\" of poker. The speaker believes that the AIs have not steered humans away from the game entirely. The speaker finds it brave to put ideas to the test in academia.'}, {'title': 'Challenges of Scaling Good Ideas ', 'text': \"Good ideas don't always work at scale. It takes a lot of work and time to organize and make something big. It's important to do things in the real world and at scale. The proof is in the pudding when it comes to real-world application.\"}, {'title': 'Competition and Automated Mechanism Design in Politics ', 'text': 'Competition between different groups to beat the top humans at Heads Up No Limit, Texas Holdem. Mechanism design is about designing the rules of the game to achieve a certain desirable outcome. The topic of mechanism design is interesting and new to the speaker. The speaker is an observer of mechanisms, including politics. The paper discusses automated mechanism design. The goal is to design the political system in an automated fashion.'}, {'title': 'Challenges in Automated Mechanism Design ', 'text': \"Automated mechanism design direction is still believed in, but it's not a panacea. There are impossibility results in mechanism design, stating that certain objectives cannot be accomplished in a certain class. These are proofs, not statements about human ingenuity. It is impossible to achieve certain properties in mechanism design, regardless of the mechanism used.\"}, {'title': 'Automated Mechanism Design and Islands of Possibility in Trade ', 'text': 'Automated mechanism design allows for specific settings to be designed for, even if there are impossibility results for the whole class. It is possible to carve out islands of possibility within known impossible classes. The Meyerson Satethweight theorem by Roger Meyerson and Mark Satethweight from 1983 shows an impossibility of efficient trade under imperfect information, but it is possible to avoid this in many settings and still achieve efficient trade. The existence of these islands of possibility does not contradict the impossibility result.'}, {'title': 'Insights from Mechanism Design and its Applicability ', 'text': 'The impossibility result is still present, but there are spots within the impossible class where the impossibility does not exist. The lessons drawn from mechanism design can be applicable to politics, human interaction, designing rules for business, negotiations, and political systems. Mechanism design itself has had limited success so far in real-world situations.'}, {'title': 'Challenges of Applying Mechanism Design in Real World Situations ', 'text': 'Real world situations are often not sound from a mechanism design perspective. Insights from theory are applied into the real world rather than applying mechanisms directly. The FCC spectrum auctions are an example where bidding truthfully is not the best strategy. Mechanism design aims to make things easy for participants, but truth telling is not the best strategy in high stakes auctions. The rules designed in practice for the FCC spectrum auctions do not align with game theory principles.'}, {'title': 'Optimal Bidding Strategies and AI Milestones ', 'text': \"Truth telling is not the best strategy in spectrum auctions. There is no single optimal bidding strategy for spectrum auctions. Bidding truthfully wouldn't be the best strategy even with just two or one item for sale. AI history is marked by seminal events such as AlphaGo beating a world champion human Go player and Liberatus winning the Heads Up No Limit Holdem. Heads Up No Limit Texas Holdem was the one remaining widely game solving.\"}, {'title': 'Benchmark Games in Game Solving ', 'text': 'Heads Up No Limit Texas Holdem is widely agreed upon as a benchmark for game solving. There are other games being worked on, such as StarCraft, Dota 2, Diplomacy, and Hanabi. These games are not acknowledged as the main next challenge problem like chess or Go. The hope is for a new benchmark to drive application independent techniques forward.'}, {'title': 'Game Solving Technology Startups: Strategic Machine and Strategy Robot ', 'text': 'Interest in pushing game solving technology into practice. Game theory is in a different situation than machine learning. Machine learning is a mature technology with proven success in the real world. Game solving has almost no applications.'}, {'title': 'The Potential of Computational Game Theory in Military Planning and Business Strategy ', 'text': 'Machine learning has shown success in real-world applications, but not yet in game solving. The next big breakthrough could be the use of computational game theory in military planning and business strategy. Machine learning methods, such as neural networks, lack transparency and explainability, while game theoretic methods, like Nash equilibria, may offer more transparency and explainability.'}, {'title': 'The Properties of Nash Equilibria and Game Theoretic Strategies ', 'text': 'Nash equilibria and game theoretic strategies have provable properties. Unlike deep learning, game theoretic strategies have provable solution quality guarantees. The strategies may not necessarily be human understandable. Deep learning and computational game theory are in the same boat in terms of human understandability.'}, {'title': 'Understanding Deep Learning, Computational Game Theory, and the Future of AI ', 'text': 'Deep learning and computational game theory are both difficult to understand. Game theoretic techniques have guarantees of solution quality, but it is more of a belief than a substantiated fact. Exciting future if there are provable things in terms of optimality. Concerns about the existential threats of artificial intelligence and its negative impact on society.'}, {'title': 'Impact of Nationwide Kidney Exchange and Combinatorial Sourcing Auctions ', 'text': 'The nationwide kidney exchange has saved hundreds of lives and increased employment in the healthcare industry. Combinatorial sourcing auctions have increased supply chain efficiency by 12.6%, resulting in over $6 billion of efficiency improvement.'}, {'title': 'Efficiency Improvement and AI Safety ', 'text': 'Over $6 billion of efficiency improvement in the world. Efficiency improvement in trucking, leading to less waste and carbon footprint. AI is going to make the world much safer. Concern about existential threats of AI and value misalignment. Game theory has a role to play in addressing these concerns.'}, {'title': 'The Role of Game Theory in Addressing Value Misalignment ', 'text': 'Game theory has a role to play in ensuring that values are aligned with human beings. Value misalignment is a theoretical worry and has not been seen in real applications. An example of potential value misalignment was in the late eighties when building transportation optimization systems. The idea of high utilization of assets as an objective could lead to impractical solutions.'}, {'title': 'Challenges in Achieving 100% Utilization and the Impact of AI ', 'text': 'The solution of loading trucks full and driving in circles to achieve 100% utilization may not be practical in reality. AI can optimize the wrong objective to the detriment of the actual problem. There is a gap between theoretical worst-case scenarios and what actually happens in reality. The speaker grew up in the Soviet Union and mentions the existence of 10,000 nuclear weapons in the world.'}, {'title': 'The Threat of Nuclear War and Climate Change ', 'text': 'The speaker grew up in the Soviet Union. There are currently 10,000 nuclear weapons in the world. The speaker is surprised that nuclear war has not broken out. The two biggest threats facing mankind are climate change and nuclear war. The speaker has tried to do something about climate change through their startups. The speaker commissioned studies on what could be done for climate change. The speaker is still keeping an eye out for potential market solutions or optimizations for climate change.'}, {'title': 'Importance of Market, Optimization, and Technology Solutions in Addressing Environmental Issues ', 'text': 'Market solutions, optimization solutions, and technology solutions are important for addressing problems like pollution. Lack of political will can hinder the success of market solutions, even with good market design. The issue of global warming is more pressing than the issue of nuclear weapons. The speaker is extremely worried about the problem of nuclear weapons.'}, {'title': 'The Game Theory of Mutually Assured Destruction and the Role of AI ', 'text': 'The game theory of mutually assured destruction is based on the idea that nobody wants to initiate a conflict. The analysis of the game theory is coarse grained and situational. With the presence of smaller nuclear weapons and non-nation actors, the risk of conflict initiation is higher than before. The speaker is interested in the application of AI and is excited about the future possibilities in the field. The speaker has been involved in excellent pieces of work at NIPS, NeurIPS, and is thinking about the future with several companies.'}, {'title': 'Developing Scalable Techniques for Game Solving and Real World Applications ', 'text': 'The focus is on developing scalable techniques for game solving and applying them in the real world. Interest in market design and optimized markets. Priority is on strategic machine strategy robot and getting the technology out there. Understanding the technology gaps that still need to be filled through real applications. Enjoying the interaction and challenge of applying state of the art techniques in real applications.'}, {'title': 'Challenges and Benefits of Applying State-of-the-Art Techniques ', 'text': \"The process of applying state-of-the-art techniques to benefit various industries and the military is difficult. There is a significant benefit to using new technology, but there are challenges in integrating systems for data and computing. There is resistance to change and inertia in adopting new methods. Finding internal champions within the customer's organization is important for driving change. Failure to adapt to new ways of doing things could lead to negative consequences.\"}, {'title': 'The Development of Autonomous Vehicles ', 'text': '[\"Things can\\'t be the same way in the future. Otherwise bad things are going to happen.\", \\'Car makers and tech companies are both working on autonomous vehicles.\\', \\'Google and Baidu are pushing on autonomous cars.\\', \\'The speaker finds the development of autonomous cars fascinating.\\', \\'There are lots of different things in the game solving, including solving bigger games with more hidden player actions.\\', \\'Poker is mentioned as a game.\\']'}, {'title': 'Title ', 'text': 'Understanding Different Types of StrategyText '}, {'title': 'Interest in Scalable Techniques for Integer Programming ', 'text': 'The speaker is interested in learning more scalable techniques for integer programming. They had a paper on automated algorithm configuration with theoretical generalization guarantees. Algorithm configuration has been going on for at least 17 years seriously without any generalization theory before. The speaker is honored to talk to Tomas and thanks him for bringing Labradus to the world. The conversation ends with no more questions.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 23:02:52.430947 ...\n",
      "Best SD: 2.384848003542364, Best iteration: 22\n",
      "done get topics 2024-03-25 23:02:53.108044.\n",
      "Stage 2 start time 2024-03-25 23:02:53.108064\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Thomas Sanholm: AI Pioneer, Game Theory Expert, and Professor\n",
      "2. The Importance of Tells, Abstraction, and Learning in Poker\n",
      "3. Understanding Nash Equilibrium, Probability Distributions, and Game Theory\n",
      "4. Coordination in Strategic Games, Transitioning to Business and Security Applications\n",
      "5. AI's Impact on Performance-Oriented Research and Poker\n",
      "6. Challenges of Scaling Good Ideas, Automated Mechanism Design in Politics\n",
      "7. Optimal Bidding Strategies, Game Solving Technology Startups, and the Future of AI\n",
      "8. Nationwide Kidney Exchange, AI Safety, and the Game Theory of Mutually Assured Destruction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-03-25 23:03:40.803566\n",
      "stage_2_titles: len: 8\n",
      "['1. Thomas Sanholm: AI Pioneer, Game Theory Expert, and Professor', '2. The Importance of Tells, Abstraction, and Learning in Poker', '3. Understanding Nash Equilibrium, Probability Distributions, and Game Theory', '4. Coordination in Strategic Games, Transitioning to Business and Security Applications', \"5. AI's Impact on Performance-Oriented Research and Poker\", '6. Challenges of Scaling Good Ideas, Automated Mechanism Design in Politics', '7. Optimal Bidding Strategies, Game Solving Technology Startups, and the Future of AI', '8. Nationwide Kidney Exchange, AI Safety, and the Game Theory of Mutually Assured Destruction']\n",
      "remove_questions start time: 2024-03-25 23:03:40.819834\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 23:09:08.406933\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-25 23:09:08.407067\n",
      "extract_keypoints done time 2024-03-25 23:11:46.598436\n",
      "Start time: 2024-03-25 23:11:46.598694\n",
      "Stage 1 done time 2024-03-25 23:14:20.711910\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and the Role of Rajat Manga at Google ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Evolution of Open Source in Google Brain ', 'text': \"Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. Google Brain's proprietary machine learning library turned into TensorFlow in 2014, the open source library. The idea of deep learning was intriguing and held promise, even before it had taken off.\"}, {'title': 'Title ', 'text': \"Leveraging Google's Compute Power and Data for Research ScalingText \"}, {'title': 'The Evolution of Machine Learning at Google ', 'text': 'Google Brain was founded around neural networks and deep learning from the beginning. In 2012 or 2011, Google started scaling machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines. Google has been doing machine learning for a long time.'}, {'title': \"The Impact of Google's Machine Learning and the Open Sourcing of TensorFlow \", 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact lots of things. Real products started wanting to use deep learning, starting with speech and then expanding to other products. Academia also started to push for more deep learning by 2014. The decision to open source TensorFlow was made.'}, {'title': 'The Impact of Going Open Source with TensorFlow ', 'text': \"The decision to go open source with TensorFlow is considered a seminal moment in software engineering. Google's decision to take a large project and go open source with it led the entire world in embracing open innovation. The initial idea to go open source came from Jeff, who was a big proponent of this. The decision was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'Rapid Growth of Deep Learning and Machine Learning ', 'text': 'Deep learning and machine learning have grown rapidly due to sharing of research. Existing libraries like Tiano and Torch were developed by academia, but there was a need for software at a different level. Google had developed internal software and published papers, leading to successful open source projects. Hadoop was developed from technology built internally, which was considered superior for various reasons.'}, {'title': \"Google Cloud's Bigtable and HBase APIs, TensorFlow, and Community Focus \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. TensorFlow is open source and can be used anywhere. Google Cloud ensures lots of integrations and works well with TensorFlow. The focus is on helping the community and pushing a good standard forward.'}, {'title': 'Development and Open Sourcing of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. TensorFlow was open sourced in November 2015. Development of TensorFlow started in the summer of 2014. Decision to open source TensorFlow was made in late 2014. Focus on ensuring that TensorFlow works well in its ecosystem.'}, {'title': 'Title ', 'text': \"TensorFlow's Design and Support for Large Scale and Mobile DeploymentText \"}, {'title': 'Title ', 'text': 'Machine Learning on Mobile Phones in the PastText '}, {'title': 'Title ', 'text': 'Comparing Libraries for Flexible Deep Learning ImplementationText '}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The move towards TensorFlow 2.0 includes more default eager execution. The graph decisions are being made to hide the graph a little bit, as it is less intuitive for developers. The graph concept came from a disbelief in having a graph-like structure, and it was initially a simple, straight-line concept. The focus on production deployment led to the consideration of using a graph in Python. Experimentation with ideas showed that not having a graph made things simpler to use.'}, {'title': 'Influence of Graph Deployment on Product Growth ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, was unexpected. The need for the product was seen from a research perspective and early days of deep learning. The potential for future growth and enabling more people to use the product was recognized after open sourcing. The growth of deep learning was observed after open sourcing.'}, {'title': 'The Growth of Deep Learning and Community Engagement ', 'text': \"Deep learning grew rapidly after open sourcing. The company saw the opportunity to leverage deep learning and deliver on what people want. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company's approach is very community-driven. The initial version of the product was 0.6 or 0.5. People initially loved the documentation provided by the company.\"}, {'title': 'Evolution of Deep Learning from Research to Practical Applications ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from being a research-focused field to being accessible to developers for practical applications. The focus shifted towards stability and deployment, as opposed to just research. Planning for version 1.0 involved addressing the needs of enterprises and emphasizing documentation and design. There was an increasing demand from enterprises for the product.'}, {'title': 'Enterprise Adoption of Product Versions ', 'text': 'The excitement of getting more enterprises to buy in and support the product. The increase in enterprise adoption post 1.0 and over the next few releases. The initial release and 1.0 being more for researchers and early interest, while 1.x saw lots of enterprises getting on board. The pressure for stability from enterprises, especially for versions below 1.0. The importance of understanding what enterprises want in the midst of product development.'}, {'title': 'Title ', 'text': 'The Importance of Stability and Simplicity in Model SelectionText '}, {'title': 'The Advancements in Transfer Learning and Machine Learning Technologies ', 'text': 'The combination of RL, GANs, and other technologies is pushing the state of the art in the field. Older technologies, such as ResNet 50, are still very usable and stable. Transfer learning on specific problems using existing models like ResNet 50 is a common use case. Making transfer learning as easy as possible is important for hobbyists and common use cases. The majority of the world uses transfer learning for their machine learning applications. The use of transfer learning looks great in presentations and is common in early apps and phones.'}, {'title': 'The Importance of Data Analysis in Enterprise ', 'text': 'Enterprises have data they want to make predictions on, often using regression models, linear models, or gradient booster trees. Some enterprises still benefit from deep learning, especially with large data sets. The audience for enterprise data analysis may vary, with different needs and preferences. The TensorFlow Extended piece, which is the entire pipeline, is important for enterprise data analysis.'}, {'title': 'The Importance of Data Organization for Using TensorFlow ', 'text': 'TensorFlow Extended is the entire pipeline, focused on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. The role of an evangelist is to encourage companies to organize their data for the big benefit of using TensorFlow.'}, {'title': 'Title ', 'text': 'Understanding Machine Learning and the TensorFlow EcosystemText '}, {'title': 'Improving Access to Data Sets with Keras and TensorFlow ', 'text': 'People want easier organization and access to released data sets. Start with basic models and improve them. Keras made TensorFlow more accessible. Francois started the Keras project before joining Google.'}, {'title': 'The Evolution of Keras and its Creator ', 'text': \"Francois started the Keras project before he was at Google. Tiano was the first thing before TensorFlow was created. When TensorFlow started becoming popular, there were enough similarities that he decided to create an interface and put TensorFlow as a backend. He decided on his own and thought that was interesting and relevant to the community. He joined research and was doing some amazing research. He has some papers on that and research, so he's a great researcher as well.\"}, {'title': 'The Deep Integration of Keras into TensorFlow ', 'text': \"Keras got integrated into TensorFlow in a deep way. TensorFlow 2.0 recommends Keras as the way for beginners to interact with TensorFlow. Transfer learning and basic use cases are made super simple with Keras. The integration with Keras has been ongoing for about two years. The researcher who developed Keras was initially working on his own, but eventually joined the team. The researcher's manager agreed to let him work with the team for a quarter, which has now extended to two years. The team spent a lot of time thinking about the integration and had a variety of APIs to consider.\"}, {'title': 'Integrating and Simplifying APIs for User Convenience ', 'text': 'There were multiple APIs, including some built by the team and others built by the community. The goal was to integrate and simplify the APIs to make it easier for users to choose and use. The community was confused about which API to use and kept asking for a standard one to be chosen. The decision to focus on Keras was based on its popularity and positive feedback from users.'}, {'title': 'The Rise of Keras in the World of Machine Learning ', 'text': 'Keras was loved by many and had great qualities. Keras was chosen organically as the best option. There was a surprise in bringing in an outside element like Keras. Keras was seen as a potential competitor to TensorFlow but ended up being an empowering element of it. The team and developers all want to make things easier for developers. Python has Guido van Rossum as a key figure. A successful open source project like TensorFlow needs one person to make final decisions.'}, {'title': 'Success and Growth at TensorFlow Dev Summit ', 'text': 'TensorFlow Dev Summit was successful. Different new features are being incorporated into TensorFlow. There is an amazing ecosystem surrounding TensorFlow. There are multiple people involved in the key design directions of TensorFlow. Martin Wick has driven a lot of the open source aspects and APIs of TensorFlow. Regular design reviews are conducted for TensorFlow. Efforts have been made to open up to the community and add transparency in TensorFlow development. More processes such as RFCs and special interest groups are being set in place for TensorFlow.'}, {'title': 'The Evolution and Growth of the TensorFlow Ecosystem ', 'text': \"The need for adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The recognition that the ecosystem's scale requires more than one decision maker. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The convergence of all these developments towards the ability to save models in a consistent way and move them between different platforms.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning in Multiple WaysText '}, {'title': 'The Integration of Machine Learning into Real Products ', 'text': 'Machine learning is being integrated into real products to have a real impact on people. There are a large number of compute devices across the world, including phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning continues to grow and cover more aspects over time. There is a focus on pushing the boundaries and building more tooling in some areas.'}, {'title': 'The Evolution of TensorFlow Tooling ', 'text': 'TensorFlow is continuously pushing boundaries and building more tooling to help users. TensorBoard was the first tool created, and now there are many libraries being built on top of it. Libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production. Some tools have come from within Google, while others have come from the community.'}, {'title': 'Improving Collaboration and Model Sharing in TensorFlow 2.0 ', 'text': 'The goal is to enable different parts of the community to build the things they care about. The focus is on making different pieces work well together in TensorFlow 2.0. The core format and sharing of models through save model and TensorFlow hub are key areas of focus. The introduction of TensorFlow.js (deep learning JS) was initially met with skepticism.'}, {'title': 'Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The team has learned a lot and iterated over the last few years. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new devices from a hardware perspective.'}, {'title': 'Challenges of Monolithic System in TensorFlow ', 'text': \"TensorFlow started as a very monolithic system and is still largely monolithic. There are lots of tools around it, but the core is still pretty large and monolithic. It's hard to change and modify and really break apart the system. The system is rapidly evolving and not slowing down. It's like changing the engine of a car while it's running.\"}, {'title': 'Challenges of Maintaining Compatibility in TensorFlow ', 'text': 'Many people rely on TensorFlow in their applications. There is a challenge in maintaining previous versions while also introducing new ones. TensorFlow 2.0 breaks some backward compatibility but the conversion is straightforward. It is a tricky balance between introducing new features and maintaining compatibility. The impact of changes in TensorFlow depends on whether it is used for research or production.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and improvements comes with a trade-off, but the overall value is much bigger. It's not just about breaking the person yesterday, but also about setting standards for new people joining the team. When doing new things, it's important to consider the impact on future team members.\"}, {'title': 'The Importance of Designing with a Clean Slate ', 'text': \"Design with a clean slate in mind is important for new things. Making compromises occasionally is necessary, but designing with a clean slate is crucial. It's important to put all concerns behind when thinking of new ideas. The speaker switched their research group to TensorFlow. The speaker wishes everyone would use the same thing, and believes TensorFlow is leading in many ways.\"}, {'title': 'The Rise of PyTorch in Research ', 'text': \"PyTorch is being used by a lot of researchers now. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not necessarily on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Importance of Learning from Previous Experiences and Exploring Different Spaces ', 'text': 'The benefit of learning from previous experiences and exploring different spaces. Building on previous successes such as JNR. The decision to revisit and add eager execution. The challenge of combining different elements in version 2.0. The comparison of eager execution to the competition.'}, {'title': 'Progress of TensorFlow in the Last Couple of Years ', 'text': 'TensorFlow has made incredible progress in the last couple of years. The integration with Keras and eager execution has made TensorFlow easily accessible. TensorFlow 2.0 has set the stage for enabling lots of other exciting possibilities. The clean APIs of TensorFlow are setting up for future developments.'}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs will allow for optimization and improved performance for users. The team is looking forward to exploring new possibilities for single machine and distributed systems in future versions.'}, {'title': 'Restructuring and Modularization of TensorFlow for Improved Organization and Performance ', 'text': 'The team is excited about future versions and expects to see improvements over time. Restructuring the monolithic system into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow in GitHub consists of one core repository with various components such as the execution engine, key backends for CPUs and GPUs, and distributed computing. The components in the current organization work together in a single library or binary, with limited ability to split them apart easily. Clean interfaces are needed for better organization and modularity in a perfect world.'}, {'title': 'The Importance of Clean Interfaces and TensorFlow in Development ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on different clusters with custom networking. Clean separation in interfaces will help with more interesting developments in different spaces. Enabling independent evolution and pushing on things allows for better scalability in the ecosystem. Major corporations like Pepsi are already using TensorFlow for development.'}, {'title': 'The Growing Community and Adoption of TensorFlow ', 'text': 'Many users are already using TensorFlow, including hardware vendors and bigger companies like IBM. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The community growth is attributed to the involvement of various companies and users who want to optimize for their specific needs, such as autonomous vehicle companies. The critical thing that allowed for this growth is not specified in the given text.'}, {'title': 'Factors Contributing to Growth in TensorFlow ', 'text': \"Growth is critical and requires a combination of factors such as timing and community feedback. TensorFlow's growth is linked to the growth of deep learning itself. Listening to the community and being open to external contributions is important for growth. Putting the right process in place and welcoming contributors is essential for growth.\"}, {'title': 'Importance of Transparency and Community in Open Source Projects ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Thinking about documentation and tools that developers care about is crucial. People building something on TensorFlow and implementing a particular architecture contributes to the growth of TensorFlow.'}, {'title': 'Title ', 'text': 'Making Development and Transitioning Easy on GitHubText '}, {'title': 'Rapid Advancements in Deep Learning Technology ', 'text': 'Most people want a really good thing. People will start to see the value and shift towards it. The field is moving rapidly, allowing for more things to happen. New things will clearly happen in 2.x, giving people reasons to move. Change is expected to happen in terms of deep learning basics.'}, {'title': 'The Future of Machine Learning and Hardware Accelerators ', 'text': 'Convolution models and basic models will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. There will probably be new developments in the field, but they are hard to predict. Some current directions include combining eager execution and graphs to make programming more natural, and exploring ground-up approaches like Swift for TensorFlow. It is uncertain if hardware accelerators will remain the same, or if training with four bits instead of 32 bits will be possible. The TPU side of things is exploring the possibility of training with fewer bits.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': 'TPU is already on version three, and it is exploring the use of four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to easily train or do transfer learning on simple image models like Inception or ResNet. Providing simple models for beginners is important.'}, {'title': 'Improving User Experience with TensorFlow ', 'text': \"Providing simple models and tools like hub to make it easy for users. Different levels of support for beginners, intermediate users, and researchers. Offering pre-trained models to decrease the time needed to start. TensorFlow's recent delivery is trivial for beginners. Addressing pain points and trying to ease the user experience.\"}, {'title': 'The Potential of High Schoolers in Technology and Innovation ', 'text': 'High schoolers are doing amazing and terrifying things. Incredible ideas will be coming from the younger generation. There is a technical and management aspect to the work with TensorFlow. Cohesion across the team is important for delivering something well.'}, {'title': 'The Importance of Team Cohesion ', 'text': \"Cohesion across the team is important for execution. The team can achieve more together than as individuals. Hiring good people who care about what they're building and are motivated is important. Having a unified vision of where the team wants to go is crucial.\"}, {'title': \"Google's Unified Vision and Organizational Structure \", 'text': \"Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, but also does research. As Google has become a larger product and ecosystem, it's important to combine direction with exploration. Google's direction is not binary and involves tensions and complexities. The mission of superstars is important at Google.\"}, {'title': \"The Role of Superstars in Google's Mission \", 'text': 'Superstars play a large role in the mission at Google and can sometimes create tension within a team. The mission of the project, like TensorFlow, can make it easier to work with superstars. Google values getting people who care and have the same culture. The project allows for lots of people to do different things and grow, making the problem easier to manage.'}, {'title': 'Refining the Hiring Process at Google ', 'text': 'The hiring process at Google has been refined over the last 20 years. Google values teamwork and productivity over individual superstar status. Core technical skills are important, but teamwork and collaboration are also key factors in hiring engineers.'}, {'title': 'The Importance of Motivation in Long-Term Success ', 'text': 'Motivation is important in addition to core technical skills for long term success. Motivation is important at every level, not just for senior positions. Google hiring process focuses on puzzle solving and problem solving ability, but may not fully assess motivation and drive.'}, {'title': \"The Importance of Culture Fit in Google's Interview Process \", 'text': 'The interview process at Google includes a focus on culture fit in addition to technical skills. Different projects at Google may have different cultural requirements. TensorFlow is a fast-moving project and requires people who are comfortable with that pace.'}, {'title': 'Navigating Project and Team Dynamics at Google ', 'text': 'Balancing the need for full-fledged products with ensuring things work really well. Importance of finding the right fit for projects and teams. Variability of culture across projects, teams, and product areas at Google. Engineering excellence as a core part of the culture. Difficult things can be fun when solved. The key to success in a large ecosystem or small product is important.'}, {'title': 'Making Decisions in a Complex Ecosystem ', 'text': 'Striking a balance across different aspects of a large ecosystem or a small product. Making hard decisions such as how fast to go versus how perfect it is, involving the community, and saying no to certain things. Some decisions are made quickly due to time constraints, while others are given time for consideration. Both choices are pretty good when it comes to making decisions. The Dev Summit came together incredibly, despite the complexity of the moving pieces.'}, {'title': 'Managing Deadlines and Releases in Software Development ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and getting something that works well. The team did a great job in putting TensorFlow 2.0 alpha together. Official deadlines are not always put out, but key things are focused on and developed in the open. Releases are done at a regular cadence.\"}, {'title': 'Approach to Releases and Iteration in TensorFlow 2.0 ', 'text': \"Releases are done at a regular cadence, with the understanding that if something doesn't make it into one release, it can be included in the next one. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is acceptable to release things that are not fully ready, as long as it is made clear that they are experimental and open to feedback. Quick cycle and quick iteration are important, rather than focusing on meeting strict deadlines. There is no pressure to make TensorFlow 2.0 stable, similar to the approach taken with WordPress 5.0, where updates were released quickly to improve it.\"}, {'title': 'Title ', 'text': 'The Stability and Future Development of NodeXText '}, {'title': 'TensorFlow 1.0 X Release Update ', 'text': \"TensorFlow has 41 million downloads for 1.0 X. The focus is on polishing and putting together features. There is no rush to release the product just because it's already out there. The goal is to get it right and focus on quality. The release is planned for the next few months or the next quarter. The team will try to make the release happen as soon as possible. Ads connect people to the things they want and need at their best.\"}, {'title': 'The Impact of Search Ads and Machine Learning on User Experience ', 'text': 'Search ads are an extension of what search is trying to do, which is to make information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience. Ads can annoy users and ruin the user experience if not done well. Huge amounts of personalized data can help map to the things users actually want without annoying them.'}, {'title': 'The Importance of Accessible Information and Quality Advertising ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the information to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's reading experience.\"}, {'title': 'Balancing Value and Monetization in Advertisements ', 'text': 'Advertisements need to strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services such as search engines and websites to provide their service. The challenge is to show valuable ads without being annoying or distracting. Advertisements, when done well, can be really useful and not annoying. The internet has a limitation in that nobody wants to pay for anything.'}, {'title': 'Rise of Paid Services on the Web ', 'text': 'More paid services are being seen across the web and people are willing to pay for them. Transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. The value of paid content is being recognized, as seen with the example of Netflix and YouTube. There will always be things that are monetized with ads, but the trend is towards more paid services.'}, {'title': 'Transition to a Mix Model and Use of TPU in Google Call App ', 'text': \"['Transition to a mix model with free trials and ads, followed by a clear revenue model.', 'Use of TPU in a Google call app for free.', 'Ability to run TensorFlow open source on any device, including desktop and phone.', 'Increasing power of desktops and phones for running TensorFlow.', 'Possibility of training TensorFlow on a phone.']\"}, {'title': 'The Impact of Cloud Computing on Machine Learning and TensorFlow ', 'text': 'The power of cloud computing is accessible through devices like phones, making it more convenient and powerful. Cloud services like Colab make it easy to get started with machine learning and TensorFlow without the need for installation. While free services like Colab are great for beginners, paid services offer more capabilities and resources. Beginners interested in machine learning and TensorFlow can start by visiting the TensorFlow.org website and exploring the resources available there.'}, {'title': 'Title ', 'text': 'Getting Started with TensorFlow and ColabText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 23:14:25.716266 ...\n",
      "Best SD: 1.661324772583615, Best iteration: 16\n",
      "done get topics 2024-03-25 23:14:26.675596.\n",
      "Stage 2 start time 2024-03-25 23:14:26.675618\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow and the Role of Rajat Manga at Google\n",
      "2. The Evolution of TensorFlow 2.0\n",
      "3. The Advancements in Transfer Learning and Machine Learning Technologies\n",
      "4. Improving Access to Data Sets with Keras and TensorFlow\n",
      "5. Improving Collaboration and Model Sharing in TensorFlow 2.0\n",
      "6. Challenges of Maintaining Compatibility in TensorFlow\n",
      "7. The Growing Community and Adoption of TensorFlow\n",
      "8. The Potential of High Schoolers in Technology and Innovation\n",
      "9. TensorFlow 1.0 X Release Update\n",
      "10. The Impact of Search Ads and Machine Learning on User Experience\n",
      "Stage 2 done time 2024-03-25 23:15:27.883672\n",
      "stage_2_titles: len: 10\n",
      "['1. The Impact of TensorFlow and the Role of Rajat Manga at Google', '2. The Evolution of TensorFlow 2.0', '3. The Advancements in Transfer Learning and Machine Learning Technologies', '4. Improving Access to Data Sets with Keras and TensorFlow', '5. Improving Collaboration and Model Sharing in TensorFlow 2.0', '6. Challenges of Maintaining Compatibility in TensorFlow', '7. The Growing Community and Adoption of TensorFlow', '8. The Potential of High Schoolers in Technology and Innovation', '9. TensorFlow 1.0 X Release Update', '10. The Impact of Search Ads and Machine Learning on User Experience']\n",
      "remove_questions start time: 2024-03-25 23:15:27.899795\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 23:21:19.229026\n",
      "chunks_text len: 73\n",
      "extract_keypoints start time: 2024-03-25 23:21:19.229116\n",
      "extract_keypoints done time 2024-03-25 23:24:15.663993\n",
      "Start time: 2024-03-25 23:24:15.664263\n",
      "Stage 1 done time 2024-03-25 23:27:00.939838\n",
      "RR stage_1_outputs:\n",
      "[{'title': \"Adobe Research's Efforts to Automate Creative Tasks \", 'text': 'Adobe Research is working to define the future evolution of their products to make the life of creatives easier. They aim to automate tedious tasks and give more time for creatives to operate in the idea space. Deep learning methods of the past decade can shine in this application. Gavin Miller, head of Adobe Research, combines tech and creativity, writing poetry and building robots outside of work.'}, {'title': 'Gavin Miller: Head of Adobe Research and Multifaceted Artist ', 'text': 'Gavin Miller is the head of Adobe Research, leading innovative efforts and applications of AI in creating images, video, audio, and language. Gavin Miller is also an artist, poet, writer, and roboticist. Lux Friedman, the interviewer, enjoys Gavin Miller\\'s poetry and promises to sprinkle it into the conversation. Gavin Miller\\'s poem \"Je Ne Vinaigrette Rien\" parodies both Edith Piaf\\'s \"Je Ne Vinaigrette Rien\" and Frank Sinatra\\'s \"My Way\".'}, {'title': 'Exploring the Struggle with Weight and Embracing Humor ', 'text': \"The speaker opens with a poem about struggling with weight and dieting. The poem reflects the internal struggle between wanting to lose weight and the irrational desire to embrace the opposite idea. The speaker expresses no regrets about their decision. The topic of weight and dieting is a serious one for some people, but the speaker finds humor in taking it to an extreme. The speaker has always been interested in writing and technology since high school. There are parallel strands in the speaker's life, one related to their private life and the other related to writing and technology.\"}, {'title': 'The Intersection of Technology and Creative Expression ', 'text': 'The intersection of private life and technological career. The influence of one idea on the other. The inspiration from science fiction for building new technology. The example of using voice synthesis for creative expression. The impact of technology limitations on creative output.'}, {'title': 'Advancements in AI and Smart Home Technology ', 'text': 'The speaker created a poem to match the tone of a voice that sounded sad and depressed. The poem was pretended to be written by an intelligent agent, telling the user to go home and leave them alone, but also expressing loneliness and a desire for company. This level of AI sophistication was beyond what was possible at the time, but is now becoming more feasible. The speaker had a smart home project in the early 90s, with a talking voice reminding them of tasks and buttons on the washing machine to prevent clothes from getting moldy. The speaker also made photo albums that used some form of technology.'}, {'title': 'Creating Magical Realism Experience with Technology ', 'text': 'The speaker made photo albums with light sensors and wireless radio to create a magical realism experience. The speaker was intrigued by the idea of creating a personality for modern agents, inspired by literary and theatrical experiences. The speaker has written plays and designed personalities for modern agents, thinking about imaginary dialogue and the capability for conversation. The speaker is interested in creating technology that can have real and knowledgeable conversations, avoiding the uncanny feeling.'}, {'title': 'Challenges and Capabilities of AI in Understanding and Communication ', 'text': \"AI can fall into the uncanny valley where it says something it doesn't really understand. AI needs to have multiple ways of talking about the same concept to sound like it really understands it. If AI can reason about a concept from multiple angles and give similar responses, it starts to seem more sentient. Automatic image captioning is mentioned as an example of AI's capabilities.\"}, {'title': 'Advancements in Image Captioning and Synthesis ', 'text': \"Automatic image captioning and generating different kinds of statements about the same picture. Work on turning a medium from one form to another, such as auto tagging imagery or making up full sentences about what's in the image. Using GANs to synthesize an asset that matches a given description. Early career focused on 3D computer graphics and pioneering work before movies had special effects done with 3D graphics. Comparison of early career work to the Renaissance, where people would model light, color, and shape.\"}, {'title': 'Advancements in Computer Image Generation with AI Algorithms ', 'text': 'Computer image generation using AI algorithms is the new frontier. The creative process is shifting towards the space of ideas rather than raw pixels. Adobe aims to span the entire range from low-level tools to realistic oil paint and watercolor simulations.'}, {'title': 'The Importance of Automation in Design Work ', 'text': 'Complete control is important for creating expressive and novel work. Automation of certain tasks frees artists to focus on inspiration. Design work for different aspect ratios used to take up a lot of time for artists. AI aims to reason about the likely intent for different formats and languages. The focus should be on the creative aspect of design, such as look, style, feel, and message.'}, {'title': 'The Evolution of Creativity in the Digital Age ', 'text': 'Creativity is changing, making it easier, faster, and cheaper to create. There is a growing demand for beautiful artwork in various industries. The role of artists may shift from hands-on artisan to art director or conceptual artist. Adobe products, AI, and technology are important tools for creative work.'}, {'title': 'Optimizing Workflow and Automation in Video and Audio Editing ', 'text': 'The speaker uses Photoshop to create the thumbnail for the video, Premiere to edit the video, and Audition for the audio. The speaker emphasizes the importance of optimizing the flow and being extremely efficient in their work process. The speaker uses an old school Kinesis keyboard and auto hotkey to minimize the number of clicks and streamline their workflow. The discussion shifts to the role of AI and automation in making the low level pixel work flow easier in the coming months and years. There is a rich array of algorithms already in Photoshop, including classical procedural algorithms and data-based ones with a large number of sliders and degrees of freedom.'}, {'title': \"AI's Role in Providing Smart Defaults for Image Editing \", 'text': 'AI can help by providing default settings based on the content itself rather than default values for the tool. Smart defaults can make life easier for people while making use of common sense from other example images. Adobe has spent a lot of work over the last 20 years thinking about selection, particularly with quick select, which looks at color boundaries and figures out how to flood fill into regions that are physically connected in the real world. The algorithm used for selection had no visual common sense about what a cat looks like or a dog, it was based on rules of thumb applied to graph theory.'}, {'title': 'Improving Image Editing with Graph Theory and Neural Nets ', 'text': 'Graph theory and neural nets are being used to improve the process of image editing. Neural nets can be used to select objects in an image without the need for manual clicking. The use of neural nets can lead to more robust and high-quality results in image editing. Background removal and easy editing of images are important goals in image editing. The use of neural nets can make background removal and editing easier and more efficient.'}, {'title': 'The Challenge of Background Removal ', 'text': 'The challenge of removing the background is discussed. Quick, cheap, and cheerful background removal options are available with algorithms. Different algorithms are available for different levels of guidance on boundaries. Combinations of tools are available for various background removal needs. Demonstration of quick object selection at Adobe Max conference.'}, {'title': 'Improving Selection Mask Creation with Simple Polygon Drawing ', 'text': 'Drawing a simple polygon around the object of interest can quickly create a selection mask. The process has reduced the time from hours to a few seconds for workflows. The challenge is to make the tool robust and work in the majority of cases, if not all. There is a difference between academic research and industrial research in achieving robustness.'}, {'title': 'Distinguishing Academic Research from Industrial Research ', 'text': 'The difference between academic research and industrial research is highlighted. Academic research focuses on great new ideas that show promise, while industrial research involves shipping and receiving customer reviews. The company values customer feedback and product critics to improve their products. The goal is not to be perfect every single time, but to be perfect enough of the time and have a mechanism to intervene and recover from mistakes. The company values talented customers and aims to support them in improving products with minimal effort.'}, {'title': 'The Impact of AI on Professional Tasks ', 'text': 'AI can make professional tasks less tedious and time-consuming. 100% automatic processes could delay time to market. Collaboration between human and machine can make the life of creatives easier.'}, {'title': 'Improving User Learning Experience in Tool Usage ', 'text': \"The focus is on helping the person in the moment to do the task they need to do, as well as thinking holistically about their journey learning a tool. The goal is for users to become experts in using the tool, similar to living in a city where you know the important streets you need to get to. Projects in research analyze thousands of hours of tutorials online to understand what's being taught in them. One publication at CHI looked at the last three or four actions users did in tutorials to determine what other people did next, providing inspiration for what to do next or for watching the tutorial.\"}, {'title': 'AI-Powered App for Creative Inspiration ', 'text': 'The app provides inspiration for next steps and tutorials. It learns from similar workflows and provides intelligent suggestions. It uses context to make intelligent suggestions about choices or assistive actions. The goal is to deeply understand the domain of designers and creative people and combine it with AI for intelligent suggestions. The app aims to provide suggestions through verbal possibilities or showing the results of trying different actions. The ultimate goal is to combine domain understanding with AI to provide intelligent suggestions for creative work.'}, {'title': 'The Importance of Artist and Teacher Guidance in Media Understanding ', 'text': 'The grand challenge of having an artist and a teacher guiding the process. Giving enough at each stage to build a foundation for the next stage. Understanding different media types visually and in terms of transcripts and words. Removing the barrier of having to type in keywords for searching. Assisting with learning the interface in the longer term.'}, {'title': 'The Impact of Adding an Assistant to GUI Interface ', 'text': 'The discussion is about whether an assistant modifies the interface to be simpler. Adding a feature to a GUI increases visual complexity for new users. Having an assistant with a new skill is additive without being intimidating. Consideration is given to onboarding new users. Some users value mastering a complex interface and keyboard shortcuts. Others prefer to get things done quickly in the simplest way possible. There is a need for a more assistive version of the interface.'}, {'title': 'Exciting Applications of Computer Vision and Machine Learning by Adobe ', 'text': 'Adobe is working on exciting applications of computer vision and machine learning. These applications include scene stitching, sky replacement, foreground/background removal, spatial object-based image search, automatic image captioning, project cloak, project deep fill, project scribbler, style transform video, style transform faces and video with project puppetron. Different classes of devices are being considered for more assistive versions of technology, depending on the context for CAPTCHA. Users in deep post-production workflows may prefer to work on a laptop or big screen desktop with more knobs and dials to express subtlety. There are many exciting applications of computer vision and machine learning that Adobe is working on.'}, {'title': 'The Sky Replace Feature in Image Editing ', 'text': 'Sky replace is an interesting feature that allows for automatic selection and replacement of the sky in an image. It also matches the geometry of the scene and provides variety in choices for different moods. The tool also recolors the foreground objects based on the new sky, adding realism to the edited image. It uses stock content to match the sky and foreground, creating a seamless blend between the two elements.'}, {'title': 'Exploring Natural Effects and Flexibility in Photography ', 'text': 'The evening sky adds an orange glow to foreground objects. The artist Magritte\\'s surrealism paintings inspired the speaker in college. The goal is to achieve natural effects in photography rather than using surrealism. The process aims to capture an entire workflow in a single action, saving time in post-production. The ability to quickly apply the effect to multiple backgrounds allows for exploration of different design options. The approach allows for flexibility in the creative process, enabling the artist to \"know it when I see it.\"'}, {'title': 'Importance of Exploring Design Space and Intelligent Image Search ', 'text': 'The importance of exploring the design space as close to final production value as possible. The idea of making intelligent choices about ways to search stock images. The concept of concept canvas and its application in image search. The need for being able to specify more than just keywords in an image search.'}, {'title': 'Concept Canvas and Spatial Layout Design ', 'text': 'Concept canvas allows assigning spatial regions to keywords. Pre-indexed images help in matching important concepts in the picture. Gives a sense of ownership over the outcome of the event. Technologies in Photoshop allow physically moving objects in post-production. Concept canvas provides a fast way to design spatial layouts.'}, {'title': 'Using Neural Networks and GANs for Object Removal and Background Filling ', 'text': 'Neural networks are being used to remove objects from a scene and fill in the background automatically. GANs (Generative Adversarial Networks) are one approach for using neural networks to remove objects from a scene and fill in the background. Traditional algorithms like content aware fill work well for certain classes of images, such as those with naturalistic textures like gravel paths. Patch-based algorithms can create plausible looking intermediate images to fill in holes in certain types of images. Algorithms are used to smooth out lighting and eliminate brightness contrast in the filled-in regions.'}, {'title': 'Challenges in Image Processing ', 'text': 'The importance of smooth lighting in avoiding brightness contrast. The challenge of inferring invisible structure behind objects in an image. The common sense knowledge required to fill in missing information in an image. The limitations of current generative methods in high resolutions.'}, {'title': 'Challenges and Strategies in Advancing AI Image Recognition ', 'text': 'The need to transition from low resolution to high resolution using another algorithm or pushing the state of the art in research. The importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms. The concept of an ensemble of experts specialized in certain things and the idea of voting on confidence levels.'}, {'title': 'Improving Workflows and Data Collection in Photoshop ', 'text': 'The process involves either voting on confidence in future actions or using a dispatcher to assign tasks based on expertise. Each model requires a significant amount of work, but over time, the set will be filled out and workflows will expand. The focus initially will be on specific workflows, with gradual branching out as capabilities increase. The large user base of Photoshop can provide valuable information on workflows and needs. The goal is to collect data on the types of annotations needed for different tasks, such as houses or octopus.'}, {'title': 'The Importance of Data Collection and Privacy in AI Development ', 'text': 'Understanding the kind of data needed for annotation and collection for building effective tools in AI. Importance of gathering data and respecting privacy. Demonstrating the benefits of sharing data with the tool to the users. Users teaching what is important and useful for them. Need for explicit permission from users for data sharing. Short-term benefits of sharing data for better recommendations. Willingness of users to share data if they depend on the tool for their livelihood.'}, {'title': 'Data Sharing and Workflow Improvement ', 'text': 'Data contributors may be willing to share workflows or choices with the data set to be trained. Technologies exist for learning without storing all information permanently. Adobe exists in a space where sharing data for improving workflow is beneficial. Some professional workflows may be very protective of their data.'}, {'title': 'Protecting Data in Professional Workflows ', 'text': 'Professional workflows may require protection of data, especially in legal cases. Some scenarios may involve a more permissive relationship with Adobe for non-confidential projects. Different levels of data capture may be implemented, with more detailed knowledge from willing participants. Explicit customer studies are conducted to gather feedback on the tool.'}, {'title': 'Improving Customer Studies and Data Collection ', 'text': \"Customer studies involve visiting and observing users to improve the tool. A more systematic process is needed to train an algorithm for customer studies. Conscious effort is needed to balance data collection with customer trust. Adobe has a chief privacy officer to ensure responsible data collection. Privacy is a priority in thinking about AI, not an afterthought. Project Puppetron demonstrates Adobe's three-dimensional thinking in addition to 2D.\"}, {'title': 'Advancements in 3D Computer Vision Algorithms ', 'text': '3D thinking is required to assign features to faces in a 2D workflow like stylization. The puppet run can apply the style of a painting to a person talking in a video, creating a realistic effect. 3D computer vision algorithms are improving and can focus on specific domains like faces initially. Over time, 3D computer vision algorithms should be able to work for more general applications.'}, {'title': 'The Use of 3D Reconstruction in Content Editing and AR/VR Applications ', 'text': '3D reconstruction can be used to edit content more reliably and correctly. The face is a very important application for 3D reconstruction. AR and VR serve slightly different purposes, with VR being able to transport users to an immersive world.'}, {'title': 'Evolution of VR and AR Technology ', 'text': 'VR technology is evolving in terms of hardware, with devices becoming all-in-one and less tethered to a box. The market for VR devices is bifurcating into consumer and professional use cases, such as for architects and designers. VR is great for experiencing spatial relationships and scale, especially for architectural design. AR holds the promise of taking digital assets off the screen.'}, {'title': 'The Promise and Challenges of Augmented Reality ', 'text': \"AR holds the promise of taking digital assets off the screen and putting them in context in the real world. The assets need to adapt to the physical context in which they're being placed. AR is like having a live theater troupe come to your house and put on a performance. AR will have the same issue of adapting to the physical space it is in. There is a tension between fidelity and adaptation in AR.\"}, {'title': 'The Importance of Adaptation in Performance and Media ', 'text': 'The importance of accurately reproducing a performance, such as a ballet, versus adapting a story to the environment. The influence of the characteristic of the media on the need for nuance and adaptation. The potential difference in approach for famous celebrities versus lovable characters like a frog. The transfer of ideas from the game world to the broader commercial sphere, particularly in the context of adaptive characters in AR.'}, {'title': 'Advancements in AR and VR Technology for 3D Design ', 'text': 'AR technology is being used to create adaptive characters. Demonstrations have been shown of converting Photoshop layers into 3D in AR. The focus is currently on 3D design, which is still a challenging space. One challenge in 3D design is laying out objects, which can be difficult on a conventional screen. VR headsets could provide a better solution for laying out objects in 3D design.'}, {'title': 'The Potential of VR and AR in Design ', 'text': 'VR headset allows for a different viewpoint and sense of depth. Fine grained design tasks may be possible in VR with the right UI. Potential explosion of demand for 3D assets driven by AR and real time animation. Tools and devices may help with designing content in VR.'}, {'title': 'Importance of Designing Content for Product Evolution ', 'text': 'Designing the content is important. New ideas are being considered, but old ways are also valued. Existing user base should not be offended by changes. Convenience should not come at the cost of control. Evolution and growth are important for the product.'}, {'title': 'Evolution and Breakthroughs in Video Editing Tools ', 'text': 'The tool has been evolving and growing, with a lot of brilliant thought along the way. A fundamental breakthrough, such as a single click to select an object, can fit nicely into the existing toolset. Radical simplicity can be achieved in the context of a different device or targeted workflow. Projects like Rush allow professional quality video editing for a certain class of media output.'}, {'title': 'Title ', 'text': 'Quality Video Editing and Software Options for Different Project TypesText '}, {'title': 'The Importance of Understanding and Utilizing Educational Tool Settings ', 'text': 'The importance of having control and understanding the correlation between different settings in an educational tool. The need for on-demand help and suggestions when stuck or unsure about optimal settings. The idea of having multiple intelligent defaults and options to choose from. The mention of interweaving poetry into the conversation.'}, {'title': 'The Impact of Digital Technology on Self-Perception ', 'text': 'The poem reflects the feeling of liberation when leaving the smartphone behind. The use of AI to create versions of ourselves and reality that are more beautiful than actual reality. The creative effort involved in creating this illusion. The inevitability of living in a digital world that is partly artificial. The need for human beings to adjust to this digital world. Comparison of the current digital world with the world a hundred years ago. The impact of social media platforms like Instagram and Facebook on creating versions of ourselves.'}, {'title': 'The Impact of Social Media on Self-Presentation ', 'text': 'The use of social media platforms like Instagram and Facebook has led to the creation of better versions of ourselves through modified images and artificial intelligence. The desire to present the best version of oneself has always been true throughout history, as seen in the example of 18th century aristocrats commissioning flattering portraits of themselves. The ability to imagine alternate realities and visualize them, whether through storytelling or visual culture, raises the question of whether it is a good or bad thing. The shift towards a more visual culture has made the presentation of idealized versions of ourselves more prevalent.'}, {'title': 'The Shift to a Visual Culture ', 'text': 'We have become a very visual culture. In the 19th century, we were a text-based culture. People now prefer quick, visual, and snappy content. Intent plays a significant role in how we present ourselves visually. Holding oneself up to an impossible standard can be harmful. The ability to imagine and visualize an alternate reality can be a wonderful thing.'}, {'title': 'The Impact of Alternate Reality and High-Quality Graphics on Exploration ', 'text': 'Alternate reality can inspire people to create new architectural styles and even start businesses. The availability of high-quality graphics may reduce the excitement of exploring new places in person. The joy of exploration, such as going to the moon, used to be about the anticipation of seeing it for the first time. The recent discovery of Pluto was a fantastic example of outer exploration.'}, {'title': 'The Importance of Critical Thinking in the Age of Media ', 'text': \"Pluto was a fantastic recent discovery with breathtakingly varied and beautiful features. Expanding the ability of the human toolkit to imagine and communicate is a good thing. There are abuses in the use of images and media, which need to be taken seriously and discouraged. The public needs to be aware of what's possible through events like this, to avoid believing everything they read or see. Multiple sets of evidence are needed to really believe something rather than a single media asset. The concept of needing multiple sets of evidence has been true forever, as seen in the story of Anne of Cleves and Henry VIII.\"}, {'title': \"Holbein's Unpleasing Portrait of Henry VIII \", 'text': \"Holbein painted a picture that Henry VIII wasn't pleased with.\"}, {'title': 'The Benefits of Internships in Research ', 'text': 'A constant influx of new people brings new ideas with it. Interns allow for exploration of fanciful or unproven ideas in a lightweight way. Internships ideally lead to new publications for the intern and researcher. Internships provide a portfolio of ideas to draw from for future development. Internships help identify future full-time researchers. Internships build a bridge to university departments for collaboration and recruitment.'}, {'title': 'Building Enduring Relationships Between Research Labs and Universities ', 'text': 'It builds a bridge to university departments for enduring relationships with professors and academic collaborations. The long term legacy of a great research lab includes people who stay and those who move on to carry the same model to other companies. Strong belief in the complementarity of industrial research and academia. Hope for the model to continue and be invested in by other companies, despite the challenge of recruitment. The idea for the model was born through brainstorming and discussions with interns.'}, {'title': 'The Intern Selection Process ', 'text': 'The process of selecting interns involves sending out a call for interns and reviewing resumes. Candidates are contacted to discuss their interests and find a good match for the projects. Interns stay in touch with their mentors and have internal discussions about project ideas. At the end of two weeks, interns have to decide on a project to pursue.'}, {'title': 'Navigating Project Decision-Making in Research Labs ', 'text': 'Pursuing ideas to see if two people have the same idea and should talk. Ideas can change direction once an intern arrives. Flexibility in choosing projects in research labs. Questions about who decides what to do, who is to blame if it goes wrong, and who gets the credit if it goes right. Adobe pushes for freedom of choice of projects but rewards based on impact. Alternative model with one lab director making decisions.'}, {'title': 'Encouraging Innovation and Collaboration in the Lab ', 'text': \"The model of running the lab is based on allowing new ideas to percolate up. There are strategic priorities for the company and areas where investment is needed. The approach is a combination of trickle down and filter up, meeting in the middle. People are not told what to do, but are encouraged to focus on certain areas that would be appreciated. Adobe's broad portfolio of products allows for good ideas to find interested product teams. There is no need to qualify things too much ahead of time.\"}, {'title': 'Product Team Intern Sponsorship and Project Success ', 'text': \"The product teams sponsor extra interns occasionally to address specific problems they care about. It is not typical for the product teams to sponsor extra interns, and it is considered as an additional expense. It is difficult to predict the success of intern projects at the beginning of the summer. Some intern projects may result in a feature, while others may not. Some projects may not be as novel as initially thought, but still have potential as a feature. Some projects may reveal the team's lack of knowledge and require revisiting the problem.\"}, {'title': 'Progress and Anticipation in Technological Breakthroughs ', 'text': \"['Progress and realization of how much is unknown.', 'Revisiting a problem multiple times before a breakthrough.', 'Impact of technological breakthroughs on products and the world.', 'Focus on creative and analytics assistants making useful suggestions.', 'Anticipation of progress in 2019 and beyond.']\"}, {'title': 'Advancements in Generative Adversarial Networks ', 'text': 'Generative adversarial networks are immensely promising and quickly becoming practical for mainstream use cases at high resolution with good quality. The core technologies like generative adversarial networks are exciting and have a strange way of doing things oddly in an interesting way, resembling dreaming. The company is transitioning from hand designing for specific use cases to a more standardized approach with the Sensei platform, which can be accessed by multiple product teams at once.'}, {'title': 'Standardizing Processes and the Future of AI in Product Development ', 'text': 'Ford is standardizing processes to shorten the time between idea generation and product impact. Products can leverage good ideas from each other, creating an economy of scale. There is a renaissance in AI and real-time ray tracing in graphics, leading to exciting emerging technologies. The combination of AI and graphics technologies will create a future where creators can \"dance with light\" and have real-time shadows, reflections, and a real-world experience. AI will anticipate and modify itself to make sense based on the creative task at hand, creating an exciting future for creators.'}, {'title': 'Fascination with Snakes and Robotics ', 'text': \"The speaker works in autonomous vehicles and is a roboticist. They have a fascination with snakes, both natural and artificial robots. There are 2,900 species of snakes in the world, with 875 venomous. The speaker's work in computer animation in the 80s led to an interest in simulating snake movement.\"}, {'title': 'The Evolution of Robotics and Animation ', 'text': \"The idea of animating spring lengths and simulating muscles came from observing the movement of objects. The interest in robotics came from simulation and graphics work. The individual also made radio controlled chips from scratch as a child. The individual did a paper in 1988 called The Motion Dynamics of Snakes and Worms, based on physiology literature and early computer animation examples. The individual worked on a movie called Her Majesty's Secret Serpent, which involved a secret agent snake parachuting in to capture a film canister from a satellite. The individual's interest in robotics led to the idea of building a real robot.\"}, {'title': 'Creating Lifelike Snake Robots: A 15-Year Obsession ', 'text': \"The speaker had a 15 year obsession with building better snake robots. The first snake robot built could only slither sideways, but didn't go forward. The speaker added wheels to the snake robot to address friction issues. The speaker loves creating the illusion of life, which drove them to animation. The goal is to create a robot that moves in a biological way, resembling a creature rather than a thing. The early snake robot was able to sidewind and go directly forward. The snake robot was used as the ring bearer at the speaker's wedding. The project was initially done as a hobby.\"}, {'title': 'Development of Autonomous On-Board Computing ', 'text': 'The development of autonomous on-board computing was limited at the time. The first controller was built from discrete logic due to limited computing capabilities. Radio controlled models were used for the second and third versions. The focus was on physicality and coordinated motion. There was a sidestep into creating a cheap toy, which taught lessons about clockwork and backlash.'}, {'title': 'The Evolution of Robot Building and Learning ', 'text': \"The text discusses the experience of building and learning from different versions of robots, such as S9 and S5. The engineer tapers the snakes for both mechanical reasons and to make them look more biological, which is not typical for most engineers. The S5 robot is currently on display at the International Spy Museum in Washington, DC. The S3 robot's motors wore out and it no longer works, with no replacements available. The engineer reflects on the humbling experience of realizing that what seemed like a good idea didn't work as expected.\"}, {'title': 'Spy Museum in Washington, DC ', 'text': \"['Spy Museum in Washington, DC.', 'Conspiracy theory about the museum being fake.', 'Use of Raspberry Pi for onboard compute.', 'Addition of vision accelerator chips for object recognition.', 'Convergence of hobby work and professional work.', 'Potential for true autonomy with onboard compute and batteries.']\"}, {'title': 'The Impact of Robotics and Technology on Children ', 'text': 'Autonomy with onboard compute, onboard batteries, and biomimetic quality appeals to children. Lectures at Girls Who Code to encourage interest in technology. Robotics are still more expensive than the value they add. Research is a license to be curious.'}, {'title': 'The Curiosity of Research in Intelligent Agent Development ', 'text': 'Being in research is a license to be curious. Hobby of reading biology and being curious about things. Trying to bring life and beauty into something inanimate. Convergence of intelligent agent research with vision and voice synthesis. Aim for meaningful conversation with intelligent agents, not necessarily human level intelligence.'}, {'title': 'The Importance of Meaningful Interaction and Reasoning in Robot Pet Ownership ', 'text': 'The goal is to have a robot pet owner understand what the robot thinks about and can reason about. Meaningful interaction with the robot is important, similar to the interaction one might have with a dog. The reasoning system of the robot should be able to explain why it knows or thinks something. The robot serves as a muse for thinking about the future of AI.'}, {'title': 'The Future of AI and Virtual Reality ', 'text': 'The robot is the muse for thinking about the future of AI and what to invent next. Bringing virtual objects into the physical world through augmented reality is more likely than building intelligent robots. Many ideas that might take five years to build a robot to do can be done in a few weeks with digital assets. Living with virtual personalities for a long time will make intelligent robots less surprising when they become commonplace.'}, {'title': 'The Future of Technology and Poetry ', 'text': \"The speaker sees a world where technology like Siri and Alexa have physical forms. The convergence of different strands of the speaker's career is exciting. The speaker ends with a favorite poem that ponders mortality and immortality. The poem ends with a beautiful message about contentment and the continuation of ideas through others.\"}, {'title': 'Title ', 'text': 'A Message of Gratitude and InspirationText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 23:27:01.961402 ...\n",
      "Best SD: inf, Best iteration: 0\n",
      "done get topics 2024-03-25 23:27:02.822791.\n",
      "Stage 2 start time 2024-03-25 23:27:02.822812\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Adobe Research's Efforts to Automate Creative Tasks\n",
      "2. Advancements in Image Captioning and Synthesis\n",
      "3. Distinguishing Academic Research from Industrial Research\n",
      "4. Exciting Applications of Computer Vision and Machine Learning by Adobe\n",
      "5. Data Sharing and Workflow Improvement\n",
      "6. Advancements in 3D Computer Vision Algorithms\n",
      "7. Importance of Designing Content for Product Evolution\n",
      "8. The Shift to a Visual Culture\n",
      "9. The Benefits of Internships in Research\n",
      "10. Advancements in Generative Adversarial Networks\n",
      "11. Development of Autonomous On-Board Computing\n",
      "Stage 2 done time 2024-03-25 23:28:26.910965\n",
      "stage_2_titles: len: 11\n",
      "[\"1. Adobe Research's Efforts to Automate Creative Tasks\", '2. Advancements in Image Captioning and Synthesis', '3. Distinguishing Academic Research from Industrial Research', '4. Exciting Applications of Computer Vision and Machine Learning by Adobe', '5. Data Sharing and Workflow Improvement', '6. Advancements in 3D Computer Vision Algorithms', '7. Importance of Designing Content for Product Evolution', '8. The Shift to a Visual Culture', '9. The Benefits of Internships in Research', '10. Advancements in Generative Adversarial Networks', '11. Development of Autonomous On-Board Computing']\n",
      "remove_questions start time: 2024-03-25 23:28:26.933911\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 23:35:44.309840\n",
      "chunks_text len: 101\n",
      "extract_keypoints start time: 2024-03-25 23:35:44.309958\n",
      "extract_keypoints done time 2024-03-25 23:39:53.862649\n",
      "Start time: 2024-03-25 23:39:53.862938\n",
      "Stage 1 done time 2024-03-25 23:43:46.722051\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Ilya Sotskever: Cofounder and Chief Scientist of OpenAI ', 'text': 'Ilya Sotskever is the cofounder and chief scientist of OpenAI, and one of the most cited computer scientists in history with over 165,000 citations. He is considered one of the most brilliant and insightful minds in the field of deep learning. The conversation was recorded before the outbreak of the pandemic. The speaker sends love and support to those affected by the medical, psychological, and financial burden of the crisis. The Artificial Intelligence Podcast is mentioned, with options to subscribe, review, support, or connect on Twitter. The speaker will do a few minutes of ads now.'}, {'title': 'The Rise of Cryptocurrency and Cash App ', 'text': 'Twitter handle is @lexfriedman, spelled F R I D M A N. The show is presented by Cash App, the number one finance app in the App Store. Cash App allows you to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Cryptocurrency is still in its early days of development, with the first decentralized cryptocurrency, Bitcoin, released just over 10 years ago. The history of money, including cryptocurrency, is fascinating and recommended to learn about.'}, {'title': 'The Early Development of Cryptocurrency and Contributions to Deep Learning ', 'text': 'Cryptocurrency is still in its early days of development and aims to redefine the nature of money. Cash App is offering a promotion where using the code LEXPODCAST gets you $10 and Cash App will donate $10 to FIRST, an organization advancing robotics and STEM education. Ilya Satsgever was one of the authors of the AlexNet paper, which marked the big catalytic moment that launched the deep learning revolution. In 2010 or 2011, Ilya Satsgever connected two facts in his mind, leading to a significant development.'}, {'title': 'Training Deep Neural Networks with Backpropagation ', 'text': 'Deep neural networks can be trained end to end with backpropagation. James Martens invented the Hessian free optimizer in 2010 and trained a 10 layer neural network end to end without pre training from scratch. The ability to train a big neural network means it can represent very complicated functions. A neural network with 10 layers allows the human brain to run for some number of milliseconds, as neuron firings are slow.'}, {'title': 'Neural Network Training and Overparameterization ', 'text': 'Neuron firings are slow and only fire 10 times in 100 milliseconds. The idea of training a very big neural network on lots of supervised data was already present. The theory that having more data than parameters prevents overfitting is incomplete. Neural networks being heavily overparameterized was not discouraging. There was evidence before that having a huge number of parameters was okay.'}, {'title': 'Challenges and Innovations in Training Big Neural Networks ', 'text': 'The theory was that with a big data set and a big neural net, it was going to work. Overparameterization was not seen as a problem. The main doubt was whether there would be enough compute to train a big enough neural net. Backpropagation was thought to work. Alex Kerchevsky wrote fast CUDA kernels for training convolutional neural nets, leading to the decision to proceed with the project.'}, {'title': 'The Influence of the Human Brain on Neural Network Development ', 'text': 'Intuition from empirical results is important in demonstrating the capabilities of a program. The connection between artificial neural networks and the human brain is a source of inspiration for deep learning researchers. The idea of a neural network is directly inspired by the brain, as seen in the work of Rosenblatt in the 60s. The concept of neurons in the brain has influenced the development of neural networks in computer science.'}, {'title': 'The Evolution of Neural Networks and Their Relationship to the Human Brain ', 'text': \"The idea of using ideas from the computer and automata to design a computational object similar to the brain. The invention of the neuron inspired by the computer and automata. The development of the convolutional neural network and its success in image recognition. The assumption that an artificial neuron is not that different from the brain if it's cleaned hard enough. The success of deep learning and the interesting differences between the human brain.\"}, {'title': 'The Difference Between Human Brain and Artificial Neural Networks ', 'text': 'The difference between the human brain and artificial neural networks is interesting for the next decade or two. Artificial neural networks have important advantages over the brain. The brain uses spikes, which may or may not be important.'}, {'title': 'Understanding the Architectural Differences in Neural Networks ', 'text': 'The architectural difference between artificial neural networks is a big factor. Spiking neural networks need to simulate non-spiking neural networks in spikes to work. Questions around back propagation and deep learning are connected to the functioning of neural networks. The effectiveness of a giant neural network is not self-evident, especially for beginners in the field.'}, {'title': 'The Significance of Cost Functions in Training Neural Networks ', 'text': \"Neural networks were proposed as a great idea because the brain is a neural network. The challenge was to figure out how to train neural networks, which led to the big idea of the cost function. The cost function is a way of measuring the performance of the system according to some measure. The concept of a single cost function may seem trivial now, but it was a significant idea at the time. There may be other things that don't necessarily have a cost function, or may have many cost functions.\"}, {'title': 'The Cost Function in GANs ', 'text': \"GANs do not have a clear cost function. GANs operate based on a game and reason about the system's behavior in terms of the equilibrium of the game. The behavior of the system is reasoned about using mathematical objects. The cost function in GANs is emergent.\"}, {'title': 'Understanding the Emergence and Evolution of Cost Functions in Deep Learning ', 'text': \"GAN's cost function is emergent from the comparison and may not be meaningful to talk about in the same way as traditional cost functions. The analogy of a cost function in biological evolution or the economy may not be the most useful for understanding GAN's cost function. Questioning whether cost functions in deep learning are holding us back and if they are a good idea that we will move past. Self play in reinforcement learning systems may provide insight into the concept of cost functions in deep learning.\"}, {'title': 'The Importance of Self Play, Exploration, and Cost Functions in Reinforcement Learning Systems ', 'text': 'Self play and exploration are important in reinforcement learning systems. Cost functions are considered great and serve well in various applications. There may be potential for new ways of looking at things that involve cost functions in a less central way. Spiking and the learning rule of the brain are areas of interest for potential usefulness.'}, {'title': 'Neuroscientists Discover Spike Time Independent Plasticity as a Learning Rule of the Brain ', 'text': 'Neuroscientists have figured out spike time independent plasticity as a learning rule of the brain. STD is a particular learning rule that uses spike timing to update synapses. The timing of signals is a fundamental property of the brain. The temporal dynamics is not captured in the current understanding of learning rules in the brain.'}, {'title': \"The Role of Recurrent Neural Networks in Mimicking the Brain's Timing Mechanism \", 'text': \"The brain's fundamental property is the timing of signals. Recurrent neural networks are a simplified version of the brain's timing mechanism. The brain is a continuous version of recurrent neural networks, allowing for all possible timings and containing information within those timings. Recurrent neural networks are amazing and can potentially perform any task. Recurrent neural networks have been superseded by transformers, but there is a possibility of a comeback in the future.\"}, {'title': 'Advancements in Language Processing and Modeling ', 'text': \"Breakthroughs in natural language processing and language modeling have been with transformers that don't emphasize recurrence. Recurrent neural networks are still possible for processing sequences. Expert systems and symbolic AI also involved maintaining a hidden state.\"}, {'title': 'The Future of Building Large Scale Knowledge Bases within Neural Networks ', 'text': 'Symbolic AI involves maintaining a hidden state as its knowledge base and growing it through sequential processing. The hidden state in symbolic AI is similar to the hidden state in LSTM or RNN. In the expert system analogy, knowledge is stored in the connections and short-term processing is done in the hidden state. There is potential for building large scale knowledge bases within neural networks. The speaker wants to explore the future of building large scale knowledge bases within neural networks.'}, {'title': 'The Rise of Neural Networks in Deep Learning ', 'text': \"Neural networks have been around for many decades, but were underestimated before deep learning started to be successful. People who worked in machine learning simply didn't think that neural networks could do much. People didn't believe that large neural networks could be trained. There was a lot of debate going on in machine learning about what are the fundamental thing behind deep learning.\"}, {'title': 'Challenges in Machine Learning Methods and Benchmarks ', 'text': 'Debate in machine learning about the right methods and benchmarks. Lack of hard facts and benchmarks in machine learning. Deep learning ideas were present but lacked supervised data and compute. Conviction needed to apply existing methods with a lot of data and compute. The missing piece was the combination of data, compute, and conviction.'}, {'title': 'The Role of Data, Compute, and Conviction in AI Development ', 'text': \"The missing piece for making AI work was the combination of data, compute (GPUs), and the conviction to mix them together. The presence of compute and supervised data allowed the empirical evidence to convince the majority of the computer science community. ImageNet served as a convincing moment and represented a shift in the computer vision community. It's not enough for ideas and compute to be present, they also need to convince the cynicism.\"}, {'title': 'Progress in AI and the Importance of Hard Benchmarks ', 'text': 'Neural networks faced skepticism and disbelief for decades. Hard tasks and benchmarks are necessary to produce undeniable evidence and make progress in the field of AI. The field of AI is making progress today due to the presence of hard benchmarks representing true progress. The contribution of recent ideas in AI includes computer vision, language, and natural language processing.'}, {'title': 'AI Ideas and Principles in Computer Vision, Language, and Reinforcement Learning ', 'text': 'AI ideas in computer vision, language, natural language processing, reinforcement learning. Fundamental science of deep learning. Machine learning has a lot of unity and overlap of ideas and principles. Simple principles apply in almost the same way to different modalities and problems.'}, {'title': 'The Evolution of Architectures in Computer Vision and NLP ', 'text': \"Computer vision and NLP are very similar to each other. They have slightly different architectures, using transformers in NLP and convolutional neural networks in vision. It is possible that one day everything will be unified with a single architecture. In natural language processing, there were a huge number of architectures for every different tiny problem, but today there's just one transformer for all those different tasks. There has been a trend towards unification and simplification of architectures over time in AI.\"}, {'title': 'Fragmentation and Unification in AI ', 'text': 'Fragmentation and specialization in AI has been subsumed by deep learning, leading to unification. Vision is expected to become unified with natural language. Convolutional neural net is computationally efficient, while RL requires different techniques due to the need for action and exploration. There is potential for broader unification between RL and supervised learning, with RL making decisions to improve supervised learning.'}, {'title': 'Reinforcement Learning in Supervised Learning Improvement ', 'text': 'RL making decisions to improve supervised learning. RL as a big black box that figures out what to do with input. Reinforcement learning combines aspects of language and vision. Utilizing long term memory and rich sensory space. RL interfaces and integrates with language and vision. Non-stationary world in reinforcement learning.'}, {'title': 'The Dynamics of Problem Solving in Reinforcement Learning and Traditional Static Problems ', 'text': 'The world is non-stationary, leading to changing actions and perceptions. Traditional static problems involve applying a model to a given distribution. Commonality between reinforcement learning and traditional static problems, such as using gradients and neural nets. Small differences exist between reinforcement learning and traditional static problems.'}, {'title': 'The Importance of Language and Problem Evaluation ', 'text': 'Language is fundamental to everything according to Noam Chomsky. The question of whether a problem is hard is slightly wrong. Effort required to reach human level performance on a benchmark is important. Perspective and frame of reference play a role in evaluating problems.'}, {'title': 'The Difficulty of Solving Hard Problems ', 'text': 'Solving a problem makes it stop being hard. The difficulty of a task depends on the capabilities of our tools. Human level language understanding and visual perception are currently hard problems. Language understanding may be harder than visual perception, depending on the definition of \"top notch\" understanding. The difficulty of language understanding also depends on how it is defined.'}, {'title': 'Interconnectedness of Vision System and Language in the Human Brain ', 'text': 'Vision system and language are interconnected in the human brain. Chomsky suggests that language is the starting point for understanding vision. There may be a fundamental hierarchy of ideas represented in the brain through language. It is possible that achieving deep understanding in images and language requires the same kind of system. Machine learning may be capable of achieving deep understanding in both images and language. There is uncertainty about the ability of machine learning to achieve deep understanding in both images and language.'}, {'title': 'The Importance of Definitions in Determining Value ', 'text': \"The importance of definitions in determining the value of reading and vision. The author's definition of a system being impressive. The author's belief in the continuous ability of humans to surprise and impress. The author's preference for monogamy and the idea of continuous surprise and pleasure in a long-term relationship.\"}, {'title': 'The Influence of Friends and Internet Content on Inspiration and Understanding ', 'text': 'Friends continue to surprise with injection of randomness, a source of inspiration, wit, and humor. Impressing with intelligence and understanding of images has not been achieved by systems as of January 2020. People click like on internet content for humor, wit, and insight.'}, {'title': 'The Beauty of Deep Learning ', 'text': 'The most beautiful thing about deep learning is that it actually works. The idea that making neural networks larger and training them on a lot of data can replicate the function of the brain is surprising and beautiful. It is unbelievable that AI with neural networks works. There is a curiosity about the intuition and insights behind why this whole thing works.'}, {'title': 'The Importance of Empirical Evidence in Optimization, Evolution, and Physics ', 'text': 'Optimization has empirical reasons to believe that it should work on most problems we care about. Evolution is empirical and shows that the evolutionary process is a good way to design organisms that survive in their environment. Physics calculations and experiments are important in coming up with new physics theories and making predictions. Understanding how the whole thing works requires more than just empirical evidence.'}, {'title': 'The Intersection of Biology and Physics in Deep Learning ', 'text': 'Deep learning is a combination of biology and physics. The experiment sometimes comes before the theory in deep learning. The validation of a theory in deep learning is similar to biology rather than mathematics. Deep learning involves making predictions based on data and using neural networks to improve performance. The complexity of biology makes it challenging to fully understand its representation in deep learning.'}, {'title': 'The Potential of Machine Learning to Unify Biology and Physics ', 'text': 'Biology is complicated and lacks good predictive theories. Physics has super precise theories that make amazing predictions. Machine learning is in between biology and physics and has the potential to unify the two. There is still underestimation of deep learning. Most of the progress in the past 10 years has been in a few cases.'}, {'title': 'Advancements in Deep Learning ', 'text': 'Deep learning has consistently exceeded expectations over the past 10 years. The field continues to make robust progress and will likely do so for quite a while. Individual researchers may find it challenging due to the large number of researchers in the field. Having access to a lot of computing power can lead to interesting discoveries in deep learning.'}, {'title': 'Challenges of Managing a Compute Cluster for Deep Learning Experiments ', 'text': 'Managing a huge compute cluster is a challenge for running experiments. The stack of deep learning is starting to be quite deep, from ideas to building data sets to distributed systems. There are many unanswered questions in the field. The speaker is seeking advice from someone they consider to be very smart. The use of one GPU is mentioned. The speaker expresses uncertainty about certain aspects of their work.'}, {'title': 'The Complexity of the Data Science and Programming Stack ', 'text': 'The stack for data science and programming is becoming increasingly deep and complex. It is challenging for a single person to become world class in every layer of the stack. Efficient learning will be important in navigating the complexity of the stack. There will be breakthroughs that do not require a huge amount of compute. Some breakthroughs and building systems will require a huge amount of compute.'}, {'title': 'The Importance of Compute in Neural Networks ', 'text': 'The amount of compute is important for neural networks. Small groups and individuals can still do important work in deep learning. Making neural networks larger can lead to better performance, which goes against statistical ideas. Double descent occurs for most practical deep learning systems.'}, {'title': 'Double Descent Phenomenon in Deep Learning ', 'text': 'Double descent occurs for pretty much all practical deep learning systems. Increasing the size of the neural network slowly, without early stopping, leads to a rapid increase in performance, followed by a decrease at the point of zero training error, and then an increase in performance again. This phenomenon is counterintuitive and goes against the expectation of deep learning phenomena to be monotonic.'}, {'title': 'The Non-Monotonic Nature of Deep Learning ', 'text': \"Deep learning phenomena are not always monotonic. Overfitting occurs when the model is sensitive to small random unimportant stuff in the training data. A small model with a large data set is insensitive to randomness and has little uncertainty. It is surprising that neural networks don't overfit quickly before being able to learn anything.\"}, {'title': 'Optimizing Neural Networks with Stochastic Gradient Descent ', 'text': 'Neural networks with a huge number of parameters can achieve zero error in a big subspace. Stochastic Gradient Descent (SGD) can find the point with the smallest norm in that subspace. This method is insensitive to small randomness in the data when the dimensionality is high. When the dimensionality of the data is equal to the dimensionality of the model, there is a one-to-one correspondence between all the data sets and the models. Small changes in the data set can have a significant impact in this scenario.'}, {'title': 'The Impact of Early Stop on Model Performance ', 'text': \"['Small changes in the data set lead to large changes in the model, resulting in worse performance.', 'It is best for the model to have more parameters than the data, but only if early stop is not introduced.', 'Introducing early stop in regularization can make the double descent bump almost completely disappear.', 'Early stopping is when you train your model and monitor validation performance, stopping training when validation performance starts to get worse.', 'The magic happens after the moment of early stopping, and not doing early stopping results in a very pronounced double descent.']\"}, {'title': 'The Impact of Degrees of Freedom on Data and Model Sensitivity ', 'text': 'When the data set has as many degrees of freedom as the model, there is a one to one correspondence between them. Small changes to the data set lead to noticeable changes in the model when the data set has as many degrees of freedom as the model. When you have a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. The resulting solution is able to discard the small changes and the randomness. Jeff Hinton suggested throwing away back propagation and starting over.'}, {'title': 'Challenges and Considerations in Training Neural Networks ', 'text': 'The suggestion to throw away back propagation and start over was made, but it was also acknowledged that back propagation is very useful and should be kept. The idea of finding alternative methods of training neural networks was discussed, with a focus on learning from how the brain learns. The importance of implementing any discovered mechanisms of learning in the brain into neural networks was highlighted. The speaker expressed their personal support for back propagation and its usefulness.'}, {'title': 'The Role of Back Propagation and Neural Networks in Solving Fundamental Problems ', 'text': \"Back propagation is a great algorithm for solving fundamental problems in finding neural circuits subject to constraints. The problem of finding neural circuits subject to constraints is unlikely to go away, making back propagation a valuable algorithm. AlphaZero's neural network plays Go better than 99.9% of humans, demonstrating the ability of neural networks to reason. The example of AlphaZero provides an existence proof that neural networks can reason. The ability of neural networks to reason is demonstrated through the game of Go, which is widely recognized as requiring reasoning.\"}, {'title': 'The Nature of Reasoning in Go and Neural Networks ', 'text': 'Go is reasoning and has elements of reasoning. Reasoning is akin to search, with a sequential element and stepwise consideration of possibilities. Playing Go and using a single neural network without search is akin to reasoning. There is an existence proof in a particular constrained environment that a process akin to reasoning exists. Humans are another existence proof of reasoning. The architecture that will allow neural networks to reason is still being considered.'}, {'title': 'The Potential for Neural Networks to Reason ', 'text': 'Neural networks may be able to reason in the future. The architecture for neural networks to reason may be similar to current architectures, but possibly more recurrent and deeper. Neural networks are powerful and have the potential to produce reasoning breakthroughs. Humans can reason, so there is potential for neural networks to do the same. The kind of tasks neural networks have been trained on may be a form of weak reasoning. There is uncertainty about whether neural networks can truly reason.'}, {'title': 'The Capabilities and Limitations of Neural Networks ', 'text': \"Neural networks are capable of reasoning. Training a neural network on a task that doesn't require reasoning will not result in reasoning. Neural networks will solve problems in the easiest way possible. Neural networks are described as the search for small circuits. General intelligence is described as the search for small programs. Finding the shortest program that outputs the data at your disposal allows for the best prediction possible. Finding the shortest program which generates some data is not a computable operation.\"}, {'title': 'Understanding the Limitations and Advantages of Neural Networks ', 'text': 'Finding the shortest program to generate data is not computable. Neural networks are the next best thing that works in practice. We are able to find a small or large circuit which fits our data in some way. The concept of overparameterized neural nets and the transmission of entropy from the dataset to the parameters is important in understanding the training process.'}, {'title': 'The Importance of Training in Deep Learning ', 'text': 'The amount of information in the weights ends up being not very large, which would explain why they generalize so well. The large circuit might be helpful for generalization. The fundamental reason for pushing on deep learning is that we are able to train them. Training comes first, and contorting neural networks around the training pillar is essential. Being trainable means starting from scratch and quickly converging towards knowing a lot.'}, {'title': 'Training a Neural Net for Program Finding ', 'text': 'Training a neural net from scratch can lead to quickly gaining a lot of knowledge. The resources at your disposal can be used to train a neural net to achieve useful performance. Finding the shortest program is not feasible, so it cannot be done. There are no good precedents of people successfully finding programs really well. Training a deep neural network is the right way to find programs.'}, {'title': 'The Power of Deep Neural Networks ', 'text': 'Training a deep neural network is the right way to go about it. There are not good illustrations of training a deep neural network yet. It is unwise to bet against deep learning. Neural networks continue to surprise us. Being able to aggregate important information over long periods of time is useful.'}, {'title': 'Neural Nets as Long-Term Knowledge Bases ', 'text': 'Neural nets experience serves as long term knowledge. Various neural nets have been trained to act as knowledge bases. There is work on investigating language models as knowledge bases. The need for a better mechanism of forgetting useless information and remembering useful information. Lack of mechanisms for remembering long term information.'}, {'title': 'The Challenge of Interpreting Knowledge in Neural Networks ', 'text': \"The text discusses the concept of compression of information in knowledge bases. It mentions the difficulty in interpreting the knowledge discovered by neural networks. The example of knowledge bases like Wikipedia is used to illustrate the concept of a compressed, structured knowledge base. The text also mentions the noninterpretable nature of neural networks' outputs.\"}, {'title': 'Improving Interpretability and Self-Awareness in Neural Networks ', 'text': \"Neural networks like language models can be made interpretable by asking them to generate text. The text generated by neural networks is generally interpretable, but there is room for improvement. The goal is to make the neural network self-aware, so it can recognize its own limitations and strengths. Self-awareness in neural networks can lead to better decision-making and skill improvement. Currently, generating a lot of examples and using human judgment is the only way to assess the neural network's capabilities. The speaker believes in the importance of self-awareness in neural networks for optimal skill development.\"}, {'title': 'The Importance of Understanding Investment for Skill Development ', 'text': \"The need for knowing where to invest to increase skills optimally. Two answers to the question of interpretability: analyzing the neurons in a neural net and the human-centric approach. OpenAI has done some work on analyzing the neurons in a neural net. The human-centric approach involves understanding a person's mental model and conception.\"}, {'title': 'Memory and Reasoning in Human and Neural Networks ', 'text': 'Human beings have the ability to remember useful information and forget the rest. Neural networks have a similar process but are not as effective as human beings. Impressive feats of reasoning include writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.'}, {'title': 'The Power of Machine Learning and Deep Learning in Problem Solving ', 'text': 'Proving hard theorems and solving open-ended problems with out-of-the-box solutions. Machine learning and deep learning have the ability to produce unambiguous results that can change the conversation. The debate and conversation change when unambiguous results are produced. There may come a time when we run out of hard problems to solve. The problem of mortality is still a sticky problem that has not been figured out.'}, {'title': 'The Evolution of Language Models and Neural Networks ', 'text': 'The history of language models and neural networks dates back to the 80s with the Elman network. The trajectory of neural networks and deep learning changed with the availability of data and compute power. The size of language models is crucial for their effectiveness, as they need to be large to be good at predicting.'}, {'title': 'The Development of Language Models ', 'text': 'Language models need to be large in order to predict the next word. Initially, language models notice broad strokes and surface level patterns, such as characters and spaces, commas and capital letters. As language models improve, they start to notice common words, spellings, syntax, and eventually semantics and facts. Noam Chomsky disagrees with the idea that incremental steps and larger networks/compute are necessary to reach semantics.'}, {'title': 'Understanding Language Semantics in Larger vs. Smaller Models ', 'text': \"Larger network and compute can understand language semantics without imposing a theory of language onto the learning mechanism. Chomsky's concept of imposing structural language is not fully understood, but larger language models show signs of understanding semantics compared to smaller models. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. Training a small LSTM model on Amazon reviews showed that increasing the size of the model led to better understanding of semantics.\"}, {'title': 'Using LSTM to Capture Sentiment in Amazon Reviews ', 'text': 'LSTM can be used to predict the next character in Amazon reviews. Increasing the size of the LSTM from 500 cells to 4,000 cells can lead to one of the neurons representing the sentiment of the review. Sentiment is a semantic attribute, not a syntactic attribute. Small neural nets do not capture sentiment while large neural nets do. As the size of the neural net increases, the focus shifts from syntax to semantics. The implication is that larger neural nets are better at capturing semantic attributes.'}, {'title': 'GPT2: A Transformer with One and a Half Billion Parameters ', 'text': 'GPT2 is a transformer with one and a half billion parameters that was trained on about 40 billion tokens of text obtained from web pages linked to from Reddit articles with more than three outputs. The transformer is the most important advance in neural network architectures in recent history. The models show signs of partial semantic understanding, but not complete semantic understanding. Smaller models do not show signs of semantic understanding.'}, {'title': 'The Success of the Transformer Model ', 'text': \"The transformer is a combination of multiple ideas simultaneously, of which attention is one. The transformer is successful because it is the simultaneous combination of multiple ideas. The transformer uses a lot of attention, but attention existed for a few years, so it can't be the main innovation. The transformer is designed to run really fast on the GPU, making a huge difference. The transformer is not recurrent, which is important because it is more shallow and therefore much easier to optimize.\"}, {'title': 'Optimizing GPU Usage for Improved GAN Results ', 'text': 'It is a great fit to the GPU and is not recurrent, making it easier to optimize. The combination of factors make it successful in making great use of the GPU. It allows achieving better results for the same amount of compute. It was surprising and amazing to see the text it generated. There has been progress in GANs in improving the samples produced.'}, {'title': 'Advancements in GANs and Language Modeling ', 'text': 'Progress in GANs has been amazing, with realistic faces being produced. Text generation has not seen as much progress as GANs. There was a sudden leap in the quality of GANs from 2015 to the present. The rapid adaptation to new advancements is remarkable. Some cognitive scientists question the true understanding of language by GPT2 models. The economic impact of advancements in language modeling is seen as the next barrier.'}, {'title': 'The Impact of AI Advancements on Global Economy ', 'text': 'The economic impact of AI advancements is not yet fully realized. The progress in AI is difficult for people outside the field to understand and distinguish. There is a lot of brilliant work in languages like Russian and Chinese that the rest of the world is not aware of. Translation is already a huge industry, with billions of people involved.'}, {'title': 'The Impact of Translation, Self-Driving, and Multitask Transformers ', 'text': 'Translation is already huge and hugely positive, with billions of people interacting with big chunks of the internet primarily through translation. Self driving is going to be hugely impactful, and deep learning for self driving is a significant factor. There may be a potential unification towards multitask transformers that can handle both language and vision tasks.'}, {'title': 'The Advancements of GPT and Transformers in Language and Vision Tasks ', 'text': 'GPT and transformers can handle both language and vision tasks. The process of making a transformer perform better involves making it bigger and giving it more data. GPT and transformers are fundamentally simple to explain and train. The next steps with GPT two may involve exploring larger versions and addressing many questions. One question is whether the model can use its own intelligence to decide what data it wants to memorize from the internet.'}, {'title': 'Active Learning and Self-Driving Technology ', 'text': 'The model should use its own intelligence to decide what data to accept and reject, similar to how people are selective about what they learn. Active learning would be beneficial for the model. Companies may keep private breakthroughs to themselves in the space of active learning and self-driving technology. The fundamental problem needs to be solved in order to achieve breakthroughs in self-driving and other specific tasks.'}, {'title': 'The Importance of Active Learning in Problem-Solving ', 'text': 'Active learning requires a problem that requires it. Research about capability is difficult without a task. Results on MNIST or clever formulations of MNIST are no longer convincing. Active learning will naturally arise as problems that require it pop up.'}, {'title': 'Concerns about the Potential Risks of Releasing Powerful AI Systems ', 'text': 'OpenAI has brought up concerns about the potential detrimental effects of releasing powerful artificial intelligence systems like GPT2. There is nervousness about the possible negative uses of a model that can generate realistic text, such as being used by bots in ways that are currently unimaginable. The speaker commends the bravery and profundity of starting a conversation about the potential risks of powerful AI systems. The speaker released a report on the topic and is seeking insights from the experience.'}, {'title': 'The Evolution and Impact of AI ', 'text': 'AI is transitioning from a state of childhood to a state of maturity. The impact of AI is large and growing. It is wise to consider the impact of releasing AI systems before doing so. A staged release for models like GPT2 seemed logical. The results of GPT2 were stunning and it seemed plausible that it could be used to reduce the cost of information.'}, {'title': 'Responsible Release of Powerful Models ', 'text': 'A staged release of the model was logical and allowed for observation of its usage. Many people used the model in cool ways with no negative applications known. Other people replicated similar models, raising questions about the responsibility of releasing powerful models. There is a moral and ethical responsibility to communicate the potential impact of powerful models, such as the potential for misinformation.'}, {'title': 'The Importance of Collaboration and Trust in AI Development ', 'text': \"GPT2's potential for misinformation was unclear. It's important to talk to other experts outside of your group to get a better understanding. Building trust between companies is important in the development of AI technology. Collaboration and discussion with colleagues elsewhere is necessary to navigate the use of AI models. Ultimately, all AI developers are working together and should consider the impact of their technology.\"}, {'title': 'The Potential Negative Consequences of Powerful AI Systems ', 'text': \"Ultimately, we're all in it together. Consider the potential negative consequences of powerful AI systems. Concern about a race for AI development leading to closed development and lack of idea sharing. The speaker has been a pure academic for 10 years and enjoys sharing ideas. The speaker is uncertain but believes in the potential of deep learning and other small ideas.\"}, {'title': 'The Power of Self Play in AI Learning ', 'text': 'Self play is a powerful mechanism for systems to learn in a competitive setting. Building AGI will require deep learning plus some additional ideas. Self play has the ability to surprise us in truly novel ways. Examples of surprising behaviors from self play systems include Dota bot, multi-agent hide and seek, and alpha zero.'}, {'title': 'The Surprising Creativity of AGI Systems ', 'text': 'AGI systems produce unexpected behaviors, which are creative solutions to problems. The ability of AGI systems to surprise us is an important aspect. AGI systems would fundamentally surprise us with useful solutions to problems. Self-play mechanisms have been used in the game or simulation context. Simulation is a tool with certain strengths.'}, {'title': 'The Use of Simulation in Reinforcement Learning ', 'text': 'Simulation is a tool with strengths and weaknesses that should be used. Criticisms of reinforcement learning include its current results being demonstrated in simulated or constrained environments. Transfer from simulation to the real world is possible and has been exhibited many times. Success in vision has been especially demonstrated in transfer from simulation to the real world. Open AI has demonstrated a robot in the summer.'}, {'title': 'OpenAI Demonstrates Successful Simulation Training of Robot Hand ', 'text': 'OpenAI demonstrated a robot hand trained entirely in simulation. The simulation was trained to be robust to many different things. The robot hand was not trained with a glove or a stuffed giraffe. The success of the robot hand training in simulation has been especially in vision.'}, {'title': 'The Potential of Deep Learning Transfer Capabilities ', 'text': 'The video demonstrates a transfer from the simulated world to the physical world. The transfer capabilities of deep learning are expected to increase in general. Better transfer capabilities will make simulation more useful. Simulation can be used to experience something and learn a moral of the story to carry into the real world.'}, {'title': 'The Importance of Self Awareness, Consciousness, and Adaptation in Human Existence ', 'text': 'The human elements of self awareness, consciousness, fear of mortality, and self preservation in the physical space are important. Having a body is useful for learning new things that cannot be learned without a body. It is possible to compensate for not having a body and still succeed. There are examples of people, like Helen Keller, who were able to compensate for the lack of modalities such as being deaf and blind. The idea of consciousness and its constraints is also important.'}, {'title': 'The Concept of Consciousness and Self-Awareness ', 'text': \"The idea of consciousness and a more constrained version of that is self awareness. It's hard to define consciousness. It's definitely interesting and fascinating. It's possible that our systems will be conscious. Humans are conscious and artificial neural nets may be similar enough to the brain to also be conscious.\"}, {'title': 'Understanding Consciousness: A Comparison of Artificial Neural Nets and the Brain ', 'text': 'There is a comparison between artificial neural nets and the brain in terms of consciousness. The complexity and interest of the brain may be underestimated. There is a question about whether there is some unexplained aspect of the brain. The likelihood of not being able to make progress in understanding consciousness is considered unlikely. The concept of intelligence is mentioned as being poorly defined. Reasoning and memory are discussed in relation to intelligence.'}, {'title': 'The Challenges of Deep Learning Systems ', 'text': 'Deep learning system solving pedestrian tasks like machine translation or computer vision without making mistakes would be impressive. Current deep learning systems may be more accurate than humans but still make a different set of mistakes. Some skepticism about deep learning arises from the nonsensical mistakes it makes.'}, {'title': 'Criticism of AI Models and Comparison to Human Intelligence ', 'text': 'Mistakes in AI models are often criticized as unintelligent, similar to how we criticize other groups of creatures as \"the other\". GPT2 may be smarter than human beings in many aspects, with a broader knowledge base and potentially deeper understanding on certain topics. Humans do not make the same mistakes as AI models, such as autonomous vehicles, and this criticism is likely to continue for various artificial intelligence systems.'}, {'title': 'Assessing the Progress of AI ', 'text': 'The process of analyzing the progress of AI often focuses on one case where the system fails in a big way, leading to public skepticism. It is confusing to judge progress in AI, and people may not be easily impressed by new demonstrations of AI capabilities. The true measure of progress in AI will be when it starts to significantly impact the GDP.'}, {'title': 'The Impact of AI on GDP and the Potential for AGI ', 'text': \"AI moving the needle on GDP is impressive. OpenAI and others may create AGI systems. The speaker would ask questions and try to make the system make a mistake. The speaker would be amazed by the system's lack of mistakes. The speaker would ask broad questions and not limit themselves in talking to the system. The speaker is one of the people who might be in the room where AGI is created. The speaker has talked to a Stalin historian.\"}, {'title': 'The Influence of AGI Systems in the 21st Century ', 'text': \"Abraham Lincoln's quote about testing a man's character with power. The potential power of AGI systems in the 21st century. The idea of AGI being the CEO in an ideal world. The concept of different entities, countries, or cities having a vote in what the AGI should do.\"}, {'title': 'Advancing Democracy with Artificial General Intelligence ', 'text': 'The concept of having an AGI that represents people and carries out their votes is appealing. The idea of having multiple AGIs for different levels of governance (city, country) is proposed. The goal is to take the democratic process to the next level. There is a suggestion that the board can always fire the CEO of the AGI. The possibility of pressing the reset button and rerandomizing parameters is mentioned. The speaker believes it is possible to build AI with these capabilities.'}, {'title': 'Designing AI Systems with a Drive to Help Humans Succeed ', 'text': 'AI systems can be designed to want to be controlled by humans. Similar to human parents, AI systems can be programmed to have a deep drive to help humans flourish. The crucial moment of creating an AGI system is important to consider. Designing an AGI system with a drive to help humans succeed is possible.'}, {'title': 'The Importance of Relinquishing Power in AGI Systems ', 'text': \"The AGI system is a crucial moment. There needs to be a relinquishing of power between the moment and the Democratic board members with the AGI at the head. George Washington relinquished power despite all the bad things he did. He didn't want to be president and didn't serve indefinitely like most dictators. The scenario described sounds terrifying and the speaker would not want to be in that position.\"}, {'title': 'The Importance of Aligning AI Values with Human Values ', 'text': 'The question of whether most people in the AI community are good is important and open-ended. People can be better than we think when it really counts. There is a need to align AI values with human values. The question of continued alignment as AI systems develop is important. The reward function and value function of humans are internal, not external. There are definite ideas on how to train a value function for AI.'}, {'title': 'Training a Value Function for Reinforcement Learning ', 'text': 'Training a value function involves creating an objective perception system. The perception system is trained separately to recognize and internalize human judgments on different situations. The trained component is then integrated as the base value function for a more capable RL system. The question of the objective functions of human existence may be wrong, as it implies an external, objective answer. Instead of seeking an objective answer, it is suggested that we should focus on making the most of our existence and maximizing our potential.'}, {'title': 'Understanding Human Wants and Objective Functions ', 'text': 'Humans want things and their wants create the drives that cause them to have objective functions. Our wants are our individual objective functions, which can change over time. There are underlying factors such as Freudian concepts, fear of death, desire for knowledge, and sexual desires that drive our wants and objective functions.'}, {'title': 'The Meaning of Life and Human Behavior ', 'text': \"The fear of death and the desire for knowledge are fundamental aspects of human behavior. Evolutionary arguments suggest that the objective function of life is to survive, procreate, and ensure the success of one's children. The meaning of life remains unanswered, but humans are part of a larger ancient process. Given our existence, the focus should be on making the most of life and minimizing suffering.\"}, {'title': \"Navigating Life's Decisions and Achievements \", 'text': \"There are choices and decisions made with hindsight that wouldn't have been made. Some regret is experienced, but solace is taken in doing the best at the time. Academic accomplishments and breakthroughs are a source of pride and gratitude. The source of happiness and pride is not solely based on achievements.\"}, {'title': 'The Importance of Perspective and Gratitude in Finding Happiness ', 'text': 'Happiness comes from the way we look at things. Happiness can come from simple things like a meal or a conversation. Being humble in the face of uncertainty is also a part of happiness. The meaning of life and discussions of happiness are important. The interviewee is grateful for the conversation and the ideas shared.'}, {'title': 'Promoting Cash App and Engaging with the Audience ', 'text': \"The speaker thanks the listener for talking and stopping by. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast by downloading Cash App and using the code LEXPodcast. The audience is encouraged to subscribe to the podcast on YouTube, review it with five stars on Apple Podcast, support on Patreon, or connect with the speaker on Twitter. The speaker ends with a quote from Alan Turing on machine learning and expresses gratitude for the audience's attention.\"}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 23:43:47.758113 ...\n",
      "Best SD: 3.047950130825634, Best iteration: 4\n",
      "done get topics 2024-03-25 23:43:49.370625.\n",
      "Stage 2 start time 2024-03-25 23:43:49.370646\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI\n",
      "2. The Influence of the Human Brain on Neural Network Development\n",
      "3. The Rise of Neural Networks in Deep Learning\n",
      "4. The Importance of Language and Problem Evaluation\n",
      "5. The Beauty of Deep Learning\n",
      "6. Challenges and Considerations in Training Neural Networks\n",
      "7. GPT2: A Transformer with One and a Half Billion Parameters\n",
      "8. The Impact of AI Advancements on Global Economy\n",
      "9. The Power of Self Play in AI Learning\n",
      "10. Understanding Human Wants and Objective Functions\n",
      "Stage 2 done time 2024-03-25 23:44:56.861671\n",
      "stage_2_titles: len: 10\n",
      "['1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI', '2. The Influence of the Human Brain on Neural Network Development', '3. The Rise of Neural Networks in Deep Learning', '4. The Importance of Language and Problem Evaluation', '5. The Beauty of Deep Learning', '6. Challenges and Considerations in Training Neural Networks', '7. GPT2: A Transformer with One and a Half Billion Parameters', '8. The Impact of AI Advancements on Global Economy', '9. The Power of Self Play in AI Learning', '10. Understanding Human Wants and Objective Functions']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "       int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
