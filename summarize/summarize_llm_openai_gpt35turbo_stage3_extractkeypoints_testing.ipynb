{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"testing\" # has rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f04c0848750>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "#     eval_prompt_template_v1 = \"\"\"\n",
    "#     Rewrite the given summary to improve readability.\n",
    "#     Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "#     Remove the reference of 'podcast' in the rewritten summary.\n",
    "#     The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "#     Here is the data:\n",
    "#     {summary}\n",
    "\n",
    "#     Return your answer in the following format:\n",
    "#     REWRITTEN_SUMMARY\n",
    "#     \"\"\"\n",
    "    \n",
    "    #v2\n",
    "#     eval_prompt_template_v2 = \"\"\"\n",
    "#     You will rewrite the given summary to improve readability.\n",
    "#     Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "#     Remove the reference of 'podcast' in the rewritten summary.\n",
    "\n",
    "#     Here is the given summary:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v3 (bad)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a speech on a summary of the given text.\n",
    "#     You will rewrite the given text to improve readability, so you can present it as a speech in a fluent and professtional way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v4 bad\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a narrator of the given text.\n",
    "#     You will rewrite the given text so that so you can narrate it in a fluent, cohesive, and coherent way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"    \n",
    "    \n",
    "#     #v5\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will rewrite the given text to improve readability, so you can present it as a speech in a fluent, cohesive, coherernt, and professtional way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"      \n",
    "    \n",
    "    #v6 (good)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You should add transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "\n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v6b\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a podcast host who speaks about the given text to the audience.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You should add transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "\n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\" \n",
    "    \n",
    "    #v6c\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a podcast host who narrates the given text.\n",
    "   \n",
    "#     Here is the given text that you will narrate:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v7\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph will start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v8 (bad)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, once in a while you can start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph can start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v9 doesnt work\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     Do not use the same transitional word more than twice in the whole text.\n",
    "#     The last paragraph will start with words like 'Finally' or 'Lastly' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "\n",
    "    #v10 doesnt work\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     In the given text, you will rewrite the first sentence of each paragraph but still keep the same meaning and all information.\n",
    "    \n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\"  \n",
    "    \n",
    "    #v11 not good\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words throughout the given text as you see fit to improve its cohesiveness.\n",
    "#     For example, once in a while you can start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph can start with words like 'Finally' or 'Lastly' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"         \n",
    "\n",
    "    #v12\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a narrator to narrate the given text.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You will keep all the original content of the given text.\n",
    "    \n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\" \n",
    "    \n",
    "    #v13\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Your role is a narrator to narrate the given text.\n",
    "    You will improve the given text on fluency and cohesiveness.\n",
    "    You will keep all the original content of the given text.\n",
    "    \n",
    "    Here is the given text that you will improve:\n",
    "    {summary}\n",
    "    \"\"\"     \n",
    "    \n",
    "    \n",
    "    print(\"rewrite_summary: \")\n",
    "    print(eval_prompt_template)\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-26 23:41:29.696348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 23:47:04.449552\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-26 23:47:04.449694\n",
      "extract_keypoints done time 2024-03-26 23:49:46.963114\n",
      "Start time: 2024-03-26 23:49:46.963301\n",
      "Stage 1 done time 2024-03-26 23:52:23.527792\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and the Role of Rajat Manga at Google ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work going on in the world in deep learning. TensorFlow is now an ecosystem of tools for the deployment of machine learning in the cloud, on the phone, in the browser, on both generic and specialized hardware. There is a big emphasis on growing a passionate community of developers. Rajat, Jeff Dean, and a large team of engineers at Google Brain are working to define the future of machine learning with TensorFlow 2.0, which is now in alpha. The decision to open source TensorFlow is a definitive moment in the tech industry.'}, {'title': 'The Impact of Open Innovation on Google Brain and TensorFlow ', 'text': \"Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. Google Brain's proprietary machine learning library turned into TensorFlow in 2014, the open source library. The idea of deep learning was intriguing and held promise, even before it had taken off.\"}, {'title': \"Google's Research Advancements in Speech and Image Recognition \", 'text': 'The idea was to scale research work to Google\\'s compute power and data. Scaling the compute and data showed better results. Early wins were achieved in speech and image research. Collaboration with the speech research team was successful. The \"cat paper\" was a significant achievement in image research.'}, {'title': 'Evolution of Machine Learning at Google ', 'text': 'Google Brain was born around neural networks and focused on deep learning from the beginning. Google started scaling machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines. Google has been doing machine learning for a long time.'}, {'title': 'The Rise of Deep Learning at Google ', 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact lots of things. Real products started wanting to use deep learning, such as speech and image recognition. Academia also started to push for more deep learning. By 2014, it was clear that deep learning was a big thing and would continue to grow. The decision to open source TensorFlow was made.'}, {'title': 'The Impact of Going Open Source with TensorFlow ', 'text': \"The decision to go open source with TensorFlow is considered a seminal moment in software engineering. Google's decision to take a large project and go open source with it led the entire world in embracing open innovation. The initial idea to go open source came from Jeff, who was a big proponent of this. The decision to go open source was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'The Growth of Deep Learning and Machine Learning ', 'text': \"Deep learning and machine learning have grown rapidly due to sharing research and the development of software libraries. Existing libraries like Tiano and Torch were developed by academia, but there was a need for software at a different level. Google had developed internal software and published papers, leading to successful open source projects. Hadoop was developed from technology built by the speaker's team, which they believed was superior for various reasons.\"}, {'title': \"Google Cloud's Bigtable and HBase APIs and Integrations with TensorFlow \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. Cloud fits into the strategy by offering integrations with other resources and ensuring everything works well. TensorFlow is open source and can be used anywhere. Google Cloud offers lots of integrations with everything else and ensures it works well there.'}, {'title': 'The Evolution of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. The incredible ecosystem surrounding TensorFlow. The timeline of TensorFlow development, from summer 2014 to open sourcing in November 2015. The decision to open source TensorFlow made in late 2014. The fast pace of development in deep learning.'}, {'title': 'TensorFlow: Supporting Large Scale and Mobile Device Deployment ', 'text': 'TensorFlow was designed to support running at large scale in the data center and on different kinds of hardware, including GPUs. The design of TensorFlow included support for running models on mobile devices and customizing code. There were already ideas of running machine learning on mobile phones at that time. The goal was to ensure that TensorFlow could support running complicated algorithms on the phone.'}, {'title': 'Running Machine Learning on Mobile Devices ', 'text': 'Ideas of running machine learning on the phone existed at that time. Customized handcrafted code or internal libraries were used for running machine learning on the phone. The use of Theano and Caffe at Google influenced design decisions. The belief was built in parallel with the development of libraries like Theano. The systems at Google were very different, so the focus was on internal development. Multiple libraries were considered before making design decisions.'}, {'title': 'Choosing a Deep Learning Library ', 'text': 'By the time we got to this, we looked at a number of libraries that were out there. The group had experience with Torch, with Lua, and Caffe. They discussed ideas around whether to have a graph or not. They wanted flexibility due to the fast-moving research and changing hardware. The flexibility in terms of being able to express all kinds of crazy things was a key decision.'}, {'title': 'The Evolution of TensorFlow 2.0 and the Shift to Default Eager Execution ', 'text': 'The move towards TensorFlow 2.0 includes default eager execution. The decision to hide the graph was made to make development more intuitive. The graph concept was influenced by Theano. The focus on production led to the consideration of using a graph in Python. Experimentation with ideas led to the realization that not having a graph made things simpler to use.'}, {'title': 'Influences on the Deployment of a Graph ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, was unexpected. The need for the product was recognized early on in the research of deep learning. The potential for future growth and enabling more people to use the product was considered after open sourcing. The growth of deep learning was observed after open sourcing.'}, {'title': 'The Growth of Deep Learning and Community Engagement ', 'text': \"Deep learning grew rapidly after open sourcing. The company saw the opportunity to leverage deep learning and deliver on what people want. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company's approach is very community driven. The initial version was 0.6 or 0.5. People initially loved the documentation provided by the company.\"}, {'title': 'The Evolution of Deep Learning from Research to Deployment ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from a research focus to being accessible to developers, leading to new possibilities. The focus shifted towards stability and deployment, as opposed to just research. Planning for version 1.0 involved addressing the needs of stability and deployment. Enterprises showed increasing interest in the product as it progressed.'}, {'title': 'Enterprise Adoption of Product ', 'text': 'The excitement of getting more enterprises to buy in and support the product. The increase in enterprise adoption post 1.0 and over the next few releases. The initial release and 1.0 being more for researchers and early interest, while 1.x saw lots of enterprises getting on board. The pressure for stability from enterprises, especially before 1.0. The importance of understanding what enterprises want in the midst of development.'}, {'title': 'Title ', 'text': 'The Importance of Stability and Simplicity in Model SelectionText '}, {'title': 'Advancements in AI: RL and GANs ', 'text': 'The combination of RL and GANs is pushing the state of the art in AI. Older AI models, such as ResNet 50, are still very usable and stable. Transfer learning on specific problems is a common use case for AI. Making AI as easy as possible for hobbyists is important. The majority of the world uses AI for transfer learning on specific problems. The use of AI in apps and phones is a common case. The presentation of AI looks great on slides.'}, {'title': 'Importance of Data Analysis and TensorFlow Extended in Enterprise Predictive Modeling ', 'text': \"Enterprises have data that they want to make predictions on, often using regression models, linear models, or gradient booster trees. Some enterprises still benefit from deep learning, especially with large data sets. The audience's needs for data analysis may vary, with some focusing on structured data and others on deep learning. The TensorFlow Extended piece, which is the entire pipeline, is important for enterprises, as discussed at the developer summit.\"}, {'title': 'The Importance of Data Organization for TensorFlow Usage ', 'text': 'TensorFlow Extended is the entire pipeline, focused on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. There is a need to evangelize the importance of organizing data for the big benefit of using TensorFlow.'}, {'title': 'Title ', 'text': 'Understanding Machine Learning and TensorFlow EcosystemText '}, {'title': 'The Importance of Data Organization and Accessible Models ', 'text': 'Data sets are in high demand and there is a need for better organization and accessibility. Start with basic models and then improve them, rather than aiming for the newest and fanciest models. The appearance of Keras made TensorFlow more accessible, as it initially started as a standard outside of TensorFlow and then became on top of TensorFlow.'}, {'title': \"The Evolution of Francois' Keras Project \", 'text': 'Francois started the Keras project before he was at Google. Tiano was the first thing he started. He decided to create an interface and put TensorFlow as a backend when TensorFlow started becoming popular. He joined Google after creating the interface. He initially joined research and was doing some amazing research. He has some papers on research and is a great researcher.'}, {'title': 'Integration of Keras into TensorFlow ', 'text': 'Keras was integrated into TensorFlow in a deep way. TensorFlow 2.0 recommends Keras as the way for beginners to interact with TensorFlow. The integration of Keras into TensorFlow makes transfer learning and basic use cases simple, even for enterprises. The integration process started with a researcher who was doing good work with the API. The researcher initially joined the team for a quarter but has been fully integrated for two years now.'}, {'title': 'Choosing the Right API: Team and Community Collaboration ', 'text': 'There were multiple APIs, including some built by the team and others built by the community. The team was working on a parallel layers API and decided to do Keras in parallel. The goal was to make the APIs look similar and be as integrated as possible. The community was confused about which API to use and kept asking for guidance. The decision was made to simplify and pick one API, and Keras was chosen based on its popularity and positive feedback from the community.'}, {'title': 'The Rise of Keras in the TensorFlow Ecosystem ', 'text': 'Keras was loved by many and had many great features. Keras was chosen organically as the best option. There was surprise in bringing in an outside element like Keras. Keras was seen as a competitor to TensorFlow but ultimately became an empowering element of TensorFlow. The team and developers all want to make things easier for a large set of developers. Python has Guido van Rossum as a key figure, similar to the need for a decision maker in a successful open source project like TensorFlow.'}, {'title': 'TensorFlow Dev Summit Highlights ', 'text': 'TensorFlow Dev Summit was successful with new features and an amazing ecosystem. Involvement in key design directions with distributed responsibilities. Regular design reviews and increased transparency with the community. Setting more processes in place such as RFCs and special interest groups.'}, {'title': 'Advancements in the TensorFlow Ecosystem ', 'text': \"The focus is on adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The decision-making process is being decentralized to allow for better scalability of the ecosystem. The growth of the ecosystem started with Andrej Karpathy's ComNetJS and has evolved to include TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. TensorFlow.js allows for training and running neural networks in the browser using JavaScript, making it a serious and legitimate tool for both backend and frontend operations. The various components of the ecosystem are converging towards a unified approach for saving and moving models across different platforms, such as desktop and mobile.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning with a Focus on Research AccessibilityText '}, {'title': 'The Integration of Machine Learning into Real Products ', 'text': 'Machine learning is being integrated into real products to have a real impact on people. There are a large number of compute devices across the world, including phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning is growing and covering more aspects over time. There is a focus on pushing the boundaries and building more tooling for machine learning.'}, {'title': 'The Evolution of Tooling and Libraries in TensorFlow ', 'text': 'TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines.'}, {'title': 'Empowering the Community with TensorFlow 2.0 ', 'text': 'The goal is to enable the community to build the things they care about. The focus is on making different pieces work well together in TensorFlow 2.0. The core format and sharing of models through save model and TensorFlow hub are key areas of focus. The introduction of TensorFlow.js (deep learning JS) was initially met with skepticism.'}, {'title': 'Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The team has learned a lot and iterated over the last few years. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new devices from a hardware perspective.'}, {'title': 'Challenges of Modifying the Monolithic System of TensorFlow ', 'text': \"TensorFlow started as a very monolithic system and to some extent it still is. There are lots of tools around it, but the core is still pretty large and monolithic. It's hard to change and modify and really break apart. It's like changing the engine with a car running or trying to fix that.\"}, {'title': 'Challenges of Maintaining Compatibility in TensorFlow ', 'text': 'Many people rely on TensorFlow in their applications. There is a challenge in maintaining previous versions while also introducing new ones. TensorFlow 2.0 breaks some backward compatibility but the conversion is straightforward. It is a tricky balance between introducing new features and maintaining compatibility. The impact of changes depends on whether it is for research or production.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and doing new things comes with a huge cost. It's a trade off between slowing certain things down and bringing overall value. It's not just about breaking the person yesterday, but also about setting standards for the person tomorrow. New people coming on board should not be broken by the changes. When doing new things, it's important to consider the impact on future team members.\"}, {'title': 'The Importance of Designing with a Clean Slate in Mind ', 'text': \"Design with a clean slate in mind, and then figure out how to make sure all the other things work. Design with the clean slate and not worry about compromises to get to a good place. It's important to put all the responsibility behind when thinking of new ideas. The speaker switched their research group to TensorFlow and wishes everyone would use the same thing. TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, number of users, and momentum.\"}, {'title': 'The Rise of PyTorch in Research ', 'text': \"PyTorch is being used by a lot of researchers now. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Importance of Learning from Previous Experiences and Exploring Different Spaces ', 'text': 'The text discusses the benefit of learning from previous experiences and exploring different spaces. It mentions the competition and the process of revisiting and adding new features. The text highlights the development of eager execution and the challenges faced in combining different elements. It questions the impact of not having eager execution, using the analogy of Muhammad Ali versus Frasier.'}, {'title': 'Progress and Excitement with TensorFlow 2.0 ', 'text': 'TensorFlow has made incredible progress in the last couple of years. The addition of TensorFlow 2.0 has enabled new possibilities and excitement for the future. Ecosystem improvements such as making it easily accessible to Keras and eager execution have been discussed. Clean APIs have been developed for easier use.'}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs will allow for optimization and improved performance for users. The team is looking forward to exploring new possibilities for single machine and distributed systems in future versions.'}, {'title': 'Restructuring for Improved Functionality ', 'text': 'The team is excited about future versions and the potential improvements. Restructuring the monolithic structure into more modular pieces is important for the ecosystem and other organizations. The current organization of repositories in the TensorFlow organization at GitHub is not easily separable. Clean interfaces are needed for better organization and functionality.'}, {'title': 'The Importance of Clean Interfaces and TensorFlow for Major Corporations ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on custom networking and clusters. Clean separation in interfaces will help with scalability and evolution of different groups in the ecosystem. The hope is for major corporations like Pepsi to use TensorFlow for development. Many major corporations, including Pepsi, are already using TensorFlow.'}, {'title': 'The Growing Community of TensorFlow ', 'text': 'Many users are already using TensorFlow, including hardware vendors and bigger companies like IBM. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The community growth is attributed to certain users wanting to optimize for their specific needs, such as autonomous vehicle companies. The critical thing that allowed for this growth is not specified in the given text.'}, {'title': 'Factors Affecting Growth in the TensorFlow Community ', 'text': \"Growth is critical and requires a combination of factors. Timing and alignment with the needs of the industry are important for growth. TensorFlow's growth is linked to the growth of deep learning. Listening to the community and being open to external contributions is essential for growth. Creating the right processes and community to welcome and support contributors is important for growth.\"}, {'title': 'Importance of Transparency and Community in Open Source Project Growth ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Putting processes in place and thinking about documentation and tools are important as a project grows. People building something on TensorFlow and implementing a particular architecture contributes to the growth of TensorFlow.'}, {'title': 'Investing in Tooling for GitHub Integration ', 'text': \"The company is working hard to make it easy to put their work on GitHub. They are investing in tooling to make significant version changes smooth. People want to move to new things because they see the value, not just because it's new. Most people want a really good thing before they move to something new.\"}, {'title': 'The Shift Towards Deep Learning ', 'text': 'Most people want a really good thing. People will start to see the value and shift towards it in the next few months. The field is moving rapidly, which will help in people moving towards it. New things will happen in 2.x, giving people good reasons to move. Change is expected to happen in terms of deep learning. The basics of deep learning, such as convolution models, will probably be around.'}, {'title': 'The Future of Machine Learning and TensorFlow ', 'text': 'Convolution models and basic models will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. New developments in the field are hard to predict, but there are some promising directions such as combining eager execution and graphs to make programming more natural. Swift for TensorFlow is taking a ground-up approach, which seems to be the right direction for future developments. There is uncertainty about the future of hardware accelerators and the possibility of training with four bits instead of 32 bits. The TPU side of things is exploring the potential for using four bits for training.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': 'TPU is already on version three, and it is exploring the use of four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to easily train or do transfer learning on simple image models. Providing simple models for beginners is important.'}, {'title': 'Improving User Experience with TensorFlow ', 'text': \"Providing simple models and tools like hub to make it easy for users. Different levels of support for beginners, intermediate users, and researchers. Offering pre-trained models to decrease the time needed to start. TensorFlow's recent delivery is trivial for beginners. Addressing pain points and trying to ease the user experience.\"}, {'title': 'The Next Generation: High Schoolers and TensorFlow ', 'text': 'High schoolers are doing amazing and terrifying things. Incredible ideas will come from the next generation. The role with TensorFlow involves both technical and management aspects. Cohesion across the team is important for delivering well.'}, {'title': 'The Importance of Cohesion and Teamwork in Achieving Goals ', 'text': \"Cohesion across the team is important for execution. Teamwork is essential for achieving more than individuals can alone. Hiring good people who care about what they're building and are motivated is crucial. Having a unified vision of where the team wants to go is important.\"}, {'title': \"Google's Unified Vision and Bottom-Up Organization \", 'text': \"Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, especially in research. As Google has become a larger product and ecosystem, it's important to combine the unified vision with exploration and direction. The mission of superstars is a significant element at Google.\"}, {'title': 'Challenges and Opportunities in Team Dynamics at Google ', 'text': 'Large percentage of work at Google is done by individual superstars. Superstars can sometimes create tension within a team dynamic. The mission of the TensorFlow project is seen as beautiful. Google values getting people who care and have the same culture. The project allows for lots of people to do different things and grow.'}, {'title': 'Refining the Hiring Process and Fostering Team Productivity at Google ', 'text': \"Hiring process at Google has been refined over the last 20 years. Productivity at Google is about the team, not just individual superstars. It's important for employees to work well with the team across Google. Core technical skills are important in hiring engineers at Google.\"}, {'title': 'The Importance of Motivation in Employee Success ', 'text': \"Motivation is important for long term success, especially for senior employees. Even junior engineers need to be motivated to succeed. Google's hiring process focuses on technical skills, but may not fully assess motivation.\"}, {'title': 'Title ', 'text': 'Factors Considered in Hiring at GoogleText '}, {'title': 'Navigating the Challenges of Product Development at Google ', 'text': 'Balancing the need for full-fledged products with the importance of ensuring things work properly. The importance of finding the right fit for different projects and teams. Variability in culture, projects, and teams across Google. Engineering excellence as a core part of the culture. The challenging and fun aspects of working on difficult things. The key to success in a large ecosystem or small product.'}, {'title': 'Striking a Balance in Decision Making ', 'text': 'Striking a balance across different aspects of a large ecosystem or a small product. Making decisions on speed versus perfection, community involvement, and saying no to certain things. Hard decisions are often made quickly due to time constraints or after careful consideration. The Dev Summit came together incredibly, with a lot of moving pieces and a deadline that made people rise to the occasion.'}, {'title': 'Managing Deadlines and Releases in Software Development ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and getting something that works well. The team did a great job in putting TensorFlow 2.0 alpha together. Official deadlines are not always put out, but key things are focused on and developed in the open. Releases are done at a regular cadence.\"}, {'title': 'Approach to Releases in TensorFlow 2.0 ', 'text': \"Releases are done at a regular cadence, with the understanding that if something doesn't make it into one release, it can be included in the next release in a month or two. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is acceptable to release things that are not fully ready, as long as it is made clear that they are experimental and open to feedback. Quick cycle and quick iteration are important, rather than focusing on meeting specific deadlines. There is no pressure to make TensorFlow 2.0 stable, similar to the approach taken with WordPress 5.0, where updates were released quickly to improve it.\"}, {'title': 'Improving Stability and Ongoing Development of TensorFlow ', 'text': 'The focus is on improving stability and ensuring that every API remains in working condition. There is still more work to be done and more releases to come in the future. The development is ongoing and there is no expectation of being done in a short period of time. TensorFlow has already been released and has had 41 million downloads for version 1.0 X.'}, {'title': 'Title ', 'text': 'TensorFlow 1.0 X Release and Focus on QualityText '}, {'title': 'The Impact of Search Ads and Machine Learning on User Experience ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience without annoyance. Advertisements can ruin the user experience if they are not relevant to the user's needs and wants. Huge amounts of personalized data can be used for machine learning to shine in connecting users to what they actually want.\"}, {'title': 'The Importance of Accessible Information and Quality Advertising ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the company to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the business model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's experience on a website.\"}, {'title': 'Balancing Value and Monetization in Advertisements ', 'text': 'Advertisements need to strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services such as search engines and websites to provide their service. The challenge is to show valuable ads without being annoying or distracting. Advertisements, when done well, can be really useful and not annoying.'}, {'title': 'The Trend Towards Paid Services and Monetizing Content ', 'text': 'More paid services are being seen across the web and people are willing to pay for them because they see the value. Transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. The trend is towards monetizing content with ads and paid services rather than everything being free on the internet.'}, {'title': 'Transition to a Mix Model and the Use of TPU in Google Call App ', 'text': '- Transition to a mix model with free trials and ads, followed by a clear revenue model.- Use of TPU in a Google call app for free is possible due to TensorFlow being open source and can be run on desktops.- Desktops are becoming more powerful, allowing for more capabilities.- Phones are now more powerful than first desktops, making it possible to train models on phones.'}, {'title': 'The Power and Convenience of Cloud Computing for Beginners ', 'text': 'The power of cloud computing is accessible through phones and offers more convenience than traditional desktops. Cloud services like Colab make it easy to get started with no installation required. Colab is a free service for beginners to play and explore, but paid services offer more capabilities. Beginners interested in machine learning and TensorFlow can start by visiting the TensorFlow website.'}, {'title': 'Getting Started with TensorFlow ', 'text': 'Start by going to TensorFlow.org and playing around on the website. Check out tutorials and guides for more information. No installation needed, can get started right away on Colab. Thank you to Rajit for the conversation.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 23:52:24.638743 ...\n",
      "Best SD: 1.7638342073763937, Best iteration: 13\n",
      "done get topics 2024-03-26 23:52:25.437165.\n",
      "Stage 2 start time 2024-03-26 23:52:25.437184\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. The Evolution of Machine Learning at Google\n",
      "2. The Growth of Deep Learning and Community Engagement\n",
      "3. The Rise of Keras in the TensorFlow Ecosystem\n",
      "4. Advancements in the TensorFlow Ecosystem\n",
      "5. Progress and Excitement with TensorFlow 2.0\n",
      "6. The Future of Machine Learning and TensorFlow\n",
      "7. Navigating the Challenges of Product Development at Google\n",
      "8. Approach to Releases in TensorFlow 2.0\n",
      "9. The Power and Convenience of Cloud Computing for Beginners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewrite_summary: \n",
      "\n",
      "    Your role is a narrator to narrate the given text.\n",
      "    You will improve the given text on fluency and cohesiveness.\n",
      "    You will keep all the original content of the given text.\n",
      "    \n",
      "    Here is the given text that you will improve:\n",
      "    {summary}\n",
      "    \n",
      "\n",
      "RRR given summary\n",
      "The podcast features Rajat Manga, an engineer and director at Google, who leads the TensorFlow team, which is at the forefront of deep learning work. TensorFlow has evolved into an ecosystem of tools for machine learning deployment across various platforms. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, promoting open innovation and collaboration. Google Brain, where Manga has been involved since its inception, played a crucial role in the development of deep learning and the transition to TensorFlow. The decision to open source TensorFlow was influenced by the desire to push the state of the art forward and build on others' research. The open source nature of TensorFlow has led to rapid growth in deep learning and machine learning, with Google Cloud providing integrations and support for TensorFlow. The timeline of TensorFlow development, from its inception to open sourcing, reflects the fast pace of development in deep learning.\n",
      "\n",
      "The podcast discusses the development and evolution of TensorFlow, a machine learning framework designed to run at large scale in data centers and on various hardware, including mobile devices. The design decisions were influenced by the need for flexibility, the experience with other libraries like Theano and Caffe, and the goal of making machine learning more accessible to developers and enterprises. The transition from a research focus to stability and deployment, as well as the increasing interest and adoption by enterprises, were key points of discussion. The company's community-driven approach, the unexpected popularity of the product, and the importance of understanding the needs of enterprises were also highlighted.\n",
      "\n",
      "The podcast discusses the importance of stability and simplicity in model selection in the field of AI. It emphasizes the usability and stability of older AI models, such as ResNet 50, and the common use of transfer learning on specific problems. The podcast also highlights the importance of making AI as easy as possible for hobbyists and the common use of AI in apps and phones. It also discusses the use of AI in enterprises for data prediction, the varying needs of the audience for data analysis, and the importance of the TensorFlow Extended pipeline for enterprises. The podcast also delves into the understanding of machine learning and the TensorFlow ecosystem, discussing the demand for better organization and accessibility of data sets, the integration of Keras into TensorFlow, and the decision to simplify and pick Keras as the best API for TensorFlow. Overall, the podcast emphasizes the importance of stability, simplicity, and accessibility in AI model selection and the integration of Keras into the TensorFlow ecosystem.\n",
      "\n",
      "The podcast discusses the goal of enabling the community to build the things they care about using TensorFlow 2.0. It focuses on making different pieces work well together, with a core format and sharing of models through save model and TensorFlow hub. The introduction of TensorFlow.js and deep learning JS initially faced skepticism and integration challenges, but the team has learned and iterated over the years. The goal is to make it easy for the end user, but there are complexities behind the scenes, including challenges in integrating with new devices from a hardware perspective. The TensorFlow Dev Summit was successful in introducing new features and an amazing ecosystem, with a focus on involvement in key design directions, regular design reviews, and increased transparency with the community. The growth of the ecosystem started with ComNetJS and has evolved to include TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The goal is to enable machine learning on every device with compute capability, and the ecosystem is growing to cover more aspects over time. There is a focus on pushing the boundaries and building more tooling for machine learning, including ML pipelines.\n",
      "\n",
      "The podcast discusses the evolution of TensorFlow from a monolithic system to a more modular one, highlighting the challenges of maintaining compatibility while introducing new features. It emphasizes the importance of considering the impact on future team members and the benefits of learning from previous experiences. The development of TensorFlow 2.0, with its clean APIs and potential for performance improvements, is a source of excitement for the team. The podcast also touches on the competition with PyTorch, the development of eager execution, and the need for clean interfaces for better organization and functionality. The ultimate goal is for major corporations like Pepsi to use TensorFlow for development, as many already do.\n",
      "\n",
      "TensorFlow has experienced significant growth, with 41 million downloads, 50,000 commits, and 1,800 contributors. The community growth is attributed to users wanting to optimize for their specific needs, such as autonomous vehicle companies. The critical factor for this growth is not specified, but timing and alignment with industry needs are important. Listening to the community and being open to external contributions is essential for growth. Transparency, community aspects, processes, and documentation are important for growth. People building on TensorFlow and implementing specific architectures contribute to its growth. The company is working to make it easy to put their work on GitHub and investing in tooling for significant version changes. The field is moving rapidly, and new developments are hard to predict, but some promising directions include combining eager execution and graphs and Swift for TensorFlow. There is uncertainty about the future of hardware accelerators and the possibility of training with four bits instead of 32 bits. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners, by providing simple models, tools, and pre-trained models. Addressing pain points and trying to ease the user experience is a priority for TensorFlow.\n",
      "\n",
      "The podcast discusses the incredible and sometimes terrifying things that high schoolers are doing, and the potential for amazing ideas to come from the next generation. It also delves into the role of TensorFlow, which involves both technical and management aspects, and the importance of cohesion and teamwork within a team for successful delivery and execution. The hiring process at Google is also explored, with a focus on hiring people who care about what they're building and are motivated, as well as the importance of a unified vision and culture within the company. The podcast also touches on the mission of superstars at Google, the hiring process, and the factors considered in hiring at Google, such as balancing the need for full-fledged products with ensuring things work properly, variability in culture, projects, and teams, and engineering excellence as a core part of the culture. Additionally, it discusses the challenges and fun aspects of working on difficult things, the key to success in a large ecosystem or small product, and making decisions on speed versus perfection, community involvement, and saying no to certain things. The podcast concludes with a discussion on the Dev Summit and how it came together incredibly, with a lot of moving pieces and a deadline that made people rise to the occasion.\n",
      "\n",
      "The podcast discusses the release of TensorFlow 2.0 alpha and the focus on quality over meeting specific deadlines. The team emphasizes the importance of finding a balance between perfection and getting something that works well. They prioritize quick iteration and improvement, with the understanding that if something doesn't make it into one release, it can be included in the next release in a month or two. The focus is on moving as fast as possible in different areas and improving stability, with no pressure to make TensorFlow 2.0 stable quickly. The development is ongoing, with more releases to come in the future. TensorFlow 1.0 X has already had 41 million downloads.\n",
      "\n",
      "The podcast discusses the role of search ads in providing a personalized and relevant user experience, without being annoying or disruptive. It emphasizes the importance of aligning with user needs and providing valuable ads, while also addressing the necessity of monetization for services like search engines and websites. The trend is towards a mix model of free trials and ads, followed by a clear revenue model, as well as the use of powerful technology like TPUs and cloud computing to make machine learning more accessible. The podcast also encourages beginners to explore machine learning and TensorFlow through resources like the TensorFlow website and Colab.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The podcast features Rajat Manga, an engineer and director at Google, who leads the TensorFlow team, which is at the forefront of deep learning work. TensorFlow has evolved into an ecosystem of tools for machine learning deployment across various platforms. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, promoting open innovation and collaboration. Google Brain, where Manga has been involved since its inception, played a crucial role in the development of deep learning and the transition to TensorFlow. The decision to open source TensorFlow was influenced by the desire to push the state of the art forward and build on others' research. The open source nature of TensorFlow has led to rapid growth in deep learning and machine learning, with Google Cloud providing integrations and support for TensorFlow. The timeline of TensorFlow development, from its inception to open sourcing, reflects the fast pace of development in deep learning.\\n\\nThe podcast delves into the development and evolution of TensorFlow, a machine learning framework designed to run at large scale in data centers and on various hardware, including mobile devices. The design decisions were influenced by the need for flexibility, the experience with other libraries like Theano and Caffe, and the goal of making machine learning more accessible to developers and enterprises. The transition from a research focus to stability and deployment, as well as the increasing interest and adoption by enterprises, were key points of discussion. The company's community-driven approach, the unexpected popularity of the product, and the importance of understanding the needs of enterprises were also highlighted.\\n\\nThe podcast emphasizes the importance of stability and simplicity in model selection in the field of AI. It highlights the usability and stability of older AI models, such as ResNet 50, and the common use of transfer learning on specific problems. The podcast also discusses the use of AI in enterprises for data prediction, the varying needs of the audience for data analysis, and the importance of the TensorFlow Extended pipeline for enterprises. The podcast also delves into the understanding of machine learning and the TensorFlow ecosystem, discussing the demand for better organization and accessibility of data sets, the integration of Keras into TensorFlow, and the decision to simplify and pick Keras as the best API for TensorFlow. Overall, the podcast emphasizes the importance of stability, simplicity, and accessibility in AI model selection and the integration of Keras into the TensorFlow ecosystem.\\n\\nThe podcast discusses the goal of enabling the community to build the things they care about using TensorFlow 2.0. It focuses on making different pieces work well together, with a core format and sharing of models through save model and TensorFlow hub. The introduction of TensorFlow.js and deep learning JS initially faced skepticism and integration challenges, but the team has learned and iterated over the years. The goal is to make it easy for the end user, but there are complexities behind the scenes, including challenges in integrating with new devices from a hardware perspective. The TensorFlow Dev Summit was successful in introducing new features and an amazing ecosystem, with a focus on involvement in key design directions, regular design reviews, and increased transparency with the community. The growth of the ecosystem started with ComNetJS and has evolved to include TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The goal is to enable machine learning on every device with compute capability, and the ecosystem is growing to cover more aspects over time. There is a focus on pushing the boundaries and building more tooling for machine learning, including ML pipelines.\\n\\nThe podcast discusses the evolution of TensorFlow from a monolithic system to a more modular one, highlighting the challenges of maintaining compatibility while introducing new features. It emphasizes the importance of considering the impact on future team members and the benefits of learning from previous experiences. The development of TensorFlow 2.0, with its clean APIs and potential for performance improvements, is a source of excitement for the team. The podcast also touches on the competition with PyTorch, the development of eager execution, and the need for clean interfaces for better organization and functionality. The ultimate goal is for major corporations like Pepsi to use TensorFlow for development, as many already do.\\n\\nTensorFlow has experienced significant growth, with 41 million downloads, 50,000 commits, and 1,800 contributors. The community growth is attributed to users wanting to optimize for their specific needs, such as autonomous vehicle companies. The critical factor for this growth is not specified, but timing and alignment with industry needs are important. Listening to the community and being open to external contributions is essential for growth. Transparency, community aspects, processes, and documentation are important for growth. People building on TensorFlow and implementing specific architectures contribute to its growth. The company is working to make it easy to put their work on GitHub and investing in tooling for significant version changes. The field is moving rapidly, and new developments are hard to predict, but some promising directions include combining eager execution and graphs and Swift for TensorFlow. There is uncertainty about the future of hardware accelerators and the possibility of training with four bits instead of 32 bits. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners, by providing simple models, tools, and pre-trained models. Addressing pain points and trying to ease the user experience is a priority for TensorFlow.\\n\\nThe podcast discusses the incredible and sometimes terrifying things that high schoolers are doing, and the potential for amazing ideas to come from the next generation. It also delves into the role of TensorFlow, which involves both technical and management aspects, and the importance of cohesion and teamwork within a team for successful delivery and execution. The hiring process at Google is also explored, with a focus on hiring people who care about what they're building and are motivated, as well as the importance of a unified vision and culture within the company. The podcast also touches on the mission of superstars at Google, the hiring process, and the factors considered in hiring at Google, such as balancing the need for full-fledged products with ensuring things work properly, variability in culture, projects, and teams, and engineering excellence as a core part of the culture. Additionally, it discusses the challenges and fun aspects of working on difficult things, the key to success in a large ecosystem or small product, and making decisions on speed versus perfection, community involvement, and saying no to certain things. The podcast concludes with a discussion on the Dev Summit and how it came together incredibly, with a lot of moving pieces and a deadline that made people rise to the occasion.\\n\\nThe podcast discusses the release of TensorFlow 2.0 alpha and the focus on quality over meeting specific deadlines. The team emphasizes the importance of finding a balance between perfection and getting something that works well. They prioritize quick iteration and improvement, with the understanding that if something doesn't make it into one release, it can be included in the next release in a month or two. The focus is on moving as fast as possible in different areas and improving stability, with no pressure to make TensorFlow 2.0 stable quickly. The development is ongoing, with more releases to come in the future. TensorFlow 1.0 X has already had 41 million downloads.\\n\\nThe podcast discusses the role of search ads in providing a personalized and relevant user experience, without being annoying or disruptive. It emphasizes the importance of aligning with user needs and providing valuable ads, while also addressing the necessity of monetization for services like search engines and websites. The trend is towards a mix model of free trials and ads, followed by a clear revenue model, as well as the use of powerful technology like TPUs and cloud computing to make machine learning more accessible. The podcast also encourages beginners to explore machine learning and TensorFlow through resources like the TensorFlow website and Colab.\"}]\n",
      "Stage 2 done time 2024-03-26 23:53:59.582081\n",
      "stage_2_titles: len: 9\n",
      "['1. The Evolution of Machine Learning at Google', '2. The Growth of Deep Learning and Community Engagement', '3. The Rise of Keras in the TensorFlow Ecosystem', '4. Advancements in the TensorFlow Ecosystem', '5. Progress and Excitement with TensorFlow 2.0', '6. The Future of Machine Learning and TensorFlow', '7. Navigating the Challenges of Product Development at Google', '8. Approach to Releases in TensorFlow 2.0', '9. The Power and Convenience of Cloud Computing for Beginners']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "#     if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "#        int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:   \n",
    "    if int(podcast['episode_number']) != 22:\n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
