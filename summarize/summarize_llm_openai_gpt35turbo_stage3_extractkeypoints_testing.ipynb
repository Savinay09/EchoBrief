{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"testing\" # has rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\"\n",
    "REWRITE_WORD_COUNT = \"at least 1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f8caba48510>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "#     eval_prompt_template_v1 = \"\"\"\n",
    "#     Rewrite the given summary to improve readability.\n",
    "#     Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "#     Remove the reference of 'podcast' in the rewritten summary.\n",
    "#     The rewritten summary should have \"\"\" + REWRITE_WORD_COUNT + \"\"\" words.\n",
    "\n",
    "#     Here is the data:\n",
    "#     {summary}\n",
    "\n",
    "#     Return your answer in the following format:\n",
    "#     REWRITTEN_SUMMARY\n",
    "#     \"\"\"\n",
    "    \n",
    "    #v2\n",
    "#     eval_prompt_template_v2 = \"\"\"\n",
    "#     You will rewrite the given summary to improve readability.\n",
    "#     Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "#     Remove the reference of 'podcast' in the rewritten summary.\n",
    "\n",
    "#     Here is the given summary:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v3 (bad)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a speech on a summary of the given text.\n",
    "#     You will rewrite the given text to improve readability, so you can present it as a speech in a fluent and professtional way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v4 bad\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a narrator of the given text.\n",
    "#     You will rewrite the given text so that so you can narrate it in a fluent, cohesive, and coherent way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"    \n",
    "    \n",
    "#     #v5\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will rewrite the given text to improve readability, so you can present it as a speech in a fluent, cohesive, coherernt, and professtional way.\n",
    "\n",
    "#     Here is the given text:\n",
    "#     {summary}\n",
    "#     \"\"\"      \n",
    "\n",
    "    \n",
    "    #v6 (good)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You should add transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "\n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "    \n",
    "    #v6b\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a podcast host who speaks about the given text to the audience.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You should add transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "\n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\" \n",
    "    \n",
    "    #v6c\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a podcast host who narrates the given text.\n",
    "   \n",
    "#     Here is the given text that you will narrate:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v7\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph will start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v7a\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases throughout the given text as you see fit to improve its cohesiveness.\n",
    "#     For example, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph will start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "\n",
    "    #v7b\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases throughout the given text as you see fit to improve its cohesiveness.\n",
    "#     For example, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph will start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "    \n",
    "#     Again, you will start a new paragraph with words when you see fit and not excessive.\n",
    "#     The ending paragraph will start with words like 'Finally' or 'Lsatly' to indicate it is the last paragraph of the given text.\n",
    "#     \"\"\"  \n",
    "    \n",
    "    #v7c\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will rewrite the first sentence of all paragraphs in the given text by removing 'The podcast' if exists.\n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"      \n",
    "    \n",
    "    #v7d\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will rewrite the first sentence of all paragraphs in the given text by removing 'The podcast' if exists.\n",
    "#     Then, you will add transitional words or phrases at the beginning of the first sentence of all paragraphs, except the first and the last paragraphs.\n",
    "    \n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"  \n",
    "    \n",
    "    #v7e (good, has transitions every paragraphs. next: try only have every other paragraphs)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     you will add transitional words or phrases at the beginning of the first sentence of all paragraphs, except the first and the last paragraphs.\n",
    "    \n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v7f (good, but run 2/3 so short!??)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     you will rewrite the first sentence of all paragraphs without mentioning 'podcast', \n",
    "#     except the first and the last paragraphs.\n",
    "    \n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"    \n",
    "    \n",
    "    # v7g (still very inconsistent)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will rewrite the first sentence of paragraphs (except the first and the last paragraphs) \n",
    "#     without mentioning 'podcast'.\n",
    "\n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"  \n",
    "    \n",
    "    #v7h (ok)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will add transitional words at the beginning of the first sentence of paragraphs except for the first and the last paragraph.\n",
    "    \n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"    \n",
    "    \n",
    "    #v7i\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will add transitional words, phrase, or sentence at the beginning of the first sentence of paragraphs except for the first and the last paragraph.\n",
    "    \n",
    "#     Here is the input text:\n",
    "#     {summary}\n",
    "#     \"\"\"      \n",
    "\n",
    "    # not work at all!\n",
    "    eval_prompt_template = \"\"\"\n",
    "    You will improve transitions between paragraphs.\n",
    "    Here is the input text:\n",
    "    {summary}\n",
    "    \"\"\"      \n",
    "\n",
    "    \n",
    "    #v8 (bad)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words or phrases through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, once in a while you can start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph can start with words like 'Finally' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "    #v9 doesnt work\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words through out the given text as you see fit to improve its cohesiveness.\n",
    "#     For examples, you will start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     Do not use the same transitional word more than twice in the whole text.\n",
    "#     The last paragraph will start with words like 'Finally' or 'Lastly' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"   \n",
    "\n",
    "    #v10 doesnt work\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     In the given text, you will rewrite the first sentence of each paragraph but still keep the same meaning and all information.\n",
    "    \n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\"  \n",
    "    \n",
    "    #v11 not good\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     You will improve the given text by adding transitional words throughout the given text as you see fit to improve its cohesiveness.\n",
    "#     For example, once in a while you can start a new paragraph with words like 'Moreover', 'Furthermore', 'In addition', etc.\n",
    "#     The last paragraph can start with words like 'Finally' or 'Lastly' to indicate it is the last paragraph.\n",
    "\n",
    "#     Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"         \n",
    "\n",
    "    #v12\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a narrator to narrate the given text.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You will keep all the original content of the given text.\n",
    "    \n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\" \n",
    "    \n",
    "    #v13\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is a narrator to narrate the given text.\n",
    "#     You will improve the given text on fluency and cohesiveness.\n",
    "#     You will keep all the original content of the given text.\n",
    "    \n",
    "#     Here is the given text that you will improve:\n",
    "#     {summary}\n",
    "#     \"\"\"     \n",
    "    \n",
    "\n",
    "    # v5b\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will rewrite the given text to improve readability, so you can present it as a speech in a fluent, \n",
    "#     cohesive, coherent, and professional way. Here is the given text:\n",
    "#     ##########\n",
    "#     {summary}\n",
    "#     ##########\n",
    "    \n",
    "#     Your answer is the improved text:\n",
    "#     IMPROVED_TEXT\n",
    "#     \"\"\"    \n",
    "    \n",
    "    # v5c (bad)\n",
    "#     eval_prompt_template = \"\"\"\n",
    "#     Your role is an instructor who is giving a narration of the given text.\n",
    "#     You will improve the given text to improve readability, so you can present it as a speech in a fluent, \n",
    "#     cohesive, coherent, and professional way. Here is the given text that you will improve on:\n",
    "#     {summary}\n",
    "#     \"\"\"    \n",
    "    \n",
    "    \n",
    "    print(\"rewrite_summary: \")\n",
    "    print(eval_prompt_template)\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-28 01:42:06.628672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-28 01:47:33.433973\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-28 01:47:33.434117\n",
      "extract_keypoints done time 2024-03-28 01:50:07.453258\n",
      "Start time: 2024-03-28 01:50:07.453445\n",
      "Stage 1 done time 2024-03-28 01:52:35.828691\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow on the Tech Industry ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Impact of Open Innovation on Machine Learning Development ', 'text': 'Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library turned into TensorFlow in 2014, the open source library. Deep learning was interesting and intriguing, showing promise but had not yet taken off.'}, {'title': \"Scaling Research Work with Google's Compute Power and Data \", 'text': 'It held some promise. The idea was to scale research work to Google\\'s compute power and data. Scaling the compute and data resulted in better performance. Early wins were achieved in speech and image research. Collaboration with the speech research team was successful. The \"cat paper\" was a significant achievement in image research.'}, {'title': \"The Evolution of Google's Machine Learning Efforts \", 'text': 'Google Brain was born around neural networks, focusing on deep learning from the beginning. In 2012 or 2011, the focus was on scaling machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines. Google has been doing machine learning for a long time, showing great promise in terms of machine learning.'}, {'title': 'The Rise of Deep Learning at Google ', 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact many things. Real products started to use deep learning, such as speech and image recognition. Academia also started to show interest in deep learning by 2014. The decision to open source TensorFlow was made.'}, {'title': 'The Impact of Going Open Source with TensorFlow ', 'text': \"The decision to go open source with TensorFlow is considered a seminal moment in software engineering. Google's decision to take a large project and go open source with it led the entire world in embracing open innovation. Jeff was a big proponent of the initial idea to go open source with TensorFlow. The decision was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'The Growth of Deep Learning and Machine Learning ', 'text': 'Deep learning and machine learning have grown rapidly due to sharing research and open source projects. Existing libraries like Tiano and Torch were done by academia, with a significantly different level. Google had done lots of software internally and published papers, leading to successful open source projects. Hadoop came off of tech that was built internally, but the company believed their tech was better for various reasons.'}, {'title': \"Google Cloud's Bigtable and HBase APIs, TensorFlow, and Community Support \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. TensorFlow is open source and can be used anywhere. Google Cloud ensures lots of integrations and works well with TensorFlow. The focus is on helping the community and pushing a good standard forward.'}, {'title': 'The Development and Open Sourcing of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. TensorFlow was open sourced in November 2015. Development of TensorFlow started in the summer of 2014. Decision to open source TensorFlow was made in late 2014. Focus on ensuring that TensorFlow works well in its ecosystem.'}, {'title': 'Scaling and Supporting Hardware for TensorFlow ', 'text': 'The team started thinking about scaling and supporting different hardware for TensorFlow. They had seen various use cases at Google, including running models at large scale in the data center and on mobile. The design of TensorFlow included support for GPUs and customization of code for mobile deployment. The team wanted to ensure that TensorFlow could support running models on mobile devices. They were also pushing for support for different kinds of hardware, including GPUs.'}, {'title': 'Challenges and Influences in Running Machine Learning on Mobile Phones ', 'text': 'Ideas of running machine learning on the phone existed at that time. Customized handcrafted code or internal libraries were used for running machine learning on the phone. The use of Theano and Caffe at Google influenced design decisions. The belief was built in parallel with the development of libraries like Theano. The systems at Google were very different, leading to the focus on internal development. Multiple libraries were considered before making design decisions.'}, {'title': 'Choosing a Deep Learning Library ', 'text': 'By the time we got to this, we looked at a number of libraries that were out there. The group had experience with Torch, Lua, Theano, and Caffe. They discussed ideas around having a graph or not. They wanted flexibility due to the fast-moving research and changing hardware. The flexibility in terms of being able to express all kinds of crazy things was a big factor.'}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The move towards TensorFlow 2.0 includes default eager execution, hiding the graph, and making development less intuitive. The idea of using a graph came from the need for production deployment and simplifying the development process. The graph was initially a simple, straight line concept, similar to Theano or cafe. Experimentation with ideas in Python led to the realization that not having a graph made things simpler to use.'}, {'title': 'Influence of Graph Deployment on Product Popularity and Growth ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, exceeded expectations. The need for the product was recognized early on, but the level of popularity was not anticipated. The potential for future growth and enabling more people to use the product was considered after open sourcing. The growth of deep learning was observed after open sourcing the product.'}, {'title': 'The Growth of Deep Learning and Community Engagement ', 'text': \"Deep learning grew rapidly after open sourcing. The company saw the opportunity to leverage deep learning and deliver on what people want. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company's approach is very community driven. The initial version of the product was 0.6 or 0.5. People initially loved the documentation provided by the company.\"}, {'title': 'The Evolution of Deep Learning from Research to Practical Applications ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from a research focus to being accessible to developers for practical applications. The focus shifted towards stability and deployment for non-research purposes. Planning for version 1.0 involved addressing the needs of stability and deployment. Enterprises showed increasing interest in the product as it progressed.'}, {'title': 'Enterprise Adoption of Product Version 1.0 ', 'text': 'Enterprise adoption of the product started to take off after the initial release. The initial release attracted researchers, hobbyists, and early interest. Enterprises are more likely to adopt a product after it reaches version 1.0 for stability. It is important to consider what enterprises want during the development process.'}, {'title': 'Variety of Priorities in Deep Learning Models ', 'text': 'Enterprise and user needs can vary, with some prioritizing stability and simplicity over the latest performance and quality. Inception and ResNet 50 are still widely used, despite being several years old, showing the value of stability and accessibility. The research crowd is interested in pushing the boundaries of deep learning with new models like RNNs, transformers, RL, and GANs.'}, {'title': 'The Advancements of RL and GANs in Technology ', 'text': 'The combination of RL and GANs is pushing the state of the art in the field. Older technology is still very usable and stable for many people. Making technology easy to use is important for the majority of the world. The use of older technology is common in apps and phones. The visual appeal of technology is important for presentations.'}, {'title': 'The Importance of TensorFlow Extended in Enterprise Data Analysis ', 'text': 'Enterprises have data that they want to make predictions on, often using regression models, linear models, or gradient booster trees. Deep learning can shine with very large data sets. The developer summit put together the whole TensorFlow Extended piece, which is the entire pipeline that enterprises care about.'}, {'title': 'The Importance of Data Organization for Using TensorFlow ', 'text': 'TensorFlow Extended is the entire pipeline, focusing on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. The role of an evangelist is to encourage companies to organize their data for the benefits of using TensorFlow.'}, {'title': 'The Importance of Data Organization and Pre-Trained Models in Machine Learning ', 'text': 'Conversations about various questions and challenges related to data and machine learning. Importance of organizing data for automation and prediction. Availability of more data sets and pre-trained models within the TensorFlow ecosystem. Interest in the recently released TensorFlow data sets.'}, {'title': 'The Impact of New Data Sets on Organization and Accessibility ', 'text': 'The release of new data sets has led to a demand for better organization and accessibility. It is important to start with basic models and then improve upon them. The appearance of Keras has made TensorFlow more accessible. Keras was initially on top of Tiano and then on top of TensorFlow.'}, {'title': 'The Evolution of Keras and TensorFlow ', 'text': 'Francois started the Keras project before he was at Google. Tiano was the first thing before TensorFlow was created. When TensorFlow started becoming popular, he decided to create an interface and put TensorFlow as a backend. He joined Google after creating the interface. He initially joined research and was doing some amazing research. He has some papers on research and is a great researcher.'}, {'title': 'Integration of Keras into TensorFlow ', 'text': 'Keras was integrated into TensorFlow in a deep way. TensorFlow 2.0 recommends Keras as the way for beginners to interact with TensorFlow. The integration of Keras into TensorFlow makes transfer learning and basic use cases simple, even for enterprises. The integration process started with a researcher who was doing good work with the API. The researcher initially joined the team for a quarter but has been fully involved for two years now. The decision to integrate Keras into TensorFlow was the result of a lot of time spent thinking about it and considering various APIs.'}, {'title': 'Consolidating Multiple APIs: Choosing Keras ', 'text': 'The company had multiple APIs, including a parallel layers API and Keras. The goal was to integrate the APIs and make them look similar. The community was confused about which API to use and which model to pick. The decision was made to simplify and pick one API, and Keras was chosen based on its popularity and positive feedback.'}, {'title': 'The Impact of Keras on TensorFlow ', 'text': 'Keras was loved by many and had great qualities. It was surprising to bring in an outside element like Keras, which was seen as a competitor to TensorFlow, but it ended up being an empowering element of TensorFlow. The team and developers all want to make things easier for a large set of developers, and that makes a difference. Python has Guido van Rossum, who held the position of benevolent dictator for life. A successful open source project like TensorFlow needs one person to make final decisions.'}, {'title': 'Success and Growth at TensorFlow Dev Summit ', 'text': 'TensorFlow Dev Summit was successful. Lots of new features and an amazing ecosystem are being incorporated into TensorFlow. Involvement in key design directions. Regular design reviews are conducted. Efforts to open up to the community and add transparency. Setting more processes in place, such as RFCs and special interest groups.'}, {'title': 'The Evolution and Growth of the TensorFlow Ecosystem ', 'text': \"The need for adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The recognition that the ecosystem's scale requires more than one decision maker and the importance of decentralizing decision-making. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The convergence of all these developments towards the ability to save models in a consistent way and move them between different platforms.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning in Various WaysText '}, {'title': 'The Integration of Machine Learning into Real Products ', 'text': 'Machine learning is being integrated into real products to have a real impact on people. ML and training are no longer limited to workstations, data centers, or the cloud, but are now running on various compute devices including phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning is growing and covering more aspects over time. There is a continuous push to push the boundaries and build more tooling.'}, {'title': 'The Evolution of TensorFlow Tooling ', 'text': 'TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow has built more tooling and things to help with ML pipelines.'}, {'title': 'Empowering the Community with TensorFlow 2.0 ', 'text': 'The goal is to enable the community to build the things they care about. The focus is on making different pieces work well together in TensorFlow 2.0. The core format and sharing of models through save model and TensorFlow hub are key pieces being pushed on. The release of TensorFlow.js (deep learning JS) was initially met with skepticism.'}, {'title': 'Challenges in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The team has iterated over the last few years and learned a lot in the process. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new devices from a hardware perspective.'}, {'title': 'Challenges of Modifying the Monolithic System of TensorFlow ', 'text': \"TensorFlow started as a very monolithic system and to some extent it still is. There are lots of tools around it, but the core is still pretty large and monolithic. It's hard to change and modify and really break apart. It's like changing the engine with a car running or trying to fix that.\"}, {'title': 'Challenges of Maintaining Backward Compatibility in TensorFlow ', 'text': 'Many people rely on TensorFlow in their applications. There is a challenge in maintaining previous versions while also moving forward with new versions. TensorFlow 2.0 breaks some backward compatibility but the conversion seems pretty straightforward. It is a tricky balance between maintaining previous versions and moving forward with new versions. For researchers writing code for a paper, the backward compatibility may not matter, but for production applications, it is important.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and improvements comes with a trade-off, but the overall value is much bigger. It's not just about breaking the person yesterday, but also about setting standards for new people joining the team. When doing new things, it's important to consider the impact on future team members.\"}, {'title': 'Designing with a Clean Slate and the Importance of TensorFlow ', 'text': \"Design with a clean slate in mind, and then figure out how to make sure all the other things work. Unless you design with a clean slate and not worry about compromises, you'll never get to a good place. It's important to put all the concerns behind when thinking of new ideas. The speaker switched their research group to TensorFlow and wishes everyone would use the same thing. TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, number of users, and momentum.\"}, {'title': 'The Rise of PyTorch in Research ', 'text': \"PyTorch is being used by a lot of researchers now. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Power of Learning from Previous Experiences and Combining Different Elements ', 'text': 'The text discusses the benefit of learning from previous experiences and exploring different spaces. It mentions the competition and the process of revisiting ideas multiple times before implementing them. The text highlights the importance of combining different elements and the power of eager execution. It also references the analogy of Muhammad Ali versus Frasier to emphasize the significance of certain decisions.'}, {'title': 'Progress and Milestones in TensorFlow ', 'text': 'TensorFlow has made incredible progress in the last couple of years. The ecosystem has been improved, making it easily accessible to Keras and eager execution. TensorFlow 2.0 is a significant milestone, enabling new possibilities and exciting developments.'}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs will allow for optimization and improved performance for both single machine and distributed systems. The team is looking forward to exploring new possibilities and spaces behind the scenes in future versions after 2.0.'}, {'title': 'Restructuring TensorFlow for Improved Modularity ', 'text': 'The team is excited about future versions and expects to see improvements over time. Restructuring the monolithic system into more modular pieces is important for other organizations and the ecosystem. The current organization of TensorFlow on GitHub consists of one core repository with multiple components that work together in a single library or binary. There is a need for cleaner interfaces to easily split the components apart.'}, {'title': 'The Importance of Clean Interfaces for Scalability ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on different clusters with custom networking. Clean separation in interfaces will help with scalability in the ecosystem. Enabling independent evolution and pushing on things allows for better scaling. Major corporations like Pepsi are already using TensorFlow. Enterprises like Pepsi are not the ones doing the development.'}, {'title': 'The Growth and Diverse User Base of TensorFlow ', 'text': 'Many users are already using TensorFlow, but not all of them are involved in core development or changes. Some users, such as hardware vendors and larger companies like IBM, are interested in customizing TensorFlow for their specific needs. Special interest groups, including autonomous vehicle companies, are also involved in using and optimizing TensorFlow. TensorFlow has been downloaded 41 million times, with a large number of commits, pull requests, and contributors. The growth of the TensorFlow community is attributed to critical factors that allowed for its initial growth and continued expansion.'}, {'title': 'Factors for Achieving Growth ', 'text': 'Growth is critical and requires a combination of factors. Timing and alignment with the needs of the industry are important for growth. Listening to the community and being open to external contributions is essential for growth. Creating the right processes and community to welcome and support contributors is important for growth.'}, {'title': 'Importance of Transparency and Community in Open Source Project Growth ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Putting processes in place and thinking about documentation and tools are important as a project grows. People building something on TensorFlow and implementing a particular architecture feeds the growth of TensorFlow.'}, {'title': 'Enhancing Developer Experience and Value on GitHub ', 'text': 'The focus is on making it easy for developers to use and contribute to the project on GitHub. There is a commitment to investing in tooling to support developers and make version changes smooth. The goal is to provide value in the new thing, rather than just change for the sake of change.'}, {'title': 'The Future of Deep Learning ', 'text': 'Most people want a really good thing. People will start to see the value and shift towards it in the next few months. The field is moving rapidly, which will help in doing more things and new things will happen in 2.x. Change is expected to happen, but the basics of deep learning, like convolution models, will probably stick around.'}, {'title': 'The Future of Machine Learning and Hardware Accelerators ', 'text': 'Convolution models and basic models will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. There will probably be new developments in the field, but they are hard to predict. Some current directions include combining eager execution and graphs to make programming more natural, and exploring ground-up approaches like Swift for TensorFlow. It is uncertain if hardware accelerators will remain the same, and if training with four bits instead of 32 bits will be possible. The TPU side of things is exploring the possibility of using TPUs for training with lower bit precision.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': \"TPU is already on version three, and it is exploring using four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to take some image model and do training or transfer learning on their kind of model, and it's important to make that easy for them.\"}, {'title': 'Title ', 'text': 'Making TensorFlow Accessible for UsersText '}, {'title': 'Young Innovators Making Waves ', 'text': 'High schoolers are doing amazing and terrifying things. Incredible ideas will be coming from the younger generation. There is a technical and management aspect to the work with TensorFlow. Cohesion across the team is important for delivering something well.'}, {'title': 'The Importance of Cohesion and Teamwork in Achieving Goals ', 'text': \"Cohesion across the team is important for execution. Teamwork is essential for achieving more than individuals can alone. Hiring good people who care about what they're building and are motivated is crucial. Having a unified vision of where the team wants to go is important.\"}, {'title': \"Google's Unified Vision and Bottom-Up Organization \", 'text': \"Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, especially in research. As Google has grown, it's important to combine the unified vision with exploration, while still staying in the overall direction. There are tensions and complexities in the direction and decision-making process. The mission of superstars is a significant element at Google.\"}, {'title': 'Challenges and Excitement at Google ', 'text': 'Large percentage of work at Google is done by individual superstars. Superstars can sometimes be against the dynamic of a team, causing tensions. The mission of the TensorFlow project is exciting and at the cutting edge. Google values getting people who care and have the same kind of culture. The project allows for lots of people to do different things and grow. There are always people challenges in different kinds of ways.'}, {'title': 'Refining the Hiring Process and Prioritizing Team Productivity at Google ', 'text': \"Hiring process at Google has been refined over the last 20 years. Productivity at Google is about the team, not just individual superstars. Core technical skills are important in hiring engineers at Google. Value is important, but if someone is hurting the team, it's a problem.\"}, {'title': 'The Importance of Motivation in the Workplace ', 'text': \"Motivation is important in addition to core technical skills. Alignment of motivation with the team's goals is crucial for long term success. Motivation is important at every level, not just for senior positions. Google's hiring process focuses on puzzle solving and problem solving abilities, but may not fully assess motivation.\"}, {'title': 'Title ', 'text': 'Factors for Hiring at GoogleText '}, {'title': 'Navigating Variability in Engineering Culture at Google ', 'text': 'Balancing the need for full-fledged products with the importance of ensuring things work well. The importance of finding the right fit for different projects and teams. Variability in culture, projects, teams, and product areas across Google. Engineering excellence as a core part of the culture. The challenging and fun aspects of working on difficult things. The key to success in a large ecosystem or small product.'}, {'title': 'Balancing Decision-Making in Ecosystems and Products ', 'text': 'Striking a balance across different aspects of a large ecosystem or a small product. Making hard decisions such as how fast to go versus how perfect it is, involving a huge community, and saying no to certain things. Some decisions are made quickly due to time constraints, while others are given time for consideration. The Dev Summit came together incredibly, with a lot of moving pieces, and the deadline made people rise to the occasion.'}, {'title': 'The Importance of Deadlines in Software Development ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and getting something that works well. The team did a great job in putting TensorFlow 2.0 alpha together. Official deadlines are not always put out, but key things are focused on and their importance is determined. Development is done in the open, both internally and externally, and releases are done at a regular cadence.\"}, {'title': 'Approach to Software Releases in TensorFlow 2.0 ', 'text': \"Releases are done at a regular cadence, with the understanding that if something isn't ready this month, it will be in the next release in a month or two. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it is clear that they are experimental and feedback is encouraged. Quick cycle and quick iteration are important, rather than focusing on strict deadlines. There is no pressure to make TensorFlow 2.0 stable, similar to the approach taken with WordPress 5.0, where updates were delivered late but followed by a rapid release of updates to improve it.\"}, {'title': 'NodeX API Stability and TensorFlow Release Updates ', 'text': 'NodeX API stability is a priority. There is still more work to be done and more releases to come. The release of TensorFlow is imminent. There have been 41 million downloads for version 1.0 X.'}, {'title': 'TensorFlow 1.0 X Release Update ', 'text': 'TensorFlow has 41 million downloads for 1.0 X. The focus is on polishing and putting together features. There is no rush to release the product. The goal is to get it right and focus on quality. The release is planned for the next few months or next quarter. The team will try to make the release happen as soon as possible. Ads connect people to the things they want and need. At their worst, ads can be annoying.'}, {'title': 'The Impact of Search Ads and Machine Learning on User Experience ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience without annoyance. Advertisements can ruin the user experience if they are not relevant to the user's needs and wants. Huge amounts of personalized data can be used for machine learning to shine in connecting users to what they actually want.\"}, {'title': 'The Importance of Accessible Information and Quality Advertising ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the information to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's reading experience.\"}, {'title': 'Balancing Value and Monetization in Advertisements ', 'text': 'Advertisements should strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services like search and websites to provide their service. Good advertisements can be useful and not annoying when done in a good balance.'}, {'title': 'The Shift Towards Paid Online Content ', 'text': 'More paid services are being seen across the web and people are willing to pay for them because they see the value. There is a transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. The trend is towards monetizing content with ads, but there is also a shift towards paid services. Examples like Netflix and YouTube show that people are willing to pay for content. The speaker and people around them have seen a change in willingness to pay for online content over the last few years.'}, {'title': 'Transition to a Mix Model and Use of TPU in Google Call App ', 'text': '- Transition to a mix model with free trials and ads, followed by a clear revenue model.- Use of TPU in a Google call app for free is possible due to TensorFlow being open source and can be run on desktops and phones.- Desktops are becoming more powerful, allowing for more capabilities in running TensorFlow.- Phones are now more powerful than first desktops, allowing for training on the phone.'}, {'title': 'The Power and Convenience of Cloud Computing ', 'text': 'Cloud computing offers more power and convenience compared to traditional desktops. Cloud services like Colab make it easy to get started with no installation needed. Colab is a free service, but paid services offer more features and capabilities. Beginners interested in machine learning and TensorFlow should start by visiting the TensorFlow website.'}, {'title': 'Getting Started with TensorFlow ', 'text': 'Start by going to TensorFlow.org and playing around on the website. Check out tutorials and guides available on the website. No installation needed, can get started right away on Colab. Easy access to resources for learning TensorFlow.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-28 01:52:36.909150 ...\n",
      "Best SD: 2.0610516452281153, Best iteration: 33\n",
      "done get topics 2024-03-28 01:52:37.706303.\n",
      "Stage 2 start time 2024-03-28 01:52:37.706323\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow and Open Innovation on Machine Learning Development\n",
      "2. Scaling and Supporting Hardware for TensorFlow and Challenges in Running Machine Learning on Mobile Phones\n",
      "3. The Importance of TensorFlow Extended in Enterprise Data Analysis and Data Organization for Using TensorFlow\n",
      "4. The Evolution of Keras and TensorFlow and Challenges of Maintaining Compatibility in Production Systems\n",
      "5. Challenges in Integrating TensorFlow.js and Deep Learning JS and The Integration of Machine Learning into Real Products\n",
      "6. The Rise of PyTorch in Research and The Power of Learning from Previous Experiences and Combining Different Elements\n",
      "7. Progress and Milestones in TensorFlow and Restructuring TensorFlow for Improved Modularity\n",
      "8. The Future of Deep Learning and Machine Learning and Hardware Accelerators\n",
      "9. Young Innovators Making Waves and Challenges and Excitement at Google\n",
      "10. TensorFlow 1.0 X Release Update and Approach to Software Releases in TensorFlow 2.0\n",
      "11. The Impact of Search Ads and Machine Learning on User Experience and The Power and Convenience of Cloud Computing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewrite_summary: \n",
      "\n",
      "    You will improve transitions between paragraphs.\n",
      "    Here is the input text:\n",
      "    {summary}\n",
      "    \n",
      "\n",
      "RRR given summary\n",
      "The podcast features Rajat Manga, an engineer and director at Google, who leads the TensorFlow team responsible for the open source library at the forefront of deep learning. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, inspiring other companies to do the same and leading to rapid growth in the field of machine learning. Google's commitment to open innovation has been a driving force behind the success of TensorFlow, and the company continues to focus on pushing the standard forward and supporting the developer community. TensorFlow is now in alpha with a large team of engineers at Google Brain working on its development, and it is designed to work seamlessly with Google Cloud and other platforms.\n",
      "\n",
      "The podcast discusses the development and evolution of TensorFlow, a machine learning framework, and its journey from supporting different hardware to becoming a widely popular and accessible tool for deep learning. The team initially focused on scaling and supporting different hardware for TensorFlow, including GPUs and mobile devices. They drew inspiration from existing libraries like Theano and Caffe, and experimented with the idea of using a graph for deployment. The decision to open source TensorFlow led to its unexpected popularity, with 41 million downloads, and a shift towards stability and deployment for non-research purposes. The company's community-driven approach, good documentation, and focus on addressing the needs of enterprises have contributed to the success and growth of TensorFlow.\n",
      "\n",
      "The podcast discusses the varying needs of enterprises and users, with some prioritizing stability and simplicity over the latest performance and quality. It highlights the continued use of older technology such as Inception and ResNet 50, despite the availability of newer models, demonstrating the value of stability and accessibility. The research community is focused on pushing the boundaries of deep learning with new models like RNNs, transformers, RL, and GANs, with the combination of RL and GANs pushing the state of the art in the field. The podcast emphasizes the common use of older technology in apps and phones, and the importance of making technology easy to use for the majority of the world. Enterprises often use regression models, linear models, or gradient booster trees to make predictions on their data, but deep learning can shine with very large data sets. The podcast also discusses the TensorFlow Extended pipeline, which focuses on stability and simplicity and aims to encourage companies to organize their data for the benefits of using TensorFlow. It also touches on the importance of organizing data for automation and prediction, the availability of more data sets and pre-trained models within the TensorFlow ecosystem, and the interest in the recently released TensorFlow data sets.\n",
      "\n",
      "The release of new data sets has led to a demand for better organization and accessibility, leading to the integration of Keras into TensorFlow. Keras was initially on top of Tiano and then on top of TensorFlow, and its integration into TensorFlow has made transfer learning and basic use cases simple, even for enterprises. The decision to integrate Keras into TensorFlow was the result of a lot of time spent thinking about it and considering various APIs. The goal was to integrate the APIs and make them look similar, and Keras was chosen based on its popularity and positive feedback. The integration process started with a researcher who was doing good work with the API and has been fully involved for two years now. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile, has led to the convergence of all these developments towards the ability to save models in a consistent way and move them between different platforms. The recognition that the ecosystem's scale requires more than one decision maker and the importance of decentralizing decision-making has led to efforts to open up to the community and add transparency, such as setting more processes in place, such as RFCs and special interest groups. The TensorFlow Dev Summit was successful, with lots of new features and an amazing ecosystem being incorporated into TensorFlow, and regular design reviews being conducted. The team and developers all want to make things easier for a large set of developers, and that makes a difference.\n",
      "\n",
      "The podcast discusses the goal of enabling the community to build the things they care about using TensorFlow 2.0. The focus is on making different pieces work well together and sharing models through save model and TensorFlow hub. The release of TensorFlow.js and deep learning JS initially faced skepticism and technical challenges, but the team has iterated and learned a lot in the process. The goal is to make it easy for the end user, but there are still challenges ahead, such as integrating with new devices from a hardware perspective. TensorFlow started as a monolithic system and is still largely monolithic, making it difficult to change and modify. There is a challenge in maintaining previous versions while also moving forward with new versions, as production systems rely on TensorFlow. The podcast also discusses the integration of machine learning into real products and the push to get machine learning on every device with compute capability. The ecosystem for machine learning is growing and covering more aspects over time, with a continuous push to build more tooling and help with ML pipelines.\n",
      "\n",
      "The podcast discusses the importance of designing with a clean slate in mind and not worrying about compromises in order to reach a good place. The speaker emphasizes the benefits of using TensorFlow for its leading position in the ecosystem, number of users, and momentum, especially for production purposes. They also mention the rise of PyTorch in research, focusing on ease of use rather than speed. The text emphasizes the value of learning from previous experiences, exploring different spaces, and revisiting ideas multiple times before implementation. It also highlights the power of combining different elements and the significance of certain decisions, using the analogy of Muhammad Ali versus Frasier.\n",
      "\n",
      "TensorFlow has made significant progress with the release of TensorFlow 2.0, which has improved the ecosystem and made it more accessible. The development team is excited about the clean APIs and potential performance improvements. Restructuring the system into more modular pieces is important for the ecosystem, and major corporations like Pepsi are already using TensorFlow. The growth of the TensorFlow community is attributed to factors such as timing, alignment with industry needs, listening to the community, and creating the right processes and community to support contributors. The focus is on making it easy for developers to use and contribute to the project on GitHub, with a commitment to investing in tooling to support developers and make version changes smooth.\n",
      "\n",
      "The podcast discusses the rapid advancements in the field of deep learning, with a focus on the staying power of convolution models, reinforcement learning, and generative adversarial networks. It also explores potential future developments, such as combining eager execution and graphs for more natural programming, and the use of hardware accelerators like TPUs for training with lower bit precision. The coevolution of TPU and TensorFlow is highlighted, with a goal of making TensorFlow as accessible and easy to use as possible, especially for beginners. The podcast emphasizes the importance of making training and transfer learning easy for users, and the uncertainty surrounding the future of hardware accelerators and training with lower bit precision.\n",
      "\n",
      "The podcast discusses the incredible and sometimes terrifying things that high schoolers are doing, and the potential for amazing ideas coming from the younger generation. It also delves into the technical and management aspects of working with TensorFlow, emphasizing the importance of cohesion and teamwork for successful execution. The podcast highlights Google's unified vision and bottom-up organization, as well as the tensions and complexities in decision-making processes. It also explores the significance of individual superstars and the hiring process at Google, focusing on the importance of motivation and alignment with the team's goals. The podcast touches on the challenges and fun aspects of working on difficult projects, as well as the need to strike a balance in decision-making and the importance of engineering excellence. Overall, the podcast provides insights into the hiring process and the key factors for success in a large ecosystem or small product at Google.\n",
      "\n",
      "The podcast discusses the upcoming release of TensorFlow 2.0, which has had 41 million downloads for version 1.0 X. The team is focused on polishing and putting together features, with no rush to release the product in order to ensure quality. Deadlines bring a sense of urgency, but it's important to strike a balance between perfection and functionality. The team has done a great job putting together TensorFlow 2.0 alpha, and development is done in the open with regular releases. The focus is on moving as fast as possible and iterating on features, with the understanding that not everything needs to be fully stable upon release. NodeX API stability is a priority, and there is still more work to be done before the imminent release of TensorFlow 2.0.\n",
      "\n",
      "The podcast discusses the importance of search ads in providing a personalized and relevant user experience. It emphasizes the need for ads to align with the user's needs and wants, and the balance between providing value to the user and monetization for the service. The trend is towards a mix model of free trials and ads, as well as a shift towards paid services. The use of machine learning and TensorFlow in providing personalized ads is also highlighted, along with the accessibility of resources for learning TensorFlow.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The podcast features Rajat Manga, an engineer and director at Google, who leads the TensorFlow team responsible for the open source library at the forefront of deep learning. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, inspiring other companies to do the same and leading to rapid growth in the field of machine learning. Google's commitment to open innovation has been a driving force behind the success of TensorFlow, and the company continues to focus on pushing the standard forward and supporting the developer community. TensorFlow is now in alpha with a large team of engineers at Google Brain working on its development, and it is designed to work seamlessly with Google Cloud and other platforms.\\n\\nThe podcast discusses the development and evolution of TensorFlow, a machine learning framework, and its journey from supporting different hardware to becoming a widely popular and accessible tool for deep learning. The team initially focused on scaling and supporting different hardware for TensorFlow, including GPUs and mobile devices. They drew inspiration from existing libraries like Theano and Caffe, and experimented with the idea of using a graph for deployment. The decision to open source TensorFlow led to its unexpected popularity, with 41 million downloads, and a shift towards stability and deployment for non-research purposes. The company's community-driven approach, good documentation, and focus on addressing the needs of enterprises have contributed to the success and growth of TensorFlow.\\n\\nThe podcast discusses the varying needs of enterprises and users, with some prioritizing stability and simplicity over the latest performance and quality. It highlights the continued use of older technology such as Inception and ResNet 50, despite the availability of newer models, demonstrating the value of stability and accessibility. The research community is focused on pushing the boundaries of deep learning with new models like RNNs, transformers, RL, and GANs, with the combination of RL and GANs pushing the state of the art in the field. The podcast emphasizes the common use of older technology in apps and phones, and the importance of making technology easy to use for the majority of the world. Enterprises often use regression models, linear models, or gradient booster trees to make predictions on their data, but deep learning can shine with very large data sets. The podcast also discusses the TensorFlow Extended pipeline, which focuses on stability and simplicity and aims to encourage companies to organize their data for the benefits of using TensorFlow. It also touches on the importance of organizing data for automation and prediction, the availability of more data sets and pre-trained models within the TensorFlow ecosystem, and the interest in the recently released TensorFlow data sets.\\n\\nThe release of new data sets has led to a demand for better organization and accessibility, leading to the integration of Keras into TensorFlow. Keras was initially on top of Tiano and then on top of TensorFlow, and its integration into TensorFlow has made transfer learning and basic use cases simple, even for enterprises. The decision to integrate Keras into TensorFlow was the result of a lot of time spent thinking about it and considering various APIs. The goal was to integrate the APIs and make them look similar, and Keras was chosen based on its popularity and positive feedback. The integration process started with a researcher who was doing good work with the API and has been fully involved for two years now. The growth and development of the ecosystem, starting with Andrej Karpathy's ComNetJS and the evolution into TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile, has led to the convergence of all these developments towards the ability to save models in a consistent way and move them between different platforms. The recognition that the ecosystem's scale requires more than one decision maker and the importance of decentralizing decision-making has led to efforts to open up to the community and add transparency, such as setting more processes in place, such as RFCs and special interest groups. The TensorFlow Dev Summit was successful, with lots of new features and an amazing ecosystem being incorporated into TensorFlow, and regular design reviews being conducted. The team and developers all want to make things easier for a large set of developers, and that makes a difference.\\n\\nThe podcast discusses the goal of enabling the community to build the things they care about using TensorFlow 2.0. The focus is on making different pieces work well together and sharing models through save model and TensorFlow hub. The release of TensorFlow.js and deep learning JS initially faced skepticism and technical challenges, but the team has iterated and learned a lot in the process. The goal is to make it easy for the end user, but there are still challenges ahead, such as integrating with new devices from a hardware perspective. TensorFlow started as a monolithic system and is still largely monolithic, making it difficult to change and modify. There is a challenge in maintaining previous versions while also moving forward with new versions, as production systems rely on TensorFlow. The podcast also discusses the integration of machine learning into real products and the push to get machine learning on every device with compute capability. The ecosystem for machine learning is growing and covering more aspects over time, with a continuous push to build more tooling and help with ML pipelines.\\n\\nThe podcast discusses the importance of designing with a clean slate in mind and not worrying about compromises in order to reach a good place. The speaker emphasizes the benefits of using TensorFlow for its leading position in the ecosystem, number of users, and momentum, especially for production purposes. They also mention the rise of PyTorch in research, focusing on ease of use rather than speed. The text emphasizes the value of learning from previous experiences, exploring different spaces, and revisiting ideas multiple times before implementation. It also highlights the power of combining different elements and the significance of certain decisions, using the analogy of Muhammad Ali versus Frasier.\\n\\nTensorFlow has made significant progress with the release of TensorFlow 2.0, which has improved the ecosystem and made it more accessible. The development team is excited about the clean APIs and potential performance improvements. Restructuring the system into more modular pieces is important for the ecosystem, and major corporations like Pepsi are already using TensorFlow. The growth of the TensorFlow community is attributed to factors such as timing, alignment with industry needs, listening to the community, and creating the right processes and community to support contributors. The focus is on making it easy for developers to use and contribute to the project on GitHub, with a commitment to investing in tooling to support developers and make version changes smooth.\\n\\nThe podcast discusses the rapid advancements in the field of deep learning, with a focus on the staying power of convolution models, reinforcement learning, and generative adversarial networks. It also explores potential future developments, such as combining eager execution and graphs for more natural programming, and the use of hardware accelerators like TPUs for training with lower bit precision. The coevolution of TPU and TensorFlow is highlighted, with a goal of making TensorFlow as accessible and easy to use as possible, especially for beginners. The podcast emphasizes the importance of making training and transfer learning easy for users, and the uncertainty surrounding the future of hardware accelerators and training with lower bit precision.\\n\\nThe podcast discusses the incredible and sometimes terrifying things that high schoolers are doing, and the potential for amazing ideas coming from the younger generation. It also delves into the technical and management aspects of working with TensorFlow, emphasizing the importance of cohesion and teamwork for successful execution. The podcast highlights Google's unified vision and bottom-up organization, as well as the tensions and complexities in decision-making processes. It also explores the significance of individual superstars and the hiring process at Google, focusing on the importance of motivation and alignment with the team's goals. The podcast touches on the challenges and fun aspects of working on difficult projects, as well as the need to strike a balance in decision-making and the importance of engineering excellence. Overall, the podcast provides insights into the hiring process and the key factors for success in a large ecosystem or small product at Google.\\n\\nThe podcast discusses the upcoming release of TensorFlow 2.0, which has had 41 million downloads for version 1.0 X. The team is focused on polishing and putting together features, with no rush to release the product in order to ensure quality. Deadlines bring a sense of urgency, but it's important to strike a balance between perfection and functionality. The team has done a great job putting together TensorFlow 2.0 alpha, and development is done in the open with regular releases. The focus is on moving as fast as possible and iterating on features, with the understanding that not everything needs to be fully stable upon release. NodeX API stability is a priority, and there is still more work to be done before the imminent release of TensorFlow 2.0.\\n\\nThe podcast discusses the importance of search ads in providing a personalized and relevant user experience. It emphasizes the need for ads to align with the user's needs and wants, and the balance between providing value to the user and monetization for the service. The trend is towards a mix model of free trials and ads, as well as a shift towards paid services. The use of machine learning and TensorFlow in providing personalized ads is also highlighted, along with the accessibility of resources for learning TensorFlow.\"}]\n",
      "Stage 2 done time 2024-03-28 01:54:40.242356\n",
      "stage_2_titles: len: 11\n",
      "['1. The Impact of TensorFlow and Open Innovation on Machine Learning Development', '2. Scaling and Supporting Hardware for TensorFlow and Challenges in Running Machine Learning on Mobile Phones', '3. The Importance of TensorFlow Extended in Enterprise Data Analysis and Data Organization for Using TensorFlow', '4. The Evolution of Keras and TensorFlow and Challenges of Maintaining Compatibility in Production Systems', '5. Challenges in Integrating TensorFlow.js and Deep Learning JS and The Integration of Machine Learning into Real Products', '6. The Rise of PyTorch in Research and The Power of Learning from Previous Experiences and Combining Different Elements', '7. Progress and Milestones in TensorFlow and Restructuring TensorFlow for Improved Modularity', '8. The Future of Deep Learning and Machine Learning and Hardware Accelerators', '9. Young Innovators Making Waves and Challenges and Excitement at Google', '10. TensorFlow 1.0 X Release Update and Approach to Software Releases in TensorFlow 2.0', '11. The Impact of Search Ads and Machine Learning on User Experience and The Power and Convenience of Cloud Computing']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "#     if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "#        int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:   \n",
    "    if int(podcast['episode_number']) != 22:\n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
