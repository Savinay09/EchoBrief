{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2c\"\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fbbe4e66650>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-24 15:58:19.409217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-24 16:02:48.372077\n",
      "chunks_text len: 65\n",
      "extract_keypoints start time: 2024-03-24 16:02:48.372216\n",
      "extract_keypoints done time 2024-03-24 16:04:48.466187\n",
      "Start time: 2024-03-24 16:04:48.466368\n",
      "Stage 1 done time 2024-03-24 16:07:06.527071\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and the Role of Rajat Manga at Google ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': \"Rajat Manga's Involvement with Google Brain and the Development of TensorFlow \", 'text': \"Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library turned into TensorFlow in 2014, the open source library. Deep learning was interesting and intriguing in some ways back then. The idea of deep learning had shown some very promising and early results. The idea of deep learning hadn't yet taken off, but it held some promise.\"}, {'title': \"The Evolution of Google's Compute Power and Data \", 'text': 'Google has significant compute power and data. Scaling compute and data leads to better results. Early wins in speech and image recognition. Birth of Google Brain focused on neural networks and deep learning.'}, {'title': \"Google's Advancements in Machine Learning \", 'text': \"The dream of what the company could become, with echoes of the open source TensorFlow community and a focus on TPUs and machine learning. The company's ability to scale machine learning to hundreds and thousands of machines, with some runs even going to 10,000 machines, showing great promise. Google's long history of machine learning, with the successful scaling of deep learning and its impact on real products such as speech and image recognition.\"}, {'title': 'The Rise of Deep Learning and Open Innovation in 2014 ', 'text': \"Academia and external push for deep learning in 2014. Recognition of the potential growth of deep learning. Decision to open source TensorFlow as a seminal moment in software engineering. Google's decision to lead the world in open innovation. Embracing the power of open innovation.\"}, {'title': 'The Growth of Open Source in Deep Learning and Machine Learning ', 'text': 'The initial idea for open source came from Jeff, who was a big proponent of it. The research group wanted to share their research and push the state of the art forward. Deep learning and machine learning have grown fast due to sharing research. There were existing libraries like Tiano and Torch, but they were done by academia and at a different level. Google had done lots of software internally that they used.'}, {'title': 'The Impact of Open Source Projects on Technology Advancement ', 'text': 'Open source projects often come out of published papers and can be very successful. Google Cloud is now providing Bigtable and H base APIs, which may not be as good as their original tech, but is what everyone is used to. The goal is to make something better and push a good standard forward, while also helping the community in various ways. The use of Google Cloud resources, such as TensorFlow open source library, fits into this goal.'}, {'title': 'The Power of TensorFlow and Google Cloud ', 'text': 'TensorFlow is open and can be used anywhere. Google Cloud ensures integrations with other platforms and works well with TensorFlow. TensorFlow has an incredible ecosystem. TensorFlow was open sourced in November 2015, after starting development in the summer of 2014.'}, {'title': 'Title ', 'text': 'The Decision to Open Source a Project for Scalable Data Center and Mobile Device SupportText '}, {'title': 'Advancements in Mobile Machine Learning ', 'text': 'Ideas of running machine learning on the phone existed at that time. Customized handcrafted code or internal libraries were used for machine learning products. The use of Theano and Caffe at Google influenced design decisions. The belief was built focused on internal systems, which were very different from existing libraries. The team looked at a number of libraries before making design decisions.'}, {'title': 'Discussion on Machine Learning Frameworks ', 'text': \"The group had experience with Torch, Lua, and Caffe. They discussed ideas around having a graph or not. They wanted flexibility due to the fast-moving research and changing hardware. The flexibility to express all kinds of crazy things was a big factor. Moving towards TensorFlow 2.0, there's more, by default, there'll be eager.\"}, {'title': 'The Use of Graphs in Python for Deployment Considerations ', 'text': 'The graph was initially hidden because it was less intuitive for developers. The graph concept came from the need to deploy production stuff. The decision to use a graph in Python was influenced by the need for deployment simplicity. The balance tilted towards using a graph due to deployment considerations. The question arises about whether the decision to use a graph was intentional.'}, {'title': 'Challenges and Success in Deep Learning Product Development ', 'text': 'Production is a good thing to focus on, but there are potential challenges and unexpected outcomes. There was a need for the product based on research and early days of deep learning. The unexpected success of 41 million downloads was not anticipated. There is potential for even more growth and enabling more people to use the product. Open sourcing led to a significant increase in attention and growth in deep learning. The company is in a good position to leverage the growth in deep learning and deliver on what people want.'}, {'title': 'The Evolution of Deep Learning for Developers ', 'text': 'The project started changing with the global population of developers. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The project is very community-driven. The documentation was a huge step up from academic projects. Deep learning shifted from being a research thing to something that developers could use for interesting things. It allowed people who had no clue about machine learning to do interesting things with it.'}, {'title': 'The Evolution of Enterprise Adoption for Version 1.0 ', 'text': 'The shift in focus from researchers to stability and deployment. The planning and preparation for version 1.0, including documentation and designs. The increasing interest and adoption by enterprises post 1.0 release. The initial interest from researchers and early adopters, followed by enterprise adoption after 1.0. The pressure for enterprise adoption before version 1.0.'}, {'title': 'The Importance of Stability in Technology ', 'text': 'TensorFlow is stable and has been growing incredibly. There are still people running models that are three or four years old, showing the need for stability in technology. Providing stability in technology allows more people to access it. There is value in providing stability and making things simpler. The field of deep learning is extremely dynamic and constantly changing.'}, {'title': 'Advancements and Usability of Machine Learning Techniques ', 'text': 'The crowd is interested in exploring new and advanced techniques in machine learning, such as RNNs, transformers, RL, and GANs. Despite the rapid advancements in the field, older models and techniques are still very usable and stable. Transfer learning, particularly using models like ResNet 50, is a common use case for many people, especially hobbyists.'}, {'title': 'The Role of Data in Enterprise Predictive Modeling ', 'text': 'Enterprises use data for making predictions. They used to rely on regression models and linear models for machine learning. Some still benefit from deep learning, especially with large datasets. The best enterprises have very large datasets where deep learning can shine.'}, {'title': 'The Importance of Organizing Data for TensorFlow 2.0 ', 'text': 'The developer summit for TensorFlow 2.0 focuses on the entire TensorFlow Extended piece, including the entire pipeline. The focus is on stability and simplicity across the entire process, not just training a model. Many companies are still old school in the way they organize their data, which is not ready or digitized. There is a need to evangelize and encourage companies to organize their data in order to benefit from TensorFlow. The speaker often finds themselves in the role of an evangelist for organizing data to get the big benefit of TensorFlow.'}, {'title': 'Challenges and Solutions in Data Organization for Machine Learning ', 'text': 'Questions range from basic to expert level, including the need for help with linear models, lack of data, and where to start with machine learning. The key to automation and making predictions is organizing data in an organized form. The TensorFlow ecosystem is providing more data sets and pre-trained models to make it easier for users. There is a demand for organized data sets and a desire to make data organization easier.'}, {'title': 'The Evolution of Keras and its Relationship with TensorFlow ', 'text': 'Start with the basics, true. Keras made TensorFlow more accessible. Francois started the Keras project before he was at Google. Keras was initially on top of Tiano and then on top of TensorFlow.'}, {'title': \"Creator's Journey from Interface Development to Joining Google and TensorFlow Team \", 'text': 'The creator decided to create an interface and use TensorFlow as a backend. The creator joined Google after creating the interface. The creator initially worked on research ideas and Keras as a side project. The creator eventually joined the TensorFlow team and integrated his work. The creator is also a great researcher with published papers.'}, {'title': 'Integration of Keras into TensorFlow 2.0 ', 'text': 'Keras got integrated into TensorFlow in a deep way. With 2.0, TensorFlow 2.0, Keras is the recommended way for a beginner to interact with TensorFlow. There was a parallel layers API that was being built. The goal was to have the APIs look similar and be as integrated as possible. There were also three other APIs that others had built over time.'}, {'title': 'Choosing Keras for TensorFlow 2.0 ', 'text': 'The community was confused about which model to use for TensorFlow 2.0. The decision was made to simplify and pick one model, and Keras was chosen based on popularity and positive feedback. Initially, there was concern that Keras might be a competitor to TensorFlow, but it ended up being an empowering element of TensorFlow. The combination of Keras and TensorFlow was surprising but ultimately successful.'}, {'title': 'The Role of Key Figures in Open Source Projects ', 'text': 'Python has Guido van Rossum, who until recently held the position of benevolent dictator for life. Successful open source projects like TensorFlow need one person who makes a final decision. The TensorFlow Dev Summit was successful with new features being incorporated and an amazing ecosystem. There are key design directions and open source contributions from multiple people, such as Martin Wick.'}, {'title': 'The Growth and Development of the TensorFlow Ecosystem ', 'text': \"Regular design reviews and transparency efforts. Setting more processes in place such as RFCs and special interest groups to grow the community. The need to scale the ecosystem beyond a lone decision maker. The growth of the ecosystem, starting with Andrej Karpathy's ComNetJS and the development of TensorFlow.js. The significance of being able to train and run neural networks in the browser using JavaScript. The development of TensorFlow Extended.\"}, {'title': 'Advancements in TensorFlow Lite for Mobile ', 'text': 'TensorFlow Lite for mobile. Convergence towards saving models in the same way for desktop and mobile. Goal to enable machine learning in multiple ways. Support for deep learning and other algorithms. Pushing the state of the art in research. Enable researchers to build the next amazing thing. Exciting developments in ML, such as BERT.'}, {'title': 'The Impact of Machine Learning Research on Compute Devices ', 'text': 'ML research and its impact on real products and people. The increasing presence of compute devices across the world. The goal of getting machine learning on every device with compute capability.'}, {'title': 'The Evolution of TensorFlow Libraries and Tooling ', 'text': 'TensorFlow has built tooling such as TensorBoard to help with training and ML pipelines. There are libraries being built on top of TensorFlow for research and production purposes. Some libraries, like TensorFlow agents and TensorFlow probability, started as research tools but are now used in production. Libraries have been developed by teams within Google as well as by the community to address different needs and interests. The goal of TensorFlow is to push the boundaries and cater to various parts of the community.'}, {'title': 'Title ', 'text': 'Enabling Others to Build with TensorFlow 2.0Text '}, {'title': 'Challenges and Progress in Integrating with the Ecosystem ', 'text': 'Integrating into the ecosystem is difficult and has many technical challenges. The process has involved a lot of learning and iteration over the last few years. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating more devices and making it easy for vendors to integrate with TensorFlow. Work is being done on compiler stuff and APIs to address these challenges.'}, {'title': 'Challenges of Managing a Monolithic System ', 'text': 'The system started as monolithic and still has a large monolithic core. It is difficult to change and modify the system due to its age and rapid evolution. The challenge is to innovate while still ensuring previous versions work. Many people rely on TensorFlow in their applications, leading to technical debt. The system is compared to changing the engine of a running car.'}, {'title': 'The Challenge of Balancing Compatibility and Innovation in TensorFlow ', 'text': \"It's easier to start from scratch every few months. Version 2.0 breaks some backward compatibility, but not too much. The conversion to version 2.0 seems pretty straightforward. There is a tricky balance between maintaining compatibility and making new changes. Production systems rely on TensorFlow, and there is concern about compatibility. It is important to keep compatibility for systems that run for a long time. Making new changes comes with a huge cost and requires consideration of many factors. It's a trade-off between making new changes and maintaining compatibility.\"}, {'title': 'The Value of New Ideas in Team Dynamics ', 'text': \"The overall value of bringing new ideas is much bigger than just breaking the person yesterday. It's important to set the tone for new people joining the team by not breaking them when they come on board. When doing new things, it's important to start with a clean slate in mind and then figure out how to make everything work. Designing with a clean slate and not worrying about compromises is essential to reaching a good place. It's important to put past responsibilities behind when thinking of new ideas.\"}, {'title': 'Comparison of TensorFlow and PyTorch ', 'text': 'TensorFlow is leading in many ways, on many dimensions in terms of ecosystem, number of users, momentum, power, production levels. Many researchers are now also using PyTorch. TensorFlow was chosen with production in mind, not just for research. PyTorch is focused on research only.'}, {'title': 'Prioritizing Ease of Use in Iterative Development ', 'text': 'The team started iterating and decided to prioritize ease of use over speed. They had the benefit of learning from previous attempts and exploring different approaches. They built on previous work, such as JNR. The team revisited the idea of eager execution multiple times before deciding to push forward with it. Eager execution has finally come together in version 2.0 after some time and effort.'}, {'title': 'The Power of Eager Execution in TensorFlow ', 'text': 'Eager execution is a powerful addition to TensorFlow. TensorFlow has made incredible progress in the last couple of years. The ecosystem has been improved to make it easily accessible to Keras and eager execution. There are other things that TensorFlow 2.0 enables us to do in the future.'}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs allow for optimization and performance improvements for both single machine and distributed systems. The team is looking forward to exploring new possibilities in future versions after 2.0.'}, {'title': 'Restructuring TensorFlow for Modularity ', 'text': 'The need to restructure the monolithic thing into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow on GitHub consists of lots of repositories, with the core one containing TensorFlow, the execution engine, key backends for CPUs and GPUs, and work for distributed stuff. All these components work together in a single library or binary, with no easy way to split them apart. In a perfect world, there would be clean interfaces for running on custom clusters with custom networking.'}, {'title': 'The Impact of Clean Separation in Spaces and the Use of TensorFlow in Major Corporations ', 'text': 'Clean separation in spaces will help with more interesting things. Enabling people to evolve and push on things independently allows for better scaling. Major corporations like Pepsi are already using TensorFlow. Some enterprises only touch small pieces of TensorFlow and do not do core development or changes.'}, {'title': 'The Impact of Companies like IBM and TensorFlow on User Needs ', 'text': \"Companies like IBM are involved in special interest groups and want to optimize for certain user needs. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. Building a community like that requires a sweet spot of timing, understanding what's needed, and growth potential.\"}, {'title': 'The Importance of Community in Open Source Projects ', 'text': \"TensorFlow's growth is linked to the growth of deep learning. Listening to the community and being open to external contributions is important. Transparency and communication are crucial for an open source project. Community aspects such as welcoming contributors and building the right processes are essential. Small projects may find it easier to manage community aspects.\"}, {'title': 'The Importance of Developer Support and Documentation in the Growth of TensorFlow ', 'text': 'As you grow, putting more processes in place and thinking about documentation and developer needs is important. Building something on TensorFlow and implementing a particular architecture that does something cool and useful feeds the growth of TensorFlow. Putting projects on GitHub contributes to the growth of TensorFlow. There is a focus on tooling and investment in developer support.'}, {'title': 'Navigating Significant Version Changes ', 'text': \"Significant version changes are always a risk, and efforts are being made to make the transition smooth. People want to move to the new thing because they see value in it, not just because it's new. Over the next few months, as people start to see the value, there will be a shift in people moving to the new thing. The field is moving rapidly, which will help in people moving to the new thing. All the new things will clearly happen in 2.x, giving people good reasons to move.\"}, {'title': 'Predicted Changes and Advancements in Deep Learning ', 'text': 'Change is expected to happen in the next five years. Basics of deep learning, such as convolution models, will likely still be around in some form. Reinforcement learning (RL) and Generative Adversarial Networks (GAN) are expected to stay based on their current status. There is a focus on combining eager execution and graphs to make programming more natural. Swift for TensorFlow is taking a similar approach to make programming more natural. More advancements are expected in the next five years.'}, {'title': 'Future Changes in Hardware Accelerators ', 'text': 'Hardware accelerators may change in the future. There is exploration of training with four bits instead of 32 bits. TPU and TensorFlow are coevolving and learning from each other and the community. Efforts are being made to make TensorFlow more accessible and easy to use, especially for beginners. Beginners want to be able to easily train with image models without worrying about the specific model.'}, {'title': 'Importance of Simple and Pre-trained Models in TensorFlow ', 'text': 'Providing simple models is important for ease of use. Different users have different needs, from beginners to researchers. Pre-trained models can decrease the time needed to start using TensorFlow. TensorFlow has made recent improvements to make it more trivial for beginners to use.'}, {'title': 'The Impact of High Schoolers on Cutting Edge Technology ', 'text': 'High schoolers are doing amazing and terrifying things, with incredible ideas coming from them as they grow up. There is a technical aspect to the work, as well as a management aspect in leading the TensorFlow project. Google has been at the forefront of exploring what it takes to build a good team, and TensorFlow is cutting edge technology. Cohesion across the team is important for delivering something well at scale.'}, {'title': 'The Power of Teamwork in Engineering ', 'text': \"Engineers can do more when working together as a team. The product generated by the team is larger than the individual contributions. The culture of the team and hiring good people are important for success. People need to care about what they're building and be motivated for the right things. Having a unified vision of where the team wants to go is crucial. Google is a bottom-up organization, and research is even more so.\"}, {'title': 'Importance of Team Alignment and Project Mission ', 'text': \"It's important to combine the larger product and ecosystem with a clear direction. Monitoring the health of the team and knowing if the team is aligned is important. Work is often done by individual superstars, which can create tensions within a team. The mission of the project, such as TensorFlow, can make it easier to align the team.\"}, {'title': 'Title ', 'text': \"Google's Emphasis on Team Culture and CollaborationText \"}, {'title': \"Refining Google's Hiring Process Over 20 Years \", 'text': 'Google has a refined hiring process over the last 20 years. Core technical skills are important, but motivation and alignment with company goals are also crucial. Motivation is important at every level, from junior to senior positions. Without motivation, even the smartest engineers may struggle to succeed.'}, {'title': \"Google's Hiring Process \", 'text': \"The Google hiring process focuses on determining the candidate's puzzle solving and problem solving abilities. The hiring process also evaluates the candidate's passion and drive to do anything. Culture fit is an important part of the interview process at Google. Each interviewer is supposed to rate the candidate on their culture fit with Google.\"}, {'title': 'Understanding the Culture and Requirements for Projects at Google ', 'text': 'The culture and requirements for projects at Google can vary. TensorFlow is a fast moving project and requires people who are comfortable with that. There is a balance between fast-moving projects and ensuring that things work really well. Finding the right fit for different projects and teams is important. The core culture at Google includes engineering excellence. Working at Google is fun but also hard at times. The culture and requirements can vary across different projects and teams at Google.'}, {'title': 'Striking a Balance for Success ', 'text': 'Difficult things are fun when solved. The key to success is striking a balance across different aspects. Making hard decisions such as how fast to go versus how perfect it is, involving the community, and saying no to certain things. Both quick and thoughtful decisions are important. The balance between deadlines and flexibility in the development of TensorFlow.'}, {'title': 'The Importance of Deadlines in Software Development ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and getting something that works well. The team did a great job in putting everything together for the release of TensorFlow 2.0 alpha. Official deadlines are not put out, but the focus is on key important things. Development is done in the open, both internally and externally, with everything available to everybody. Releases are done at a regular cadence.\"}, {'title': 'The Importance of Quick Iteration and Improvement in Software Development ', 'text': \"Quick iteration and improvement is important in different areas. It's okay to release experimental features and gather feedback. Quick cycle and iteration is prioritized over meeting deadlines. Pressure to make TensorFlow 2.0 stable is not mentioned. Comparison with WordPress 5.0 release and its approach to updates.\"}, {'title': 'Title ', 'text': 'The Progress of TensorFlow 1.0 X Release CandidateText '}, {'title': 'The Impact of Ads on User Experience ', 'text': 'The focus is on getting things right and not rushing. The goal is to release the project in the next few months or next quarter. Ads can have both positive and negative impacts on user experience. Connecting users to the things they need and want through ads is seen as a beautiful opportunity.'}, {'title': 'The Future of Personalized Advertising and Machine Learning ', 'text': 'Machine learning requires huge amounts of personalized data to be effective. Google is leading the world in personalized advertising. The future of ads is aligned with what users need. Search ads are an extension of making information accessible. There is a minimum quality level for search ads to be shown.'}, {'title': 'The Importance of Advertising on the Web ', 'text': 'Advertising is a key part and has been adapted to the web. The balance between showing valuable ads and annoying ads is important. Monetization is necessary for search engines and websites to provide their services. Annoying ads can detract from the user experience. The value of the ad to the user is important.'}, {'title': 'The Evolution of Revenue Streams in the Internet Age ', 'text': 'The model of funding businesses like Google is a significant revenue stream. The internet has a mix of paid and free services, with a transition towards more paid services. Advertisements, when done well, can be useful and not annoying. People are willing to pay for services they see value in, such as Netflix and paid apps. It may take a long time for everything on the internet to be paid for, if at all.'}, {'title': 'Changing Attitudes Towards Paying for Online News ', 'text': 'People are willing to pay for newspaper content on good news websites. There has been a change in attitude towards paying for online content in the past few years. Hopeful for a transition to a mixed model where users can try content for free with ads, but also have a clear revenue model. Open source TensorFlow is mentioned as a side note.'}, {'title': 'The Power and Benefits of Cloud Computing ', 'text': 'Cloud computing allows for easy access and use of powerful resources without the need for installation. Cloud services like Colab are free and great for getting started and exploring new things. The power of desktops continues to increase, but mobile phones are now even more powerful than early desktops. Cloud computing is particularly beneficial for students and courses, making it easy to get started and access resources.'}, {'title': 'Exploring TensorFlow and Machine Learning ', 'text': 'Start by going to TensorFlow.org and exploring the website. Check out tutorials and guides available on the website. You can use Colab to start experimenting with TensorFlow without any installation needed. Rajit and Lex had a conversation about machine learning and TensorFlow.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-24 16:07:07.549870 ...\n",
      "Best SD: 1.311109554714178, Best iteration: 36\n",
      "done get topics 2024-03-24 16:07:08.212935.\n",
      "Stage 2 start time 2024-03-24 16:07:08.212953\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow and Rajat Manga's Role at Google\n",
      "2. Advancements in Mobile Machine Learning and Deployment Considerations\n",
      "3. Usability of Machine Learning Techniques and Data Organization Challenges\n",
      "4. The Evolution of Keras and its Integration with TensorFlow\n",
      "5. The Growth and Development of the TensorFlow Ecosystem and Challenges in Integration\n",
      "6. Balancing Compatibility and Innovation in TensorFlow, Comparison with PyTorch\n",
      "7. Restructuring TensorFlow for Modularity and the Impact of Major Corporations\n",
      "8. Navigating Significant Version Changes and Future Changes in Hardware Accelerators\n",
      "9. The Power of Teamwork in Engineering and Google's Hiring Process\n",
      "10. Quick Iteration and Improvement in Software Development, Importance of Deadlines\n",
      "11. Exploring TensorFlow and Machine Learning, The Impact of Ads on User Experience and the Future of Personalized Advertising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "Rajat Manga, director of Google's TensorFlow team, discusses the evolution of TensorFlow from a proprietary machine learning library to an open source ecosystem. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, inspiring other companies to do the same. Google's significant compute power and data have allowed for the scaling of deep learning, leading to early successes in speech and image recognition. The company's commitment to open innovation and the sharing of research has contributed to the rapid growth of deep learning and machine learning. Google Cloud now provides integrations with TensorFlow, allowing for its use anywhere and ensuring compatibility with other platforms. TensorFlow 2.0, currently in alpha, is being developed by a large team of engineers at Google Brain, with a focus on growing a passionate community of developers. The podcast also touches on the history of Google Brain, the push for deep learning in academia, and the impact of open source projects on the tech industry.\n",
      "\n",
      "The podcast discusses the decision to open source a project for scalable data center and mobile device support, focusing on the evolution of machine learning on mobile devices and the development of TensorFlow 2.0. The team considered various libraries and ultimately chose to use a graph in Python for deployment simplicity. The unexpected success of the open sourced project led to significant growth in deep learning and a shift from research-focused to community-driven development. The project's focus shifted from researchers to stability and deployment, leading to the planning and preparation for version 1.0 and increased interest and adoption by enterprises. The podcast also highlights the challenges and unexpected outcomes of enterprise adoption before version 1.0, as well as the project's impact on enabling developers to use machine learning for interesting applications. Overall, the decision to open source the project has led to a thriving community, extensive documentation, and a shift in focus towards practical applications of deep learning.\n",
      "\n",
      "The podcast discusses the interest in exploring new machine learning techniques such as RNNs, transformers, RL, and GANs, while also highlighting the continued usability and stability of older models and techniques. Transfer learning, particularly using models like ResNet 50, is common among hobbyists, while enterprises use data for predictions, with some still benefiting from deep learning, especially with large datasets. The developer summit for TensorFlow 2.0 focuses on stability and simplicity across the entire process, not just training a model. Many companies still organize their data in an old-school way, and there is a need to evangelize and encourage companies to organize their data to benefit from TensorFlow. The speaker often finds themselves in the role of an evangelist for organizing data to get the big benefit of TensorFlow. The key to automation and making predictions is organizing data in an organized form. The TensorFlow ecosystem is providing more data sets and pre-trained models to make it easier for users, and there is a demand for organized data sets and a desire to make data organization easier. The field of deep learning is dynamic and constantly changing, but there is value in providing stability and making things simpler.\n",
      "\n",
      "The podcast discusses the evolution of Keras and its integration into TensorFlow. Francois, the creator of Keras, initially developed it as an interface on top of Tiano and later integrated it with TensorFlow. After joining Google, he worked on research ideas and eventually integrated Keras into the TensorFlow team. With the release of TensorFlow 2.0, Keras became the recommended way for beginners to interact with TensorFlow. The decision to simplify and choose Keras as the primary model for TensorFlow 2.0 was based on its popularity and positive feedback. Initially, there were concerns about Keras competing with TensorFlow, but it ultimately became an empowering element of the platform. The podcast also highlights the importance of having a single decision-maker in successful open source projects, as well as the success of the TensorFlow Dev Summit and the contributions from multiple individuals, such as Martin Wick.\n",
      "\n",
      "The podcast discusses the importance of regular design reviews and transparency efforts in growing the community, as well as the need to scale the ecosystem beyond a lone decision maker. It highlights the development of TensorFlow.js, which allows for training and running neural networks in the browser using JavaScript, and the significance of enabling machine learning in multiple ways. The podcast also covers the goal of getting machine learning on every device with compute capability, as well as the development of tooling and libraries on top of TensorFlow for research and production purposes. It emphasizes the challenges and complexities behind integrating into the ecosystem, as well as the ongoing work to address technical challenges and make it easier for vendors to integrate with TensorFlow. Overall, the podcast showcases the exciting developments in machine learning research and its impact on real products and people, as well as the goal of pushing the boundaries and catering to various parts of the community.\n",
      "\n",
      "The podcast discusses the challenges of transitioning to TensorFlow version 2.0 while maintaining backward compatibility. It emphasizes the importance of balancing new changes with compatibility for long-running systems. The team prioritized ease of use over speed and learned from previous attempts to improve the ecosystem and enable eager execution in version 2.0. The podcast highlights the potential for performance improvements and clean APIs in the new version, as well as the excitement for exploring new possibilities in future versions. However, the system's monolithic core and rapid evolution present challenges in innovating while ensuring previous versions still work. The comparison is made to changing the engine of a running car, emphasizing the technical debt and reliance on TensorFlow in applications. Overall, the podcast emphasizes the trade-off between making new changes and maintaining compatibility, as well as the potential for progress and innovation with TensorFlow 2.0 and beyond.\n",
      "\n",
      "The podcast discusses the need to restructure TensorFlow into more modular pieces for better scalability and to meet the needs of different users. Currently, TensorFlow on GitHub consists of many repositories, making it difficult to split the components apart. Major corporations like Pepsi are already using TensorFlow, and companies like IBM are involved in special interest groups to optimize for certain user needs. The growth of TensorFlow is linked to the growth of deep learning, and building a community requires timing, understanding, and growth potential. Transparency, communication, and welcoming contributors are crucial for an open source project. As projects grow, more processes and documentation are needed. Implementing architectures on TensorFlow and putting projects on GitHub contribute to its growth, with a focus on tooling and developer support.\n",
      "\n",
      "The podcast discusses the significant version changes in the field of deep learning and TensorFlow. Efforts are being made to make the transition to new versions smooth, with a focus on providing value to users. The field is rapidly evolving, and it is expected that people will start moving to the new versions over the next few months. Despite the changes, basics of deep learning such as convolution models are expected to remain, while advancements in reinforcement learning and Generative Adversarial Networks are anticipated. There is also a focus on making programming more natural, with the introduction of Swift for TensorFlow. Hardware accelerators may change in the future, with exploration of training with four bits instead of 32 bits. Efforts are being made to make TensorFlow more accessible and easy to use, especially for beginners, by providing simple pre-trained models. Overall, the podcast highlights the ongoing developments in TensorFlow and the efforts to make it more user-friendly and accessible for a wide range of users.\n",
      "\n",
      "The podcast discusses the importance of engineers working together as a team to create a larger product than individual contributions. It emphasizes the significance of team culture, hiring the right people, and having a unified vision for success. Google's bottom-up organization and emphasis on research are highlighted, along with the importance of monitoring the team's health and alignment. The hiring process at Google focuses on technical skills, motivation, and culture fit, with different projects requiring different types of individuals. The core culture at Google includes engineering excellence, and the company is at the forefront of exploring what it takes to build a good team. The podcast also touches on the management aspect of leading projects, such as TensorFlow, and the importance of cohesion across the team for delivering something well at scale.\n",
      "\n",
      "The podcast discusses the importance of quick iteration and improvement in different areas, prioritizing quick cycle and iteration over meeting deadlines. It compares the approach to updates in TensorFlow 2.0 with the release of WordPress 5.0. The podcast emphasizes the balance between deadlines and flexibility in development, making hard decisions, and involving the community. It highlights the team's success in putting everything together for the release of TensorFlow 2.0 alpha, with a focus on key important things and regular cadence of releases. The pressure to make TensorFlow 2.0 stable is not mentioned, and the development is done in the open, both internally and externally, with everything available to everybody.\n",
      "\n",
      "Rajit and Lex discuss the importance of getting machine learning and TensorFlow right before rushing to release a project. They explore the impact of ads on user experience, noting that while personalized advertising can be beneficial, there is a balance between valuable and annoying ads. They also discuss the necessity of monetization for search engines and websites, and the potential transition towards more paid services on the internet. They mention the benefits of open source TensorFlow and the ease of access to powerful resources through cloud computing, particularly for students and courses. They express hope for a mixed model where users can access content for free with ads, but also have a clear revenue model. Overall, they emphasize the importance of taking the time to do things right in the world of machine learning and advertising, while also acknowledging the potential for positive change in the future.\n",
      "RRR rewritten summary\n",
      "[{'text': \"Rajat Manga, director of Google's TensorFlow team, discusses the evolution of TensorFlow from a proprietary machine learning library to an open source ecosystem. The decision to open source TensorFlow in 2015 was a pivotal moment in the tech industry, inspiring other companies to do the same. Google's significant compute power and data have allowed for the scaling of deep learning, leading to early successes in speech and image recognition. The company's commitment to open innovation and the sharing of research has contributed to the rapid growth of deep learning and machine learning. Google Cloud now provides integrations with TensorFlow, allowing for its use anywhere and ensuring compatibility with other platforms. TensorFlow 2.0, currently in alpha, is being developed by a large team of engineers at Google Brain, with a focus on growing a passionate community of developers. The history of Google Brain, the push for deep learning in academia, and the impact of open source projects on the tech industry are also discussed.\\n\\nThe podcast delves into the decision to open source a project for scalable data center and mobile device support, focusing on the evolution of machine learning on mobile devices and the development of TensorFlow 2.0. The unexpected success of the open sourced project led to significant growth in deep learning and a shift from research-focused to community-driven development. The project's focus shifted from researchers to stability and deployment, leading to the planning and preparation for version 1.0 and increased interest and adoption by enterprises. The challenges and unexpected outcomes of enterprise adoption before version 1.0, as well as the project's impact on enabling developers to use machine learning for interesting applications, are also highlighted.\\n\\nThe interest in exploring new machine learning techniques such as RNNs, transformers, RL, and GANs is discussed, along with the continued usability and stability of older models and techniques. The developer summit for TensorFlow 2.0 focuses on stability and simplicity across the entire process, not just training a model. The importance of evangelizing and encouraging companies to organize their data to benefit from TensorFlow is emphasized, along with the value of providing stability and making things simpler in the dynamic field of deep learning.\\n\\nThe evolution of Keras and its integration into TensorFlow is explored, along with the decision to simplify and choose Keras as the primary model for TensorFlow 2.0. The importance of having a single decision-maker in successful open source projects, as well as the success of the TensorFlow Dev Summit and the contributions from multiple individuals, such as Martin Wick, are also highlighted.\\n\\nThe challenges of transitioning to TensorFlow version 2.0 while maintaining backward compatibility are discussed, emphasizing the trade-off between making new changes and maintaining compatibility, as well as the potential for progress and innovation with TensorFlow 2.0 and beyond.\\n\\nThe need to restructure TensorFlow into more modular pieces for better scalability and to meet the needs of different users is emphasized, along with the ongoing developments in TensorFlow and the efforts to make it more user-friendly and accessible for a wide range of users.\\n\\nThe importance of engineers working together as a team to create a larger product than individual contributions is highlighted, along with the significance of team culture, hiring the right people, and having a unified vision for success.\\n\\nThe importance of quick iteration and improvement in different areas, prioritizing quick cycle and iteration over meeting deadlines, is discussed, along with the balance between deadlines and flexibility in development.\\n\\nRajit and Lex discuss the importance of getting machine learning and TensorFlow right before rushing to release a project, as well as the impact of ads on user experience and the potential transition towards more paid services on the internet. They express hope for a mixed model where users can access content for free with ads, but also have a clear revenue model.\"}]\n",
      "Stage 2 done time 2024-03-24 16:08:44.522457\n",
      "stage_2_titles: len: 11\n",
      "[\"1. The Impact of TensorFlow and Rajat Manga's Role at Google\", '2. Advancements in Mobile Machine Learning and Deployment Considerations', '3. Usability of Machine Learning Techniques and Data Organization Challenges', '4. The Evolution of Keras and its Integration with TensorFlow', '5. The Growth and Development of the TensorFlow Ecosystem and Challenges in Integration', '6. Balancing Compatibility and Innovation in TensorFlow, Comparison with PyTorch', '7. Restructuring TensorFlow for Modularity and the Impact of Major Corporations', '8. Navigating Significant Version Changes and Future Changes in Hardware Accelerators', \"9. The Power of Teamwork in Engineering and Google's Hiring Process\", '10. Quick Iteration and Improvement in Software Development, Importance of Deadlines', '11. Exploring TensorFlow and Machine Learning, The Impact of Ads on User Experience and the Future of Personalized Advertising']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
