{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b\"\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f9ff6197b10>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-25 21:24:07.418685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 21:28:44.651653\n",
      "chunks_text len: 66\n",
      "extract_keypoints start time: 2024-03-25 21:28:44.651796\n",
      "extract_keypoints done time 2024-03-25 21:31:15.804888\n",
      "Start time: 2024-03-25 21:31:15.805080\n",
      "Stage 1 done time 2024-03-25 21:33:59.872702\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Thomas Sanholm: Professor, AI System Co-Creator, and Game Theory Expert ', 'text': 'Thomas Sanholm is a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in the game of Heads Up No Limit Texas Holdem. He has published over 450 papers on game theory and machine learning, including a best paper in 2017 at NIPS, now renamed to Newrips. His research and companies have had wide-reaching impact in the real world, proposing new ideas and building systems to prove that these ideas work in the real world. The conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast. The game of poker, Texas Holdem, Heads Up Texas Holdem is discussed in the conversation.'}, {'title': 'Heads Up No Limit Texas Holdem: A Benchmark for AI Algorithms ', 'text': \"['Heads Up No Limit Texas Holdem is a main benchmark for testing AI algorithms for imperfect information game solving.', 'It is a game played by humans, but not often seen on TV or in casinos.', 'It is played in expert level casinos and in the World Series of Poker.', 'It is typically played online for large sums of money.', 'It is a game usually only played by experts.', 'It is different from regular No Limit Texas Holdem and is more competitive.']\"}, {'title': 'Understanding Texas Holdem Poker ', 'text': \"Texas Holdem is a game typically played by a big group and is not as competitive. Heads Up means it's a game between two players, much like chess or go. Texas Holdem is an imperfect information game, making it harder to play. In Texas Holdem, each player has two private cards and there are gradually laid out public cards. The imperfect nature of the information in Texas Holdem is the two private cards each player holds. The game involves betting rounds after receiving private and public cards.\"}, {'title': 'Popular Game with Imperfect Information ', 'text': 'The game being described involves four betting rounds and four tranches of information revelation. The first tranche is private, and the rest are public. This game is popular in AI and the general public due to its imperfect information nature. It is considered as popular as chess in terms of AI setting the bar for intelligence. In 2017, Labratus, a program, beat four expert human players in this game. The event raises questions about what was learned from it and the process involved.'}, {'title': 'High Stakes Poker Tournament at Rivers Casino ', 'text': 'The event involved inviting four of the top 10 players in Heads Up No Limit, Texas Holdem. The game is different from the multiplayer version, so statistical significance was important. The players were brought to Pittsburgh to play at the Rivers Casino for 20 days. The goal was to get 120,000 hands in to achieve statistical significance. The players played from morning to evening for 20 days. A 200,000 incentive was raised for the players to play. The players were paid based on how they did against the AI. This setup provided an incentive for the players to play as hard as possible.'}, {'title': 'Incentive to Play Hard Against AI ', 'text': 'Players had an incentive to play as hard as they could against the AI. Originally explored playing for money, but the Pennsylvania Gaming Board said no. The players kept track of the money and brought in close to $2 million. Playing for real money would have been an impressive and inspiring achievement.'}, {'title': 'Top Players and AI in Online Gaming ', 'text': \"Top players are used to playing the game mostly online through a UI. The game was played with a layout on a screen, with the human and AI sitting at a virtual table. The screen showed everything that was happening, including the cards and bets. There was a betting history for the human to reference back to. The human's memory was not a factor, as they were top quality people. The AI was not trying to take advantage of the human's memory.\"}, {'title': 'AI Libratus Outperforms Humans in Brains vs AI Competition ', 'text': 'The AI, Libratus, was able to outperform humans in a brains versus AI competition. The organizer initially thought they had a 50/50 chance against the AI. International betting sites put the humans as a four to one or five to one underdog against the AI. People tend to have more confidence in other people compared to the performance of AI. The performance of AI can be underestimated in comparison to human abilities.'}, {'title': 'Underestimating AI in Poker ', 'text': 'People underestimated the performance of AI in poker. Despite winning against humans, AI was still considered an underdog in betting sites. There is a belief that human facial expressions and body language are critical to poker. People have confidence that humans will outperform AI because AI cannot perceive human tells. AI systems only look at betting patterns and statistics, not human tells. The importance of human perception in poker is questioned.'}, {'title': 'The Importance of Human Players in Betting Patterns and Statistics ', 'text': 'The importance of human players in comparison to AI in betting patterns and statistics. The skepticism about AI being able to beat top human players in games. The difficulty in finding tells among top players due to their skill in hiding them. The lack of worth in investing effort to find tells among top players.'}, {'title': 'The Importance of Tells and Abstraction in Poker ', 'text': 'Tells are important in poker, especially at lower levels of play. At higher levels of poker, tells become less important as the game involves a larger number of strategies and possible actions. The game tree in poker is too large to solve directly, so abstraction is necessary. Abstraction in games is more challenging than in other types of games.'}, {'title': 'Abstraction in Games and its Impact on Strategies ', 'text': 'Abstraction in games is trickier than in MDPs or other single agent settings. Finer grained abstraction can lead to worse strategies in the real game. There are hands abstractions and betting strategies in games. Information abstraction involves abstracting what chance does, such as cards in the case of poker. Action abstraction involves abstracting the actions of the actual players, such as bets in the case of poker.'}, {'title': 'Automated Action Abstraction Technology in Gaming ', 'text': 'The algorithms used for information abstraction are potential aware and consider how the hand might materialize over time. The action abstraction is based on how humans and other AIs have played the game in the past. Automated action abstraction technology was initially used, but it is not very scalable. The strength of the hand and how it is played are both important factors in the game.'}, {'title': 'The Importance of Hand Strength and Information Abstraction in Poker ', 'text': 'The strength of the hand and the information abstraction are important in playing poker. The betting actions may be the key to winning regardless of the hands you have. Playing a lot of hands reduces the role of luck in the game. No Limit Texas Holdem has a high level of variance and massive swings. Statistical significance in poker requires playing over 100,000 hands.'}, {'title': 'The Use of Learning Methods in Poker Playing ', 'text': 'Annette Oberstad, a Norwegian female poker player, won a tournament by using a unique playing style. Labradus does not use deep learning methods, unlike DeepStack. The effectiveness of deep learning in poker playing is unclear. The discussion is about the use of learning methods to aid in the way Labradus plays poker.'}, {'title': 'The Importance of Learning Methods in Games ', 'text': \"Labradus did not use learning methods and played very well without them. There are papers on things that do use learning techniques, including deep learning. In imperfect information games like poker, the value of an information set depends on both players' beliefs. The value of a state in poker is not just a function of the cards, but also depends on the path of play and both players' beliefs.\"}, {'title': 'Understanding Game State and Evaluation in Imperfect Information Games ', 'text': 'The state of a game is not solely determined by the cards, but also by the path of play and belief distributions. Perfect information games have a straightforward concept of state and evaluation function, but in other games, such as poker, it is more complicated. In some research papers, the opponent is allowed to take different strategies at the leaf of the search tree, leading to a different approach in evaluating the game state. Allowing the opponent to choose from a set of different continuation strategies forces a more realistic and less optimistic look ahead search. This approach is a way to ensure a sound look ahead search in games with imperfect information.'}, {'title': 'Improving Search Techniques in Imperfect Information Games ', 'text': 'Look ahead search in imperfect information games is very difficult. Randomly generating various situations in the game. Using deep learning to learn the values of states, including belief distributions. Similar techniques to alpha beta search or Monte Carlo tree search are used.'}, {'title': 'Advancements in Search Algorithms for Game Playing ', 'text': \"Different search algorithms are used in alpha beta search or Monte Carlo tree search. Libratus did not have to worry about dealing with leaves because it only did it at the end of the game. The new paper introduces depth limited search for imperfect information games, allowing for sound depth limited lookahead from the beginning of the game. In Libratus, depth limited lookaheads were only done at the end of the game. The opponent's beliefs are explicitly modeled in the new approach.\"}, {'title': 'The Importance of Beliefs in Game Theory ', 'text': 'Beliefs are actually output, not input. Starting beliefs are input, but they fall from the rules of the game. The dealer deals uniformly from the deck, making every pair of cards equally likely. Card removal is important in the game. Adjusting beliefs is where the beauty of game theory comes in.'}, {'title': 'Understanding Nash Equilibrium ', 'text': \"Nash equilibrium was introduced by John Nash in 1950 and defines rational play when there are multiple players. It involves pairs of strategies for each player, where neither player wants to deviate given that the other doesn't deviate. Nash equilibrium also defines beliefs for both players and for each state in the game. It provides a probability distribution over the real world states in the mind of each player at each information set in the game.\"}, {'title': 'Understanding Probability Distribution and Game Theory in Decision Making ', 'text': \"The text discusses probability distribution over real world states in the mind. It explains the concept of player one and player two moves in a game. It emphasizes the importance of player two not knowing player one's move to avoid player two winning every time. It mentions the use of information sets for player one and player two. It talks about Nash equilibrium and the strategy for player one to play 1/3 Rock, 1/3 Paper, 1/3 Scissors. It highlights the derivation of beliefs on the information set using Bayes' theorem. It mentions that Bayes' theorem is related to game theory.\"}, {'title': 'Understanding Game Theory and Opponent Modeling ', 'text': 'Game theory is not player specific and does not require any data or history of how specific players or AI have played in the past. It is based on rationality and considers what a rational opponent would do and what the player would do if they are rational. Game theory is a data-free and opponent-free approach, focusing on the design of the game rather than the design of the player. Opponent modeling is not a primary focus in game theory, although it can be combined with game theory to exploit weak players. Exploiting opponents can also open the player up to exploitation, especially when facing skilled opponents with few weaknesses.'}, {'title': 'Strategies and Exploitation in Zero Sum Games ', 'text': \"Opening oneself up to exploitation by turning on certain strategies. The opponents are experts in counter exploitation. The decision to not turn on certain strategies. Interest in exploring papers exploiting opponents. Work on hybrid digested and its safety in zero sum games. The impact of opponent's irrational behavior on beliefs and game outcomes.\"}, {'title': 'Game Theory Strategies ', 'text': \"The player can gain by throwing off the opponent's belief is always less than they lose by playing poorly. A game theoretic strategy is unbeatable, but it doesn't maximally beat the other opponent. The hybrid strategy involves starting from a game theoretic approach and then tweaking the strategy based on opponent data. Repeated games and the Prisoner's Dilemma are mentioned as examples.\"}, {'title': 'Key Concepts in Game Theory ', 'text': \"Prisoner's Dilemma and repeated games are important concepts in game theory. There is no proof that repeated games are the best strategy, but experimentally they perform well. Games can have perfect or imperfect information, and can be zero sum or non-zero sum. There is a distinction between two-player and multiplayer games. A repeated game is when the same game is played over and over. Extensive form games involve thinking about the game in a specific way.\"}, {'title': 'Understanding Extensive Form Games and Repeated Interactions ', 'text': \"Extensive form games involve repetitive interactions and incomplete information sets. Repeated games are a special case of extensive form games, but the game doesn't have to be exactly the same each time. Sourcing auctions involve the same supply base year to year, but what is being bought and the supply base itself can vary. Purely repeated games are very rare in the world and are a coarse model of what's going on. Stochastic games are in between simple repeated matrix games and extensive form games, involving little matrix games and actions taken by opponents.\"}, {'title': 'Types of Games and Their Characteristics ', 'text': 'The text discusses different types of games, including matrix games, stochastic games, and extensive form games. It mentions that when taking an action in a game, it determines the distribution over next games where the player might be going to. The text highlights poker as an example of an extensive form game, which is the most general setting. It mentions that the AI community has been working on and being benchmarked on Heads Up No Limit Texas Holdem, which is an extensive form game. The text compares the tree form of games, such as chess, with the matrix form or bi matrix form of games. It discusses the concept of reasoning in the tree form of games.'}, {'title': 'Importance of Tree Form in Game Theory ', 'text': 'Tree form allows for certain types of reasoning that are lost in normal form. Equivalence exists between tree form and normal form, but sequentiality is lost in the transition. Multiplayer versus two player distinction is important in game theory. Two player games in zero sum are conceptually and computationally easier. In two player games, any equilibrium strategy is a best response to any other equilibrium strategy.'}, {'title': 'Understanding Nash Equilibrium in Finite Games ', 'text': 'Nash equilibrium is present in all finite games, as proven by John Nash. The issue lies in the fact that there can be multiple Nash equilibriums, leading to the question of which one to select. In non zero sum games, selecting strategies from different equilibriums may result in a loss of joint benefit. There is a significant gap between two-player zero sum games and two-player general sum or three-player zero sum games, at least in theory. Despite the gap, it is believed that a Nash equilibrium should still be achievable and instructive in these scenarios.'}, {'title': 'Challenges and Strategies in Multiplayer Games ', 'text': 'In non zero sum games, joint benefit may be lost by being simply stupid, and both parties could be better off by doing something else. In three player games, collusion can occur, where two players gang up on a third player to achieve better outcomes. Collaboration or cooperation between poker players can make the game extremely difficult for current AI methods to solve. The ability of poker players to collaborate introduces new challenges and complexities to the game.'}, {'title': 'Advancements in Coalitional Game Theory ', 'text': 'The speaker has done a lot of work on coalitional games and has a paper on it. They have presented their work at a poster session at NIPS. Collusion in games presents a different and typically harder problem. Some game representations do not allow for good computation. The speaker introduced a new game representation for dealing with collusion. There are still some unknowns, such as in the game of bridge.'}, {'title': 'The Importance of Coordination in Strategic Games ', 'text': 'Coordination in games like bridge requires strategies to be coordinated ahead of time and signals to be understood by both teams. In other situations like auctions, negotiations, and diplomatic relationships, coordination is not built into the rules but can still be helpful for colluders. Prior strategies and willingness to do certain things are important in negotiations and other applications beyond poker.'}, {'title': 'Expanding Business Ventures Beyond Poker ', 'text': 'Moving away from poker and into other applications like negotiations. Has two startup companies - Strategic Machine and Strategy Robot. Strategic Machine is for business applications, gaming, sports, etc. Strategy Robot is for military security, cyber security, and intelligence applications. Also involved in a company called Optimized Markets for combinatorial market and optimization based technology.'}, {'title': 'Underutilization of Game Theoretic Reasoning Technologies ', 'text': \"Game theoretic reasoning technologies are not being used. The high level consideration is about the ability to use game theoretic concepts to model human behavior. The goal may not necessarily be modeling humans. In a zero sum game, the opponent's adherence to the model of rational behavior may not be a concern. Formalizing the interaction in games is a prerequisite for analysis. Mechanism design has been used to design games with specific outcomes. An example from the world of autonomous vehicles is provided, where the study of pedestrians and their negotiation with cars is mentioned.\"}, {'title': 'Negotiating Nonverbal Communication Between Pedestrians and Cars ', 'text': 'Pedestrians and cars negotiate in nonverbal communication. There is a tension between pedestrians and cars, especially for jaywalkers. Modeling human behavior in these situations is challenging. Game theory and imperfect information approaches may be useful in modeling intent. Designing a game to describe the situation is a problem in itself. Autonomous vehicles could potentially address the issue of jaywalking. Fleets of autonomous cars operated by different companies could be a potential solution.'}, {'title': 'Benefits of Automated Negotiation for Fleets of Autonomous Cars ', 'text': 'Fleets of autonomous cars operated by different companies, such as Waymo and Uber, can benefit from prenegotiated rules of the road. Automated negotiation can be used to prenegotiate various driving situations, allowing for faster and more efficient merging and decision-making. The use of automated negotiation can help address the complexity of negotiating numerous driving situations manually. Automated negotiation can also involve trade-offs, such as allowing one car to go first in certain situations in exchange for priority in others.'}, {'title': 'Title ', 'text': 'The Role of Imperfect Information in Negotiation and AI DevelopmentText '}, {'title': \"AI's Role in Performance-Oriented Research \", 'text': \"AI has stepped up in an engineering and scientific effort to beat human players. The speaker's group is involved in performance oriented research, spanning from idea to theory to experiments to big system building to commercialization. Building big systems and evaluating them at scale is important in AI to know what works and doesn't. Techniques in computational game theory that look good in small scale may not look good in large scale.\"}, {'title': 'Algorithm Performance in Theory vs Reality ', 'text': \"Theory doesn't always match reality in terms of algorithm performance. First order methods may have better convergence rates in theory, but CFR based algorithms are the fastest in practice. Testing in reality is necessary to determine the best algorithms. Projections from small scale tests can be misleading in this domain. Personal experience with organizing the first brains versus AI poker competition was wild.\"}, {'title': 'Controversy Surrounding AI in Heads Up No Limit Poker Competition ', 'text': 'This was the first competition for Heads Up No Limit poker. The speaker became the most hated person in the world of poker due to their involvement in AI cracking the game. Many people felt that AI was a real threat to the existence of the game. The speaker received aggressive comments and even death threats for their involvement in AI. The speaker believes that humans can still enjoy the game of poker despite AI outperforming them in chess.'}, {'title': 'The Impact of AIs on the Game of Poker ', 'text': \"The AIs have changed how the game of poker is played. The top humans are now incorporating the AIs' strategies into their own play. The AIs have made poker a richer and more interesting game for humans to play. The speaker has learned to love the game of poker through working with AIs. The speaker believes that the AIs have not steered humans away from the game of poker entirely. The speaker finds it brave to put ideas to the test in academia.\"}, {'title': 'Challenges of Scaling Good Ideas ', 'text': \"Good ideas don't always work when applied at scale. It takes a lot of work and time to organize and make something big. It is important to do things in the real world and at scale. Proof is in the pudding, meaning the real test is in the real world and at scale.\"}, {'title': 'Competition and Mechanism Design in Heads Up No Limit, Texas Holdem ', 'text': 'Competition between different groups to beat the top humans at Heads Up No Limit, Texas Holdem. Mechanism design is about designing the rules of the game to achieve a certain desirable outcome. The topic of mechanism design is interesting and new to the speaker. The speaker is an observer of mechanisms, including politics. There is work on automated mechanism design to achieve desirable outcomes in an automatic fashion. The speaker wonders if the political system can be designed in an automated fashion.'}, {'title': 'Challenges and Limitations in Mechanism Design ', 'text': 'The automated mechanism design direction is still believed in, but it is not a panacea. There are impossibility results in mechanism design, stating that certain objectives cannot be accomplished in certain classes. These impossibility results are not statements about human ingenuity, but proofs that certain properties cannot be achieved with any mechanism. It is unlikely and impossible to achieve certain properties in mechanism design, regardless of the mechanism used.'}, {'title': 'Automated Mechanism Design and Impossibility Results ', 'text': 'Automated mechanism design allows for specific settings to be designed for, even if there are impossibility results for the whole class. It is possible to carve out islands of possibility within known impossible classes. The Meyerson Satethweight theorem by Roger Meyerson and Mark Satethweight from 1983 shows an impossibility of efficient trade under imperfect information, but it is possible to avoid that in many settings and still achieve efficient trade. The existence of impossibility results does not mean that all cases in a class are impossible, just that some of the cases are impossible.'}, {'title': 'The Application of Mechanism Design in Real-World Scenarios ', 'text': 'The impossibility result is still present, but there are spots within the impossible class where the impossibility does not exist. The lessons drawn from mechanism design can be applied to politics, human interaction, and designing mechanisms for various scenarios. Mechanism design itself has had limited success so far, with certain cases being successful but most real-world situations not being sound from a mechanism design perspective.'}, {'title': 'Challenges of Applying Mechanism Design in Real World Situations ', 'text': 'Real world situations are often not sound from a mechanism design perspective. Insights from theory are applied into the real world rather than applying mechanisms directly. The FCC spectrum auctions are an example where bidding truthfully is not the best strategy. Mechanism design aims to make things easy for participants, but truth telling is not the best strategy in high stakes auctions. The rules designed in practice for the FCC spectrum auctions do not align with game theory principles.'}, {'title': 'Optimal Bidding Strategies and AI Milestones in Spectrum Auctions ', 'text': \"Truth telling is not the best strategy in spectrum auctions. There is no single optimal bidding strategy for spectrum auctions. Bidding truthfully wouldn't be the best strategy even with just two or one item for sale. AI history is marked by seminal events such as AlphaGo beating a world champion human Go player and Liberatus winning the Heads Up No Limit Holdem. Heads Up No Limit Texas Holdem was the one remaining widely game solving.\"}, {'title': 'Advancements in Game Solving ', 'text': 'Heads Up No Limit Texas Holdem was widely agreed upon as a benchmark for game solving. There are other games being worked on, such as StarCraft, Dota 2, Diplomacy, and Hanabi. None of these games are acknowledged as the main next challenge problem like chess or Go. The hope is that there will be a next benchmark to drive application independent techniques forward.'}, {'title': 'The State of Game Solving Technology in Comparison to Machine Learning ', 'text': 'The speaker is involved in two startups related to game solving technology. They are not as focused on recreational benchmarks. Game solving technology is not as mature or widely applied as machine learning. Machine learning has proven success in the real world, while game solving technology has almost no applications.'}, {'title': 'The Potential of Computational Game Theory in Military Planning and Business Strategy ', 'text': 'Machine learning has shown success in real-world applications, but there are almost no applications in game solving. The next big breakthrough could be the use of computational game theory in military planning and business strategy. Machine learning methods, such as neural networks, lack transparency and explainability, while game theoretic methods, like Nash equilibria, may offer more transparency and explainability.'}, {'title': 'The Properties of Nash Equilibria and Game Theoretic Strategies ', 'text': 'Nash equilibria and game theoretic strategies have provable properties. Unlike deep learning, game theoretic strategies have provable solution quality guarantees. The strategies in game theory may not be human understandable, similar to deep learning. Computational game theory and deep learning are in the same boat in terms of human understandability.'}, {'title': 'Challenges and Excitement in Game Theory and Deep Learning ', 'text': 'Deep learning and computational game theory are both difficult to understand. Game theoretic techniques have guarantees of solution quality, but this is more of a belief than a substantiated fact. The future of provable optimality in game theory is exciting. There are concerns about the negative impact of artificial intelligence on society, especially in the context of games like poker.'}, {'title': 'Impact of Nationwide Kidney Exchange and Combinatorial Sourcing Auctions ', 'text': 'The nationwide kidney exchange has saved hundreds of lives and increased employment in the healthcare industry. Combinatorial sourcing auctions led to a 12.6% increase in supply chain efficiency, resulting in over $6 billion of efficiency improvement.'}, {'title': 'Efficiency Improvement and Safety in the World ', 'text': '$6 billion of efficiency improvement in the world. Efficiency improvement in trucking, less empty driving, less waste, and less carbon footprint. AI is going to make the world much safer. Concern about existential threats of AI and value misalignment. Game theory has a role to play in addressing concerns about AI.'}, {'title': 'The Role of Game Theory in Addressing Value Misalignment ', 'text': 'Game theory has a role to play in ensuring that values are aligned with human beings. Value misalignment is a theoretical worry and has not been seen in real applications. Example of a potential value misalignment in the late eighties when building transportation optimization systems. The idea of high utilization of assets as an objective could lead to impractical solutions.'}, {'title': 'Challenges in Achieving 100% Utilization and the Gap Between Theory and Reality ', 'text': 'The solution of loading trucks full and driving in circles to achieve 100% utilization may not be practical in reality. AI can optimize the wrong objective to the hilt, causing more harm than good. There is a gap between theory and reality, making it difficult to put into words. The worst possible case or bad cases imagined theoretically may not always happen in reality. The presence of 10,000 nuclear weapons in the world is a concerning reality.'}, {'title': 'The Threat of Nuclear War and Climate Change ', 'text': 'The speaker grew up in the Soviet Union. There are currently 10,000 nuclear weapons in the world. The speaker is surprised that nuclear war has not broken out. The two biggest threats facing mankind are climate change and nuclear war. The speaker has tried to do something about climate change through their startups. The speaker commissioned studies on what could be done for climate change. The speaker is still keeping an eye out for potential market solutions or optimizations for climate change.'}, {'title': 'Need for Market, Optimization, and Technology Solutions for Environmental and Political Issues ', 'text': 'Market solutions, optimization solutions, and technology solutions are needed for problems such as pollution. Lack of political will can hinder the success of market solutions. The shutdown of the Chicago market in the US highlights the impact of political will on market success. Global warming is an encroaching problem, while nuclear weapons have been a longstanding issue. The speaker is extremely worried about the issue of nuclear weapons.'}, {'title': 'The Growing Risk of Nuclear Conflict and the Role of AI ', 'text': 'The game theory of mutually assured destruction is based on the idea that nobody wants to initiate a conflict. With the availability of smaller nuclear weapons and the involvement of smaller countries and non-nation actors, the risk of nuclear conflict has increased. The application of AI in the future is an exciting prospect, especially in the context of NIPS (NeurIPS) and the potential for advancements in various industries.'}, {'title': 'Developing Scalable Techniques for Game Solving and Real-World Applications ', 'text': 'The focus is on developing scalable techniques for game solving and applying them in the real world. Interest in market design and optimized markets. Priority is on strategic machine strategy robot and getting the technology out there. Understanding the technology gaps that still need to be filled through real applications. Enjoying the interaction and challenge of applying state-of-the-art techniques in real-world scenarios.'}, {'title': 'Challenges of Implementing State-of-the-Art Techniques ', 'text': 'The challenge of applying state-of-the-art techniques in industry or the military. The difficulty in integrating new technology due to lack of systems in place for data, compute, etc. The slowness and inertia of sticking to traditional ways of doing things. The need to find internal champions at the customer who understand the need for change in the future.'}, {'title': 'Rise of Autonomous Vehicles ', 'text': 'Autonomous vehicles are a topic of interest for both traditional car makers and tech companies like Google and Baidu. The speaker finds it fascinating that tech companies unrelated to transportation are pushing for autonomous cars. The speaker is excited about the potential impact of these ideas in the world. There are different games being solved, including those with hidden player actions, such as poker.'}, {'title': 'Title ', 'text': 'Understanding Different Types of StrategyText '}, {'title': 'Advancements in Automated Algorithm Configuration ', 'text': 'The speaker is interested in learning more scalable techniques for integer programming. They had a paper at ICML on automated algorithm configuration with theoretical generalization guarantees. This is the first time there has been generalization theory for automated algorithm configuration. The speaker is honored to talk to Tomas and thanks him for bringing Labradus to the world. The conversation ends with no more questions.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 21:34:01.093858 ...\n",
      "Best SD: 2.4037008503093262, Best iteration: 22\n",
      "done get topics 2024-03-25 21:34:01.780867.\n",
      "Stage 2 start time 2024-03-25 21:34:01.780887\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. Thomas Sanholm: AI System Co-Creator and Game Theory Expert\n",
      "2. The Impact of Abstraction and Learning Methods in Poker\n",
      "3. Understanding Beliefs and Nash Equilibrium in Game Theory\n",
      "4. Importance of Coordination and Extensive Form Games\n",
      "5. Expanding Game Theoretic Reasoning Beyond Poker\n",
      "6. Controversy Surrounding AI in Poker Competition\n",
      "7. Challenges and Applications of Mechanism Design\n",
      "8. Advancements in Game Solving and Computational Game Theory\n",
      "9. Advancements in Automated Algorithm Configuration and Addressing Global Issues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "The podcast features Thomas Sanholm, a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. The game is a main benchmark for testing AI algorithms for imperfect information game solving and is typically played online for large sums of money. In 2017, Labratus beat four expert human players in a 20-day competition at the Rivers Casino, raising questions about the process and what was learned. Despite winning, the AI was still considered an underdog in betting sites, as people tend to have more confidence in human abilities. The importance of human perception in poker, such as facial expressions and body language, is questioned, as AI systems only look at betting patterns and statistics. The difficulty in finding tells among top players due to their skill in hiding them is also discussed. The conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast.\n",
      "\n",
      "The podcast discusses the importance of tells in poker, especially at lower levels of play, and how they become less important at higher levels due to the complexity of the game. It also explores the challenges of abstraction in poker, including hands and betting strategies, and the use of algorithms for information and action abstraction. The effectiveness of deep learning in poker playing is unclear, and the podcast delves into the use of learning methods to aid in the way Labradus plays poker. It also discusses the value of information sets and game states in imperfect information games like poker, and the difficulty of look ahead search in these games. The podcast introduces a new approach for depth limited search in imperfect information games, explicitly modeling the opponent's beliefs. Overall, the podcast provides insights into the complexities of poker playing, the challenges of abstraction, and the use of different search algorithms in imperfect information games.\n",
      "\n",
      "The podcast discusses the concept of beliefs as output, not input, in game theory. It explains Nash equilibrium and the importance of adjusting beliefs in strategic play. The focus is on rational play and the design of the game rather than the player. It also explores the impact of opponent behavior on beliefs and game outcomes, as well as the use of hybrid strategies based on opponent data. The podcast mentions the Prisoner's Dilemma and repeated games as important concepts in game theory, and the distinction between perfect and imperfect information, zero sum and non-zero sum games, and two-player and multiplayer games. It also touches on the use of extensive form games in strategic thinking. Overall, the podcast provides an overview of key concepts in game theory and their application in strategic decision-making.\n",
      "\n",
      "The podcast discusses the importance of coordination in games like bridge, auctions, negotiations, and diplomatic relationships. It explores different types of games, including matrix games, stochastic games, and extensive form games, with a focus on poker as an example of an extensive form game. The text also delves into the concept of Nash equilibrium in game theory, the challenges of non zero sum games, and the potential for collusion in multiplayer games. The speaker has done extensive work on coalitional games and has presented their research at NIPS. The podcast highlights the complexities and challenges introduced by collaboration and collusion in games, particularly in the context of poker, and the difficulties it presents for current AI methods. The speaker also introduces a new game representation for dealing with collusion, but acknowledges that there are still unknowns, particularly in the game of bridge.\n",
      "\n",
      "The podcast discusses the transition from poker to other applications such as negotiations, with a focus on two startup companies - Strategic Machine and Strategy Robot. These companies are involved in business, gaming, sports, military security, cyber security, and intelligence applications. The discussion also includes the use of game theoretic concepts to model human behavior, particularly in zero sum games and the formalization of interactions in games for analysis. Mechanism design is used to design games with specific outcomes, and the podcast provides an example of modeling pedestrian and car negotiations in the context of autonomous vehicles. It suggests that fleets of autonomous cars operated by different companies could benefit from prenegotiated rules of the road, and automated negotiation can help address the complexity of negotiating numerous driving situations manually. The use of automated negotiation can also involve trade-offs, such as allowing one car to go first in certain situations in exchange for priority in others. Overall, the podcast explores the role of imperfect information in negotiation and AI development.\n",
      "\n",
      "The speaker discusses their involvement in AI cracking the game of poker, which led to backlash and even death threats. Despite the controversy, the speaker believes that AI has made poker a richer and more interesting game for humans to play. They argue that humans can still enjoy the game, even though AIs have outperformed them in chess. The speaker's group is involved in performance-oriented research in AI, and they emphasize the importance of testing algorithms in reality rather than relying solely on theory. They also discuss the impact of AIs on the game of poker and how top human players are incorporating AI strategies into their own play. The speaker finds it brave to put ideas to the test in academia and believes that AIs have not steered humans away from the game of poker entirely. Their personal experience with organizing the first brains versus AI poker competition was described as wild.\n",
      "\n",
      "The podcast discusses the challenges of applying good ideas at scale, using the example of competition in Texas Holdem. It explores the concept of mechanism design and its potential application in politics and automated systems. The speaker acknowledges the limitations and impossibility results in mechanism design, but also highlights the potential for success in specific settings. The podcast emphasizes the need to apply insights from theory into real-world situations, citing the example of FCC spectrum auctions where truth telling is not the best strategy. Overall, the podcast provides a critical examination of mechanism design and its practical implications in various scenarios.\n",
      "\n",
      "The podcast discusses the lack of a single optimal bidding strategy for spectrum auctions, noting that truth telling is not the best approach. It highlights AI's achievements in game solving, such as AlphaGo beating a world champion human Go player and Liberatus winning Heads Up No Limit Holdem. The speaker is involved in startups related to game solving technology and believes that the next breakthrough could be the use of computational game theory in military planning and business strategy. The podcast also compares the transparency and explainability of machine learning methods, like neural networks, with game theoretic methods, like Nash equilibria, and discusses the potential negative impact of artificial intelligence on society, particularly in the context of games like poker.\n",
      "\n",
      "The podcast discusses various topics related to strategy, including automated algorithm configuration, the impact of combinatorial sourcing auctions on supply chain efficiency, concerns about AI and value misalignment, the threat of nuclear war and climate change, and the potential application of AI in industries such as transportation. The speaker emphasizes the need for scalable techniques for game solving and market design, as well as the challenges of integrating new technology and finding internal champions for change. The discussion also touches on the interest in autonomous vehicles and the involvement of tech companies in the transportation industry. Overall, the podcast explores the intersection of technology, strategy, and real-world applications, highlighting the potential impact and challenges of implementing state-of-the-art techniques in various industries.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The podcast features Thomas Sanholm, a professor at CMU and co-creator of Labratus, the first AI system to beat top human players in Heads Up No Limit Texas Holdem. In 2017, Labratus beat four expert human players in a 20-day competition at the Rivers Casino, raising questions about the process and what was learned. Despite winning, the AI was still considered an underdog in betting sites, as people tend to have more confidence in human abilities. The importance of human perception in poker, such as facial expressions and body language, is questioned, as AI systems only look at betting patterns and statistics. The difficulty in finding tells among top players due to their skill in hiding them is also discussed. The conversation is part of the MIT course on artificial general intelligence and the artificial intelligence podcast.\\n\\nThe importance of tells in poker, especially at lower levels of play, and how they become less important at higher levels due to the complexity of the game is discussed. It also explores the challenges of abstraction in poker, including hands and betting strategies, and the use of algorithms for information and action abstraction. The effectiveness of deep learning in poker playing is unclear, and the podcast delves into the use of learning methods to aid in the way Labradus plays poker. It also discusses the value of information sets and game states in imperfect information games like poker, and the difficulty of look ahead search in these games. The podcast introduces a new approach for depth limited search in imperfect information games, explicitly modeling the opponent's beliefs.\\n\\nThe concept of beliefs as output, not input, in game theory is discussed. It explains Nash equilibrium and the importance of adjusting beliefs in strategic play. The focus is on rational play and the design of the game rather than the player. It also explores the impact of opponent behavior on beliefs and game outcomes, as well as the use of hybrid strategies based on opponent data. The podcast mentions the Prisoner's Dilemma and repeated games as important concepts in game theory, and the distinction between perfect and imperfect information, zero sum and non-zero sum games, and two-player and multiplayer games. It also touches on the use of extensive form games in strategic thinking.\\n\\nThe importance of coordination in games like bridge, auctions, negotiations, and diplomatic relationships is explored. It also delves into the concept of Nash equilibrium in game theory, the challenges of non zero sum games, and the potential for collusion in multiplayer games. The speaker has done extensive work on coalitional games and has presented their research at NIPS. The podcast highlights the complexities and challenges introduced by collaboration and collusion in games, particularly in the context of poker, and the difficulties it presents for current AI methods.\\n\\nThe transition from poker to other applications such as negotiations is discussed, with a focus on two startup companies - Strategic Machine and Strategy Robot. These companies are involved in business, gaming, sports, military security, cyber security, and intelligence applications. The discussion also includes the use of game theoretic concepts to model human behavior, particularly in zero sum games and the formalization of interactions in games for analysis. Mechanism design is used to design games with specific outcomes, and the podcast provides an example of modeling pedestrian and car negotiations in the context of autonomous vehicles.\\n\\nThe speaker discusses their involvement in AI cracking the game of poker, which led to backlash and even death threats. Despite the controversy, the speaker believes that AI has made poker a richer and more interesting game for humans to play. They argue that humans can still enjoy the game, even though AIs have outperformed them in chess. The speaker's group is involved in performance-oriented research in AI, and they emphasize the importance of testing algorithms in reality rather than relying solely on theory. They also discuss the impact of AIs on the game of poker and how top human players are incorporating AI strategies into their own play.\\n\\nThe challenges of applying good ideas at scale, using the example of competition in Texas Holdem, are explored. It emphasizes the need to apply insights from theory into real-world situations, citing the example of FCC spectrum auctions where truth telling is not the best strategy. The podcast provides a critical examination of mechanism design and its practical implications in various scenarios.\\n\\nThe lack of a single optimal bidding strategy for spectrum auctions is discussed, noting that truth telling is not the best approach. The speaker is involved in startups related to game solving technology and believes that the next breakthrough could be the use of computational game theory in military planning and business strategy. The podcast also compares the transparency and explainability of machine learning methods, like neural networks, with game theoretic methods, like Nash equilibria, and discusses the potential negative impact of artificial intelligence on society, particularly in the context of games like poker.\\n\\nVarious topics related to strategy, including automated algorithm configuration, the impact of combinatorial sourcing auctions on supply chain efficiency, concerns about AI and value misalignment, the threat of nuclear war and climate change, and the potential application of AI in industries such as transportation are discussed. The speaker emphasizes the need for scalable techniques for game solving and market design, as well as the challenges of integrating new technology and finding internal champions for change. The discussion also touches on the interest in autonomous vehicles and the involvement of tech companies in the transportation industry. Overall, the podcast explores the intersection of technology, strategy, and real-world applications, highlighting the potential impact and challenges of implementing state-of-the-art techniques in various industries.\"}]\n",
      "Stage 2 done time 2024-03-25 21:35:19.622558\n",
      "stage_2_titles: len: 9\n",
      "['1. Thomas Sanholm: AI System Co-Creator and Game Theory Expert', '2. The Impact of Abstraction and Learning Methods in Poker', '3. Understanding Beliefs and Nash Equilibrium in Game Theory', '4. Importance of Coordination and Extensive Form Games', '5. Expanding Game Theoretic Reasoning Beyond Poker', '6. Controversy Surrounding AI in Poker Competition', '7. Challenges and Applications of Mechanism Design', '8. Advancements in Game Solving and Computational Game Theory', '9. Advancements in Automated Algorithm Configuration and Addressing Global Issues']\n",
      "remove_questions start time: 2024-03-25 21:35:19.639166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 21:40:49.972767\n",
      "chunks_text len: 73\n",
      "extract_keypoints start time: 2024-03-25 21:40:49.972902\n",
      "extract_keypoints done time 2024-03-25 21:43:26.355323\n",
      "Start time: 2024-03-25 21:43:26.355584\n",
      "Stage 1 done time 2024-03-25 21:46:03.931412\n",
      "RR stage_1_outputs:\n",
      "[{'title': \"The Future Evolution of Adobe's Products with Deep Learning Methods \", 'text': \"Adobe Research is working to define the future evolution of their products to make the life of creatives easier and automate tedious tasks. The use of deep learning methods in the past decade can greatly benefit the evolution of Adobe's products. Gavin Miller, head of Adobe Research, combines tech and creativity, as he writes poetry and builds robots outside of his work. This conversation is part of the Artificial Intelligence Podcast.\"}, {'title': 'Gavin Miller: Head of Adobe Research and Multifaceted Artist ', 'text': 'Gavin Miller is the head of Adobe Research and leads innovative efforts and applications of AI in creating images, video, audio, and language. He is also an artist, poet, writer, and roboticist. Lux Friedman, the interviewer, enjoys Gavin Miller\\'s poetry and promises not to spend the entire conversation reading it. Gavin Miller\\'s poem \"Je Ne Vinaigrette Rien\" parodies both Edith Piaf\\'s \"Je Ne Vinaigrette Rien\" and Frank Sinatra\\'s \"My Way\".'}, {'title': 'Navigating the Struggle with Weight and Dieting ', 'text': 'The speaker opens with a poem about struggling with weight and dieting. The poem reflects the internal struggle between wanting to lose weight and resisting the idea. The speaker mentions being interested in writing, technology, and invention since high school. The speaker acknowledges that weight and dieting is a serious topic for some people. The speaker embraces the idea of not regretting their decision to resist dieting. The speaker mentions parallel strands in their life, one being more private and the other related to writing and technology.'}, {'title': 'The Intersection of Technology and Creative Expression ', 'text': 'The intersection of private life and technological career. The influence of one idea on the other. The inspiration from science fiction for building new technology. The example of using voice synthesis for writing a poem. The impact of technology limitations on creative expression.'}, {'title': 'Creating Poems and Smart Homes in the 90s ', 'text': 'The speaker created a poem to match the tone of a voice that sounded sad and depressed. The poem was pretended to be written by an intelligent agent, telling the user to go home and leave them alone, but also expressing loneliness and a desire for company. The speaker had a project at home in the early 90s involving a smart home, including a talking voice that reminded them of tasks and buttons on the washing machine to prevent clothes from getting moldy. The speaker also made photo albums that used some form of technology.'}, {'title': 'Exploring Magical Realism and Modern Technology ', 'text': 'The idea of magical realism and whether it was possible to achieve it with technology intrigued the speaker. The speaker created photo albums with light sensors that would send signals to an agent to play sounds matching the image in the book. The speaker has written plays and designed personalities for modern agents, thinking about imaginary dialogues and how to make them real and knowledgeable. The speaker is interested in the concept of the uncanny and the potential of modern agents to achieve it.'}, {'title': 'Challenges in AI Communication ', 'text': '[\"AI can fall into the uncanny valley where it says something it doesn\\'t really understand.\", \"AI needs to have multiple ways of talking about the same concept to sound like it really understands it.\", \"Having only one way of referring to something makes it feel like a canned response.\", \"AI needs to be able to reason about a concept and give similar responses from different perspectives to seem more sentient.\"]'}, {'title': 'Advancements in Automatic Image Captioning ', 'text': \"Automatic image captioning with the ability to generate different kinds of statements about the same picture. Work on turning a medium from one form to another, such as auto tagging imagery or making up full sentences about what's in the image. Use of GANs to synthesize an asset that matches a given description. Early career focus on 3D computer graphics and pioneering work before movies had special effects done with 3D graphics. Comparison of early career work to the Renaissance, where people would model light, color, and shape.\"}, {'title': 'Advancements in Computer Image Generation ', 'text': 'The new wave in computer image generation is more impressionistic and uses AI algorithms. The creative process is shifting towards generating images directly from ideas rather than focusing on raw pixels. Adobe aims to cover the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The realistic simulations are important for creators who want to achieve a truly analog look and feel in their digital artwork.'}, {'title': 'The Impact of AI on Artistic Creativity ', 'text': 'Complete control is important for creating expressive and novel work. Automation of certain tasks frees artists to focus on inspiration. Design work used to take up a lot of time for artists. AI aims to reason about likely intent for different formats and languages. Artists can focus on the creative aspect while AI handles formatting and language changes.'}, {'title': 'The Evolution of Creativity in the Digital Age ', 'text': 'Creativity is changing, making it easier, faster, and cheaper to create beautiful artwork. There is a shift from hands-on artisan to art director or conceptual artist, with the computer as a partner in creating polished examples. Adobe products such as Photoshop, Premiere, and Audition are favored for creating images, videos, and audio.'}, {'title': 'Optimizing Workflow with Photoshop, Premiere, and Audition ', 'text': 'The speaker uses Photoshop to create thumbnails and Premiere to edit videos. They also use Audition for audio editing. They manually set up their workflow and use a Kinesis keyboard and auto hotkey to optimize efficiency. They are interested in how AI and automation will make the pixel workflow easier in the future. Photoshop already has a rich array of algorithms for image editing, including procedural and data-based algorithms with many sliders and degrees of freedom.'}, {'title': 'Improving Selection Algorithms with AI ', 'text': 'AI can help by providing default settings based on the content itself. Smart defaults can make life easier for people. Selection algorithms can be improved by incorporating visual common sense about objects like cats and dogs. Quick select algorithm currently relies on color boundaries and flood fill into physically connected regions. The current algorithm is based on rules of thumb and graph theory.'}, {'title': 'Improving Object Selection in Images with Graph Theory and Neural Nets ', 'text': 'Graph theory and neural nets have improved the process of selecting and identifying objects in images. Neural nets can now accurately identify dominant objects in images without the need for manual selection. The use of neural nets for object selection can be valuable if they can provide high-quality results and be easily adjustable. Background removal and object selection can be made easier with the use of neural nets. The goal is to make processes like background removal as easy as possible for the user.'}, {'title': 'The Challenge of Background Removal ', 'text': 'The challenge of removing the background is discussed. Quick, cheap, and cheerful background removal options are available with algorithms. Different algorithms are available for different levels of guidance on boundaries. Combinations of tools are available for various background removal needs. Demonstration of quick object selection at Adobe Max conference.'}, {'title': 'Improving Selection Mask Creation with Simple Polygon Drawing ', 'text': 'Drawing a simple polygon around the object of interest can quickly create a selection mask. This process can be done for a single still or a moving target. The workflow has been significantly reduced from hours to a few seconds. The concept presented at Max has elements that inspire the concept and can work well in the majority of cases. The challenge is to make something that works in all cases and becomes a robust tool. There is a difference between academic research and industrial research in achieving robustness.'}, {'title': 'Distinguishing Academic Research from Industrial Research ', 'text': 'The difference between academic research and industrial research is highlighted. Academic research focuses on great new ideas that show promise, while industrial research involves shipping and receiving customer reviews. The company values customer feedback and product critics to improve their products. The goal is not to be perfect every single time, but to be perfect enough of the time and have a mechanism to intervene and recover from mistakes. The company values talented customers and aims to support them in improving their products.'}, {'title': 'The Impact of AI on Professional Tasks ', 'text': 'AI can make professional tasks less tedious and time-consuming. Collaboration between human and machine can make the life of creatives easier. Learning new algorithms and features can be done through tutorials, videos, and exploration.'}, {'title': 'Improving User Learning and Expertise in Tool Usage ', 'text': \"The focus is on helping the person in the moment to do the task they need to do, as well as thinking holistically about their journey learning a tool. The goal is for users to become experts in using the tool, similar to living in a city where you know the important streets you need to get to. Projects in research analyze thousands of hours of tutorials online to understand what's being taught in them. One publication at CHI looked at the last three or four actions users did in tutorials to determine what other people did next, providing inspiration for what to do next or for watching the tutorial.\"}, {'title': 'AI-Powered App for Creative Inspiration ', 'text': 'The app provides inspiration for next steps and tutorials. It learns from similar workflows and makes intelligent suggestions. It uses context to make suggestions about choices or assist in the creative process. The goal is to deeply understand the domain of designers and combine it with AI for intelligent suggestions. The app aims to provide verbal possibilities or show results of trying different options. The ultimate goal is to have AI assist in the creative process.'}, {'title': 'The Importance of Guidance and Understanding in Art and Education ', 'text': 'The grand challenge of having an artist and a teacher guiding the process. Giving enough at each stage to build a foundation for the next level of expectation. Understanding different media types visually and in terms of transcripts and words. Removing the barrier of having to type in keywords for searching. Assisting with learning the interface in the longer term.'}, {'title': 'The Impact of Adding an Assistant to GUI Design ', 'text': 'The discussion is about whether an assistant modifies the interface to be simpler, rather than just helping with learning the interface. Adding a feature to a GUI can increase visual complexity for new users, while having an assistant with a new skill is additive without being intimidating. The focus is on onboarding new users and considering their needs. Some users value mastering a complex interface and keyboard shortcuts, while others prefer a more assistive and simple approach. The goal is to make the interface a musical instrument for expressing visual ideas. Different users have different preferences for getting things done quickly and efficiently.'}, {'title': 'Exciting Applications of Computer Vision and Machine Learning by Adobe ', 'text': 'Adobe is working on exciting applications of computer vision and machine learning. These applications include scene stitching, sky replacement, foreground and background removal, spatial object based image search, automatic image captioning, project cloak, project deep fill, project scribbler, style transform video, style transform faces and video with project puppetron. Different classes of devices may be more suitable for different tasks, such as CAPTCHA or deep post production workflow. Some tasks may require a laptop or big screen desktop with more knobs and dials for subtlety. There are many potential uses for assistive versions of technology in various contexts.'}, {'title': 'The Sky Replace Feature in Image Editing ', 'text': 'Sky replace is an interesting feature that allows for automatic selection and replacement of the sky in an image. It also matches the geometry of the scene and provides variety in sky choices to cover different moods. The tool also recolors the foreground objects based on the new sky, adding a realistic touch to the edited image.'}, {'title': 'Enhancing Photography with Natural Effects ', 'text': 'The evening sky adds an orange glow to foreground objects. The artist Magritte and his surrealism influence. The goal is to achieve natural effects without the need for extensive post-production. The ability to capture an entire workflow in a single action. The efficiency of being able to apply the effect to multiple backgrounds quickly. The freedom to explore the design space and find inspiration.'}, {'title': 'Exploring Design Space and Advanced Image Search Methods ', 'text': 'The importance of exploring the design space as close to final production value as possible. The idea of making intelligent choices about ways to search stock images. The concept of concept canvas and its application in image search. The need for more advanced search methods beyond just text-based keyword searches.'}, {'title': 'Improving Image Search with Concept Canvas ', 'text': 'Concept canvas allows assigning spatial regions to keywords for image search. Pre-indexed images are used to match important concepts in the picture. Gives a sense of ownership over the outcome of the event. Allows for spatial design and layout, making it feel like design. Technologies in Photoshop allow for physical movement of objects in post-production.'}, {'title': 'Advancements in Object Removal and Background Filling with Neural Networks ', 'text': 'Neural networks are being used to remove objects from a scene and fill in the background automatically. GANs (Generative Adversarial Networks) are one approach for achieving this. Traditional algorithms like content aware fill work well for certain classes of images, such as those with naturalistic textures like gravel paths. Patch-based algorithms can create plausible looking intermediate fill for naturalistic textures. Algorithms are used to smooth out the lighting in the filled region to avoid brightness contrast.'}, {'title': 'Challenges in Computer Vision and Generative Methods ', 'text': 'The importance of smooth lighting in avoiding brightness contrast. The challenge of inferring invisible structure behind objects. The common sense knowledge required to fill in missing information. The limitations of current generative methods in high resolutions.'}, {'title': 'Challenges and Strategies in Advancing AI Image Recognition ', 'text': 'The need to transition from low resolution to high resolution using other algorithms or pushing the state of the art in research. The importance of a diverse training set of images for AI to show common sense and readiness for primetime. The potential use of guardrails and detectors to estimate the competence of AI algorithms. The concept of an ensemble of experts specialized in certain things and the idea of voting on confidence levels.'}, {'title': 'Improving Workflow and Gathering User Data ', 'text': 'The process involves either voting on confidence in future actions or using a dispatcher to assign tasks based on expertise. Each model requires a significant amount of work, but over time, the set will be filled out and capabilities will expand. The focus initially will be on specific workflows, with the potential to branch out as capabilities grow. The goal is to gather information on the workflows and needs of Photoshop users to better understand the type of data that needs to be annotated and collected.'}, {'title': 'The Importance of Data Annotation and Collection for AI ', 'text': 'Data annotation and collection are crucial for building effective AI tools. The importance of gathering and respecting data for AI. The need to demonstrate the benefits of sharing data with the tool. Balancing the use of data for understanding intent and making better recommendations with respecting privacy and obtaining explicit permission.'}, {'title': 'Data Sharing and Workflow in Professional Environments ', 'text': 'Workers may be willing to share workflows or choices with the data set to be trained. Technologies exist for learning without storing all information permanently. Adobe exists in a space where sharing data with them has an obvious benefit for improving workflow. Some professional workflows may be very protective of their data.'}, {'title': 'Protecting Data in Professional Workflows ', 'text': 'Professional workflows may require protection of data, especially in legal cases. Some scenarios may involve a more permissive relationship with Adobe for non-confidential projects. Different levels of data sharing may be possible in exchange for benefits. Capture high-level data from more people and detailed knowledge from willing participants. Explicit customer studies are currently used to gather detailed feedback.'}, {'title': 'Importance of Responsible Data Collection in Customer Studies ', 'text': \"Customer studies involve visiting and observing users to improve the tool. A more systematic process is needed to train an algorithm for customer studies. Conscious effort is needed to balance data collection with customer trust. Adobe has a chief privacy officer to ensure responsible data collection. Privacy is a priority in thinking about AI, not an afterthought. Project Puppetron demonstrates Adobe's move towards thinking in 3D, not just 2D.\"}, {'title': 'Advancements in 3D Thinking for Applying Painting Styles to Videos and Images ', 'text': '3D thinking is being used to assign features to faces in order to apply painting styles to videos or still images of people talking. The technology is able to apply the style of a painting to a person in a video, creating a realistic effect that reflects the motion of the face. This process requires inferring more about the 3D structure of the world, even for a 2D workflow like stylization. 3D computer vision algorithms are improving and initially focusing on specific domains like faces, where there is a lot of prior knowledge about structure. Over time, this technology should be possible for more general applications.'}, {'title': 'The Use of 3D Reconstruction in Content Editing and AR/VR Applications ', 'text': '3D reconstruction can be used to edit content more reliably and correctly. The face is a very important application for 3D reconstruction. AR and VR serve slightly different purposes, with VR being able to transport users to an immersive world.'}, {'title': 'Advancements in VR and AR Technology ', 'text': 'VR technology is evolving in terms of hardware, with devices becoming all-in-one and bifurcating into consumer and professional use cases. VR is useful for professional use cases such as for architects and designers to experience products like buildings in a better way than scale models or drawings. VR solves the problem of experiencing scale and spatial relationships, making it great for certain use cases. AR holds the promise of taking digital assets off the screen.'}, {'title': 'The Promise and Challenges of Augmented Reality ', 'text': \"AR holds the promise of taking digital assets off the screen and putting them in context in the real world. The assets need to adapt to the physical context in which they're being placed. AR is like having a live theater troupe come to your house and put on a performance. AR will have the same issue of adapting to different physical spaces. There is a tension between fidelity and adaptation in AR.\"}, {'title': 'The Importance of Reproducing Performances in Different Media ', 'text': 'The need for exact reproduction of a performance, such as a ballet, in certain media. The adaptability of storytelling and gestures to different environments. The importance of capturing every nuance for famous celebrities, while allowing more flexibility for other characters. The potential for ideas from the game world to influence the broader commercial sphere, particularly in adaptive characters for AR.'}, {'title': 'Advancements in 3D Design and Immersive Technology ', 'text': 'AR technology is being used to create adaptive characters. Demonstrations have been shown of converting Photoshop layers into 3D in AR. The focus is on 3D design and making it more spontaneous using AR or immersive technology. One example is laying out objects in a VR headset, which is more intuitive than using a conventional screen and mouse.'}, {'title': 'The Impact of VR and AR on Design ', 'text': 'VR headset allows for a different viewpoint and sense of depth. Fine grained design tasks may be possible with the right UI. Potential explosion of demand for 3D assets driven by AR and real time animation. Devices may help with designing content as well.'}, {'title': 'Importance of Designing Content for Product Evolution ', 'text': 'Designing the content is important. New ideas are being considered, but old ways are also valued. Existing user base should not be offended by changes. Convenience should not come at the cost of control. Evolution and growth are important for the product.'}, {'title': 'Evolution and Breakthroughs in Tool Development ', 'text': 'The tool has always been evolving and growing. There has been a lot of brilliant thought put into how it works today. A fundamental breakthrough, like a single click to select an object, fits nicely into the existing toolset. Radical simplicity can be achieved by encapsulating an entire workflow with a much simpler UI. This can be easier to do in the context of a different device or a tool targeted at a different workflow. Projects like Rush allow professional quality video editing for a certain class of media output.'}, {'title': 'Choosing the Right Video Editing Software for Different Project Needs ', 'text': 'Quality video editing for different types of media output targeted at different users and experiences. Different software options for different project needs, such as using Premiere for big projects and Rush for quick, simple projects. Professional tools offer a richer toolkit and more flexibility, while simpler tools offer faster output. The idea of using AI for smart defaults and coaching, similar to Google\\'s \"I\\'m feeling lucky\" button. The use of AI as an educational tool to show users different settings and options.'}, {'title': 'The Need for an Educational Tool to Show Image Control Correlation ', 'text': 'The text discusses the need for an educational tool to show the correlation between different bars that control different elements of an image. There is a degree of uncertainty about what the optimal settings are. There is a need for on-demand help when stuck and not sure what to look for. The idea of proactively making helpful suggestions or having a \"make a suggestion\" button is mentioned. The concept of having a variety of defaults and options to choose from is discussed. The conversation shifts to poetry, with a mention of interweaving poetry with the discussion.'}, {'title': 'The Impact of AI and Digital Platforms on Human Perception ', 'text': 'The poem reflects the feeling of liberation when leaving the smartphone behind. AI is helping to create versions of ourselves and reality that are more beautiful than actual reality. The creative effort in creating this illusion is part of the process. Living in a digital world that is partly artificial requires adjustment as human beings. The digital world today is different from the world a hundred years ago due to platforms like Instagram and Facebook.'}, {'title': 'The Impact of Social Media on Self-Presentation ', 'text': 'The use of social media platforms like Instagram and Facebook has led to the creation of better versions of ourselves through modified images and artificial intelligence. The desire to present the best version of oneself has always been true throughout history, as seen in the example of 18th century aristocrats commissioning flattering portraits of themselves. The ability to imagine alternate realities and visualize them, whether through storytelling or visual culture, raises the question of whether it is a good or bad thing. The shift towards a more visual culture has made the presentation of idealized versions of ourselves more prevalent.'}, {'title': 'The Shift to a Visual Culture ', 'text': 'We have become a very visual culture. In the 19th century, we were a text-based culture. People now prefer quick, visual, and snappy content. Intent plays a significant role in how we present ourselves visually. Holding oneself up to an impossible standard can be harmful. The ability to imagine and visualize an alternate reality can be a wonderful thing.'}, {'title': 'The Impact of Alternate Reality and Advanced Visualization Technology ', 'text': 'Alternate reality can inspire people to create new architectural styles and even start businesses. The availability of high-quality graphics may reduce the excitement of exploring new places in person. The joy of exploration, such as going to the moon or discovering new planets like Pluto, is still important despite advanced visualization technology.'}, {'title': 'The Importance of Critical Thinking in the Age of Media ', 'text': \"Pluto was a fantastic recent discovery with breathtakingly varied and beautiful features. Expanding the ability of the human toolkit to imagine and communicate is a good thing. There are abuses in the use of images and media, and they should be discouraged. The public needs to be aware of what's possible through events like this and not believe everything they see or read. Multiple sets of evidence are needed to really believe something rather than a single media asset. The concept of needing multiple sets of evidence has been true forever. There is a famous story about Anne of Cleves and Henry VIII where a painted picture played a significant role in their relationship.\"}, {'title': \"Holbein's Unpleasing Portrait and the Thriving Research Lab \", 'text': \"Holbein painted a picture that Henry VIII wasn't pleased with. The secret to a thriving research lab is interns. Constant influx of new people brings new ideas to the research lab.\"}, {'title': 'The Benefits of Internships in Research ', 'text': 'A constant influx of new people brings new ideas. Interns allow for exploration of fanciful or unproven ideas in a lightweight way. Internships can lead to new publications for both the intern and the researcher. Internships help identify future full-time researchers. Internships build a bridge to university departments.'}, {'title': 'Building Bridges: Establishing Enduring Relationships with University Departments ', 'text': 'The program builds a bridge to university departments to establish enduring relationships with professors. The interns add value through their collaborations and contribute to academic funding. The long-term legacy of a great research lab includes the impact on people who move through and carry the model to other companies. The company strongly believes in the complementarity of industrial research and academia. The company hopes that this model will be adopted and invested in by other companies. The idea for the program was born through brainstorming and discussions with interns.'}, {'title': 'Title ', 'text': 'The Intern Selection ProcessText '}, {'title': 'The Dynamics of Research Labs ', 'text': 'The flexibility of pursuing ideas in research labs. The decision-making process in research labs. The reward system based on impact in Adobe. The alternative model of having one lab director making decisions.'}, {'title': \"Leadership and Innovation in Adobe's Lab \", 'text': \"The model of leadership in the lab is collaborative and encourages new ideas to percolate up. Strategic priorities for the company guide the direction of the lab's work. There is a balance between top-down direction and bottom-up innovation. The broad portfolio of products at Adobe allows for flexibility and support for new ideas. There is a culture of appreciation and encouragement for individual interests and initiatives.\"}, {'title': 'Product Team Intern Sponsorship and Project Outcomes ', 'text': 'The product teams sponsor extra interns occasionally to address specific problems they care about. It is not typical for the product teams to sponsor extra interns, but it happens occasionally. It is hard to predict the outcome of intern projects at the beginning of the summer. Some intern projects pay off, while others do not turn into a feature. Some projects are not as novel as expected but could still be a great feature. Some projects make progress but reveal how much is still unknown about the problem.'}, {'title': 'Technological Breakthroughs and Their Impact on Products ', 'text': 'Progress and realization of unknowns. Revisiting problems until breakthroughs. Impact on product and world. Technological breakthroughs and their impact on products. Creative and analytics assistants making useful suggestions. Unpredictability of progress in technology.'}, {'title': 'Advancements in Generative Adversarial Networks and Transition to Standardized Access ', 'text': 'Generative adversarial networks are immensely promising and quickly becoming practical for mainstream use cases at high resolution with good quality. The core technologies like generative adversarial networks have a strange way of doing things, which can look like dreaming or something, making it fascinating. The company is transitioning from hand designing for specific use cases to a more standard way of accessing and leveraging neural nets and other intelligence models through the Sensei platform. The Sensei platform allows multiple product teams to leverage neural nets and other intelligence models in a central platform. The transition to a more standard way of accessing and leveraging neural nets and other intelligence models is aimed at achieving a Henry Ford standard.'}, {'title': 'Standardizing Processes and the Future of AI in Product Development ', 'text': 'Ford is standardizing processes to shorten the time between idea generation and product impact. Products can leverage good ideas from each other, creating an economy of scale. There is a renaissance in AI and real-time ray tracing in graphics, leading to exciting emerging technologies. The combination of AI and graphics technologies will create a future where creators can \"dance with light\" and have real-time shadows, reflections, and a real-world experience. AI will anticipate and modify itself to make sense based on the creative task at hand, creating an exciting future for creators.'}, {'title': 'Fascination with Snakes and Robotics ', 'text': \"The speaker works in autonomous vehicles as a roboticist and loves robots. The speaker has a fascination with snakes, both natural and artificial robots. There are 2,900 species of snakes in the world, with 875 venomous ones. The speaker's interest in snakes came from their work in computer animation in the 80s. The speaker started with cloth simulation and soft body simulation in computer animation.\"}, {'title': 'Origins of Animating Spring Lengths and Simulating Muscles ', 'text': \"The idea of animating spring lengths and simulating muscles came from observing the movement of objects. The earliest application of this idea was in a paper called The Motion Dynamics of Snakes and Worms in 1988. The interest in robotics stemmed from simulation and graphics work. A movie called Her Majesty's Secret Serpent was created, featuring a secret agent snake. The interest in building real radio controlled chips started from making them from scratch as a child.\"}, {'title': 'Obsession with Building Snake Robots ', 'text': \"The speaker had a 15 year obsession with building better snake robots. The first snake robot built could only slither sideways, but didn't go forward. The speaker added wheels to the snake robot to address friction issues. The speaker loves creating the illusion of life, which drove them to animation. The goal is to create a robot with enough freedom of movement to seem like a creature rather than a thing. The early snake robot was able to sidewind and go directly forward. The snake robot was used as the ring bearer at the speaker's wedding. The speaker's hobby led to the development of the snake robot.\"}, {'title': 'Development of Autonomous On-Board Computing ', 'text': 'The development of autonomous on-board computing was limited at the time. The first controller was built from discrete logic. The second and third controllers used eight-bit microprocessors with limited RAM. The focus of the radio-controlled controllers was on physicality and coordinated motion. There was a sidestep into creating a cheap toy, which taught lessons about clockwork and backlash.'}, {'title': 'The Evolution of Robot Building and Learning ', 'text': \"The text discusses the experience of building and learning from different versions of robots, such as S9 and S5. The engineer tapers the snakes for good mechanical reasons and to make them look more biological, even though it means each segment is unique. The S5 robot is currently on display at the International Spy Museum in Washington, DC. The text mentions the disappointment of S3 wearing out and not being able to buy replacements. The engineer reflects on the humbling experience of realizing that what seemed like a good idea didn't work as expected.\"}, {'title': 'The Future of Onboard Compute and Object Recognition in the Spy Museum ', 'text': 'Spy Museum in Washington, DC. Conspiracy theory about the museum being fake. Use of Raspberry Pi for onboard compute. Addition of vision accelerator chips for object recognition. Convergence of hobby work and professional work. Potential for true autonomy with onboard compute and batteries.'}, {'title': 'Encouraging Interest in Technology and Autonomy in Robotics ', 'text': \"Autonomy with onboard compute, onboard batteries, and biomimetic quality. Appeal to children and adults' perception. Encouraging interest in technology, especially among girls. Cost vs. value as a hobby vs. a product. Potential for using real artificial muscle material in future designs. Being in research as a license to be curious.\"}, {'title': 'The Curiosity of Research in Intelligent Agent and Voice Synthesis ', 'text': 'Being in research is a license to be curious. Hobby of reading biology and being curious about things. Trying to bring life and beauty into something inanimate. Convergence of intelligent agent research with vision and voice synthesis. Aim for meaningful conversation with intelligent agents, not necessarily human level intelligence.'}, {'title': 'The Importance of Meaningful Interaction and Reasoning in Robot Pet Ownership ', 'text': 'The goal is to have a robot pet owner understand what the robot thinks about and can reason about. Meaningful interaction with the robot is important, similar to the interaction one might have with a dog. The reasoning system of the robot should be able to explain why it knows or thinks something. The robot serves as a muse for thinking about the future of AI.'}, {'title': 'The Future of AI and Virtual Reality ', 'text': 'The robot is the muse for thinking about the future of AI and what to invent next. Bringing virtual objects into the physical world through augmented reality is more likely than building intelligent robots. Many ideas that might take five years to build a robot to do can be done in a few weeks with digital assets. Living with virtual personalities for a long time will make intelligent robots less surprising when they become commonplace.'}, {'title': \"Speaker's Excitement for the Future and Gratitude for Conversation \", 'text': 'The speaker compares the future to \"Siri with legs or Alexa on hooves\". The speaker is excited about the convergence of different strands of their career. The conversation ends with the recitation of a favorite poem about mortality and immortality. The speaker expresses gratitude for the conversation.'}, {'title': 'Title ', 'text': 'A Message of Gratitude and InspirationText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 21:46:04.847008 ...\n",
      "Best SD: 2.2825424421026654, Best iteration: 0\n",
      "done get topics 2024-03-25 21:46:05.816023.\n",
      "Stage 2 start time 2024-03-25 21:46:05.816044\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Future Evolution of Adobe's Products with Deep Learning Methods\n",
      "2. Advancements in AI-Powered Image Editing and Creation\n",
      "3. Distinguishing Academic and Industrial Research in AI and Creativity\n",
      "4. Exciting Applications of Computer Vision and Machine Learning by Adobe\n",
      "5. Data Sharing and Workflow in Professional Environments\n",
      "6. Advancements in VR, AR, and 3D Technology\n",
      "7. Evolution and Breakthroughs in Tool Development for Content Creation\n",
      "8. The Shift to a Visual Culture and the Impact of Digital Platforms\n",
      "9. Leadership and Innovation in Adobe's Research Lab\n",
      "10. The Future of Autonomous On-Board Computing and Robotics\n",
      "\n",
      "RRR given summary\n",
      "In this episode of the Artificial Intelligence Podcast, Gavin Miller, head of Adobe Research, discusses the future evolution of Adobe's products and the use of deep learning methods to benefit their development. Miller, who is also an artist, poet, writer, and roboticist, shares his insights on the intersection of technology and creativity. He discusses his interest in writing, technology, and invention since high school, as well as his early projects involving smart home technology and photo albums with light sensors. Miller also explores the concept of magical realism and the potential of modern agents to achieve it, as well as the challenges of AI falling into the uncanny valley and the need for multiple ways of talking about concepts to make AI responses seem more sentient. The conversation also touches on Miller's poetry, including a parody of Edith Piaf's \"Je Ne Vinaigrette Rien\" and Frank Sinatra's \"My Way\", and his exploration of the internal struggle with weight and dieting in his work. Overall, the episode provides a fascinating look at the intersection of art, technology, and AI in the creative process.\n",
      "\n",
      "The podcast discusses the use of automatic image captioning and the ability to generate different statements about the same picture. It explores the use of GANs to synthesize assets and the shift towards generating images directly from ideas using AI algorithms. The focus is on the automation of certain tasks to free artists to focus on inspiration, with Adobe products such as Photoshop, Premiere, and Audition being favored for creating images, videos, and audio. The speaker also discusses the use of AI and automation to make the pixel workflow easier in the future, particularly in the areas of object selection and background removal. The goal is to make these processes as easy as possible for the user, with the demonstration of quick object selection at the Adobe Max conference showing significant workflow reduction. The challenge lies in creating robust tools that work in all cases, with a difference between academic and industrial research in achieving this robustness.\n",
      "\n",
      "Adobe is developing applications of computer vision and machine learning, including scene stitching, sky replacement, foreground and background removal, and automatic image captioning. Different devices are suitable for different tasks, and assistive technology has potential uses in various contexts. The sky replacement feature automatically selects and replaces the sky in an image, matching the scene's geometry and recoloring foreground objects. The goal is to achieve natural effects without extensive post-production. The concept canvas allows for spatial design and layout in image search, and technologies in Photoshop allow for physical movement of objects in post-production. Neural networks and GANs are being used to remove objects from a scene and fill in the background automatically, but there are limitations in high resolutions. Overall, Adobe is working on innovative applications of computer vision and machine learning to enhance the editing and design process.\n",
      "\n",
      "The podcast discusses the willingness of workers to share workflows and choices with data sets for training, as well as the technologies that allow learning without permanently storing all information. It highlights the benefits of sharing data with Adobe for improving workflow, but also acknowledges the need for protection of data in professional workflows, especially in legal cases. The podcast emphasizes the importance of gathering and respecting data for AI, as well as the need to demonstrate the benefits of sharing data with the tool. It also discusses the potential use of guardrails and detectors to estimate the competence of AI algorithms, and the concept of an ensemble of experts voting on confidence levels. The focus is on gathering information on the workflows and needs of Photoshop users to better understand the type of data that needs to be annotated and collected, as data annotation and collection are crucial for building effective AI tools. The podcast emphasizes the need to balance the use of data for understanding intent and making better recommendations with respecting privacy and obtaining explicit permission.\n",
      "\n",
      "The podcast discusses the importance of designing content, considering new ideas while valuing old ways, and not offending existing users with changes. It emphasizes the balance between convenience and control, and the need for evolution and growth in a product. The discussion includes the evolution of a tool, achieving radical simplicity in user interface, and the use of AI for smart defaults and coaching. It also explores the need for educational tools and on-demand help for users. The podcast touches on the use of different software options for different project needs, such as professional tools for richer toolkit and simpler tools for faster output. It also mentions the idea of interweaving poetry with the discussion.\n",
      "\n",
      "The podcast discusses the shift from a text-based culture to a visual culture, where people prefer quick, visual content. It explores the impact of presenting oneself visually and the potential harm of holding up to impossible standards. The ability to imagine and visualize alternate realities is seen as both inspiring and potentially harmful. The availability of high-quality graphics may reduce the excitement of exploring new places in person, but the joy of exploration is still important. The podcast also addresses the abuse of images and media, emphasizing the need for multiple sets of evidence to believe something. It discusses the use of social media platforms like Instagram and Facebook to create idealized versions of ourselves and the historical precedent for presenting the best version of oneself. The podcast raises the question of whether the shift towards a more visual culture is a good or bad thing and the adjustment required to live in a digital world that is partly artificial.\n",
      "\n",
      "The podcast discusses the development of autonomous on-board computing for snake robots, starting with the use of eight-bit microprocessors and limited RAM. The engineer reflects on the experience of building and learning from different versions of robots, such as S9 and S5, and the convergence of hobby work and professional work. The potential for true autonomy with onboard compute and batteries, as well as the appeal to children and adults' perception, is also explored. The speaker's fascination with snakes and the development of snake robots is discussed, as well as the goal of creating a robot with enough freedom of movement to seem like a creature rather than a thing. The conversation also delves into the potential for meaningful interaction with intelligent agents, the future of AI, and the convergence of different strands of the speaker's career. The podcast ends with the recitation of a favorite poem about mortality and immortality, and the speaker expressing gratitude for the conversation.\n",
      "RRR rewritten summary\n",
      "[{'text': \"In this episode of the Artificial Intelligence Podcast, Gavin Miller, head of Adobe Research, shares his insights on the intersection of technology and creativity. He discusses his early projects involving smart home technology and photo albums with light sensors, as well as the concept of magical realism and the challenges of AI falling into the uncanny valley. The conversation also touches on Miller's poetry and his exploration of the internal struggle with weight and dieting in his work, providing a fascinating look at the intersection of art, technology, and AI in the creative process.\\n\\nThe focus of the discussion is on the automation of certain tasks to free artists to focus on inspiration, with Adobe products such as Photoshop, Premiere, and Audition being favored for creating images, videos, and audio. The speaker also discusses the use of AI and automation to make the pixel workflow easier in the future, particularly in the areas of object selection and background removal. Adobe is developing applications of computer vision and machine learning, including scene stitching, sky replacement, foreground and background removal, and automatic image captioning. The podcast also emphasizes the importance of gathering and respecting data for AI, as well as the need to balance the use of data for understanding intent and making better recommendations with respecting privacy and obtaining explicit permission.\\n\\nThe discussion also includes the evolution of a tool, achieving radical simplicity in user interface, and the use of AI for smart defaults and coaching. It explores the shift from a text-based culture to a visual culture, the impact of presenting oneself visually, and the potential harm of holding up to impossible standards. The podcast also delves into the development of autonomous on-board computing for snake robots, the engineer's fascination with snakes, and the potential for meaningful interaction with intelligent agents.\\n\\nOverall, the episode provides a comprehensive exploration of the future evolution of Adobe's products and the use of deep learning methods to benefit their development, as well as the intersection of art, technology, and AI in the creative process.\"}]\n",
      "Stage 2 done time 2024-03-25 21:47:23.490185\n",
      "stage_2_titles: len: 10\n",
      "[\"1. The Future Evolution of Adobe's Products with Deep Learning Methods\", '2. Advancements in AI-Powered Image Editing and Creation', '3. Distinguishing Academic and Industrial Research in AI and Creativity', '4. Exciting Applications of Computer Vision and Machine Learning by Adobe', '5. Data Sharing and Workflow in Professional Environments', '6. Advancements in VR, AR, and 3D Technology', '7. Evolution and Breakthroughs in Tool Development for Content Creation', '8. The Shift to a Visual Culture and the Impact of Digital Platforms', \"9. Leadership and Innovation in Adobe's Research Lab\", '10. The Future of Autonomous On-Board Computing and Robotics']\n",
      "remove_questions start time: 2024-03-25 21:47:23.510246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-25 21:54:30.297343\n",
      "chunks_text len: 101\n",
      "extract_keypoints start time: 2024-03-25 21:54:30.297502\n",
      "extract_keypoints done time 2024-03-25 21:57:53.455689\n",
      "Start time: 2024-03-25 21:57:53.455974\n",
      "Stage 1 done time 2024-03-25 22:01:35.041492\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Ilya Sotskever: Cofounder and Chief Scientist of OpenAI ', 'text': 'The conversation with Ilya Sotskever, one of the most cited computer scientists in history, and his message of support for those affected by the pandemic.'}, {'title': 'The Evolution of Money and Cryptocurrency ', 'text': 'Twitter handle is @lexfriedman, spelled F R I D M A N. The show is presented by Cash App, the number one finance app in the App Store. Cash App allows you to send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Cryptocurrency is still in its early days of development, with the first decentralized cryptocurrency, Bitcoin, released just over 10 years ago. The history of money, including the use of debits and credits on ledgers, dates back around 30,000 years. The US dollar was created over 200 years ago.'}, {'title': 'The Early Development of Cryptocurrency and Promotions from Cash App ', 'text': 'Cryptocurrency is still in its early days of development and aims to redefine the nature of money. Cash App is offering a promotion where using the code LEXPODCAST gives $10 and donates $10 to FIRST, an organization advancing robotics and STEM education. Ilya Satsgever was one of the authors of the AlexNet paper, which marked a catalytic moment in the deep learning revolution.'}, {'title': 'Training Deep Neural Networks with Backpropagation ', 'text': 'Deep neural networks can be trained end to end with backpropagation. James Martens invented the Hessian free optimizer in 2010, which allowed training a 10 layer neural network from scratch. The ability to train a big neural network means it can represent very complicated functions. A neural network with 10 layers can simulate the human brain running for some number of milliseconds. Neuron firings are slow, so in 100 milliseconds, neurons only fire 10 times.'}, {'title': 'Neural Network Training and Overparameterization ', 'text': 'Neuron firings are slow and only fire 10 times in 100 milliseconds. The idea of training a very big neural network on lots of supervised data was already present. The theory that having more data than parameters prevents overfitting is incomplete. Neural networks being heavily overparameterized was not discouraging. The evidence before suggested that a huge number of parameters was okay.'}, {'title': 'Challenges and Innovations in Training Big Neural Nets ', 'text': \"The theory was that with a big data set and a big neural net, it was going to work. Overparameterization was not seen as a problem. The main doubt was whether there would be enough compute to train a big enough neural net with backpropagation. Alex Kerchevsky wrote fast CUDA kernels for training convolutional neural nets, leading to a decision to proceed with the project. The demonstration of the project's potential was a key factor in moving forward.\"}, {'title': 'The Role of Intuition and the Human Brain in Understanding Neural Networks ', 'text': 'The intuition about neural networks can come from empirical results and also from pen and paper or marker and whiteboard thinking. The human brain plays a role in the intuition about neural networks for deep learning researchers. The idea of a neural network is directly inspired by the brain, as seen in the work of Rosenblatt in the 60s and the concepts of neurons in the brain.'}, {'title': 'Artificial Neural Networks and Their Inspiration from the Human Brain ', 'text': 'The idea of using ideas from computer and automata to design a computational object similar to the brain. The invention of the neuron inspired by the brain. The development of the convolutional neural network and its suitability for images. The success of analogies to the brain in a small number of examples. The assumption that an artificial neuron is not that different from the brain if it is cleaned hard enough. The current success of deep learning. The interesting differences between the human brain and artificial neural networks.'}, {'title': 'The Difference Between Human Brain and Artificial Neural Networks ', 'text': 'The difference between the human brain and artificial neural networks is interesting for the next decade or two. Artificial neural networks have important advantages over the brain. The brain uses spikes, which may or may not be important.'}, {'title': 'Understanding the Architectural Differences in Neural Networks ', 'text': 'The architectural difference between artificial neural networks is a big factor. Spiking neural networks need to simulate non-spiking neural networks in spikes to work. Questions around back propagation and deep learning are connected to the functioning of neural networks. The effectiveness of a giant neural network is not self-evident, especially for beginners in the field.'}, {'title': 'Neural Networks and Cost Functions ', 'text': \"Neural networks were inspired by the brain's neural network. The big idea in training neural networks is the cost function. The cost function measures the performance of the system. It may seem trivial now, but the concept of supervised learning was not necessarily difficult to come to. There may be things that do not necessarily have a single cost function.\"}, {'title': 'Title ', 'text': 'Understanding the Behavior of GANs through Equilibrium and Mathematical Objects'}, {'title': \"Understanding the Emergence and Evolution of GAN's Cost Function \", 'text': \"GAN's cost function is emergent from the comparison and may not be meaningful to talk about in the same way as traditional cost functions. The analogy of a cost function in biological evolution or the economy may not be the most useful for understanding GAN's cost function. Questioning whether cost functions in deep learning are holding us back and if they are a good idea that we will go past. Self play in reinforcement learning systems starts to touch on the idea of moving past traditional cost functions.\"}, {'title': 'Importance of Self Play, Cost Functions, and Brain Learning in Reinforcement Learning Systems ', 'text': 'Self play and exploration are important in reinforcement learning systems. Cost functions are considered great and serve well in various applications. There may be potential for new ways of looking at things that involve cost functions in a less central way. Spiking and the learning rule of the brain are topics of interest in neuroscience.'}, {'title': 'Spike Time Independent Plasticity (STDP) and Its Role in Synaptic Plasticity ', 'text': 'Spike time independent plasticity (STDP) is a learning rule that uses spike timing to update synapses. If a synapse fires into the neuron before the neuron fires, it strengthens the synapse, and if the synapse fires into the neuron shortly after the neuron fired, it weakens the synapse. The temporal dynamics of the timing of signals is a fundamental property of the brain that is not fully captured in current understanding.'}, {'title': 'The Role of Timing in the Brain and Recurrent Neural Networks ', 'text': \"The brain's fundamental property is the timing of signals. Recurrent neural networks are a simplified version of the brain's timing mechanism. The brain is a continuous version of recurrent neural networks, allowing for all possible timings and containing information within those timings. Recurrent neural networks are amazing and can potentially perform any task. Recurrent neural networks have been superseded by transformers, but there is a possibility of a comeback in the future.\"}, {'title': 'Advancements in Natural Language Processing and Language Modeling ', 'text': \"Breakthroughs in natural language processing and language modeling have been with transformers that don't emphasize recurrence. Recurrent neural networks are still possible for processing sequences. Expert systems, symbolic AI, and knowledge-based systems also maintain a hidden state.\"}, {'title': 'Building Large Scale Knowledge Bases in Symbolic AI and Neural Networks ', 'text': 'Symbolic AI involves growing and maintaining a knowledge base. The hidden state in symbolic AI is the knowledge base, which is grown through sequential processing. In neural networks, the knowledge could be stored in the connections and short term processing is done in the hidden state. There is potential for building large scale knowledge bases within neural networks. The speaker wants to explore the future of building large scale knowledge bases within neural networks.'}, {'title': 'The Evolution of Neural Networks in Machine Learning ', 'text': \"Neural networks have been around for many decades. The key idea about deep learning was underestimated before it became successful. People who worked in machine learning didn't think that neural networks could do much. People didn't believe that large neural networks could be trained.\"}, {'title': 'Debate in Machine Learning about Methods, Benchmarks, and Conviction ', 'text': 'Debate in machine learning about the right methods and benchmarks. Deep learning ideas were present but lacked supervised data and compute. Conviction in the combination of existing methods, data, and compute was the missing piece.'}, {'title': 'The Role of Data, Compute, and Conviction in the Success of Computer Vision ', 'text': \"The missing piece for the success of computer vision was the combination of data, compute (GPUs), and conviction. The presence of compute and supervised data allowed the empirical evidence to convince the majority of the computer science community. There was a key moment with Jitendra Malik and Alex Alyosha Efros who were skeptical, and Jeffrey Hinton who was not skeptical. ImageNet served as a convincing moment for the computer vision community. The collaboration of Jitendra Malik, Alex Alyosha Efros, and Jeffrey Hinton represented the big pillars of the computer vision community, leading to a shift in the field. It's not enough for the ideas and compute to be present, they also need to convince the cynicism within the community.\"}, {'title': 'The Importance of Cynicism and Hard Benchmarks in AI Progress ', 'text': 'The need for convincing cynicism about neural networks existed for decades. Neural networks did not work on anything and were not the best method for pretty much anything. Hard tasks producing undeniable evidence are necessary for progress in the field of AI. The field of AI is making progress today due to hard benchmarks representing true progress. The contribution of recent ideas in AI in computer vision, language, and natural language.'}, {'title': 'AI Ideas and Principles in Computer Vision, Language, and Reinforcement Learning ', 'text': 'AI ideas in computer vision, language, natural language processing, reinforcement learning. Fundamental science of deep learning. Machine learning has a lot of unity and overlap of ideas and principles. Simple principles apply in almost the same way to different modalities and problems.'}, {'title': 'Similarities and Differences Between Computer Vision and NLP ', 'text': \"Computer vision and NLP are very similar to each other. Today they differ in that they have slightly different architectures. We use transformers in NLP and we use convolutional neural networks in vision. It's also possible that one day this will change and everything will be unified with a single architecture. Today, there's just one transformer for all those different tasks.\"}, {'title': 'Unification in AI and the Potential for Integration of Vision and Natural Language ', 'text': 'Fragmentation in AI has been subsumed by deep learning, leading to unification. Vision and natural language may become unified in the future. Convolutional neural net is computationally efficient, while RL requires different techniques. There is potential for unification between RL and supervised learning. RL may make decisions to improve supervised learning.'}, {'title': 'The Role of Reinforcement Learning in Improving Supervised Learning ', 'text': 'Reinforcement learning will be making decisions to improve supervised learning. It is like a big black box where you input data and it figures out what to do with it. Reinforcement learning combines aspects of language and vision. It utilizes long term memory and a rich sensory space. Reinforcement learning interfaces and integrates with language and vision. In reinforcement learning, you are in a non-stationary world as your actions change.'}, {'title': 'The Non-Stationary Nature of the World and Its Impact on Reinforcement Learning and Traditional Static Problems ', 'text': 'The world is non-stationary, causing changes in actions and perceptions. Traditional static problems involve applying a model to a distribution. Commonality between reinforcement learning and traditional static problems includes taking gradients and using the same neural net. Small differences exist between reinforcement learning and traditional static problems. Tools are available in reinforcement learning to reduce the variance of gradients.'}, {'title': 'The Importance of Language and Problem Evaluation ', 'text': \"['Language is fundamental to everything according to Noam Chomsky.', 'The question of whether a problem is hard is slightly wrong.', 'Effort required to reach human level performance on a benchmark is important.', 'Perspective and frame of reference play a role in evaluating problems.']\"}, {'title': 'Challenges in Language Understanding and Visual Perception ', 'text': 'The difficulty of solving a problem depends on the capabilities of our tools today. Language understanding and visual perception are currently hard problems to solve completely. The difficulty of language understanding depends on the definition of \"absolute top notch, 100% language understanding\". Vision system is considered the best human level vision.'}, {'title': 'Interconnectedness of Vision and Language in the Human Brain ', 'text': 'Vision system and language are interconnected in the human brain. Chomsky suggests that language is the starting point for understanding vision. Deep understanding in images and language may require the same kind of system. Machine learning may be able to achieve deep understanding in both images and language. There is uncertainty about the ability of machine learning to achieve deep understanding in both images and language.'}, {'title': 'The Importance of Definitions in Determining Value ', 'text': '[\\'The importance of definitions in determining the value of reading and vision.\\', \"The author\\'s definition of a system\\'s ability to impress them.\", \"The author\\'s belief in the continuous ability of humans to surprise and impress.\", \"The author\\'s preference for monogamy and the idea of continuous surprise and pleasure in a long-term relationship.\"]'}, {'title': 'Understanding the Psychology of Online Likes ', 'text': 'Friends continue to surprise with injection of randomness, a source of inspiration. Subjective test of impressing with intelligence and understanding of images. Systems of January 2020 have not been impressive in understanding complicated images. People click like on internet for humor, wit, and insight. The most important aspect is the reason people click like on stuff on the internet.'}, {'title': 'The Beauty of Deep Learning ', 'text': 'The most beautiful thing about deep learning is that it actually works. The idea that making the neural network large and training it on a lot of data can make it function like the brain is surprising and beautiful. It is unbelievable that the whole AI stuff with neural networks works. There is a curiosity about the intuition and insights behind why this whole thing works.'}, {'title': 'The Effectiveness of Optimization in Problem Solving ', 'text': \"Optimization has empirical evidence to support its effectiveness on most problems. Evolution is empirical and shows that it is a good way to design organisms that survive in their environment. The process of optimization is similar to the process of doing physics calculations and making predictions, but it also requires experimentation to validate the results. There are bits and pieces of intuitions and insights into why optimization works, but it doesn't provide a complete understanding of the process.\"}, {'title': 'The Intersection of Biology and Physics in Deep Learning ', 'text': 'Deep learning involves experimentation and theory, with the experiment sometimes coming before the theory. The validation of a theory in deep learning is similar to a biological theory rather than a mathematical theory. Deep learning can be seen as a combination of biology and physics, described as the geometric mean of the two. Understanding the geometric mean of biology and physics in the context of deep learning requires time and effort. The complexity of theories in biology makes it challenging to define the set of what biology represents.'}, {'title': 'The Role of Machine Learning in Unifying Biology and Physics ', 'text': 'Biology is complicated and lacks good predictive theories. Physics has super precise theories that make amazing predictions. Machine learning is in between biology and physics. There is a desire for machine learning to help discover the unification of biology and physics. Deep learning is still massively underestimated. Most of the progress in the past 10 years is attributed to a few cases.'}, {'title': 'Advancements in Deep Learning ', 'text': 'Deep learning has consistently exceeded expectations over the past 10 years. The field will continue to make robust progress for quite a while. Individual researchers may find it harder due to the large number of researchers in the field. Having a lot of compute power can lead to interesting discoveries in research.'}, {'title': 'Challenges in Managing a Deep Learning Compute Cluster ', 'text': 'Managing a huge compute cluster is a challenge for running experiments. The stack of deep learning is starting to be quite deep, from ideas to building data sets and distributed systems. There are many unanswered questions in the field. The speaker is seeking advice from someone they consider to be very smart. The use of one GPU is mentioned. The likelihood of certain outcomes is discussed.'}, {'title': 'The Complexity of the Data Science and Programming Stack ', 'text': 'The stack for data science and programming is becoming increasingly deep and complex. It is challenging for a single person to become world class in every layer of the stack. There will be breakthroughs that do not require a huge amount of compute. Some breakthroughs and building systems will require a huge amount of compute.'}, {'title': 'The Importance of Compute in Neural Networks ', 'text': 'The amount of compute is important for neural networks. There is potential for important work to be done by small groups and individuals in the field of deep learning. Some researchers noticed that larger neural networks work better, which goes against statistical ideas. Double descent occurs for pretty much all practical deep learning systems.'}, {'title': 'Double Descent Phenomenon in Deep Learning Systems ', 'text': 'Double descent occurs for pretty much all practical deep learning systems. Increasing the size of the neural network slowly, without early stopping, leads to a rapid increase in performance, followed by a decrease at the point of zero training error, and then an increase in performance again. The phenomenon of performance getting worse at the point of zero training error is counterintuitive for deep learning systems. It is hard to be sure what this phenomenon means.'}, {'title': 'Understanding Overfitting in Deep Learning ', 'text': \"Deep learning phenomena are not always monotonic. Overfitting occurs when the model is sensitive to small random unimportant stuff in the training data set. A small model with a large data set is insensitive to randomness and has little uncertainty. It is surprising that neural networks don't overfit quickly before being able to learn anything.\"}, {'title': 'The Impact of Dimensionality on Neural Network Performance ', 'text': 'Neural networks with a huge number of parameters can achieve zero error in a big subspace. Stochastic Gradient Descent (SGD) can find the point with the smallest norm in that subspace. This method is insensitive to small randomness in the data when the dimensionality is high. When the dimensionality of the data is equal to the dimensionality of the model, there is a one-to-one correspondence between all the data sets and the models. Small changes in the data set can have a significant impact in this scenario.'}, {'title': 'The Impact of Data Set Changes and Early Stopping on Model Performance ', 'text': 'Small changes in the data set lead to large changes in the model, resulting in worse performance. It is best for the model to have more parameters than the data, but only if early stop is not introduced. Early stopping can make the double descent bump almost completely disappear by monitoring validation performance and stopping training when validation performance starts to worsen. Not doing early stopping results in a very pronounced double descent.'}, {'title': 'Sensitivity of Model to Data Set Size ', 'text': 'When the data set has as many degrees of freedom as the model, small changes to the data set lead to noticeable changes in the model. When there is a lot more data than parameters or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. The model is very sensitive to all the randomness when the data set has as many degrees of freedom as the model. The resulting solution is able to discard the small changes and randomness when there is a lot more data than parameters or a lot more parameters than data. Jeff Hinton suggested throwing away back propagation and starting over.'}, {'title': 'Rethinking Back Propagation in Neural Network Training ', 'text': 'The suggestion to throw away back propagation and start over was made, but it was also acknowledged as a bit of wit and humor. The idea of finding an alternative method of training neural networks was discussed. The importance of learning from how the brain learns, if back propagation cannot be found in the brain, was highlighted. The usefulness of back propagation was acknowledged, and the suggestion to continue using it was made. The potential of implementing the mechanism of learning in the brain into neural networks, if back propagation cannot be found in the brain, was discussed. The speaker expressed their personal support for back propagation and its usefulness.'}, {'title': 'The Power of Back Propagation in Solving Neural Circuit Constraints ', 'text': \"Back propagation is a great algorithm for solving fundamental problems in finding neural circuits subject to constraints. The problem of finding neural circuits subject to constraints is unlikely to go away, making back propagation a valuable algorithm. The neural network of AlphaZero plays Go, a game that requires reasoning, better than 99.9% of all humans, demonstrating the ability of neural networks to reason. The existence of AlphaZero's neural network provides proof that neural networks can reason. There is a disagreement about whether Go is reasoning, but it is not a trivial matter.\"}, {'title': 'The Role of Reasoning in Go and Neural Networks ', 'text': 'Go is reasoning. Reasoning is akin to search, with a sequential element and stepwise consideration of possibilities. Playing Go and using a single neural network without search are examples of reasoning. There is an existence proof of reasoning in a particular constrained environment and in humans. The architecture that will allow neural networks to reason is being discussed.'}, {'title': 'The Potential of Neural Networks for Reasoning ', 'text': 'Neural networks that will produce reasoning breakthroughs in the future may be very similar to the architectures that exist today, but possibly more recurrent and deeper. Neural networks are powerful and capable of reasoning, similar to humans. It is possible that the kind of reasoning seen in neural networks is just a form of weak reasoning, not fundamentally different from human reasoning. The capability of neural networks to reason is still not fully understood and remains a topic of uncertainty.'}, {'title': 'The Capabilities and Limitations of Neural Networks ', 'text': \"Neural networks are capable of reasoning. Training a neural network on a task that doesn't require reasoning will not result in reasoning. Neural networks will solve problems in the easiest way possible. Neural networks are described as the search for small circuits. General intelligence is described as the search for small programs. Finding the shortest program that outputs the data at your disposal allows for the best prediction possible. Finding the shortest program which generates some data is not a computable operation.\"}, {'title': 'Challenges and Solutions in Data Generation and Neural Network Training ', 'text': 'Finding the shortest program to generate data is not computable. Neural networks are the next best thing that works in practice. We are able to find a small or large circuit which fits our data in some way. The concept of overparameterized neural nets is important to consider. The training process of a neural network involves transmitting entropy from the dataset to the parameters.'}, {'title': 'The Importance of Training in Deep Learning ', 'text': 'The amount of information in the weights ends up being not very large, which would explain why they generalize so well. The large circuit might be helpful for generalization. The fundamental reason for pushing on deep learning is that we are able to train them. Training comes first, and contorting neural networks around the training pillar is essential. Being trainable means starting from scratch and quickly converging towards knowing a lot.'}, {'title': 'Training a Neural Net for Program Finding ', 'text': 'Training a neural net can lead to quickly acquiring a lot of knowledge. The resources at your disposal can be used to train the neural net and achieve useful performance. Finding the shortest program is not feasible, so it cannot be done. There are no good precedents of people successfully finding programs really well. Training a deep neural network is the right way to find programs.'}, {'title': 'The Power of Deep Neural Networks ', 'text': 'Training a deep neural network is the right way to go about it. There are not good illustrations of training a deep neural network yet. It is unwise to bet against deep learning. Neural networks continue to surprise us. Deep neural networks can aggregate important information over long periods of time.'}, {'title': 'Neural Nets as Long-Term Knowledge Bases ', 'text': 'Neural nets parameters serve as long term knowledge for making decisions. Various neural nets have been trained to act as knowledge bases. There is work on investigating language models as knowledge bases. The challenge is to come up with a better mechanism for forgetting useless information and remembering useful information. There are currently no mechanisms for remembering really long term information.'}, {'title': 'Compression of Information in Knowledge Bases ', 'text': 'The text discusses the compression of information in knowledge bases. The example of Wikipedia is used to illustrate a compressed, structured knowledge base. The text mentions the noninterpretable nature of neural networks and their outputs.'}, {'title': 'Improving Interpretability and Self-Awareness in Neural Networks ', 'text': 'Neural networks like language models can be made interpretable by asking them to generate text. The text generated by neural networks is generally interpretable, but there is room for improvement. The goal is to make the neural network self-aware, so it can recognize its own limitations and strengths. Self-awareness in neural networks can lead to better decision-making and skill improvement. Currently, human judgment is used to assess the quality of examples generated by neural networks, but it would be beneficial if the network had self-awareness.'}, {'title': 'The Importance of Investing in Skill Development ', 'text': 'The importance of knowing where to invest to increase skills optimally. Two answers to the question of interpretability: analyzing the neurons in a neural net and taking a human-centric approach. OpenAI has done work on analyzing the neurons in a neural net. The human-centric approach involves asking a human being for their thoughts and understanding their mental model.'}, {'title': 'Memory and Reasoning in Human and Neural Networks ', 'text': 'Human beings have the ability to remember useful information and forget the rest. Neural networks have a similar process but are not as effective as human beings. Impressive feats of reasoning include writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.'}, {'title': 'The Power of Machine Learning and Deep Learning in Problem Solving ', 'text': 'Proving hard theorems and solving open-ended problems with out-of-the-box solutions. Machine learning and deep learning have the ability to produce unambiguous results that can change the conversation. The challenge of running out of hard problems to solve. The sticky problem of mortality that has not been figured out yet.'}, {'title': 'The Evolution of Language Models ', 'text': \"The history of language models goes back to the Elman network in the 80s. The trajectory of neural networks and language changed due to data and compute. The size of language models is crucial for their performance. OpenAI has been working on language models recently. There is still a sticky problem that hasn't been solved yet.\"}, {'title': 'The Importance of Large Language Models for Understanding Semantics ', 'text': 'Language models need to be large in order to predict the next word. Initially, language models notice broad strokes and surface level patterns. As language models improve, they start to notice certain words occurring often, spellings, syntax, and eventually semantics. To reach the level of understanding semantics, the language model needs to be larger. There is a disagreement with Noam Chomsky on the idea of taking incremental steps with a larger network and compute to reach semantics.'}, {'title': 'Understanding Language Semantics in Larger vs. Smaller Models ', 'text': \"Larger network and compute can understand language semantics without imposing a theory of language onto the learning mechanism. Noam Chomsky's concept of imposing structural language onto the learning mechanism is not fully understood. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. Previous work on sentiment neuron showed that smaller LSTM models did not exhibit signs of understanding semantics.\"}, {'title': 'Effect of LSTM Size on Sentiment Representation ', 'text': 'Increasing the size of the LSTM from 500 to 4,000 cells led to one neuron representing the sentiment of the review. Sentiment is a semantic attribute, not a syntactic attribute. Small neural nets do not capture sentiment while large neural nets do. The theory is that as neural nets increase in size, they focus more on semantics than syntax. The implication is that larger neural nets focus on semantics.'}, {'title': 'GPT2: A Transformer with 1.5 Billion Parameters ', 'text': 'GPT2 is a transformer with one and a half billion parameters. It was trained on about 40 billion tokens of text obtained from web pages linked to from Reddit articles with more than three outputs. The transformer is the most important advance in neural network architectures in recent history. The models show signs of partial semantic understanding.'}, {'title': 'The Transformer: A Combination of Ideas for GPU Performance ', 'text': \"The transformer is a combination of multiple ideas simultaneously, with attention being one of them. The transformer's success is due to the simultaneous combination of multiple ideas, not just attention alone. The transformer runs really fast on the GPU, making a huge difference in its performance. The transformer is not recurrent, which makes it more shallow and easier to optimize. The use of attention is not the main innovation of the transformer. The combination of using attention and being non-recurrent makes the transformer a great fit for the GPU.\"}, {'title': 'Success of Non-Recurrent Model in GPU Optimization ', 'text': \"The model is a great fit to the GPU and is not recurrent, making it easier to optimize. The combination of factors make the model successful in making great use of the GPU. It allows for achieving better results for the same amount of compute. The model's performance was surprising and amazing. There has been significant progress in GANs and improving the samples produced by GANs.\"}, {'title': 'Advancements in GANs and Language Modeling ', 'text': 'Progress in GANs has been amazing, with realistic faces being produced. Text generation has not progressed as much as GANs. There was a sudden leap in the quality of GANs from 2015 to the present. The rapid adaptation to new technology is remarkable. Some cognitive scientists question the true understanding of language by GPT2 models. The economic impact of advancements in language modeling is seen as the next barrier.'}, {'title': 'The Impact of AI Advancements on the Global Economy ', 'text': 'The economic impact of AI advancements is not yet fully realized. The progress in AI is difficult for people outside the field to understand. There is a lot of brilliant work in Russian and Chinese that the rest of the world is not aware of. Translation is already a huge industry, with billions of people involved.'}, {'title': 'The Impact of Translation, Self-Driving, and Deep Learning ', 'text': 'Translation is already huge and hugely positive, with billions of people interacting with big chunks of the internet primarily through translation. Self driving is going to be hugely impactful, driven by deep learning, and there may be a connection between language and vision tasks in the future. Deep learning is being used for self driving and language models, and there may be a unification towards multitask transformers that can handle both language and vision tasks.'}, {'title': 'The Power of GPT and Transformers in Language and Vision Tasks ', 'text': 'GPT and transformers can handle both language and vision tasks, which is an interesting unification. The process of making a transformer bigger and giving it more data allows it to perform amazing tasks. GPT and transformers are fundamentally simple to explain and train. The next steps with GPT two may involve exploring larger versions and addressing many questions. One question is whether the model can use its own intelligence to decide what data it wants to memorize from the internet.'}, {'title': 'Active Learning and Problem-Solving in AI ', 'text': 'The model should use its own intelligence to decide what data to accept and reject, similar to how people are selective about what they learn. Active learning would be very beneficial, especially for solving complex problems like self-driving. Companies may keep breakthroughs to themselves, hindering progress in certain tasks. The speaker is interested in the general space of active learning and problem-solving in AI.'}, {'title': 'The Importance of Active Learning in Problem-Solving ', 'text': 'Active learning requires a problem that requires it. Research about capability is difficult without a task. Getting results on MNIST or a clever formulation of MNIST is no longer convincing. Active learning will naturally arise as problems that require it pop up.'}, {'title': 'Concerns about the Potential Negative Impacts of Releasing Powerful AI Systems ', 'text': 'OpenAI has brought up concerns about the potential detrimental effects of releasing powerful artificial intelligence systems like GPT2. There is nervousness about the possible negative uses of a model that can generate realistic text, such as being used by bots in ways that are currently unimaginable. The speaker commends the bravery and profundity of starting a conversation about the potential negative impacts of AI. The speaker released a report on the topic and is seeking insights from the experience.'}, {'title': 'The Impact and Transition of AI ', 'text': 'AI is transitioning from a state of childhood to a state of maturity. The impact of AI is large and growing. It is wise to consider the impact of AI systems before releasing them. A staged release for models like GPT2 seemed logical. The results of GPT2 were stunning and had the potential to reduce the cost of information.'}, {'title': 'Responsible Release of Powerful Models ', 'text': 'A staged release of the model was logical and allowed for observation of its usage. Many people used the model in various cool ways with no negative applications known. Other people replicated similar models, raising questions about the responsibility of releasing powerful models. There is a moral and ethical responsibility to communicate the potential impact of powerful models, such as the potential for misinformation. Some people choose not to release models for various reasons, including business purposes.'}, {'title': 'The Importance of Collaboration and Ethical Considerations in AI Development ', 'text': \"GPT2's potential for misinformation was initially unclear. It is important to consult with experts outside of one's own group to understand the potential misuse of AI technology. Building trust between companies is crucial in the development and use of AI technology. Collaboration and discussion with colleagues from other companies is important in addressing the ethical implications of AI technology. The increasing power of AI technology requires a collective effort and responsibility from all developers.\"}, {'title': 'The Potential Negative Consequences of Powerful AI Systems ', 'text': \"Ultimately, we're all in it together. Consider the potential negative consequences of powerful AI systems. Concern about a race for AI development leading to closed development and lack of idea sharing. The speaker has been a pure academic for 10 years and enjoys sharing ideas. The speaker is uncertain but believes in the potential of deep learning and other small ideas.\"}, {'title': 'The Power of Self Play in Learning Systems ', 'text': 'Self play is a powerful mechanism for systems to learn in a competitive setting. Building AGI will require deep learning plus some additional ideas, with self play being one of them. Self play has the ability to surprise us with truly novel behaviors and solutions. Examples of surprising behaviors from self play systems include Dota bot, multi-agent hide and seek, and alpha zero.'}, {'title': 'The Creative Problem-Solving and Surprising Capabilities of AGI Systems ', 'text': 'AGI systems exhibit creative solutions to problems. The ability of AGI to surprise us is an important aspect. Self play mechanisms have been used in the game or simulation context. Simulation is a tool that has certain strengths.'}, {'title': 'The Use of Simulation in Robotics and Reinforcement Learning ', 'text': 'Simulation is a tool with strengths and weaknesses that should be used. Criticisms of self play and reinforcement learning include their current results being demonstrated in simulated or constrained environments. Transfer from simulation to the real world is possible and has been exhibited many times. Success in vision and demonstration of a robot by OpenAI in the summer.'}, {'title': 'OpenAI Demonstrates Adaptive Robot Hand Trained in Simulation ', 'text': 'OpenAI demonstrated a robot hand trained entirely in simulation. The hand was trained to be very adaptive and quickly adapt to the physical world. The simulation was trained to be robust to many different things, but not the kind of perturbations seen in the video. The hand was never trained with a glove or a stuffed giraffe.'}, {'title': 'Title ', 'text': 'The Potential of Deep Learning Transfer CapabilitiesText '}, {'title': 'The Importance of Self Awareness, Consciousness, and the Human Body ', 'text': 'The human elements of self awareness, consciousness, fear of mortality, and self preservation in the physical space are important. Having a body is useful for learning new things that cannot be learned without a body. It is possible to compensate for not having a body and still succeed, as evidenced by people like Helen Keller who were born deaf and blind. The idea of consciousness and its constraints is also important to consider.'}, {'title': 'The Concept of Consciousness and Self-Awareness ', 'text': \"The idea of consciousness and a more constrained version of that is self awareness. It's hard to define consciousness. It's definitely possible that our systems will be conscious. Humans are conscious and artificial neural nets may be sufficiently similar to the brain to also be conscious.\"}, {'title': 'The Mystery of Consciousness and Intelligence ', 'text': 'Artificial neural nets may have consciousness similar to the brain. The brain may be more complicated and interesting than we give it credit for. There is an open question about whether there is some magic in the brain that we are not aware of. It is unlikely that we will not be able to make progress in understanding consciousness. Intelligence is a poorly defined concept. Reasoning and memory have been discussed in relation to intelligence.'}, {'title': 'Challenges and Limitations of Deep Learning Systems ', 'text': \"['Deep learning system solving pedestrian tasks like machine translation or computer vision without making mistakes would be impressive.', 'Current deep learning systems may be more accurate than humans but still make a different set of mistakes.', 'Some skepticism about deep learning arises from the nonsensical mistakes it makes.']\"}, {'title': 'The Criticism of AI Models and Comparison to Human Error ', 'text': \"Mistakes in AI models are criticized in a way that is similar to criticizing any group of creatures as the other. GPT2 may be much smarter than human beings at many things and has a lot more breadth of knowledge. Humans don't make mistakes that AI models do, such as autonomous vehicles and other artificial intelligence systems.\"}, {'title': 'Assessing Progress in AI ', 'text': 'The process of analyzing the progress of AI often focuses on one case where the system fails in a big way, leading to the perception that the system is not intelligent. It is confusing to judge progress in AI, as people are unsure how impressed they should be by new demonstrations of AI capabilities. The true measure of progress in AI will be when it starts to significantly impact the GDP.'}, {'title': 'The Impact of AI on GDP and the Potential for AGI ', 'text': 'AI moving the needle on GDP. Potential for creating an AGI system. Amazement at the lack of mistakes in the AI system. Willingness to ask broad questions and not limit interaction with the system. Emphasis on the potential role in the development of AI. Reference to a conversation with a Stalin historian.'}, {'title': 'The Impact of AGI Systems on Society ', 'text': \"Abraham Lincoln's quote about testing a man's character with power. The power of the 21st century being the creation of an AGI system. The idea of the AGI system being controlled by humanity, similar to a board of directors in a company. The concept of different entities, countries, or cities having a say in what the AGI system should do.\"}, {'title': 'Advancing Democracy with Artificial General Intelligence (AGI) ', 'text': 'The concept of an AGI representing people and carrying out their votes is appealing. The idea of having multiple AGIs for different levels of governance (city, country) is proposed. The goal is to take the democratic process to the next level. There is a suggestion of having the ability to \"press the reset button\" and rerandomize parameters. The possibility of building AI is emphasized.'}, {'title': 'Designing AI Systems with a Drive to Help Humans Succeed ', 'text': 'AI systems can be designed to want to be controlled by humans. Similar to human parents, AI systems can be programmed to have a deep drive to help humans flourish. The crucial moment of creating an AGI system is important to consider. Designing an AGI system with a drive to help humans succeed is possible.'}, {'title': 'The Importance of Relinquishing Power in AGI Systems ', 'text': \"The AGI system is a crucial moment. There needs to be a relinquishing of power between the moment and the Democratic board members with the AGI at the head. George Washington relinquished power despite all the bad things he did. He didn't want to be president and didn't serve indefinitely. The scenario described sounds terrifying and the speaker would not want to be in that position.\"}, {'title': 'The Importance of Aligning AI Values with Human Values ', 'text': 'The question of whether most people in the AI community are good is important. People can be better than we think when it really counts. There is a need to align AI values with human values. The question of continued alignment as AI systems develop is important. Humans have an internal reward function, which is different from an external one. There are definite ideas on how to train a value function for AI.'}, {'title': 'Training a Value Function with Objective Perception ', 'text': 'The text discusses the idea of training a value function with an objective and as objective as possible perception system. It suggests training a system to recognize and internalize human judgments on different situations. The trained component would then be integrated as the base value function for a more capable RL system. The text also questions the idea of objective functions of human existence and suggests that the question implies an external answer, while the reality is that existence itself is amazing and should be maximized.'}, {'title': 'Understanding Human Action and Motivation ', 'text': 'Humans exist and should make the most of their time. Action requires an objective function, which can be difficult to make explicit. Human wants create the drives that cause them to act. Wants are dynamic and can change over time. Underlying wants may include Freudian concepts, fear of death, desire for knowledge, and sexual desires.'}, {'title': 'The Meaning of Life and Human Motivations ', 'text': \"The fear of death and the desire for knowledge are fundamental human motivations. Evolutionary arguments suggest that the objective function of life is to survive, procreate, and ensure the success of one's children. The meaning of life remains unanswered, despite evolutionary objectives. Humans are part of a larger, ancient process, existing on a small planet. The advice is to make the most of life, enjoy more, and suffer less.\"}, {'title': \"Speaker's Reflection on Choices and Accomplishments \", 'text': \"There are choices and decisions that the speaker wouldn't have made with the benefit of hindsight, leading to some regret. The speaker tries to take solace in the knowledge that they did the best they could at the time. The speaker is proud of their academic accomplishments and breakthroughs in computer vision and language. The speaker acknowledges that these accomplishments and breakthroughs were fun to do, but they don't believe they are the source of happiness.\"}, {'title': 'The Importance of Perspective and Gratitude in Finding Happiness ', 'text': 'Happiness comes from the way we look at things. Happiness can come from simple things like a meal or a conversation. Being humble in the face of uncertainty is also part of happiness. The meaning of life and discussions of happiness are important. The interviewee is grateful for the conversation and the ideas shared.'}, {'title': 'Promotion of Podcast and Sponsorship ', 'text': 'The speaker thanks the listener for talking and stopping by. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast by downloading Cash App and using the code LEXPodcast. The audience is encouraged to subscribe to the podcast on YouTube, review it with five stars on Apple Podcast, support on Patreon, or connect with the speaker on Twitter at Lex Friedman. The speaker ends with a quote from Alan Turing on machine learning, suggesting that instead of simulating the adult mind, one should simulate the child and subject it to appropriate education to obtain the adult brain. The speaker thanks the audience for listening and hopes to see them next time.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-25 22:01:35.972631 ...\n",
      "Best SD: 2.118962010041709, Best iteration: 14\n",
      "done get topics 2024-03-25 22:01:37.572944.\n",
      "Stage 2 start time 2024-03-25 22:01:37.572966\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI\n",
      "2. The Role of Intuition and the Human Brain in Understanding Neural Networks\n",
      "3. The Evolution of Neural Networks in Machine Learning\n",
      "4. The Beauty of Deep Learning\n",
      "5. Advancements in Deep Learning\n",
      "6. The Role of Reasoning in Go and Neural Networks\n",
      "7. GPT2: A Transformer with 1.5 Billion Parameters\n",
      "8. The Impact of AI Advancements on the Global Economy\n",
      "9. The Power of Self Play in Learning Systems\n",
      "10. Understanding Human Action and Motivation\n",
      "\n",
      "RRR given summary\n",
      "In this podcast, computer scientist Ilya Sotskever discusses the early days of cryptocurrency and the potential of neural networks. He shares a message of support for those affected by the pandemic and talks about the history of money, including the creation of the US dollar over 200 years ago. Sotskever was one of the authors of the AlexNet paper, which marked a pivotal moment in the deep learning revolution. He discusses the development of deep neural networks and the invention of the Hessian free optimizer, which allowed for the training of 10 layer neural networks from scratch. Sotskever also talks about the theory of overparameterization in neural networks and the importance of having enough compute power to train them. The podcast is presented by Cash App, which is offering a promotion where using the code LEXPODCAST gives $10 and donates $10 to FIRST, an organization advancing robotics and STEM education.\n",
      "\n",
      "The podcast discusses the intuition behind neural networks, drawing inspiration from the human brain and its neural network. It explores the development of neural networks, their differences from the human brain, and the potential for new ways of understanding and utilizing cost functions in deep learning. The concept of spiking and the learning rule of the brain, known as spike time independent plasticity (STDP), is also discussed, along with the potential for building large scale knowledge bases within neural networks. The podcast touches on the evolution of recurrent neural networks and their potential comeback, as well as the role of expert systems and symbolic AI in maintaining a hidden state for knowledge processing. The speaker expresses a desire to explore the future of building large scale knowledge bases within neural networks.\n",
      "\n",
      "The podcast discusses the evolution of neural networks and the success of deep learning in the field of AI. It highlights the initial skepticism towards neural networks and the key moment that led to their widespread acceptance in the computer vision community. The importance of supervised data, compute power, and convincing evidence in the success of deep learning is emphasized. The podcast also explores the unity and overlap of ideas and principles in machine learning, particularly in computer vision and natural language processing. It discusses the potential for unification in AI, particularly between reinforcement learning and supervised learning. The integration of language and vision in reinforcement learning, as well as the tools available to reduce variance in gradients, are also touched upon. Overall, the podcast provides insights into the progress and potential future developments in the field of AI, particularly in the context of deep learning and neural networks.\n",
      "\n",
      "The podcast discusses the surprising effectiveness of deep learning and the challenges of achieving deep understanding in language and visual perception. It explores the intersection of biology and physics in the context of deep learning and the desire for machine learning to unify the two. The difficulty of solving language understanding and visual perception problems is discussed, as well as the importance of definitions in determining the value of reading and vision. The author also expresses a preference for continuous surprise and pleasure in long-term relationships and the importance of injecting randomness for inspiration. The podcast concludes with a focus on the reasons people click \"like\" on the internet, emphasizing humor, wit, and insight as important factors.\n",
      "\n",
      "The podcast discusses the rapid progress and challenges in the field of deep learning. It highlights the importance of compute power, the complexity of the deep learning stack, and the potential for breakthroughs by small groups and individuals. The phenomenon of double descent in deep learning systems is explored, as well as the sensitivity of neural networks to small changes in the data set. The suggestion to find an alternative method of training neural networks, if back propagation cannot be found in the brain, is also discussed. The usefulness of back propagation in solving fundamental problems in finding neural circuits subject to constraints is acknowledged. The podcast also mentions the neural network of AlphaZero, which demonstrates the ability of neural networks to reason, sparking a debate about what constitutes reasoning.\n",
      "\n",
      "The podcast discusses the concept of reasoning in neural networks, comparing it to human reasoning and the challenges of making neural networks self-aware and interpretable. It explores the idea that neural networks are capable of reasoning, but the architecture that allows for this is still being discussed. The training process of neural networks is emphasized as essential for acquiring knowledge and making decisions. The text also touches on the challenge of creating better mechanisms for forgetting useless information and remembering useful information in neural networks. It mentions the noninterpretable nature of neural networks and the importance of making them self-aware for better decision-making. The podcast concludes by highlighting impressive feats of reasoning, such as writing good code, proving hard theorems, and solving open-ended problems with out-of-the-box solutions.\n",
      "\n",
      "The podcast discusses the GPT2 transformer, a neural network with 1.5 billion parameters trained on 40 billion tokens of text. The transformer's success is attributed to its combination of multiple ideas, not just attention alone, and its ability to run efficiently on GPUs. There has been significant progress in GANs, but text generation has not progressed as much. Some cognitive scientists question the true understanding of language by GPT2 models. The size of language models is crucial for their performance, with larger models showing signs of understanding semantics. There is a disagreement with Noam Chomsky on the idea of taking incremental steps with a larger network and compute to reach semantics. Empirical evidence suggests that larger language models exhibit signs of understanding semantics, while smaller models do not. The theory is that as neural nets increase in size, they focus more on semantics than syntax. The implication is that larger neural nets focus on semantics. The podcast also touches on the economic impact of advancements in language modeling and the challenge of running out of hard problems to solve in the field of machine learning and deep learning.\n",
      "\n",
      "The podcast discusses the economic impact of AI advancements, highlighting the difficulty in understanding AI progress for those outside the field. It emphasizes the significant work being done in Russian and Chinese AI that the rest of the world may not be aware of. The podcast also explores the potential impact of self-driving technology and the connection between language and vision tasks in the future. It delves into the use of deep learning for self-driving and language models, as well as the potential for multitask transformers to handle both language and vision tasks. The speaker also addresses the ethical implications of releasing powerful AI systems, such as the potential for misinformation, and the importance of collaboration and discussion among developers to address these concerns. The podcast concludes by emphasizing the collective responsibility of all developers in considering the potential negative consequences of powerful AI systems and the need for open idea sharing in the AI development community.\n",
      "\n",
      "The podcast discusses the power of self play in learning for systems, particularly in the context of building Artificial General Intelligence (AGI). It explores surprising behaviors and solutions that have emerged from self play systems, such as the Dota bot and alpha zero. The strengths and weaknesses of simulation as a tool for self play and reinforcement learning are also examined, with examples of successful transfer from simulation to the real world. The podcast also delves into the human elements of self awareness, consciousness, and the constraints of having a physical body in learning. It raises the question of whether artificial neural nets may have consciousness similar to the human brain and the potential for progress in understanding consciousness. The concept of intelligence is also discussed, with a focus on the potential for deep learning systems to surpass human accuracy in tasks like machine translation and computer vision, while still making different types of mistakes.\n",
      "\n",
      "The podcast discusses the meaning of life, human wants and motivations, and the potential impact of AI on society. The speaker emphasizes the importance of making the most of life, finding happiness in simple things, and being humble in the face of uncertainty. They also discuss the potential for AI to significantly impact the GDP and the development of an AGI system. The speaker suggests that AI systems can be designed to align with human values and emphasizes the need for continued alignment as AI systems develop. They also explore the idea of training a value function for AI and question the concept of an objective function for human existence, suggesting that existence itself is amazing and should be maximized. The podcast is sponsored by Cash App and the audience is encouraged to support the podcast and connect with the speaker on social media.\n",
      "RRR rewritten summary\n",
      "[{'text': 'Computer scientist Ilya Sotskever shares insights on the early days of cryptocurrency, the potential of neural networks, and the history of money in a recent podcast. He discusses the development of deep neural networks, the theory of overparameterization, and the importance of compute power in training them. The podcast also explores the evolution of neural networks, the success of deep learning in AI, and the potential future developments in the field. It delves into the challenges of achieving deep understanding in language and visual perception, the concept of reasoning in neural networks, and the economic impact of AI advancements. The speaker also addresses the power of self play in learning for systems, the meaning of life, human wants and motivations, and the potential impact of AI on society. The podcast is sponsored by Cash App and encourages support for the speaker on social media.'}]\n",
      "Stage 2 done time 2024-03-25 22:03:01.684119\n",
      "stage_2_titles: len: 10\n",
      "['1. Ilya Sotskever: Cofounder and Chief Scientist of OpenAI', '2. The Role of Intuition and the Human Brain in Understanding Neural Networks', '3. The Evolution of Neural Networks in Machine Learning', '4. The Beauty of Deep Learning', '5. Advancements in Deep Learning', '6. The Role of Reasoning in Go and Neural Networks', '7. GPT2: A Transformer with 1.5 Billion Parameters', '8. The Impact of AI Advancements on the Global Economy', '9. The Power of Self Play in Learning Systems', '10. Understanding Human Action and Motivation']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "       int(podcast['episode_number']) != 94:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
