{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b2\" # no rewritten\n",
    "\n",
    "SUMMARY_NUM_WORDS = 1500\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100\n",
    "TOPIC_SUMMARY_WORD_COUNT = \"at least 500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fe90915d490>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a \"\"\" + TOPIC_SUMMARY_WORD_COUNT + \"\"\" word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "#   final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-26 01:00:09.246188\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-26 01:06:02.703482\n",
      "chunks_text len: 73\n",
      "extract_keypoints start time: 2024-03-26 01:06:02.703634\n",
      "extract_keypoints done time 2024-03-26 01:09:10.098392\n",
      "Start time: 2024-03-26 01:09:10.098657\n",
      "Stage 1 done time 2024-03-26 01:12:16.463616\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'Advancements in Adobe Research for Creative Automation ', 'text': 'Adobe Research is working to define the future evolution of their products to make the life of creatives easier. They aim to automate tedious tasks and give more time for creatives to operate in the idea space. Deep learning methods of the past decade can shine in this application. Gavin Miller, head of Adobe Research, combines tech and creativity, writing poetry and building robots outside of work.'}, {'title': 'Gavin Miller: Head of Adobe Research and Multifaceted Artist ', 'text': 'Gavin Miller is the head of Adobe Research, leading innovative efforts and applications of AI in creating images, video, audio, and language. Gavin Miller is also an artist, poet, writer, and roboticist. Lux Friedman, the interviewer, enjoys Gavin Miller\\'s poetry and promises not to spend the entire conversation reading it. Gavin Miller\\'s poem \"Je Ne Vinaigrette Rien\" parodies both Edith Piaf\\'s \"Je Ne Vinaigrette Rien\" and Frank Sinatra\\'s \"My Way\". The poem includes both deep and profound verses as well as light and silly ones.'}, {'title': 'Exploring the Struggle with Weight and Embracing Humor ', 'text': \"The speaker opens with a poem about struggling with weight and dieting. The poem reflects the internal struggle between wanting to lose weight and the irrational desire to embrace the opposite idea. The speaker expresses no regrets about their decision. The topic of weight and dieting is a serious one for some people, but the speaker finds humor in taking it to an extreme. The speaker has always been interested in writing and technology since high school. There are parallel strands in the speaker's life, one related to their private life and the other related to writing and technology.\"}, {'title': 'The Intersection of Technology and Creative Expression ', 'text': 'The intersection of private life and technological career. The influence of one idea on the other. The inspiration from science fiction for building new technology. The example of using voice synthesis for writing a poem. The impact of technology limitations on creative expression.'}, {'title': 'Advancements in AI and Smart Home Technology ', 'text': 'The speaker created a poem to match the tone of a voice that sounded sad and depressed. The poem was pretended to be written by an intelligent agent, telling the user to go home and leave them alone, while also expressing loneliness and a desire for company. This level of AI sophistication was beyond what was possible at the time, but is now becoming more feasible. The speaker had a smart home project in the early 90s, with a talking voice reminding them of tasks and buttons on the washing machine to prevent clothes from getting moldy. The speaker also made photo albums that used some form of technology.'}, {'title': 'Creating Magical Realism Photo Albums with Light Sensors and Wireless Radio ', 'text': 'The speaker made photo albums with light sensors and wireless radio to create a magical realism experience. The speaker was intrigued by the idea of creating a personality for modern agents, inspired by their experience in writing plays. The speaker wanted to create a technology that could have real and knowledgeable conversations, not just imaginary dialogue.'}, {'title': 'Challenges and Capabilities of AI in Understanding and Communication ', 'text': '[\"AI can fall into the uncanny valley where it says something it doesn\\'t really understand.\", \\'AI needs to have multiple ways of talking about the same concept to sound like it really understands it.\\', \\'If AI can reason about a concept from multiple angles and give similar responses, it starts to seem more sentient.\\', \"Automatic image captioning is mentioned as an example of AI\\'s capabilities.\"]'}, {'title': 'Advancements in Automatic Image Captioning and 3D Computer Graphics ', 'text': \"Automatic image captioning with the ability to generate different kinds of statements about the same picture. Work on turning a medium from one form to another, such as auto tagging imagery or making up full sentences about what's in the image. Use of GANs to synthesize an asset that matches a given description. Early career focus on 3D computer graphics and pioneering work in the field. Comparison of early 3D graphics work to the Renaissance period, where people would model light, color, and shape.\"}, {'title': 'Advancements in Computer Image Generation ', 'text': 'The new wave in computer image generation is more impressionistic and uses AI algorithms. The creative process is shifting towards generating images directly from ideas rather than focusing on raw pixels. Adobe aims to span the entire range of tools, from low-level analog workflows to realistic oil paint and watercolor simulations. The realistic simulations are important for people who want to create something really.'}, {'title': 'The Importance of Automation in Design Work ', 'text': 'Complete control is important for creating expressive and novel work. Automation of certain tasks frees artists to focus on inspiration. Design work used to take up a lot of time for artists. AI can reason about different formats and languages to assist with design work. The focus should be on the creative aspect of design.'}, {'title': 'The Evolution of Creativity in the Digital Age ', 'text': 'Creativity is changing, making it easier, faster, and cheaper to create beautiful artwork. There is a shift from hands-on artisan to art director or conceptual artist, with the computer as a partner in creating polished examples. Adobe products such as Photoshop, Premiere, and Audition are favored for creating images, videos, and audio.'}, {'title': 'Optimizing Workflow and Automation in Video and Audio Editing ', 'text': 'The speaker uses Photoshop to create the thumbnail for the video, Premiere to edit the video, and Audition for the audio. The speaker emphasizes the importance of optimizing the flow and being extremely efficient in their work process. The speaker uses an old school Kinesis keyboard and auto hotkey to minimize the number of clicks and streamline their workflow. The discussion shifts to the role of AI and automation in making the low level pixel work flow easier in the coming months and years. There is a rich array of algorithms already in Photoshop, including classical procedural algorithms and ones based on data, with a large number of sliders and degrees of freedom.'}, {'title': 'Improving User Experience with AI-Driven Default Settings ', 'text': 'AI can help by providing default settings based on the content itself rather than default values for the tool. Smart defaults can make life easier for people while making use of common sense from other example images. Adobe has spent a lot of work over the last 20 years thinking about selection, such as with quick select, which looks at color boundaries and figures out how to flood fill into physically connected regions. The algorithm used for selection had no visual common sense about what a cat looks like or a dog, it was based on rules of thumb applied to graph theory.'}, {'title': 'Improving Object Identification in Images with Graph Theory and Neural Nets ', 'text': 'Graph theory and neural nets have improved the process of selecting and identifying objects in images. Neural nets can now accurately identify dominant objects in images without the need for manual selection. This technology is valuable for tasks like background removal and can be a great starting point for further editing. The goal is to make tasks like background removal as easy as possible for users.'}, {'title': 'Challenges and Algorithms for Quick Background Removal ', 'text': 'The challenge of removing the background for images with flowing hair. Research on quick, cheap, and cheerful background removal. Algorithms for quick background removal with a single click. Algorithms for more guided background removal with touch-ups. Combinations of tools for various types of background removal. Demonstration of quick object selection at Adobe Max conference.'}, {'title': 'Improving Selection Workflow with Polygon Drawing ', 'text': 'Drawing a simple polygon around the object of interest allows for quick selection. Selection mask can be pulled from a moving target in just a few seconds. Workflow time reduced from hours to seconds. Touching up may be required after initial selection. The difference between academic and industrial research is mentioned.'}, {'title': 'Distinguishing Academic Research from Industrial Research ', 'text': 'The difference between academic research and industrial research is highlighted. Academic research focuses on great new ideas that show promise, while industrial research involves shipping and receiving customer reviews. The company values customer feedback and product critics to improve their products. The goal is not to be perfect every single time, but to be perfect enough of the time and have a mechanism to intervene and recover from mistakes. The company values talented customers and aims to support them in improving their products.'}, {'title': 'The Impact of AI on Professional Tasks ', 'text': 'AI can make professional tasks less tedious and time-consuming. It is important to support professionals in their work with AI. 100% automatic processes could delay time to market. Collaboration between human and machine is important for making the life of creatives easier.'}, {'title': 'The Importance of Supporting Learning in Tutorials ', 'text': \"The importance of helping the person in the moment to do the task they need to do. Thinking holistically about the journey of learning a tool, like Adobe University. Research projects analyzing thousands of hours of tutorials online to understand what's being taught in them. Publication at CHI looking at the last three or four actions people did in tutorials to provide inspiration for what to do next.\"}, {'title': 'Intelligent Workflow Assistance App ', 'text': \"The app uses the context of the user's workflow to make intelligent suggestions. It aims to provide inspiration and learning from similar workflows. The goal is to combine domain understanding with AI and pattern matching to make intelligent suggestions. The suggestions can be verbal or shown as results of trying a certain action. The ultimate goal is to provide assistive suggestions for designers and creative people.\"}, {'title': 'Enhancing Learning with Artist and Teacher Guidance ', 'text': 'The goal is to have an artist and a teacher guide the process. Giving enough at each stage to build a foundation for the next level of expectation. Understanding different media types visually and in terms of transcripts and words is important. Removing the barrier of having to type in keywords for searching. Ultimately, the tool should assist with learning the interface.'}, {'title': 'The Impact of Assistants on Interface Complexity ', 'text': 'The discussion is about whether an assistant modifies the interface to be simpler or just assists with learning the existing interface. Adding a feature to a GUI increases visual complexity for new users, while having an assistant with a new skill is additive without being intimidating. Consideration is given to onboarding new users and how to make the interface easier for them. Some users value mastering a complex interface and keyboard shortcuts, while others prefer a more assistive version for quick and simple tasks.'}, {'title': 'Exciting Applications of Computer Vision and Machine Learning by Adobe ', 'text': 'Adobe is working on exciting applications of computer vision and machine learning. These applications include scene stitching, sky replacement, foreground/background removal, spatial object-based image search, automatic image captioning, project cloak, project deep fill, project scribbler, style transform video, style transform faces and video with project puppetron. Different classes of devices are being considered for a more assistive version of the technology, depending on the context for CAPTCHA. Users in deep post-production workflows may prefer to use a laptop or big screen desktop with more knobs and dials to express subtlety. There are many exciting applications of computer vision and machine learning that Adobe is working on.'}, {'title': 'Automated Sky Replacement for Enhanced Visual Effects ', 'text': 'Sky replace is a compound action that automatically selects the sky and looks for stock content that matches the geometry of the scene. It adds variety in choices for different moods and mats in the sky behind the foreground. It uses the foreground of the other image to recolor the foreground of the image being edited. It can change the color of foreground objects based on the new sky being added. It is an exciting tool with visual elements that can provide links to other similar tools.'}, {'title': 'Achieving Natural Effects in Photography with Innovative Technology ', 'text': 'The evening sky adds an orange glow to foreground objects. The speaker was inspired by Magritte\\'s surrealism, particularly the painting \"The Empire of Light\". The goal is to achieve natural effects in photography without relying on post-production editing. The technology allows for capturing an entire workflow in a single action, saving time and effort. The ability to quickly apply different backgrounds allows for exploration of the design space and finding inspiration.'}, {'title': 'Exploring the Design Space and Advanced Image Search Methods ', 'text': 'The importance of exploring the design space as close to final production value as possible. The idea of making intelligent choices about ways to search stock images. The concept of concept canvas and its application in image search. The need for more advanced search methods beyond just text-based keyword searches.'}, {'title': 'Improving Image Search with Concept Canvas ', 'text': 'Concept canvas allows assigning spatial regions to keywords for image search. Pre-indexed images help in matching important concepts in the picture. Spatial design and layout in concept canvas make it feel like design. Giving enough control makes people feel a sense of ownership over the outcome. Technologies in Photoshop allow physically moving elements in post-production.'}, {'title': 'Advancements in Object Removal and Background Filling with Neural Networks ', 'text': 'Neural networks are being used to remove objects from a scene and fill in the background automatically. GANs (Generative Adversarial Networks) are one approach for achieving this. Traditional algorithms like content aware fill work well for certain classes of images, such as naturalistic textures like gravel paths. Patch-based algorithms can create plausible looking intermediate fill for certain types of images. Algorithms are used to smooth out lighting and eliminate brightness contrast in the filled region.'}, {'title': 'Challenges in Image Processing ', 'text': 'The importance of smooth lighting in avoiding brightness contrast. The challenge of inferring invisible structure behind objects in an image. The common sense knowledge required to fill in missing information in an image. The limitations of current generative methods in handling high resolutions.'}, {'title': 'Challenges and Considerations for Improving Algorithm Performance ', 'text': \"The algorithm is only stable at low resolutions and requires further research to improve its performance at high resolutions. The algorithm may not demonstrate common sense when presented with images outside of its training set. A diverse training set of images is necessary to ensure the algorithm's readiness for primetime. There may be a need for guardrails or a detector to estimate the algorithm's competence in different tasks. The concept of an ensemble of experts specialized in certain tasks may be considered for improving the algorithm's performance.\"}, {'title': 'Improving Workflow Capabilities through Voting and Task Assignment ', 'text': 'The process involves either voting on confidence in future actions or using a dispatcher to assign tasks based on expertise. Each model requires a significant amount of work, but over time, the set will be filled out and capabilities will increase. The focus initially will be on specific workflows, with expansion as capabilities grow. The goal is to gather information on existing workflows and user needs, particularly in popular software like Photoshop. The aim is to understand the type of data needed for annotation and collection, such as specific objects like houses or octopus.'}, {'title': 'The Importance of Data Gathering and Annotation in AI ', 'text': 'Understanding the kind of data needed for annotation and collection is crucial for building effective tools in AI. The topic of data gathering and its purpose is significant in the world of AI. It is important to train customers in using products and also to learn from them about what is important and useful. Respecting privacy and obtaining explicit permission for data usage is essential. Demonstrating the benefits of sharing data with the tool is necessary, either in understanding intent for better recommendations or for quick evolution of the tool.'}, {'title': 'The Importance of Sharing Data for Workflow Improvement ', 'text': 'The importance of sharing workflows and choices with the data set for training. Technologies for learning without storing all information permanently. The willingness to share data with Adobe for improving workflow, compared to social networks. The potential benefit of sharing data with Adobe for improving future workflow.'}, {'title': 'Protecting Sensitive Data in Professional Workflows ', 'text': 'Professional workflows may require protection of sensitive data, such as in legal cases. Some scenarios may involve a more permissive relationship with Adobe for non-confidential projects. There is a possibility of capturing high-level data from more people and detailed knowledge from willing participants. Explicit customer studies are conducted to gather feedback on the tool.'}, {'title': 'Importance of Responsible Data Collection in Customer Studies ', 'text': \"Customer studies involve visiting and observing users to improve the tool. A more systematic process is needed to train an algorithm for customer studies. Conscious effort is needed to balance data collection with customer trust. Adobe has a chief privacy officer to ensure responsible data collection. Privacy is a priority in AI development, not an afterthought. Project Puppetron demonstrates Adobe's move towards thinking in 3D, not just 2D.\"}, {'title': 'Advancements in 3D Thinking for Applying Painting Styles to Videos and Images ', 'text': '3D thinking is being used to assign features to faces in order to apply painting styles to videos or still images of people talking. The technology is able to apply the style of a painting to a person in a video in a way that looks natural and reflects the motion of the face. This process requires inferring more about the 3D structure of the world, even for a 2D workflow like stylization. 3D computer vision algorithms are improving and initially focusing on specific domains like faces, where there is a lot of prior knowledge about structure. Over time, these algorithms should be able to work for more general applications.'}, {'title': 'The Importance of 3D Reconstruction in Virtual Reality ', 'text': '3D reconstruction can be invisible to the user but allows for more reliable and correct edits. The face is a very important application for 3D reconstruction. VR can transport users to an immersive world, similar to a widescreen television.'}, {'title': 'Evolution of VR and AR Technology ', 'text': 'VR technology is evolving in terms of hardware and becoming more affordable. There is a market bifurcation between consumer and professional use cases for VR and AR. VR is useful for experiencing spatial relationships and scale, especially for architects and designers. AR holds the promise of taking digital assets off the screen.'}, {'title': 'The Promise and Challenges of Augmented Reality ', 'text': \"AR holds the promise of taking digital assets off the screen and putting them in context in the real world. The assets need to adapt to the physical context in which they're being placed. AR is like having a live theater troupe come to your house and put on a performance. AR will have the same issue of adapting to different physical spaces. There is a tension between fidelity and adaptation in AR.\"}, {'title': 'The Impact of Media Characteristics on Performance Reproduction and Story Adaptation ', 'text': 'The importance of accurately reproducing a performance, such as a ballet, versus adapting a story to the environment. The influence of the media characteristic on the need for nuance and adaptation. The difference in expectations for famous celebrities versus lovable characters like a frog in storytelling. The potential for ideas from the game world to impact the broader commercial sphere, particularly in adaptive characters for AR.'}, {'title': 'Advancements in AR and VR Technology for 3D Design ', 'text': 'AR technology is being used to create adaptive characters. Demonstrations have been shown of converting Photoshop layer stacks into 3D in AR. Excitement is focused on 3D design, which is still a challenging space. 3D design involves laying out objects, which can be difficult on a conventional screen. VR headsets offer a potential solution for laying out objects in 3D design.'}, {'title': 'The Potential Impact of VR and AR on Design ', 'text': 'VR headset allows for a different viewpoint and sense of depth. Fine grained design tasks may be possible in VR with the right UI. Potential explosion of demand for 3D assets driven by AR and real time animation. Tools and devices in VR may help with designing content.'}, {'title': 'Importance of Designing Content for Product Evolution ', 'text': 'Designing the content is important. New ideas are being considered, but old ways are also valued. Existing user base should not be offended by changes. Convenience should not come at the cost of control. Evolution and growth are important for the product.'}, {'title': 'Evolution and Breakthroughs in Video Editing Tools ', 'text': 'The tool has been evolving and growing, with a lot of brilliant thought along the way. A fundamental breakthrough, such as a single click to select an object, can fit into the existing toolset as an optional mode or starting point. Radical simplicity can be achieved in the context of a different device or targeted workflow, such as a mobile device or a tool focused on spontaneity and velocity. Projects like Rush enable professional quality video editing for a specific class of media output targeted differently.'}, {'title': 'Choosing the Right Video Editing Software for Different Project Types ', 'text': 'Quality video editing for different types of media output and user experiences. Different software options for different project types, such as using Premiere for big projects and Rush for quick, simple projects. Professional tools offer a richer toolkit and more flexibility. The use of AI for smart defaults and coaching, similar to Google\\'s \"I\\'m feeling lucky\" button. The idea of using one button to provide a good set of settings as an educational tool.'}, {'title': 'The Importance of an Educational Tool for Image Editing ', 'text': 'The need for an educational tool to show the correlation between different bars that control different elements of an image. The uncertainty about the optimal settings and the need for on-demand help. The suggestion of a \"make a suggestion\" button to provide helpful suggestions based on knowledge of workflows. The idea of having a variety of defaults and options to choose from. The mention of poetry and the intention to interleave it with the conversation.'}, {'title': 'The Impact of AI and the Digital World on Human Perception ', 'text': 'The poem reflects the feeling of liberation when leaving the smartphone behind. AI is helping to create versions of ourselves and reality that are more beautiful than actual reality. The digital world is partly artificial and better than the world of a hundred years ago. The creative effort in creating this illusion is part of the process. The question of how human beings should adjust to live in this digital world is raised.'}, {'title': 'The Impact of Social Media on Self-Presentation ', 'text': 'The use of social media platforms like Instagram and Facebook has led to the creation of better versions of ourselves online. The use of image modification tools and artificial intelligence has allowed for the creation of adjusted or fake versions of ourselves and reality. The human desire to present the best version of oneself has always been true throughout history. The shift towards a more visual culture has made it easier to imagine alternate realities and visualize them. In the past, storytelling and poetry were used to imagine alternate realities, but now visual culture plays a significant role.'}, {'title': 'The Impact of Visual Culture on Self-Presentation ', 'text': 'Our culture has become very visual and quick. Intent plays a big role in how we present ourselves visually. People may judge others based on their appearance, but the reality will be revealed when they meet in person. Holding oneself to an impossible standard based on visual appearance can be harmful. The ability to imagine and visualize an alternate reality can be a wonderful thing.'}, {'title': 'The Impact of Alternate Reality and Visualization Technology on Exploration ', 'text': '- Alternate reality can inspire people to create new architectural styles and even start businesses.- The availability of high-quality graphics may reduce the excitement of exploring new places in person.- The joy of exploration, such as going to the moon or discovering new planets like Pluto, is still important despite advances in visualization technology.'}, {'title': 'The Importance of Critical Thinking in Communication and Imaging ', 'text': \"Pluto was a fantastic recent discovery with breathtakingly varied and beautiful features. Expanding the ability of the human toolkit to imagine and communicate is a good thing. Abuses in communication and imaging should be taken seriously and discouraged. The public needs to be aware of what's possible through events like this and not believe everything they see or read. Multiple sets of evidence may be needed to believe something rather than a single media asset. The concept of needing multiple sets of evidence to believe something has been true forever. There is a famous story about Anne of Cleves and Henry VIII involving a painted picture by Holbein.\"}, {'title': \"Holbein's Unpleasing Portrait and the Thriving Research Lab \", 'text': \"Holbein painted a picture that Henry VIII wasn't pleased with. The secret to a thriving research lab is interns. Constant influx of new people brings new ideas to the research lab.\"}, {'title': 'The Benefits of Internships in Research ', 'text': 'A constant influx of new people brings new ideas with it. Interns allow for exploration of fanciful or unproven ideas in a lightweight way. Internships can lead to new publications for both the intern and the researcher. Internships provide a portfolio of ideas to draw from for future development. Internships are a great way to identify future full-time researchers. Internships build a bridge to university departments for collaboration and recruitment.'}, {'title': 'Building Enduring Relationships Between Companies and University Departments ', 'text': 'The program builds a bridge to university departments to establish enduring relationships with professors. The interns add value through their collaborations and contribute to a virtuous cycle. The long-term legacy of a great research lab includes the impact on people who move through and carry the same model to other companies. The company strongly believes in industrial research and its complementarity with academia. The company hopes that this model will continue to propagate and be invested in by other companies. The idea for the program was born through brainstorming and discussions with the interns.'}, {'title': 'Title ', 'text': 'The Intern Selection ProcessText '}, {'title': 'The Dynamics of Research Labs ', 'text': 'The flexibility of pursuing ideas in research labs. The decision-making process in research labs. The reward system based on impact in Adobe. The alternative model of having one lab director making decisions.'}, {'title': 'Encouraging Innovation and Flexibility in the Lab ', 'text': \"The model of running the lab is based on allowing new ideas to percolate up. The lab director is not a dictator but rather encourages and appreciates certain priorities and ideas. The company has strategic priorities and areas for investment, but also allows for individual interpretation and interest in pursuing certain ideas. Adobe's broad portfolio of products allows for flexibility and support for new ideas within the company. There is a collaborative and flexible approach to implementing new ideas within the company.\"}, {'title': 'Product Team Intern Sponsorship and Project Success ', 'text': \"The product teams sponsor extra interns occasionally to address specific problems they care about. It is not typical for the product teams to sponsor extra interns, and it is considered as an additional expense. It is difficult to predict the success of intern projects at the beginning of the summer. Some intern projects may result in valuable features, while others may not. Some projects may reveal the team's lack of knowledge and require revisiting.\"}, {'title': 'Progress and Realization in Problem Solving and Technological Breakthroughs ', 'text': \"['Progress and realization of how much is unknown.', 'Revisiting a problem multiple times before a breakthrough.', 'Impact on product and technological breakthroughs.', 'Creative and analytics assistants making useful suggestions.', 'Unpredictability of progress in these areas.']\"}, {'title': 'Advancements in Generative Adversarial Networks and Sensei Platform ', 'text': 'Core technologies like generative adversarial networks are immensely promising. The quick practical application of these technologies for mainstream use cases at high resolution with good quality is exciting. The strange and interesting way in which these technologies operate, resembling dreaming. The development of a Sensei platform for pulling neural nets and other intelligence models into a central platform, accessible by multiple product teams at once. Transitioning from hand designing for specific use cases to a more standardized approach, allowing for easier access and utilization.'}, {'title': 'Standardizing Processes and Emerging Technologies at Ford ', 'text': 'Ford is standardizing processes to shorten the time between idea generation and product impact. Products can leverage good ideas from each other, creating an economy of scale. There is a renaissance in AI and real-time ray tracing in graphics, leading to exciting emerging technologies. The combination of AI and graphics technologies will create a real-world experience with real-time shadows, reflections, and magical properties brought by AI. This future is exciting for creators and will enhance the creative process.'}, {'title': 'Fascination with Snakes and Robots ', 'text': \"The speaker works in autonomous vehicles as a roboticist and loves robots. The speaker has a fascination with snakes, both natural and artificial robots. There are 2,900 species of snakes in the world, with 875 venomous. The speaker's work in computer animation in the 80s led to an interest in simulating snake movement.\"}, {'title': 'The Evolution of Animating Spring Lengths and Simulating Muscles ', 'text': \"The idea of animating spring lengths and simulating muscles came from the observation of objects bouncing and then stopping. The earliest application of this idea was in a paper called The Motion Dynamics of Snakes and Worms in 1988. The research involved reading physiology literature on how snakes and worms move and creating early computer animation examples. Interest in robotics stemmed from simulation and graphics work. A movie called Her Majesty's Secret Serpent was created, featuring a secret agent snake parachuting in to capture a film canister from a satellite. The idea of building a real radio controlled chip came from making them from scratch as a child. This led to the start of building a real radio controlled chip.\"}, {'title': 'Building Better Snake Robots ', 'text': \"The speaker had a 15 year obsession with building better snake robots. The first snake robot built could only slither sideways, but didn't go forward. The speaker added wheels to the snake robot to address friction issues. The speaker loves creating the illusion of life, which drove them to animation. The goal is to create a robot that moves in a biological way, resembling a creature rather than a thing. The early snake robot was able to sidewind and go directly forward. The snake robot was used as the ring bearer at the speaker's wedding. The project was initially done as a hobby.\"}, {'title': 'Early Development of Autonomous Systems ', 'text': 'The speaker initially started building and experimenting with autonomous systems as a hobby. The onboard compute at the time was incredibly limited, leading to the use of discrete logic and 8-bit microprocessors with minimal RAM. The early systems were radio controlled rather than autonomous, focusing on coordinated motion and physicality. The speaker also delved into creating clockwork toys, learning about backlash and other mechanical aspects.'}, {'title': 'The Experience of Building and Learning from a Robotic Project ', 'text': 'The text discusses the experience of building and learning from a robotic project. The speaker mentions building a better engineered version of a previous project, S3, which had worn out motors and was no longer functional. The project S5 is described as being biologically inspired and different from typical robotic designs, with unique segments and a tapering shape. The speaker mentions that S5 is currently on display at the International Spy Museum in Washington, DC. The text also mentions the disappointment of not being able to buy replacements for a meaningful project, S3. The speaker reflects on the humbling experience of realizing that a seemingly good idea did not work as expected.'}, {'title': 'Spy Museum in Washington, DC ', 'text': \"['Spy Museum in Washington, DC.', 'Conspiracy theory about the museum being fake.', 'Use of Raspberry Pi for onboard compute.', 'Addition of vision accelerator chips for object recognition.', 'Desktop level compute enabling true autonomy with onboard compute and batteries.']\"}, {'title': 'Title ', 'text': 'Informative Features of Robots for Children and AdultsText '}, {'title': 'The Curiosity and Intelligence in Research ', 'text': 'Being in research is a license to be curious. Hobby forced the person to read biology and be curious about things. Person tries to bring life and beauty into something inanimate. Intelligent agent research will converge with vision and voice synthesis to give it a sense of having intelligence. Turing test is a high bar and having a meaningful conversation with the intelligent agent is more important.'}, {'title': 'The Importance of Meaningful Interaction and Reasoning in Robot Pet Ownership ', 'text': 'The goal is to have a robot pet owner understand what the robot thinks about and can reason about. Meaningful interaction with the robot is important, similar to the interaction with a dog. The reasoning system of the robot should be able to explain why it knows or thinks something. The robot serves as a muse for thinking about the future of AI.'}, {'title': 'The Future of AI and Virtual Reality ', 'text': 'The robot is the muse for thinking about the future of AI and what to invent next. Bringing virtual objects into the physical world through augmented reality is more likely than building intelligent robots. Many ideas that might take five years to build a robot to do can be done in a few weeks with digital assets. Living with virtual personalities for a long time will make intelligent robots less surprising when they become commonplace.'}, {'title': \"Speaker's Excitement for the Future and Gratitude for Conversation \", 'text': 'The speaker compares the future to a world with \"Siri with legs or Alexa on hooves\". The speaker is excited about the convergence of different strands of their career. The conversation ends with the recitation of a favorite poem about mortality and immortality. The speaker expresses gratitude for the conversation.'}, {'title': 'Title ', 'text': 'Creating Inspiration and EmpowermentText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-26 01:12:23.853377 ...\n",
      "Best SD: inf, Best iteration: 0\n",
      "done get topics 2024-03-26 01:12:24.739199.\n",
      "Stage 2 start time 2024-03-26 01:12:24.739221\n",
      "RRRRRR summary_num_words: 1500\n",
      "RRRRR titles:\n",
      "1. Advancements in Adobe Research for Creative Automation\n",
      "2. Advancements in Automatic Image Captioning and 3D Computer Graphics\n",
      "3. Distinguishing Academic Research from Industrial Research\n",
      "4. Exciting Applications of Computer Vision and Machine Learning by Adobe\n",
      "5. The Importance of Sharing Data for Workflow Improvement\n",
      "6. Advancements in 3D Thinking for Applying Painting Styles to Videos and Images\n",
      "7. Importance of Designing Content for Product Evolution\n",
      "8. The Impact of Visual Culture on Self-Presentation\n",
      "9. Holbein's Unpleasing Portrait and the Thriving Research Lab\n",
      "10. Advancements in Generative Adversarial Networks and Sensei Platform\n",
      "11. Early Development of Autonomous Systems\n",
      "Stage 2 done time 2024-03-26 01:14:22.184870\n",
      "stage_2_titles: len: 11\n",
      "['1. Advancements in Adobe Research for Creative Automation', '2. Advancements in Automatic Image Captioning and 3D Computer Graphics', '3. Distinguishing Academic Research from Industrial Research', '4. Exciting Applications of Computer Vision and Machine Learning by Adobe', '5. The Importance of Sharing Data for Workflow Improvement', '6. Advancements in 3D Thinking for Applying Painting Styles to Videos and Images', '7. Importance of Designing Content for Product Evolution', '8. The Impact of Visual Culture on Self-Presentation', \"9. Holbein's Unpleasing Portrait and the Thriving Research Lab\", '10. Advancements in Generative Adversarial Networks and Sensei Platform', '11. Early Development of Autonomous Systems']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "#     if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "#         #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "#         continue\n",
    "    \n",
    "#     if int(podcast['episode_number']) != 12 and int(podcast['episode_number']) != 23 and \\\n",
    "#        int(podcast['episode_number']) != 94 and int(podcast['episode_number']) != 22:    \n",
    "    if int(podcast['episode_number']) != 23:           \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
