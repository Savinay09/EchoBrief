{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "VERSION=\"v2b\"\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f664f842b90>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-24 14:46:04.446163\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-24 14:51:11.208687\n",
      "chunks_text len: 72\n",
      "extract_keypoints start time: 2024-03-24 14:51:11.208847\n",
      "extract_keypoints done time 2024-03-24 14:53:50.541169\n",
      "Start time: 2024-03-24 14:53:50.541425\n",
      "Stage 1 done time 2024-03-24 14:56:14.931393\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow and the Role of Rajat Manga at Google ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work going on in the world in deep learning. TensorFlow is now an ecosystem of tools for the deployment of machine learning in the cloud, on the phone, in the browser, on both generic and specialized hardware. There is a big emphasis on growing a passionate community of developers. Rajat, Jeff Dean, and a large team of engineers at Google Brain are working to define the future of machine learning with TensorFlow 2.0, which is now in alpha. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Evolution of Open Source in Google Brain ', 'text': 'Open innovation can inspire companies to open source their code and engage in the open exchange of ideas. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library at Google Brain turned into TensorFlow in 2014, the open source library. Deep learning was interesting and intriguing, and held promise, but had not yet taken off.'}, {'title': \"Scaling Research Work with Google's Compute Power and Data \", 'text': 'The idea was to scale research work to Google\\'s compute power and data. Scaling the compute and data showed better results. Early wins were achieved in speech and image research. Collaboration with the speech research team was successful. The \"cat paper\" was a significant achievement in image research.'}, {'title': 'Google Brain and Machine Learning at Scale ', 'text': 'Google Brain was founded around neural networks and deep learning from the beginning. The mission was to scale machine learning to hundreds and thousands of machines. Google has been doing machine learning for a long time.'}, {'title': 'The Rise of Deep Learning at Google ', 'text': 'Google has been doing machine learning for a long time. Deep learning was new, but as they scaled it up, they showed that it was possible and would impact many things. Real products started to use deep learning, starting with speech and then expanding to other areas like image recognition. Academia also started to show interest in deep learning around 2014. Google decided to open source TensorFlow, indicating the growing importance of deep learning both internally and externally.'}, {'title': \"The Impact of Google's Decision to Open Source TensorFlow \", 'text': \"The decision to go open source with TensorFlow is considered a seminal moment in software engineering. Google's decision to open source a large project with significant IP was a powerful statement in favor of open innovation. The initial idea to go open source came from Jeff, a big proponent of the concept. The decision was influenced by the research group's desire to push the state of the art forward and build on others' research.\"}, {'title': 'Advancements in Deep Learning and Machine Learning ', 'text': \"Deep learning and machine learning have grown rapidly due to sharing of research. Existing libraries like Tiano and Torch were developed by academia, but there was a significant difference in level. Google had developed software internally and published papers, leading to successful open source projects. Hadoop was developed from technology built by the speaker's team, which they believed was superior for various reasons.\"}, {'title': \"Google Cloud's Bigtable and HBase APIs, TensorFlow, and Community Focus \", 'text': 'Google Cloud is providing Bigtable and HBase APIs. The goal is to provide something better and push a good standard forward. TensorFlow is open source and can be used anywhere. Google Cloud ensures lots of integrations and works well with TensorFlow. The focus is on helping the community and pushing a good standard forward.'}, {'title': 'The Evolution of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. The incredible ecosystem surrounding TensorFlow. The timeline of the development and open sourcing of TensorFlow. The fast pace of development in deep learning.'}, {'title': 'Title ', 'text': 'The Evolution of TensorFlow for Large Scale and Mobile DeploymentText '}, {'title': 'Challenges and Influences in Running Machine Learning on Mobile Phones ', 'text': 'Ideas of running machine learning on the phone existed at that time. Customized handcrafted code or internal libraries were used for running machine learning on the phone. The use of Theano and Caffe at Google influenced design decisions. The belief was built in parallel with the development of libraries like Theano. The systems at Google were very different, so the focus was on building internal tools. Multiple libraries were considered before making design decisions.'}, {'title': 'Exploring Libraries for Machine Learning ', 'text': 'We looked at a number of libraries such as Theano, Torch, Lua, and Caffe. The group discussed ideas around having a graph or not. The key decisions were based on the limitations seen in prior technologies and the fast pace of research and hardware changes. Flexibility in expressing various concepts was a key factor in the decision-making process.'}, {'title': 'The Evolution of TensorFlow 2.0 ', 'text': 'The move towards TensorFlow 2.0 includes more default eager execution and hiding the graph. The graph decision came from the need for simplicity and intuition in development. The graph was influenced by Theano and the need for simplicity in deployment. Experimentation with ideas in Python led to the realization that not having a graph made things simpler to use.'}, {'title': 'Influences and Considerations in the Deployment of a Graph ', 'text': 'The decision to use a graph for deployment was influenced by the complexity of other ideas. The popularity of the product, with 41 million downloads, was unexpected. The need for the product was recognized early on in the research perspective and early days of deep learning. The potential for future growth and enabling more people to use the product was considered after open sourcing. The growth of deep learning was observed after open sourcing, leading to the realization of potential for more people to use the product.'}, {'title': 'Rapid Growth of Deep Learning After Open Sourcing ', 'text': 'Deep learning grew rapidly after open sourcing. The company saw the opportunity to leverage deep learning and deliver on what people want. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel. The company is very community driven. The initial version was 0.6 or 0.5. People initially loved the documentation provided by the company.'}, {'title': 'The Evolution of Deep Learning from Research to Practical Applications ', 'text': 'Documentation was initially well-received and seen as a significant improvement from academic projects. Deep learning transitioned from a research focus to being accessible to developers for practical applications. The focus shifted towards stability and deployment for non-research purposes. Planning for version 1.0 involved addressing the needs of stability and deployment. Enterprises showed increasing interest in the product as it progressed.'}, {'title': 'Enterprise Adoption of a Product ', 'text': 'Excitement around enterprise adoption of a product. Initial interest from researchers, hobbies, and early adopters. Pressure for stability from enterprises before version 1.0. Importance of understanding what enterprises want.'}, {'title': 'Prioritizing Stability and Simplicity in Technology Adoption ', 'text': 'Enterprise and user needs can vary, with some prioritizing stability and simplicity over the latest performance and quality. Inception and ResNet 50 are still widely used, even though they are several years old. There is value in providing stability and simplicity, as it allows more people to access the technology. The research crowd is interested in pushing the boundaries with new technologies like RNNs, transformers, RL, and GANs.'}, {'title': 'The Advancements in AI with RL and GANs ', 'text': 'The combination of RL and GANs is pushing the state of the art in AI. Older AI models, such as ResNet 50, are still very usable and stable. Transfer learning on specific problems is a common use case for AI. Making AI as easy as possible for hobbyists is important. The majority of the world uses AI for transfer learning on specific problems. AI applications on phones often use transfer learning. Transfer learning looks great in presentations.'}, {'title': 'The Importance of Predictive Modeling and Deep Learning in Enterprise Data Analysis ', 'text': 'Enterprises have data they want to make predictions on, often using regression models, linear models, or gradient booster trees. Some enterprises still benefit from deep learning, especially with large data sets. The audience for these predictions may vary, with some focusing on structured data. The TensorFlow Extended piece, which is the entire pipeline, is important for enterprises. The best enterprises probably have very large data sets where deep learning can shine.'}, {'title': 'The Importance of Data Organization for Using TensorFlow ', 'text': 'TensorFlow Extended is the entire pipeline, focused on stability and simplicity. Companies often have old school data organization, which hinders the use of TensorFlow. The role of an evangelist is to encourage companies to organize their data for the big benefit of using TensorFlow.'}, {'title': 'Importance of Data Organization and Availability in Machine Learning ', 'text': 'Conversations about various questions related to data and machine learning. Need for organized data for automation and prediction. Availability of more data sets and pre-trained models within the TensorFlow ecosystem.'}, {'title': 'The Impact of New Data Sets on Organization and Accessibility ', 'text': 'The release of new data sets has led to a demand for better organization and accessibility. Starting with basic models and improving them is a recommended approach. The appearance of Keras has made TensorFlow more accessible. Keras initially started on top of Tiano and later moved to TensorFlow.'}, {'title': 'The Origins and Evolution of the Keras Project ', 'text': 'Francois started the Keras project before he was at Google. Tiano was the first thing he started. He decided to create an interface and put TensorFlow as a backend when TensorFlow started becoming popular. He joined Google after creating the interface. He initially joined research and was doing some amazing research. He has some papers on research and is a great researcher.'}, {'title': 'Integration of Keras into TensorFlow ', 'text': \"Keras got integrated into TensorFlow in a deep way. TensorFlow 2.0 recommends Keras as the way for beginners to interact with TensorFlow. The integration with Keras makes transfer learning and basic use cases simple, even for enterprises. The initial quarter of collaboration with the researcher turned into two years of full integration. The researcher's work with the API was well-received and led to the integration with TensorFlow.\"}, {'title': 'Choosing Keras as the Standard API for Simplified Integration ', 'text': 'The company had multiple APIs, including a parallel layers API and Keras. The goal was to integrate the APIs and simplify the process for users. The community was confused about which API to use and needed a standard recommendation. The decision was made to focus on Keras due to its popularity and positive feedback from users.'}, {'title': 'The Impact of Keras on TensorFlow ', 'text': 'Keras was loved by many and had great qualities. It was surprising to bring in an outside element like Keras, which became an empowering part of TensorFlow. The team and developers all want to make things easier for a large set of developers. Python has Guido van Rossum, who held the position of benevolent dictator for life. A successful open source project like TensorFlow needs one person to make final decisions.'}, {'title': 'Success and Growth at the TensorFlow Dev Summit ', 'text': 'TensorFlow Dev Summit was successful. Different new features are being incorporated into TensorFlow. There is an amazing ecosystem surrounding TensorFlow. There are multiple people involved in the key design directions of TensorFlow. Regular design reviews are conducted for TensorFlow. Efforts have been made to open up to the community and add transparency in TensorFlow development. More processes such as RFCs and special interest groups are being set in place for TensorFlow.'}, {'title': 'The Importance of Transparency and Processes in Scaling the Ecosystem ', 'text': \"The need for adding transparency and setting more processes in place, such as RFCs and special interest groups, to grow the community and scale the ecosystem. The recognition that the ecosystem cannot scale with a lone decision maker. The growth of the ecosystem, starting with Andrej Karpathy's ComNetJS and the development of TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile. The convergence of all these developments towards the ability to save models in the same way and move them between different platforms.\"}, {'title': 'Title ', 'text': 'Enabling Machine Learning Research and ApplicationText '}, {'title': 'The Integration of Machine Learning into Real Products ', 'text': 'Machine learning is being integrated into real products to have a real impact on people. There are a large number of compute devices across the world, including phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning is growing and covering more aspects over time. There is a focus on pushing the boundaries and building more tooling for machine learning.'}, {'title': 'The Evolution of Tooling and Libraries in TensorFlow ', 'text': 'TensorFlow has built more tooling and things to help with ML pipelines. TensorFlow was the first tool they started with. There are lots of libraries being built on top of TensorFlow. Some libraries are for research, while others are for production. Some libraries come from within Google, while others come from the community.'}, {'title': 'Title ', 'text': 'Enabling Community Collaboration and Model Sharing in TensorFlow 2.0Text '}, {'title': 'Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'TensorFlow.js and deep learning JS were initially difficult projects to integrate into the ecosystem. There have been many technical challenges to overcome in the development of TensorFlow.js. The team has learned a lot and iterated over the last few years. The goal is to make it easy for the end user, but there are many complexities behind the scenes. There are still challenges ahead, such as integrating with new devices from a hardware perspective.'}, {'title': 'Challenges of Modifying the Monolithic System of TensorFlow ', 'text': \"TensorFlow started as a very monolithic system and to some extent it still is. There are lots of tools around it, but the core is still pretty large and monolithic. It's hard to change and modify and really break apart. It's like changing the engine with a car running or trying to fix that.\"}, {'title': 'Challenges of Using TensorFlow in Applications ', 'text': 'The challenge of so many people relying on TensorFlow in their applications. The responsibility for previous versions to still work to some degree. The balance between breaking back compatibility and making the conversion straightforward. The impact on production applications versus research code.'}, {'title': 'The Importance of Maintaining Compatibility in Production Systems ', 'text': \"Production systems rely on TensorFlow, both at Google and across the world. It is important to maintain compatibility for systems that run for a long time. Making new changes and improvements comes with a huge cost. The trade-off is between slowing certain things down and bringing overall value. It's not just about breaking the person yesterday, but also about setting standards for the person tomorrow. New things should be done with consideration for the people who will come on board in the future.\"}, {'title': 'Importance of Designing with a Clean Slate ', 'text': 'Design with a clean slate in mind is important for new things. Making compromises occasionally is necessary, but designing with a clean slate is crucial. It is important to put all concerns behind when thinking of new ideas. The speaker has switched their research group to TensorFlow and believes it is leading in many ways.'}, {'title': 'The Rise of PyTorch in Research ', 'text': \"PyTorch is being used by a lot of researchers now. TensorFlow was chosen with production in mind, not just for research. PyTorch focuses on research and making things easy, not on speed. PyTorch doesn't worry about graphs and just runs things. There are things to learn from PyTorch's approach.\"}, {'title': 'The Importance of Learning from Previous Experiences and Exploring Different Spaces ', 'text': 'The text discusses the benefit of learning from previous experiences and exploring different spaces. It mentions the competition and the process of revisiting and adding new features. The text highlights the importance of eager execution and the effort put into combining different elements. It also references Muhammad Ali versus Frasier as a comparison.'}, {'title': 'Title ', 'text': 'The Advancements of TensorFlowText '}, {'title': 'Excitement for Clean APIs and Performance Improvements in Version 2.0 ', 'text': 'The development team is excited about the clean APIs and the potential for performance improvements with version 2.0. The clean APIs will allow for optimization and improved performance for both single machine and distributed systems. The team is looking forward to exploring new possibilities and spaces behind the scenes in future versions after 2.0.'}, {'title': 'Restructuring TensorFlow for Improved Modularity ', 'text': 'The team is excited about future versions and expects to see improvements over time. Restructuring the monolithic structure into more modular pieces is important for the ecosystem and other organizations. The current organization of TensorFlow on GitHub consists of multiple repositories, with the core one containing the execution engine, key backends for CPUs and GPUs, and distributed computing capabilities. The current setup does not easily allow for splitting the components apart, and clean interfaces are needed for a perfect world scenario.'}, {'title': 'The Importance of Clean Interfaces and TensorFlow in Development ', 'text': 'In a perfect world, clean interfaces would allow for easy implementation on different clusters with custom networking. Clean separation in interfaces will help with more interesting developments in different spaces. Enabling independent evolution and pushing on things allows for better scalability in the ecosystem. Major corporations like Pepsi are already using TensorFlow for development.'}, {'title': 'The Growth and Impact of TensorFlow ', 'text': 'Many users are already using TensorFlow, including hardware vendors and bigger companies like IBM. TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The community growth is attributed to the involvement of various users and companies, including autonomous vehicle companies. The critical thing that allowed for this growth is not specified in the given text.'}, {'title': 'Importance of Community Involvement for Growth ', 'text': 'Growth is critical and needs to continue. Factors need to come together for growth. Listening to the community and their needs is important. Being open to external contributions and helping contributors. Putting the right process in place for community involvement.'}, {'title': 'Importance of Transparency and Community in Open Source Project Growth ', 'text': 'Transparency is important for an open source project. Community aspects are important to work on as a project grows. Putting processes in place and thinking about documentation and tools are important as a project grows. People building something on TensorFlow and implementing a particular architecture feeds the growth of TensorFlow.'}, {'title': 'Investing in Developer Support and Project Contribution on GitHub ', 'text': 'The company is working hard to make it easy to use and contribute to their projects on GitHub. They are investing in tooling to support developers and make version changes smooth. People want to move to new things because they see value in them, not just because they are new.'}, {'title': 'Anticipated Shift Towards Advanced Deep Learning in the Next Few Months ', 'text': 'Most people want a really good thing. People will start to see the value and shift towards it in the next few months. The field is moving rapidly, which will help in doing more things and new things will happen in 2.x. Change is expected to happen, especially in terms of deep learning basics such as convolution models.'}, {'title': 'The Future of Machine Learning and TensorFlow ', 'text': 'Convolution models and basic concepts will likely still be around in some form in five years. Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) are very likely to stay based on their current status. There will probably be new developments in the field, but they are hard to predict. Some current directions include combining eager execution and graphs to make programming more natural, and the use of Swift for TensorFlow. It is expected to see more developments in these areas in the next five years. Uncertainty remains regarding the future of hardware accelerators and the potential for training with fewer bits.'}, {'title': 'The Evolution of TPU and TensorFlow ', 'text': 'TPU is already on version three, and it is exploring the use of four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving, learning from each other and from the community and applications. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to easily train or do transfer learning on simple image models like Inception or ResNet. Providing simple models for beginners is important.'}, {'title': 'Improving User Experience with TensorFlow ', 'text': \"Providing simple models and tools like hub to make it easy for users. Different levels of support for beginners, intermediate users, and researchers. Offering pre-trained models to decrease the time needed to start. TensorFlow's recent delivery is trivial for beginners. Addressing pain points and trying to ease the user experience.\"}, {'title': 'The Impact of High Schoolers and TensorFlow on Technology ', 'text': 'High schoolers are doing amazing and terrifying things, and will contribute incredible ideas as they grow up. There is a technical aspect and a management aspect to the role with TensorFlow. Google has been at the forefront of exploring what it takes to build a good team. TensorFlow is one of the most cutting edge technologies in the world. Cohesion across the team is important for delivering something well.'}, {'title': 'The Importance of Team Cohesion ', 'text': \"Cohesion across the team is important for execution. Teamwork is essential for achieving more than individuals can alone. Hiring good people who care about what they're building and are motivated is crucial. Having a unified vision of where the team wants to go is important.\"}, {'title': \"Google's Unified Vision and Organizational Structure \", 'text': \"Google has a somewhat unified vision of where they want to go. Google is a bottom-up organization in some sense, but also does research. As Google has become a larger product and ecosystem, it's important to combine a mix of direction and exploration. The mission of superstars is a key element at Google.\"}, {'title': 'Challenges and Opportunities at Google ', 'text': 'Large percentage of work at Google is done by individual superstars. Superstars can sometimes create tension within a team dynamic. The mission of the project in TensorFlow is exciting and at the cutting edge. Google values getting people who care and have the same kind of culture. The project allows for lots of people to do different things and grow. There are always people challenges in different kinds of ways.'}, {'title': 'Refining the Hiring Process at Google ', 'text': \"The hiring process at Google has been refined over the last 20 years. Google values teamwork and productivity over individual superstar status. Core technical skills are important, but teamwork and collaboration are also crucial in hiring engineers. It's important for individuals to work well with the team across Google. If a superstar is hurting the team, then that's a problem.\"}, {'title': 'The Importance of Motivation in the Workplace ', 'text': \"Motivation is important in addition to core technical skills. Alignment of motivation with the team's goals is crucial for long term success. Motivation is important at every level, not just for senior positions. Google's hiring process focuses on puzzle solving and problem solving abilities, but may not fully assess motivation.\"}, {'title': 'Title ', 'text': \"The Importance of Culture Fit in Google's Interview ProcessText \"}, {'title': 'Navigating Project and Team Dynamics at Google ', 'text': 'Balancing the need for full-fledged products with ensuring things work really well. Importance of finding the right fit for projects and teams. Variability of culture across projects, teams, and product areas at Google. Engineering excellence as a core part of the culture. Difficult things can be fun when solved. The key to success in a large ecosystem or small product is.'}, {'title': 'Making Decisions and Balancing Priorities in Ecosystems and Products ', 'text': 'Striking a balance across different aspects of a large ecosystem or a small product. Making decisions on speed versus perfection, community involvement, and saying no to certain things. Hard decisions are often made quickly due to time constraints or after careful consideration. The Dev Summit came together incredibly, with a lot of moving pieces and a deadline that made people rise to the occasion.'}, {'title': 'The Importance of Deadlines in Software Development ', 'text': \"Deadlines bring a sense of urgency to get the right things together. It's important to strike a good balance between perfection and getting something that works well. The team did a great job in putting TensorFlow 2.0 alpha together. Official deadlines are not always put out, but key things are focused on and their importance is determined. Development is done in the open, both internally and externally, and releases are done at a regular cadence.\"}, {'title': 'Approach to Software Releases ', 'text': \"Releases are done at a regular cadence, with the understanding that if something isn't ready this month, it can be included in the next release in a month or two. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it's clear that they are experimental and feedback is encouraged. Quick cycle and quick iteration are important, rather than focusing on meeting a deadline with everything perfect. There is no pressure to make TensorFlow 2.0 stable, similar to the approach taken with WordPress 5.0, where updates were released quickly to improve it.\"}, {'title': 'NodeX API Stability and Future Development ', 'text': 'NodeX API stability is a priority. There will be ongoing updates and releases for NodeX. The development and improvement of NodeX will continue beyond the next two months. TensorFlow has already had 41 million downloads for version 1.0 X.'}, {'title': 'TensorFlow 1.0 X Release Update ', 'text': 'TensorFlow has 41 million downloads for 1.0 X. The focus is on polishing and putting together features. There is no rush to release the product. The goal is to get it right and focus on quality. The release is planned for the next few months or next quarter. Spring is a relative concept, spoken like a true developer. The interviewee previously led a team at Google on search ads. Ads can connect people to the things they want and need.'}, {'title': 'The Impact of Personalized Advertising on User Experience ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. Machine learning can connect users to the things they want and need, providing a personalized experience. Ads can annoy users and ruin the user experience if not personalized to their needs and wants. Huge amounts of data can be used for personalized advertising and user experience.\"}, {'title': 'The Importance of Accessible and Relevant Information in Advertising ', 'text': \"The goal is to make the world's information accessible, including products and other things that people care about. It is important for the information and ads to align with what the users need. In search ads, there is a minimum quality level before an ad is shown. Advertising is a key part of the model and has been adapted to the web. There are aspects of ads that can be annoying, such as ads that interrupt the user's experience on a website.\"}, {'title': 'Finding the Balance: Monetization and User Value in Advertisements ', 'text': 'Advertisements need to strike a balance between being valuable to the user and providing monetization to the service. Monetization is necessary for services such as search engines and websites to provide their service. The challenge is to find a balance between showing valuable ads and not being annoying or distracting. Advertisements, when done well, can be really useful and not annoying. One of the limitations of the internet is that nobody wants to pay for anything.'}, {'title': 'Rise of Paid Services on the Web ', 'text': 'More paid services are being seen across the web and people are willing to pay for them because they see the value. Transition towards a mix model where maybe you get to try something out for free, maybe with ads. People are willing to pay for newspaper content and good news websites across the web. The trend is towards monetizing content with ads, but there is also a shift towards paid services. Examples like Netflix and YouTube show that people are willing to pay for content. The speaker and people around them have seen a change in willingness to pay for online content in recent years.'}, {'title': 'Transition to a Mix Model and Use of TPU in Google Call App ', 'text': \"['Transition to a mix model with free trials and ads, followed by a clear revenue model.', 'Use of TPU in a Google call app for free.', 'Ability to run TensorFlow open source on any device, including desktop and phone.', 'Increasing power of desktops and phones for running TensorFlow.', 'Training TensorFlow on a phone.']\"}, {'title': 'The Power and Convenience of Cloud Computing ', 'text': 'Cloud computing offers more power and convenience compared to traditional desktops. Cloud services like Colab make it easy to get started with no installation needed. Colab is a free service, but paid services offer more features and capabilities. Beginners interested in machine learning and TensorFlow should start by visiting the TensorFlow website.'}, {'title': 'Getting Started with TensorFlow ', 'text': 'Start by going to TensorFlow.org and playing around on the website. Check out tutorials and guides available on the website. No installation needed, can start using Colab right away. Easy access to resources for getting started with TensorFlow.'}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-24 14:56:15.930380 ...\n",
      "Best SD: 1.5587661999529314, Best iteration: 24\n",
      "done get topics 2024-03-24 14:56:16.742417.\n",
      "Stage 2 start time 2024-03-24 14:56:16.742437\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow and the Role of Rajat Manga at Google\n",
      "2. The Evolution of TensorFlow 2.0\n",
      "3. The Importance of Predictive Modeling and Deep Learning in Enterprise Data Analysis\n",
      "4. The Impact of New Data Sets on Organization and Accessibility\n",
      "5. Success and Growth at the TensorFlow Dev Summit\n",
      "6. Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS\n",
      "7. The Rise of PyTorch in Research\n",
      "8. The Growth and Impact of TensorFlow\n",
      "9. Navigating Project and Team Dynamics at Google\n",
      "10. TensorFlow 1.0 X Release Update\n",
      "11. The Impact of Personalized Advertising on User Experience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "The podcast features Rajat Manga, director of Google's TensorFlow team, discussing the development and open sourcing of TensorFlow, an open source library at the forefront of deep learning. The decision to open source TensorFlow has had a significant impact on the tech industry, inspiring other companies to do the same. Google Brain, founded in 2011, evolved into TensorFlow in 2014, with a focus on scaling machine learning to Google's compute power and data. Early successes were achieved in speech and image research, leading to the decision to open source TensorFlow. This move has been influential in promoting open innovation and the exchange of ideas in the field of machine learning. Google Cloud provides integrations and support for TensorFlow, with a focus on helping the community and pushing a good standard forward.\n",
      "\n",
      "The podcast discusses the fast pace of development in deep learning and the evolution of TensorFlow for large scale and mobile deployment. The decision-making process involved considering multiple libraries and discussing the concept of having a graph. The move towards TensorFlow 2.0 includes more default eager execution and hiding the graph, influenced by the need for simplicity and intuition in development. The podcast also explores the varying priorities of enterprise and user needs, with some prioritizing stability and simplicity over the latest performance and quality. The podcast also touches on the value of providing stability and simplicity to allow more people to access the technology, as well as the common use of transfer learning in AI applications. It concludes by discussing the company's transition from a research focus to being accessible to developers for practical applications, with a focus on stability and deployment for non-research purposes, and the increasing interest from enterprises as the product progressed.\n",
      "\n",
      "The podcast also covers the availability of data sets and pre-trained models within the TensorFlow ecosystem, as well as the integration of Keras into TensorFlow. The podcast highlights the initial development of Keras and its integration with TensorFlow, as well as the decision to focus on Keras as the recommended API due to its popularity and positive feedback. It also touches on the need for a single decision-maker in successful open source projects like TensorFlow.\n",
      "\n",
      "The TensorFlow Dev Summit was successful, with new features being incorporated and an amazing ecosystem surrounding TensorFlow. Multiple people are involved in key design directions, with regular design reviews and efforts to open up to the community. Processes such as RFCs and special interest groups are being set in place to grow the community and scale the ecosystem. The recognition that the ecosystem cannot scale with a lone decision maker has led to the development of TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile, with a focus on enabling machine learning research and application. The goal is to get machine learning on every device with compute capability, and the ecosystem for machine learning is growing and covering more aspects over time, with a focus on pushing boundaries and building more tooling for machine learning.\n",
      "\n",
      "The podcast discusses the challenges and complexities of integrating TensorFlow.js and deep learning JS into the ecosystem. The team has learned a lot and iterated over the last few years to make it easier for end users, but there are still challenges ahead, such as integrating with new devices from a hardware perspective. TensorFlow started as a monolithic system and is still quite large and difficult to modify. There is a balance between breaking back compatibility and making the conversion straightforward, as production systems rely on TensorFlow. The trade-off is between slowing certain things down and bringing overall value. New changes and improvements should be done with consideration for future users. TensorFlow has built more tooling and things to help with ML pipelines, and there are lots of libraries being built on top of TensorFlow for both research and production, from both Google and the community.\n",
      "\n",
      "The podcast discusses the importance of designing with a clean slate in mind, emphasizing the need to put all concerns behind when thinking of new ideas. The speaker has switched their research group to TensorFlow, praising its focus on production and performance. They also mention the benefits of learning from PyTorch's approach and exploring different spaces. The text highlights the excitement for the clean APIs and potential performance improvements with TensorFlow version 2.0, as well as the restructuring of the monolithic structure into more modular pieces. The team is looking forward to exploring new possibilities and spaces in future versions. The podcast also mentions the use of TensorFlow by major corporations like Pepsi for development.\n",
      "\n",
      "The podcast discusses the widespread use of TensorFlow, with 41 million downloads and 1,800 contributors. The growth is attributed to community involvement and the involvement of various companies, including those in autonomous vehicles. The company is focused on listening to the community, being open to contributions, and putting processes in place for community involvement. They are also investing in tooling to support developers and make version changes smooth. The field is rapidly evolving, with potential developments in areas such as combining eager execution and graphs, the use of Swift for TensorFlow, and uncertainty regarding the future of hardware accelerators. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners, by providing simple models and tools, different levels of support, and pre-trained models. The company is also focused on addressing pain points and improving the user experience.\n",
      "\n",
      "The podcast discusses the role of high schoolers in contributing to cutting-edge technology, specifically in the context of Google's TensorFlow project. It emphasizes the importance of teamwork, cohesion, and motivation in building successful teams and achieving project goals. The hiring process at Google is focused on finding individuals who not only possess technical skills but also fit well within the company's culture and values. The podcast also highlights the challenges of balancing speed and perfection, community involvement, and decision-making within a large ecosystem like Google. It emphasizes the importance of engineering excellence and problem-solving abilities in the company's culture. Overall, the podcast provides insights into the dynamics of teamwork, hiring, and decision-making within Google, particularly in the context of the TensorFlow project.\n",
      "\n",
      "The podcast discusses the development and release of TensorFlow 2.0, which has already had 41 million downloads for version 1.0 X. The focus is on polishing and putting together features, with no rush to release the product. The goal is to get it right and focus on quality, with the release planned for the next few months or next quarter. The interviewee, who previously led a team at Google on search ads, emphasizes the importance of striking a balance between perfection and getting something that works well. Development is done in the open, both internally and externally, with releases done at a regular cadence. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it's clear that they are experimental and feedback is encouraged. There is no pressure to make TensorFlow 2.0 stable, and NodeX API stability is a priority, with ongoing updates and releases planned beyond the next two months.\n",
      "\n",
      "The podcast discusses the importance of personalized search ads in providing a valuable user experience and the challenges of finding a balance between valuable ads and annoying ones. It also explores the shift towards a mix model of free trials and ads, as well as the increasing willingness of people to pay for online content. The use of machine learning and the power of cloud computing in running TensorFlow are also highlighted, with a focus on the accessibility of resources for beginners interested in machine learning. The podcast emphasizes the goal of making the world's information, including products and services, accessible to users, and the need for ads to align with user needs and wants. It also touches on the evolving revenue models for online content and the trend towards paid services.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The discussion with Rajat Manga, director of Google's TensorFlow team, delves into the development and open sourcing of TensorFlow, an open source library at the forefront of deep learning. The decision to open source TensorFlow has had a significant impact on the tech industry, inspiring other companies to do the same. Google Brain, founded in 2011, evolved into TensorFlow in 2014, with a focus on scaling machine learning to Google's compute power and data. Early successes were achieved in speech and image research, leading to the decision to open source TensorFlow. This move has been influential in promoting open innovation and the exchange of ideas in the field of machine learning. Google Cloud provides integrations and support for TensorFlow, with a focus on helping the community and pushing a good standard forward.\\n\\nThe discussion covers the fast pace of development in deep learning and the evolution of TensorFlow for large scale and mobile deployment. The decision-making process involved considering multiple libraries and discussing the concept of having a graph. The move towards TensorFlow 2.0 includes more default eager execution and hiding the graph, influenced by the need for simplicity and intuition in development. The discussion also explores the varying priorities of enterprise and user needs, with some prioritizing stability and simplicity over the latest performance and quality. It also touches on the value of providing stability and simplicity to allow more people to access the technology, as well as the common use of transfer learning in AI applications. It concludes by discussing the company's transition from a research focus to being accessible to developers for practical applications, with a focus on stability and deployment for non-research purposes, and the increasing interest from enterprises as the product progressed.\\n\\nThe discussion also covers the availability of data sets and pre-trained models within the TensorFlow ecosystem, as well as the integration of Keras into TensorFlow. It highlights the initial development of Keras and its integration with TensorFlow, as well as the decision to focus on Keras as the recommended API due to its popularity and positive feedback. It also touches on the need for a single decision-maker in successful open source projects like TensorFlow.\\n\\nThe TensorFlow Dev Summit was successful, with new features being incorporated and an amazing ecosystem surrounding TensorFlow. Multiple people are involved in key design directions, with regular design reviews and efforts to open up to the community. Processes such as RFCs and special interest groups are being set in place to grow the community and scale the ecosystem. The recognition that the ecosystem cannot scale with a lone decision maker has led to the development of TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for mobile, with a focus on enabling machine learning research and application. The goal is to get machine learning on every device with compute capability, and the ecosystem for machine learning is growing and covering more aspects over time, with a focus on pushing boundaries and building more tooling for machine learning.\\n\\nThe discussion delves into the challenges and complexities of integrating TensorFlow.js and deep learning JS into the ecosystem. The team has learned a lot and iterated over the last few years to make it easier for end users, but there are still challenges ahead, such as integrating with new devices from a hardware perspective. TensorFlow started as a monolithic system and is still quite large and difficult to modify. There is a balance between breaking back compatibility and making the conversion straightforward, as production systems rely on TensorFlow. The trade-off is between slowing certain things down and bringing overall value. New changes and improvements should be done with consideration for future users. TensorFlow has built more tooling and things to help with ML pipelines, and there are lots of libraries being built on top of TensorFlow for both research and production, from both Google and the community.\\n\\nThe discussion emphasizes the importance of designing with a clean slate in mind, emphasizing the need to put all concerns behind when thinking of new ideas. The speaker has switched their research group to TensorFlow, praising its focus on production and performance. They also mention the benefits of learning from PyTorch's approach and exploring different spaces. The text highlights the excitement for the clean APIs and potential performance improvements with TensorFlow version 2.0, as well as the restructuring of the monolithic structure into more modular pieces. The team is looking forward to exploring new possibilities and spaces in future versions. The discussion also mentions the use of TensorFlow by major corporations like Pepsi for development.\\n\\nThe discussion delves into the widespread use of TensorFlow, with 41 million downloads and 1,800 contributors. The growth is attributed to community involvement and the involvement of various companies, including those in autonomous vehicles. The company is focused on listening to the community, being open to contributions, and putting processes in place for community involvement. They are also investing in tooling to support developers and make version changes smooth. The field is rapidly evolving, with potential developments in areas such as combining eager execution and graphs, the use of Swift for TensorFlow, and uncertainty regarding the future of hardware accelerators. The goal is to make TensorFlow as accessible and easy to use as possible, especially for beginners, by providing simple models and tools, different levels of support, and pre-trained models. The company is also focused on addressing pain points and improving the user experience.\\n\\nThe discussion also covers the role of high schoolers in contributing to cutting-edge technology, specifically in the context of Google's TensorFlow project. It emphasizes the importance of teamwork, cohesion, and motivation in building successful teams and achieving project goals. The hiring process at Google is focused on finding individuals who not only possess technical skills but also fit well within the company's culture and values. The discussion also highlights the challenges of balancing speed and perfection, community involvement, and decision-making within a large ecosystem like Google. It emphasizes the importance of engineering excellence and problem-solving abilities in the company's culture. Overall, the discussion provides insights into the dynamics of teamwork, hiring, and decision-making within Google, particularly in the context of the TensorFlow project.\\n\\nThe discussion delves into the development and release of TensorFlow 2.0, which has already had 41 million downloads for version 1.0 X. The focus is on polishing and putting together features, with no rush to release the product. The goal is to get it right and focus on quality, with the release planned for the next few months or next quarter. The interviewee, who previously led a team at Google on search ads, emphasizes the importance of striking a balance between perfection and getting something that works well. Development is done in the open, both internally and externally, with releases done at a regular cadence. The focus is on moving as fast as possible in different areas, with the ability to iterate and improve on things. It is okay to put out experimental features that aren't fully ready, as long as it's clear that they are experimental and feedback is encouraged. There is no pressure to make TensorFlow 2.0 stable, and NodeX API stability is a priority, with ongoing updates and releases planned beyond the next two months.\\n\\nThe discussion covers the importance of personalized search ads in providing a valuable user experience and the challenges of finding a balance between valuable ads and annoying ones. It also explores the shift towards a mix model of free trials and ads, as well as the increasing willingness of people to pay for online content. The use of machine learning and the power of cloud computing in running TensorFlow are also highlighted, with a focus on the accessibility of resources for beginners interested in machine learning. The discussion emphasizes the goal of making the world's information, including products and services, accessible to users, and the need for ads to align with user needs and wants. It also touches on the evolving revenue models for online content and the trend towards paid services.\"}]\n",
      "Stage 2 done time 2024-03-24 14:58:01.527698\n",
      "stage_2_titles: len: 11\n",
      "['1. The Impact of TensorFlow and the Role of Rajat Manga at Google', '2. The Evolution of TensorFlow 2.0', '3. The Importance of Predictive Modeling and Deep Learning in Enterprise Data Analysis', '4. The Impact of New Data Sets on Organization and Accessibility', '5. Success and Growth at the TensorFlow Dev Summit', '6. Challenges and Progress in Integrating TensorFlow.js and Deep Learning JS', '7. The Rise of PyTorch in Research', '8. The Growth and Impact of TensorFlow', '9. Navigating Project and Team Dynamics at Google', '10. TensorFlow 1.0 X Release Update', '11. The Impact of Personalized Advertising on User Experience']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_stage3_extractkeypoints_{VERSION}.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
