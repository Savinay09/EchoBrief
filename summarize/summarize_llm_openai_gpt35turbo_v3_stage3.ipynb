{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc25033aed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7fc196b56c90>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_summary_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'summary': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'summary': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'summary': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'summary': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_1(chunks_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following podcast an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# Use less word to try solve the warning/error:\n",
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). \n",
    "# Running this sequence through the model will result in indexing errors\n",
    "# 75-100\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'summaries': [stage_1_outputs[chunk_id]['summary'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['summaries_concat'] = ' '.join(topic_data['summaries'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['summaries_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 is not technical. skip\n",
      "episode 2 is not technical. skip\n",
      "episode 3 already processed. skip\n",
      "episode 4 already processed. skip\n",
      "episode 5 already processed. skip\n",
      "episode 6 already processed. skip\n",
      "episode 7 already processed. skip\n",
      "episode 8 is not technical. skip\n",
      "episode 9 already processed. skip\n",
      "episode 10 already processed. skip\n",
      "episode 11 already processed. skip\n",
      "episode 12 is not technical. skip\n",
      "episode 13 already processed. skip\n",
      "episode 14 already processed. skip\n",
      "episode 15 already processed. skip\n",
      "episode 16 is not technical. skip\n",
      "episode 17 already processed. skip\n",
      "episode 18 already processed. skip\n",
      "episode 19 already processed. skip\n",
      "episode 20 already processed. skip\n",
      "episode 21 already processed. skip\n",
      "Start time: 2024-03-17 14:24:45.767887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 done time 2024-03-17 14:26:26.545051\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-17 14:26:37.860623 ...\n",
      "Best SD: 2.0548046676563256, Best iteration: 6\n",
      "done get topics 2024-03-17 14:26:39.183920.\n",
      "topics out: 12\n",
      "Stage 2 start time 2024-03-17 14:26:39.183944\n",
      "RRRRRR summary_num_words: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "In this podcast, Rajat Manga, director of Google's TensorFlow team, discusses the evolution of TensorFlow from a software library to an ecosystem of tools for deploying machine learning in various platforms. The conversation revolves around the evolution of Google Brain and the decision to open source TensorFlow, exploring the origins and goals of Google Brain and the potential of deep learning in the early days of the proprietary machine learning library. The podcast also discusses the impact of scaling data and computing power on machine learning and early wins in speech and image recognition, as well as the significance of open innovation in research and the impact it has had on the growth of deep learning and machine learning.\n",
      "\n",
      "The podcast also delves into the early stages of designing TensorFlow for mobile devices and GPUs, the challenges of running complex algorithms on phones, and the need to customize code for mobile deployment. It provides insight into the early efforts to make TensorFlow compatible with various hardware and the growing interest in running machine learning models on mobile devices. The conversation also explores the decision-making process behind using graphs in production, specifically in the context of Python, and the unexpected popularity and growth of deep learning, with 41 million downloads and the potential for even more people to get involved in the future.\n",
      "\n",
      "The discussion also explores the integration of Google Cloud resources with the open source library TensorFlow and the potential impact on the tech community. The speaker reflects on the development of TensorFlow, including the considerations for large-scale data center usage and support for different hardware, and the decision-making process leading up to the open sourcing of the framework. The podcast also discusses the process of scaling up a product and the shift towards enterprise adoption, emphasizing the need for stability, deployment, documentation, and design. It explores the evolution of the product's appeal from early interest to enterprise adoption, highlighting the importance of stability in enterprise environments.\n",
      "\n",
      "The podcast also discusses the decision to simplify and integrate multiple APIs in machine learning, particularly in the context of the development of Keras 2.0. The goal was to streamline the process and make it more user-friendly, ultimately leading to the integration of Keras into TensorFlow. The discussion explores the decision-making process in open source projects, the dynamics of decision-making, and the impact on the development of new features and the overall ecosystem. It also touches on the expansion and scalability of the TensorFlow ecosystem, the increasing accessibility of TensorFlow and Keras, and the evolution of Keras and its transition to using TensorFlow as a backend.\n",
      "\n",
      "The podcast explores the convergence of machine learning models across different platforms, such as desktop and mobile, with the goal of enabling researchers to build the next amazing thing in machine learning. It discusses the importance of pushing the state of the art in machine learning research and integrating this research into real products to have a meaningful impact on people. The conversation also explores the potential for machine learning to be utilized across a wide range of compute devices, beyond just workstations, data centers, and the cloud. The speaker emphasizes the goal of making machine learning accessible on every device with compute capability and highlights the development of tooling such as TensorBoard and TensorFlow extended to support the entire ML pipeline.\n",
      "\n",
      "The podcast also discusses the challenges and innovations in integrating TensorFlow.js into the ecosystem, including technical difficulties and the iterative process. It also touches on the evolution of TensorFlow from a monolithic system to one with many tools around it, as well as the challenges in maintaining back compatibility while innovating with TensorFlow 2.0. The speakers emphasize the importance of balancing the need for updates and changes with the potential impact on existing systems, and the value of designing with a clean slate in mind when implementing new ideas and processes.\n",
      "\n",
      "The podcast discusses the competition between the machine learning frameworks TensorFlow and PyTorch, and their impact on research and development. The speakers emphasize the importance of considering both research and production needs, and acknowledge the value of learning from the different approaches taken by PyTorch. They also discuss the evolution of graphs in computing, the challenges faced in implementing eager execution, and the integration of eager execution and the influence of PyTorch on the development of TensorFlow 2.0. The conversation explores the challenges and successes in combining these features, as well as the potential impact on the future of the platform.\n",
      "\n",
      "The podcast discusses the restructuring of TensorFlow into more modular pieces, which is expected to benefit organizations and individuals in the ecosystem. It emphasizes the importance of clean interfaces for flexibility and scalability in distributed computing. The potential for major corporations to utilize TensorFlow, the growth and success of open source communities, and the impact of TensorFlow 2.0 are also discussed. The speakers highlight the importance of transparency, community building, and the ease of transitioning between different versions of TensorFlow.\n",
      "\n",
      "The podcast explores the future of TensorFlow and TPUs, discussing potential advancements in hardware accelerators and TPU technology. The speakers also touch on the accessibility of TensorFlow for beginners and the challenges they may face, as well as the efforts to make the platform more user-friendly. The conversation delves into the uncertainty of planning for the future in such a dynamic field, while also acknowledging the inevitability of change and the likelihood of certain technologies remaining relevant.\n",
      "\n",
      "The podcast discusses the importance of team cohesion in the development of cutting-edge technologies, such as TensorFlow, and the significance of hiring motivated individuals who align with the company's goals. The speaker emphasizes the impact of a cohesive team in delivering high-quality results and highlights the challenges and tensions that arise when managing a diverse team. The conversation also delves into the Google hiring process, which evaluates not only technical skills but also a candidate's passion and culture fit.\n",
      "\n",
      "This podcast explores the value of quick iteration and experimentation in software development, using examples such as WordPress 5.0 and TensorFlow 2.0. It discusses the pressure to make software stable, the benefits of releasing updates quickly, and the importance of gathering feedback from users. The development of TensorFlow 2.0 is highlighted, emphasizing the team's commitment to creating a great product and the balance between meeting deadlines and producing high-quality work.\n",
      "\n",
      "The podcast discusses the potential of machine learning in improving the user experience of search ads, emphasizing the importance of personalized data and aligning with user needs in advertising. It explores the shift towards more paid services across the web and the willingness of consumers to pay for valuable content, as well as the future of using TensorFlow and TPUs to empower students in education. The speaker also highlights the benefits of using cloud computing for machine learning, particularly for beginners, and provides recommendations for getting started with machine learning and TensorFlow.\n",
      "RRR rewritten summary\n",
      "[{'text': \"Rajat Manga, director of Google's TensorFlow team, discusses the evolution of TensorFlow from a software library to an ecosystem of tools for deploying machine learning in various platforms. The conversation revolves around the evolution of Google Brain and the decision to open source TensorFlow, exploring the origins and goals of Google Brain and the potential of deep learning in the early days of the proprietary machine learning library. The impact of scaling data and computing power on machine learning and early wins in speech and image recognition are also discussed, as well as the significance of open innovation in research and its impact on the growth of deep learning and machine learning.\\n\\nThe early stages of designing TensorFlow for mobile devices and GPUs, the challenges of running complex algorithms on phones, and the need to customize code for mobile deployment are explored. Insight is provided into the early efforts to make TensorFlow compatible with various hardware and the growing interest in running machine learning models on mobile devices. The decision-making process behind using graphs in production, specifically in the context of Python, and the unexpected popularity and growth of deep learning, with 41 million downloads and the potential for even more people to get involved in the future, are also discussed.\\n\\nThe integration of Google Cloud resources with the open source library TensorFlow and the potential impact on the tech community are explored. The development of TensorFlow, including the considerations for large-scale data center usage and support for different hardware, and the decision-making process leading up to the open sourcing of the framework are reflected upon. The process of scaling up a product and the shift towards enterprise adoption, emphasizing the need for stability, deployment, documentation, and design, is also discussed.\\n\\nThe decision to simplify and integrate multiple APIs in machine learning, particularly in the context of the development of Keras 2.0, is explored. The goal was to streamline the process and make it more user-friendly, ultimately leading to the integration of Keras into TensorFlow. The discussion also explores the decision-making process in open source projects, the dynamics of decision-making, and the impact on the development of new features and the overall ecosystem. It also touches on the expansion and scalability of the TensorFlow ecosystem, the increasing accessibility of TensorFlow and Keras, and the evolution of Keras and its transition to using TensorFlow as a backend.\\n\\nThe convergence of machine learning models across different platforms, such as desktop and mobile, with the goal of enabling researchers to build the next amazing thing in machine learning is discussed. The importance of pushing the state of the art in machine learning research and integrating this research into real products to have a meaningful impact on people is emphasized. The potential for machine learning to be utilized across a wide range of compute devices, beyond just workstations, data centers, and the cloud, is also explored. The speaker emphasizes the goal of making machine learning accessible on every device with compute capability and highlights the development of tooling such as TensorBoard and TensorFlow extended to support the entire ML pipeline.\\n\\nThe challenges and innovations in integrating TensorFlow.js into the ecosystem, including technical difficulties and the iterative process, are discussed. The evolution of TensorFlow from a monolithic system to one with many tools around it, as well as the challenges in maintaining back compatibility while innovating with TensorFlow 2.0, are also touched upon. The importance of balancing the need for updates and changes with the potential impact on existing systems, and the value of designing with a clean slate in mind when implementing new ideas and processes, is emphasized.\\n\\nThe competition between the machine learning frameworks TensorFlow and PyTorch, and their impact on research and development, is discussed. The importance of considering both research and production needs, and learning from the different approaches taken by PyTorch, is acknowledged. The evolution of graphs in computing, the challenges faced in implementing eager execution, and the integration of eager execution and the influence of PyTorch on the development of TensorFlow 2.0 are explored. The challenges and successes in combining these features, as well as the potential impact on the future of the platform, are also discussed.\\n\\nThe restructuring of TensorFlow into more modular pieces, which is expected to benefit organizations and individuals in the ecosystem, is emphasized. The importance of clean interfaces for flexibility and scalability in distributed computing is also highlighted. The potential for major corporations to utilize TensorFlow, the growth and success of open source communities, and the impact of TensorFlow 2.0 are also discussed. The importance of transparency, community building, and the ease of transitioning between different versions of TensorFlow is also highlighted.\\n\\nThe future of TensorFlow and TPUs is explored, discussing potential advancements in hardware accelerators and TPU technology. The accessibility of TensorFlow for beginners and the challenges they may face, as well as the efforts to make the platform more user-friendly, are also discussed. The uncertainty of planning for the future in such a dynamic field, while also acknowledging the inevitability of change and the likelihood of certain technologies remaining relevant, is delved into.\\n\\nThe importance of team cohesion in the development of cutting-edge technologies, such as TensorFlow, and the significance of hiring motivated individuals who align with the company's goals, is emphasized. The impact of a cohesive team in delivering high-quality results and the challenges and tensions that arise when managing a diverse team are highlighted. The Google hiring process, which evaluates not only technical skills but also a candidate's passion and culture fit, is also delved into.\\n\\nThe value of quick iteration and experimentation in software development, using examples such as WordPress 5.0 and TensorFlow 2.0, is explored. The pressure to make software stable, the benefits of releasing updates quickly, and the importance of gathering feedback from users are discussed. The development of TensorFlow 2.0 is highlighted, emphasizing the team's commitment to creating a great product and the balance between meeting deadlines and producing high-quality work.\\n\\nThe potential of machine learning in improving the user experience of search ads, emphasizing the importance of personalized data and aligning with user needs in advertising, is discussed. The shift towards more paid services across the web and the willingness of consumers to pay for valuable content, as well as the future of using TensorFlow and TPUs to empower students in education, are explored. The benefits of using cloud computing for machine learning, particularly for beginners, and recommendations for getting started with machine learning and TensorFlow are also provided.\"}]\n",
      "Stage 2 done time 2024-03-17 14:27:29.671195\n",
      "stage_2_titles: len: 12\n",
      "['1. The Evolution of Machine Learning at Google and the Impact of Open Sourcing TensorFlow', '2. The Rise of Deep Learning and the Importance of Graphs in Production', '3. The Future of Machine Learning and AI in Enterprises', '4. The Evolution of TensorFlow and Keras in Open Source Projects', '5. Advancing Research and Integration of Machine Learning in the Future', '6. Challenges and Innovations in Scaling TensorFlow and Integrating with Hardware', '7. The Battle of PyTorch vs. TensorFlow and Optimizing Performance with TensorFlow 2.0', '8. Restructuring the TensorFlow Ecosystem for Modularity and Building a Strong Open Source Community', '9. The Future of TensorFlow and TPUs in Simplifying for Beginners and Machine Learning 3.0', '10. The Importance of Team Cohesion in Cutting Edge Technology and Navigating Team Dynamics in the Workplace', '11. The Importance of Quick Iteration in Software Development and Developing TensorFlow 2.0', '12. The Power of Machine Learning in Search Ads and the Future of Online Advertising']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 23 already processed. skip\n",
      "episode 24 already processed. skip\n",
      "episode 25 already processed. skip\n",
      "episode 26 is not technical. skip\n",
      "episode 27 is not technical. skip\n",
      "episode 28 already processed. skip\n",
      "episode 29 is not technical. skip\n",
      "episode 30 already processed. skip\n",
      "episode 31 already processed. skip\n",
      "episode 32 already processed. skip\n",
      "episode 33 is not technical. skip\n",
      "episode 34 already processed. skip\n",
      "episode 35 already processed. skip\n",
      "episode 36 already processed. skip\n",
      "episode 37 is not technical. skip\n",
      "episode 38 already processed. skip\n",
      "episode 39 is not technical. skip\n",
      "episode 40 already processed. skip\n",
      "episode 41 already processed. skip\n",
      "episode 42 already processed. skip\n",
      "episode 43 already processed. skip\n",
      "episode 44 already processed. skip\n",
      "episode 45 is not technical. skip\n",
      "episode 46 is not technical. skip\n",
      "episode 47 already processed. skip\n",
      "episode 48 already processed. skip\n",
      "episode 49 already processed. skip\n",
      "episode 50 already processed. skip\n",
      "episode 51 is not technical. skip\n",
      "episode 52 already processed. skip\n",
      "episode 53 already processed. skip\n",
      "episode 54 is not technical. skip\n",
      "episode 55 is not technical. skip\n",
      "episode 56 already processed. skip\n",
      "episode 57 already processed. skip\n",
      "episode 58 is not technical. skip\n",
      "episode 59 is not technical. skip\n",
      "episode 60 already processed. skip\n",
      "episode 61 already processed. skip\n",
      "episode 62 already processed. skip\n",
      "episode 63 is not technical. skip\n",
      "episode 64 is not technical. skip\n",
      "episode 65 already processed. skip\n",
      "episode 66 already processed. skip\n",
      "episode 67 is not technical. skip\n",
      "episode 68 already processed. skip\n",
      "episode 69 already processed. skip\n",
      "episode 70 already processed. skip\n",
      "episode 71 already processed. skip\n",
      "episode 72 already processed. skip\n",
      "episode 73 already processed. skip\n",
      "episode 74 already processed. skip\n",
      "episode 75 already processed. skip\n",
      "episode 76 already processed. skip\n",
      "episode 77 is not technical. skip\n",
      "episode 78 is not technical. skip\n",
      "episode 79 already processed. skip\n",
      "episode 80 already processed. skip\n",
      "episode 81 already processed. skip\n",
      "episode 82 is not technical. skip\n",
      "episode 83 already processed. skip\n",
      "episode 84 is not technical. skip\n",
      "episode 85 is not technical. skip\n",
      "episode 86 already processed. skip\n",
      "episode 87 is not technical. skip\n",
      "episode 88 is not technical. skip\n",
      "episode 89 already processed. skip\n",
      "episode 90 already processed. skip\n",
      "episode 91 already processed. skip\n",
      "episode 92 already processed. skip\n",
      "episode 93 already processed. skip\n",
      "episode 94 already processed. skip\n",
      "episode 95 already processed. skip\n",
      "episode 96 is not technical. skip\n",
      "episode 97 already processed. skip\n",
      "episode 98 already processed. skip\n",
      "episode 99 already processed. skip\n",
      "episode 101 is not technical. skip\n",
      "episode 102 is not technical. skip\n",
      "episode 103 already processed. skip\n",
      "episode 104 already processed. skip\n",
      "episode 105 is not technical. skip\n",
      "episode 106 already processed. skip\n",
      "episode 107 is not technical. skip\n",
      "episode 108 already processed. skip\n",
      "episode 109 already processed. skip\n",
      "episode 110 already processed. skip\n",
      "episode 111 already processed. skip\n",
      "episode 112 is not technical. skip\n",
      "episode 113 already processed. skip\n",
      "episode 114 already processed. skip\n",
      "episode 115 already processed. skip\n",
      "episode 116 is not technical. skip\n",
      "episode 117 is not technical. skip\n",
      "episode 118 already processed. skip\n",
      "episode 119 already processed. skip\n",
      "episode 120 already processed. skip\n",
      "episode 121 is not technical. skip\n",
      "episode 122 already processed. skip\n",
      "episode 123 is not technical. skip\n",
      "episode 124 is not technical. skip\n",
      "episode 125 is not technical. skip\n",
      "episode 126 already processed. skip\n",
      "episode 127 is not technical. skip\n",
      "episode 128 is not technical. skip\n",
      "episode 129 already processed. skip\n",
      "episode 130 already processed. skip\n",
      "episode 131 already processed. skip\n",
      "episode 132 already processed. skip\n",
      "episode 133 already processed. skip\n",
      "episode 134 is not technical. skip\n",
      "episode 135 is not technical. skip\n",
      "episode 136 is not technical. skip\n",
      "episode 137 is not technical. skip\n",
      "episode 138 is not technical. skip\n",
      "episode 139 already processed. skip\n",
      "episode 140 is not technical. skip\n",
      "episode 141 already processed. skip\n",
      "episode 142 is not technical. skip\n",
      "episode 143 is not technical. skip\n",
      "episode 144 already processed. skip\n",
      "episode 145 is not technical. skip\n",
      "episode 146 already processed. skip\n",
      "episode 147 already processed. skip\n",
      "episode 148 already processed. skip\n",
      "episode 149 is not technical. skip\n",
      "episode 150 is not technical. skip\n",
      "episode 151 already processed. skip\n",
      "episode 152 is not technical. skip\n",
      "episode 153 already processed. skip\n",
      "episode 154 is not technical. skip\n",
      "episode 155 already processed. skip\n",
      "episode 156 is not technical. skip\n",
      "episode 157 already processed. skip\n",
      "episode 158 is not technical. skip\n",
      "episode 159 is not technical. skip\n",
      "episode 160 already processed. skip\n",
      "episode 161 is not technical. skip\n",
      "episode 162 is not technical. skip\n",
      "episode 163 is not technical. skip\n",
      "episode 164 is not technical. skip\n",
      "episode 165 is not technical. skip\n",
      "episode 166 is not technical. skip\n",
      "episode 167 is not technical. skip\n",
      "episode 168 already processed. skip\n",
      "episode 169 is not technical. skip\n",
      "episode 170 is not technical. skip\n",
      "episode 171 is not technical. skip\n",
      "episode 172 is not technical. skip\n",
      "episode 173 already processed. skip\n",
      "episode 174 is not technical. skip\n",
      "episode 175 is not technical. skip\n",
      "episode 176 is not technical. skip\n",
      "episode 177 already processed. skip\n",
      "episode 178 is not technical. skip\n",
      "episode 179 is not technical. skip\n",
      "episode 180 is not technical. skip\n",
      "episode 181 already processed. skip\n",
      "episode 182 is not technical. skip\n",
      "episode 183 already processed. skip\n",
      "episode 184 is not technical. skip\n",
      "episode 185 is not technical. skip\n",
      "episode 186 already processed. skip\n",
      "episode 187 already processed. skip\n",
      "episode 188 already processed. skip\n",
      "episode 189 is not technical. skip\n",
      "episode 190 already processed. skip\n",
      "episode 191 is not technical. skip\n",
      "episode 192 is not technical. skip\n",
      "episode 193 already processed. skip\n",
      "episode 194 is not technical. skip\n",
      "episode 195 already processed. skip\n",
      "episode 196 is not technical. skip\n",
      "episode 197 is not technical. skip\n",
      "episode 198 is not technical. skip\n",
      "episode 199 is not technical. skip\n",
      "episode 200 is not technical. skip\n",
      "episode 201 is not technical. skip\n",
      "episode 202 is not technical. skip\n",
      "episode 203 is not technical. skip\n",
      "episode 204 is not technical. skip\n",
      "episode 205 is not technical. skip\n",
      "episode 206 already processed. skip\n",
      "episode 207 is not technical. skip\n",
      "episode 208 already processed. skip\n",
      "episode 209 already processed. skip\n",
      "episode 210 is not technical. skip\n",
      "episode 211 is not technical. skip\n",
      "episode 212 is not technical. skip\n",
      "episode 213 already processed. skip\n",
      "episode 214 is not technical. skip\n",
      "episode 215 already processed. skip\n",
      "episode 216 is not technical. skip\n",
      "episode 217 already processed. skip\n",
      "episode 218 already processed. skip\n",
      "episode 219 already processed. skip\n",
      "episode 220 is not technical. skip\n",
      "episode 221 already processed. skip\n",
      "episode 222 already processed. skip\n",
      "episode 223 is not technical. skip\n",
      "episode 224 already processed. skip\n",
      "episode 225 already processed. skip\n",
      "episode 226 is not technical. skip\n",
      "episode 227 is not technical. skip\n",
      "episode 228 is not technical. skip\n",
      "episode 229 is not technical. skip\n",
      "episode 230 is not technical. skip\n",
      "episode 231 is not technical. skip\n",
      "episode 232 is not technical. skip\n",
      "episode 233 is not technical. skip\n",
      "episode 234 is not technical. skip\n",
      "episode 235 already processed. skip\n",
      "episode 236 is not technical. skip\n",
      "episode 237 is not technical. skip\n",
      "episode 238 is not technical. skip\n",
      "episode 239 is not technical. skip\n",
      "episode 240 is not technical. skip\n",
      "episode 241 already processed. skip\n",
      "episode 242 is not technical. skip\n",
      "episode 243 is not technical. skip\n",
      "episode 244 is not technical. skip\n",
      "episode 245 is not technical. skip\n",
      "episode 246 already processed. skip\n",
      "episode 247 already processed. skip\n",
      "episode 248 is not technical. skip\n",
      "episode 249 is not technical. skip\n",
      "episode 250 already processed. skip\n",
      "episode 251 is not technical. skip\n",
      "episode 252 already processed. skip\n",
      "episode 253 is not technical. skip\n",
      "episode 254 is not technical. skip\n",
      "episode 255 is not technical. skip\n",
      "episode 256 is not technical. skip\n",
      "episode 257 already processed. skip\n",
      "episode 258 already processed. skip\n",
      "episode 259 is not technical. skip\n",
      "episode 260 is not technical. skip\n",
      "episode 261 already processed. skip\n",
      "episode 262 is not technical. skip\n",
      "episode 263 is not technical. skip\n",
      "episode 264 is not technical. skip\n",
      "episode 265 is not technical. skip\n",
      "episode 266 already processed. skip\n",
      "episode 267 is not technical. skip\n",
      "episode 269 is not technical. skip\n",
      "episode 270 is not technical. skip\n",
      "episode 271 already processed. skip\n",
      "episode 272 is not technical. skip\n",
      "episode 273 is not technical. skip\n",
      "episode 274 is not technical. skip\n",
      "episode 275 is not technical. skip\n",
      "episode 276 is not technical. skip\n",
      "episode 277 is not technical. skip\n",
      "episode 278 is not technical. skip\n",
      "episode 279 is not technical. skip\n",
      "episode 280 already processed. skip\n",
      "episode 281 is not technical. skip\n",
      "episode 284 is not technical. skip\n",
      "episode 285 is not technical. skip\n",
      "episode 286 is not technical. skip\n",
      "episode 288 is not technical. skip\n",
      "episode 289 is not technical. skip\n",
      "episode 290 is not technical. skip\n",
      "episode 292 is not technical. skip\n",
      "episode 293 is not technical. skip\n",
      "episode 294 already processed. skip\n",
      "episode 295 is not technical. skip\n",
      "episode 296 is not technical. skip\n",
      "episode 297 is not technical. skip\n",
      "episode 298 is not technical. skip\n",
      "episode 299 already processed. skip\n",
      "episode 300 is not technical. skip\n",
      "episode 301 is not technical. skip\n",
      "episode 302 already processed. skip\n",
      "episode 303 is not technical. skip\n",
      "episode 304 is not technical. skip\n",
      "episode 305 is not technical. skip\n",
      "episode 306 already processed. skip\n",
      "episode 307 already processed. skip\n",
      "episode 308 is not technical. skip\n",
      "episode 309 already processed. skip\n",
      "episode 310 is not technical. skip\n",
      "episode 311 is not technical. skip\n",
      "episode 312 is not technical. skip\n",
      "episode 313 is not technical. skip\n",
      "episode 314 is not technical. skip\n",
      "episode 315 is not technical. skip\n",
      "episode 316 is not technical. skip\n",
      "episode 317 is not technical. skip\n",
      "episode 318 is not technical. skip\n",
      "episode 319 is not technical. skip\n",
      "episode 320 is not technical. skip\n",
      "episode 321 is not technical. skip\n",
      "episode 322 already processed. skip\n",
      "episode 323 is not technical. skip\n",
      "episode 324 is not technical. skip\n",
      "episode 325 already processed. skip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = summarize_stage_1(chunks_text)['stage_1_outputs']\n",
    "    # Split the titles and summaries\n",
    "    stage_1_summaries = [e['summary'] for e in stage_1_outputs]\n",
    "    stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_summaries)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    summary_embeds = generate_embeddings(stage_1_summaries)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "        summary_similarity_matrix[row, col] = similarity\n",
    "        summary_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "    time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(summary_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "    chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "    print(f\"topics out: {len(topics)}\")\n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_v3_stage3.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "    time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
